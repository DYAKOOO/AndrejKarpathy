1
00:00:00,000 --> 00:00:04,200
hi everyone so today we are once again

2
00:00:02,399 --> 00:00:07,020
continuing our implementation of make

3
00:00:04,200 --> 00:00:09,840
more now so far we've come up to here

4
00:00:07,020 --> 00:00:11,400
montalia perceptrons and our neural net

5
00:00:09,840 --> 00:00:12,480
looked like this and we were

6
00:00:11,400 --> 00:00:13,380
implementing this over the last few

7
00:00:12,480 --> 00:00:15,240
lectures

8
00:00:13,380 --> 00:00:16,680
now I'm sure everyone is very excited to

9
00:00:15,240 --> 00:00:18,359
go into recurring neural networks and

10
00:00:16,680 --> 00:00:20,340
all of their variants and how they work

11
00:00:18,359 --> 00:00:21,539
and the diagrams look cool and it's very

12
00:00:20,340 --> 00:00:23,460
exciting and interesting and we're going

13
00:00:21,539 --> 00:00:25,080
to get a better result but unfortunately

14
00:00:23,460 --> 00:00:28,619
I think we have to remain here for one

15
00:00:25,080 --> 00:00:30,539
more lecture and the reason for that is

16
00:00:28,619 --> 00:00:31,859
we've already trained this multilio

17
00:00:30,539 --> 00:00:33,600
perceptron right and we are getting

18
00:00:31,859 --> 00:00:34,800
pretty good loss and I think we have a

19
00:00:33,600 --> 00:00:37,680
pretty decent understanding of the

20
00:00:34,800 --> 00:00:39,120
architecture and how it works but the

21
00:00:37,680 --> 00:00:42,300
line of code here that I take an issue

22
00:00:39,120 --> 00:00:45,120
with is here lost up backward that is we

23
00:00:42,300 --> 00:00:46,920
are taking a pytorch auto grad and using

24
00:00:45,120 --> 00:00:48,899
it to calculate all of our gradients

25
00:00:46,920 --> 00:00:50,879
along the way and I would like to remove

26
00:00:48,899 --> 00:00:52,200
the use of lost at backward and I would

27
00:00:50,879 --> 00:00:55,140
like us to write our backward pass

28
00:00:52,200 --> 00:00:56,460
manually on the level of tensors and I

29
00:00:55,140 --> 00:00:58,860
think that this is a very useful

30
00:00:56,460 --> 00:01:00,360
exercise for the following reasons

31
00:00:58,860 --> 00:01:02,520
I actually have an entire blog post on

32
00:01:00,360 --> 00:01:05,460
this topic but I'd like to call back

33
00:01:02,520 --> 00:01:07,320
propagation a leaky abstraction

34
00:01:05,460 --> 00:01:09,060
and what I mean by that is back

35
00:01:07,320 --> 00:01:11,460
propagation does doesn't just make your

36
00:01:09,060 --> 00:01:12,659
neural networks just work magically it's

37
00:01:11,460 --> 00:01:14,159
not the case they can just Stack Up

38
00:01:12,659 --> 00:01:16,260
arbitrary Lego blocks of differentiable

39
00:01:14,159 --> 00:01:17,700
functions and just cross your fingers

40
00:01:16,260 --> 00:01:19,740
and back propagate and everything is

41
00:01:17,700 --> 00:01:22,020
great things don't just work

42
00:01:19,740 --> 00:01:23,759
automatically it is a leaky abstraction

43
00:01:22,020 --> 00:01:25,439
in the sense that you can shoot yourself

44
00:01:23,759 --> 00:01:28,500
in the foot if you do not understanding

45
00:01:25,439 --> 00:01:31,560
its internals it will magically not work

46
00:01:28,500 --> 00:01:32,820
or not work optimally and you will need

47
00:01:31,560 --> 00:01:34,979
to understand how it works under the

48
00:01:32,820 --> 00:01:36,240
hood if you're hoping to debug it and if

49
00:01:34,979 --> 00:01:37,799
you are hoping to address it in your

50
00:01:36,240 --> 00:01:39,960
neural nut

51
00:01:37,799 --> 00:01:42,360
um so this blog post here from a while

52
00:01:39,960 --> 00:01:43,860
ago goes into some of those examples so

53
00:01:42,360 --> 00:01:46,020
for example we've already covered them

54
00:01:43,860 --> 00:01:48,900
some of them already for example the

55
00:01:46,020 --> 00:01:51,479
flat tails of these functions and how

56
00:01:48,900 --> 00:01:53,520
you do not want to saturate them too

57
00:01:51,479 --> 00:01:55,140
much because your gradients will die the

58
00:01:53,520 --> 00:01:56,399
case of dead neurons which I've already

59
00:01:55,140 --> 00:01:58,619
covered as well

60
00:01:56,399 --> 00:02:00,540
the case of exploding or Vanishing

61
00:01:58,619 --> 00:02:02,820
gradients in the case of repair neural

62
00:02:00,540 --> 00:02:05,579
networks which we are about to cover

63
00:02:02,820 --> 00:02:07,560
and then also you will often come across

64
00:02:05,579 --> 00:02:10,080
some examples in the wild

65
00:02:07,560 --> 00:02:11,760
this is a snippet that I found uh in a

66
00:02:10,080 --> 00:02:13,440
random code base on the internet where

67
00:02:11,760 --> 00:02:15,480
they actually have like a very subtle

68
00:02:13,440 --> 00:02:18,120
but pretty major bug in their

69
00:02:15,480 --> 00:02:20,099
implementation and the bug points at the

70
00:02:18,120 --> 00:02:21,300
fact that the author of this code does

71
00:02:20,099 --> 00:02:23,160
not actually understand by propagation

72
00:02:21,300 --> 00:02:25,260
so they're trying to do here is they're

73
00:02:23,160 --> 00:02:27,420
trying to clip the loss at a certain

74
00:02:25,260 --> 00:02:28,560
maximum value but actually what they're

75
00:02:27,420 --> 00:02:30,180
trying to do is they're trying to

76
00:02:28,560 --> 00:02:32,040
collect the gradients to have a maximum

77
00:02:30,180 --> 00:02:34,200
value instead of trying to clip the loss

78
00:02:32,040 --> 00:02:36,180
at a maximum value and

79
00:02:34,200 --> 00:02:38,280
um indirectly they're basically causing

80
00:02:36,180 --> 00:02:41,760
some of the outliers to be actually

81
00:02:38,280 --> 00:02:43,800
ignored because when you clip a loss of

82
00:02:41,760 --> 00:02:46,860
an outlier you are setting its gradient

83
00:02:43,800 --> 00:02:48,959
to zero and so have a look through this

84
00:02:46,860 --> 00:02:50,280
and read through it but there's

85
00:02:48,959 --> 00:02:51,480
basically a bunch of subtle issues that

86
00:02:50,280 --> 00:02:53,519
you're going to avoid if you actually

87
00:02:51,480 --> 00:02:55,260
know what you're doing and that's why I

88
00:02:53,519 --> 00:02:56,940
don't think it's the case that because

89
00:02:55,260 --> 00:02:59,040
pytorch or other Frameworks offer

90
00:02:56,940 --> 00:03:00,599
autograd it is okay for us to ignore how

91
00:02:59,040 --> 00:03:02,640
it works

92
00:03:00,599 --> 00:03:04,800
now we've actually already covered

93
00:03:02,640 --> 00:03:07,319
covered autograd and we wrote micrograd

94
00:03:04,800 --> 00:03:09,180
but micrograd was an autograd engine

95
00:03:07,319 --> 00:03:11,099
only on the level of individual scalars

96
00:03:09,180 --> 00:03:13,620
so the atoms were single individual

97
00:03:11,099 --> 00:03:14,940
numbers and uh you know I don't think

98
00:03:13,620 --> 00:03:16,620
it's enough and I'd like us to basically

99
00:03:14,940 --> 00:03:19,140
think about back propagation on level of

100
00:03:16,620 --> 00:03:21,659
tensors as well and so in a summary I

101
00:03:19,140 --> 00:03:23,220
think it's a good exercise I think it is

102
00:03:21,659 --> 00:03:25,080
very very valuable you're going to

103
00:03:23,220 --> 00:03:27,060
become better at debugging neural

104
00:03:25,080 --> 00:03:28,860
networks and making sure that you

105
00:03:27,060 --> 00:03:30,239
understand what you're doing it is going

106
00:03:28,860 --> 00:03:31,260
to make everything fully explicit so

107
00:03:30,239 --> 00:03:33,360
you're not going to be nervous about

108
00:03:31,260 --> 00:03:34,680
what is hidden away from you and

109
00:03:33,360 --> 00:03:37,560
basically in general we're going to

110
00:03:34,680 --> 00:03:40,080
emerge stronger and so let's get into it

111
00:03:37,560 --> 00:03:42,120
a bit of a fun historical note here is

112
00:03:40,080 --> 00:03:43,920
that today writing your backward pass by

113
00:03:42,120 --> 00:03:45,720
hand and manually is not recommended and

114
00:03:43,920 --> 00:03:48,299
no one does it except for the purposes

115
00:03:45,720 --> 00:03:49,860
of exercise but about 10 years ago in

116
00:03:48,299 --> 00:03:52,260
deep learning this was fairly standard

117
00:03:49,860 --> 00:03:53,459
and in fact pervasive so at the time

118
00:03:52,260 --> 00:03:55,260
everyone used to write their own

119
00:03:53,459 --> 00:03:57,480
backward pass by hand manually including

120
00:03:55,260 --> 00:03:59,580
myself and it's just what you would do

121
00:03:57,480 --> 00:04:01,680
so we used to ride backward pass by hand

122
00:03:59,580 --> 00:04:04,440
and now everyone just calls lost that

123
00:04:01,680 --> 00:04:07,140
backward uh we've lost something I want

124
00:04:04,440 --> 00:04:11,099
to give you a few examples of this so

125
00:04:07,140 --> 00:04:13,620
here's a 2006 paper from Jeff Hinton and

126
00:04:11,099 --> 00:04:15,420
Russell selectinov in science that was

127
00:04:13,620 --> 00:04:17,940
influential at the time and this was

128
00:04:15,420 --> 00:04:19,799
training some architectures called

129
00:04:17,940 --> 00:04:22,019
restricted bolstery machines and

130
00:04:19,799 --> 00:04:26,160
basically it's an auto encoder trained

131
00:04:22,019 --> 00:04:27,540
here and this is from roughly 2010 I had

132
00:04:26,160 --> 00:04:30,300
a library for training researchable

133
00:04:27,540 --> 00:04:32,400
machines and this was at the time

134
00:04:30,300 --> 00:04:34,199
written in Matlab so python was not used

135
00:04:32,400 --> 00:04:36,660
for deep learning pervasively it was all

136
00:04:34,199 --> 00:04:39,360
Matlab and Matlab was this a scientific

137
00:04:36,660 --> 00:04:41,580
Computing package that everyone would

138
00:04:39,360 --> 00:04:44,220
use so we would write Matlab which is

139
00:04:41,580 --> 00:04:46,199
barely a programming language as well

140
00:04:44,220 --> 00:04:48,120
but I've had a very convenient tensor

141
00:04:46,199 --> 00:04:49,500
class and was this a Computing

142
00:04:48,120 --> 00:04:51,600
environment and you would run here it

143
00:04:49,500 --> 00:04:53,160
would all run on a CPU of course but you

144
00:04:51,600 --> 00:04:54,900
would have very nice plots to go with it

145
00:04:53,160 --> 00:04:57,840
and a built-in debugger and it was

146
00:04:54,900 --> 00:05:00,720
pretty nice now the code in this package

147
00:04:57,840 --> 00:05:03,000
in 2010 that I wrote for fitting

148
00:05:00,720 --> 00:05:05,040
research multiple machines to a large

149
00:05:03,000 --> 00:05:07,020
extent is recognizable but I wanted to

150
00:05:05,040 --> 00:05:09,240
show you how you would well I'm creating

151
00:05:07,020 --> 00:05:11,820
the data in the XY batches I'm

152
00:05:09,240 --> 00:05:13,259
initializing the neural nut so it's got

153
00:05:11,820 --> 00:05:15,240
weights and biases just like we're used

154
00:05:13,259 --> 00:05:17,220
to and then this is the training Loop

155
00:05:15,240 --> 00:05:19,979
where we actually do the forward pass

156
00:05:17,220 --> 00:05:21,540
and then here at this time they didn't

157
00:05:19,979 --> 00:05:23,520
even necessarily use back propagation to

158
00:05:21,540 --> 00:05:25,259
train neural networks so this in

159
00:05:23,520 --> 00:05:28,139
particular implements contrastive

160
00:05:25,259 --> 00:05:30,539
Divergence which estimates a gradient

161
00:05:28,139 --> 00:05:32,580
and then here we take that gradient and

162
00:05:30,539 --> 00:05:34,560
use it for a parameter update along the

163
00:05:32,580 --> 00:05:36,600
lines that we're used to

164
00:05:34,560 --> 00:05:38,039
um yeah here

165
00:05:36,600 --> 00:05:39,600
but you can see that basically people

166
00:05:38,039 --> 00:05:41,940
are meddling with these gradients uh

167
00:05:39,600 --> 00:05:43,380
directly and inline and themselves uh it

168
00:05:41,940 --> 00:05:45,300
wasn't that common to use an auto grad

169
00:05:43,380 --> 00:05:47,820
engine here's one more example from a

170
00:05:45,300 --> 00:05:49,919
paper of mine from 2014

171
00:05:47,820 --> 00:05:51,360
um called the fragmented embeddings

172
00:05:49,919 --> 00:05:53,340
and here what I was doing is I was

173
00:05:51,360 --> 00:05:55,199
aligning images and text

174
00:05:53,340 --> 00:05:56,880
um and so it's kind of like a clip if

175
00:05:55,199 --> 00:05:58,259
you're familiar with it but instead of

176
00:05:56,880 --> 00:06:00,180
working on the level of entire images

177
00:05:58,259 --> 00:06:01,800
and entire sentences it was working on

178
00:06:00,180 --> 00:06:03,720
the level of individual objects and

179
00:06:01,800 --> 00:06:05,520
little pieces of sentences and I was

180
00:06:03,720 --> 00:06:08,039
embedding them and then calculating very

181
00:06:05,520 --> 00:06:10,380
much like a clip-like loss and I dig up

182
00:06:08,039 --> 00:06:13,259
the code from 2014 of how I implemented

183
00:06:10,380 --> 00:06:14,520
this and it was already in numpy and

184
00:06:13,259 --> 00:06:16,500
python

185
00:06:14,520 --> 00:06:19,199
and here I'm planting the cost function

186
00:06:16,500 --> 00:06:20,940
and it was standard to implement not

187
00:06:19,199 --> 00:06:23,699
just the cost but also the backward pass

188
00:06:20,940 --> 00:06:26,039
manually so here I'm calculating the

189
00:06:23,699 --> 00:06:28,620
image embeddings sentence embeddings the

190
00:06:26,039 --> 00:06:31,259
loss function I calculate this course

191
00:06:28,620 --> 00:06:32,580
this is the loss function and then once

192
00:06:31,259 --> 00:06:34,620
I have the loss function I do the

193
00:06:32,580 --> 00:06:36,539
backward pass right here so I backward

194
00:06:34,620 --> 00:06:38,220
through the loss function and through

195
00:06:36,539 --> 00:06:41,280
the neural nut and I append

196
00:06:38,220 --> 00:06:42,780
regularization so everything was done by

197
00:06:41,280 --> 00:06:44,460
hand manually and you were just right

198
00:06:42,780 --> 00:06:46,500
out the backward pass and then you would

199
00:06:44,460 --> 00:06:47,940
use a gradient Checker to make sure that

200
00:06:46,500 --> 00:06:49,440
your numerical estimate of the gradient

201
00:06:47,940 --> 00:06:51,900
agrees with the one you calculated

202
00:06:49,440 --> 00:06:53,520
during back propagation so this was very

203
00:06:51,900 --> 00:06:55,139
standard for a long time but today of

204
00:06:53,520 --> 00:06:56,819
course it is standard to use an auto

205
00:06:55,139 --> 00:06:58,560
grad engine

206
00:06:56,819 --> 00:06:59,819
um but it was definitely useful and I

207
00:06:58,560 --> 00:07:01,139
think people sort of understood how

208
00:06:59,819 --> 00:07:03,120
these neural networks work on a very

209
00:07:01,139 --> 00:07:04,680
intuitive level and so I think it's a

210
00:07:03,120 --> 00:07:06,479
good exercise again and this is where we

211
00:07:04,680 --> 00:07:08,220
want to be okay so just as a reminder

212
00:07:06,479 --> 00:07:09,840
from our previous lecture this is The

213
00:07:08,220 --> 00:07:11,759
jupyter Notebook that we implemented at

214
00:07:09,840 --> 00:07:13,380
the time and

215
00:07:11,759 --> 00:07:15,000
we're going to keep everything the same

216
00:07:13,380 --> 00:07:16,620
so we're still going to have a two layer

217
00:07:15,000 --> 00:07:18,660
multiplayer perceptron with a batch

218
00:07:16,620 --> 00:07:20,280
normalization layer so the forward pass

219
00:07:18,660 --> 00:07:22,020
will be basically identical to this

220
00:07:20,280 --> 00:07:23,639
lecture but here we're going to get rid

221
00:07:22,020 --> 00:07:24,660
of lost and backward and instead we're

222
00:07:23,639 --> 00:07:26,039
going to write the backward pass

223
00:07:24,660 --> 00:07:27,660
manually

224
00:07:26,039 --> 00:07:29,520
now here's the starter code for this

225
00:07:27,660 --> 00:07:31,500
lecture we are becoming a back prop

226
00:07:29,520 --> 00:07:34,020
ninja in this notebook

227
00:07:31,500 --> 00:07:36,000
and the first few cells here are

228
00:07:34,020 --> 00:07:37,979
identical to what we are used to so we

229
00:07:36,000 --> 00:07:40,139
are doing some imports loading the data

230
00:07:37,979 --> 00:07:41,580
set and processing the data set none of

231
00:07:40,139 --> 00:07:43,319
this changed

232
00:07:41,580 --> 00:07:44,699
now here I'm introducing a utility

233
00:07:43,319 --> 00:07:46,620
function that we're going to use later

234
00:07:44,699 --> 00:07:47,699
to compare the gradients so in

235
00:07:46,620 --> 00:07:49,139
particular we are going to have the

236
00:07:47,699 --> 00:07:50,759
gradients that we estimate manually

237
00:07:49,139 --> 00:07:53,280
ourselves and we're going to have

238
00:07:50,759 --> 00:07:54,479
gradients that Pi torch calculates and

239
00:07:53,280 --> 00:07:55,919
we're going to be checking for

240
00:07:54,479 --> 00:07:58,919
correctness assuming of course that

241
00:07:55,919 --> 00:08:00,300
pytorch is correct

242
00:07:58,919 --> 00:08:03,000
um then here we have the initialization

243
00:08:00,300 --> 00:08:05,160
that we are quite used to so we have our

244
00:08:03,000 --> 00:08:06,780
embedding table for the characters the

245
00:08:05,160 --> 00:08:08,340
first layer second layer and the batch

246
00:08:06,780 --> 00:08:09,780
normalization in between

247
00:08:08,340 --> 00:08:11,880
and here's where we create all the

248
00:08:09,780 --> 00:08:13,500
parameters now you will note that I

249
00:08:11,880 --> 00:08:16,080
changed the initialization a little bit

250
00:08:13,500 --> 00:08:18,240
uh to be small numbers so normally you

251
00:08:16,080 --> 00:08:20,099
would set the biases to be all zero here

252
00:08:18,240 --> 00:08:22,800
I am setting them to be small random

253
00:08:20,099 --> 00:08:24,539
numbers and I'm doing this because

254
00:08:22,800 --> 00:08:26,220
if your variables are initialized to

255
00:08:24,539 --> 00:08:28,080
exactly zero sometimes what can happen

256
00:08:26,220 --> 00:08:30,360
is that can mask an incorrect

257
00:08:28,080 --> 00:08:32,520
implementation of a gradient

258
00:08:30,360 --> 00:08:34,020
um because uh when everything is zero it

259
00:08:32,520 --> 00:08:35,279
sort of like simplifies and gives you a

260
00:08:34,020 --> 00:08:37,440
much simpler expression of the gradient

261
00:08:35,279 --> 00:08:39,240
than you would otherwise get and so by

262
00:08:37,440 --> 00:08:41,760
making it small numbers I'm trying to

263
00:08:39,240 --> 00:08:43,140
unmask those potential errors in these

264
00:08:41,760 --> 00:08:46,500
calculations

265
00:08:43,140 --> 00:08:48,420
you also notice that I'm using uh B1 in

266
00:08:46,500 --> 00:08:50,940
the first layer I'm using a bias despite

267
00:08:48,420 --> 00:08:52,800
batch normalization right afterwards

268
00:08:50,940 --> 00:08:54,180
um so this would typically not be what

269
00:08:52,800 --> 00:08:55,920
you do because we talked about the fact

270
00:08:54,180 --> 00:08:57,899
that you don't need the bias but I'm

271
00:08:55,920 --> 00:08:58,980
doing this here just for fun

272
00:08:57,899 --> 00:09:00,360
um because we're going to have a

273
00:08:58,980 --> 00:09:01,800
gradient with respect to it and we can

274
00:09:00,360 --> 00:09:03,540
check that we are still calculating it

275
00:09:01,800 --> 00:09:05,160
correctly even though this bias is

276
00:09:03,540 --> 00:09:07,260
asparious

277
00:09:05,160 --> 00:09:10,320
so here I'm calculating a single batch

278
00:09:07,260 --> 00:09:11,700
and then here I'm doing a forward pass

279
00:09:10,320 --> 00:09:13,680
now you'll notice that the forward pass

280
00:09:11,700 --> 00:09:15,600
is significantly expanded from what we

281
00:09:13,680 --> 00:09:16,620
are used to here the forward pass was

282
00:09:15,600 --> 00:09:17,880
just

283
00:09:16,620 --> 00:09:19,620
um here

284
00:09:17,880 --> 00:09:22,080
now the reason that the forward pass is

285
00:09:19,620 --> 00:09:24,000
longer is for two reasons number one

286
00:09:22,080 --> 00:09:26,519
here we just had an F dot cross entropy

287
00:09:24,000 --> 00:09:28,380
but here I am bringing back a explicit

288
00:09:26,519 --> 00:09:29,820
implementation of the loss function

289
00:09:28,380 --> 00:09:32,040
and number two

290
00:09:29,820 --> 00:09:35,339
I've broken up the implementation into

291
00:09:32,040 --> 00:09:37,080
manageable chunks so we have a lot a lot

292
00:09:35,339 --> 00:09:38,820
more intermediate tensors along the way

293
00:09:37,080 --> 00:09:40,500
in the forward pass and that's because

294
00:09:38,820 --> 00:09:42,839
we are about to go backwards and

295
00:09:40,500 --> 00:09:45,720
calculate the gradients in this back

296
00:09:42,839 --> 00:09:48,420
propagation from the bottom to the top

297
00:09:45,720 --> 00:09:49,920
so we're going to go upwards and just

298
00:09:48,420 --> 00:09:51,959
like we have for example the lock props

299
00:09:49,920 --> 00:09:53,700
tensor in a forward pass in the backward

300
00:09:51,959 --> 00:09:55,200
pass we're going to have a d-lock probes

301
00:09:53,700 --> 00:09:56,519
which is going to store the derivative

302
00:09:55,200 --> 00:09:58,680
of the loss with respect to the lock

303
00:09:56,519 --> 00:10:00,899
props tensor and so we're going to be

304
00:09:58,680 --> 00:10:02,940
prepending D to every one of these

305
00:10:00,899 --> 00:10:04,920
tensors and calculating it along the way

306
00:10:02,940 --> 00:10:07,320
of this back propagation

307
00:10:04,920 --> 00:10:09,120
so as an example we have a b and raw

308
00:10:07,320 --> 00:10:12,600
here we're going to be calculating a DB

309
00:10:09,120 --> 00:10:14,640
in raw so here I'm telling pytorch that

310
00:10:12,600 --> 00:10:16,740
we want to retain the grad of all these

311
00:10:14,640 --> 00:10:18,480
intermediate values because here in

312
00:10:16,740 --> 00:10:20,279
exercise one we're going to calculate

313
00:10:18,480 --> 00:10:22,560
the backward pass so we're going to

314
00:10:20,279 --> 00:10:25,080
calculate all these D values D variables

315
00:10:22,560 --> 00:10:26,760
and use the CNP function I've introduced

316
00:10:25,080 --> 00:10:29,399
above to check our correctness with

317
00:10:26,760 --> 00:10:31,440
respect to what pi torch is telling us

318
00:10:29,399 --> 00:10:32,820
this is going to be exercise one uh

319
00:10:31,440 --> 00:10:34,680
where we sort of back propagate through

320
00:10:32,820 --> 00:10:36,240
this entire graph

321
00:10:34,680 --> 00:10:37,500
now just to give you a very quick

322
00:10:36,240 --> 00:10:40,380
preview of what's going to happen in

323
00:10:37,500 --> 00:10:43,320
exercise two and below here we have

324
00:10:40,380 --> 00:10:45,600
fully broken up the loss and back

325
00:10:43,320 --> 00:10:47,820
propagated through it manually in all

326
00:10:45,600 --> 00:10:49,140
the little Atomic pieces that make it up

327
00:10:47,820 --> 00:10:50,899
but here we're going to collapse the

328
00:10:49,140 --> 00:10:53,339
laws into a single cross-entropy call

329
00:10:50,899 --> 00:10:56,940
and instead we're going to analytically

330
00:10:53,339 --> 00:10:59,519
derive using math and paper and pencil

331
00:10:56,940 --> 00:11:01,140
the gradient of the loss with respect to

332
00:10:59,519 --> 00:11:02,339
the logits and instead of back

333
00:11:01,140 --> 00:11:04,440
propagating through all of its little

334
00:11:02,339 --> 00:11:05,880
chunks one at a time we're just going to

335
00:11:04,440 --> 00:11:07,440
analytically derive what that gradient

336
00:11:05,880 --> 00:11:09,180
is and we're going to implement that

337
00:11:07,440 --> 00:11:10,680
which is much more efficient as we'll

338
00:11:09,180 --> 00:11:12,660
see in the in a bit

339
00:11:10,680 --> 00:11:14,640
then we're going to do the exact same

340
00:11:12,660 --> 00:11:16,140
thing for patch normalization so instead

341
00:11:14,640 --> 00:11:18,600
of breaking up bass drum into all the

342
00:11:16,140 --> 00:11:20,820
old tiny components we're going to use

343
00:11:18,600 --> 00:11:22,920
uh pen and paper and Mathematics and

344
00:11:20,820 --> 00:11:25,380
calculus to derive the gradient through

345
00:11:22,920 --> 00:11:27,000
the bachelor Bachelor layer so we're

346
00:11:25,380 --> 00:11:28,500
going to calculate the backward

347
00:11:27,000 --> 00:11:30,240
passthrough bathroom layer in a much

348
00:11:28,500 --> 00:11:31,620
more efficient expression instead of

349
00:11:30,240 --> 00:11:33,600
backward propagating through all of its

350
00:11:31,620 --> 00:11:36,480
little pieces independently

351
00:11:33,600 --> 00:11:38,040
so there's going to be exercise three

352
00:11:36,480 --> 00:11:40,019
and then in exercise four we're going to

353
00:11:38,040 --> 00:11:42,720
put it all together and this is the full

354
00:11:40,019 --> 00:11:44,399
code of training this two layer MLP and

355
00:11:42,720 --> 00:11:46,140
we're going to basically insert our

356
00:11:44,399 --> 00:11:48,480
manual back prop and we're going to take

357
00:11:46,140 --> 00:11:50,760
out lost it backward and you will

358
00:11:48,480 --> 00:11:53,579
basically see that you can get all the

359
00:11:50,760 --> 00:11:55,980
same results using fully your own code

360
00:11:53,579 --> 00:11:59,279
and the only thing we're using from

361
00:11:55,980 --> 00:12:01,680
pytorch is the torch.tensor to make the

362
00:11:59,279 --> 00:12:03,300
calculations efficient but otherwise you

363
00:12:01,680 --> 00:12:04,740
will understand fully what it means to

364
00:12:03,300 --> 00:12:06,420
forward and backward and neural net and

365
00:12:04,740 --> 00:12:08,100
train it and I think that'll be awesome

366
00:12:06,420 --> 00:12:10,019
so let's get to it

367
00:12:08,100 --> 00:12:13,140
okay so I read all the cells of this

368
00:12:10,019 --> 00:12:14,700
notebook all the way up to here and I'm

369
00:12:13,140 --> 00:12:15,899
going to erase this and I'm going to

370
00:12:14,700 --> 00:12:18,720
start implementing backward pass

371
00:12:15,899 --> 00:12:20,459
starting with d lock problems so we want

372
00:12:18,720 --> 00:12:22,079
to understand what should go here to

373
00:12:20,459 --> 00:12:23,760
calculate the gradient of the loss with

374
00:12:22,079 --> 00:12:25,079
respect to all the elements of the log

375
00:12:23,760 --> 00:12:26,880
props tensor

376
00:12:25,079 --> 00:12:28,380
now I'm going to give away the answer

377
00:12:26,880 --> 00:12:30,240
here but I wanted to put a quick note

378
00:12:28,380 --> 00:12:32,459
here that I think would be most

379
00:12:30,240 --> 00:12:34,500
pedagogically useful for you is to

380
00:12:32,459 --> 00:12:36,660
actually go into the description of this

381
00:12:34,500 --> 00:12:38,399
video and find the link to this Jupiter

382
00:12:36,660 --> 00:12:40,140
notebook you can find it both on GitHub

383
00:12:38,399 --> 00:12:41,339
but you can also find Google collab with

384
00:12:40,140 --> 00:12:43,139
it so you don't have to install anything

385
00:12:41,339 --> 00:12:45,180
you'll just go to a website on Google

386
00:12:43,139 --> 00:12:47,959
collab and you can try to implement

387
00:12:45,180 --> 00:12:50,519
these derivatives or gradients yourself

388
00:12:47,959 --> 00:12:53,279
and then if you are not able to come to

389
00:12:50,519 --> 00:12:55,440
my video and see me do it and so work in

390
00:12:53,279 --> 00:12:57,959
Tandem and try it first yourself and

391
00:12:55,440 --> 00:12:59,100
then see me give away the answer and I

392
00:12:57,959 --> 00:13:00,240
think that'll be most valuable to you

393
00:12:59,100 --> 00:13:01,440
and that's how I recommend you go

394
00:13:00,240 --> 00:13:03,540
through this lecture

395
00:13:01,440 --> 00:13:06,959
so we are starting here with d-log props

396
00:13:03,540 --> 00:13:08,639
now d-lock props will hold the

397
00:13:06,959 --> 00:13:11,339
derivative of the loss with respect to

398
00:13:08,639 --> 00:13:13,680
all the elements of log props

399
00:13:11,339 --> 00:13:18,180
what is inside log blobs the shape of

400
00:13:13,680 --> 00:13:19,620
this is 32 by 27. so it's not going to

401
00:13:18,180 --> 00:13:21,899
surprise you that D log props should

402
00:13:19,620 --> 00:13:23,760
also be an array of size 32 by 27

403
00:13:21,899 --> 00:13:26,040
because we want the derivative loss with

404
00:13:23,760 --> 00:13:27,540
respect to all of its elements so the

405
00:13:26,040 --> 00:13:29,519
sizes of those are always going to be

406
00:13:27,540 --> 00:13:33,120
equal

407
00:13:29,519 --> 00:13:36,899
now how how does log props influence the

408
00:13:33,120 --> 00:13:40,680
loss okay loss is negative block probes

409
00:13:36,899 --> 00:13:42,860
indexed with range of N and YB and then

410
00:13:40,680 --> 00:13:47,820
the mean of that now just as a reminder

411
00:13:42,860 --> 00:13:51,540
YB is just a basically an array of all

412
00:13:47,820 --> 00:13:52,980
the correct indices

413
00:13:51,540 --> 00:13:54,720
um so what we're doing here is we're

414
00:13:52,980 --> 00:13:57,300
taking the lock props array of size 32

415
00:13:54,720 --> 00:13:58,920
by 27.

416
00:13:57,300 --> 00:14:00,959
right

417
00:13:58,920 --> 00:14:03,120
and then we are going in every single

418
00:14:00,959 --> 00:14:06,300
row and in each row we are plugging

419
00:14:03,120 --> 00:14:07,980
plucking out the index eight and then 14

420
00:14:06,300 --> 00:14:10,920
and 15 and so on so we're going down the

421
00:14:07,980 --> 00:14:12,899
rows that's the iterator range of N and

422
00:14:10,920 --> 00:14:15,240
then we are always plucking out the

423
00:14:12,899 --> 00:14:17,760
index of the column specified by this

424
00:14:15,240 --> 00:14:20,160
tensor YB so in the zeroth row we are

425
00:14:17,760 --> 00:14:23,339
taking the eighth column in the first

426
00:14:20,160 --> 00:14:26,519
row we're taking the 14th column Etc and

427
00:14:23,339 --> 00:14:28,459
so log props at this plugs out

428
00:14:26,519 --> 00:14:30,899
all those

429
00:14:28,459 --> 00:14:32,760
log probabilities of the correct next

430
00:14:30,899 --> 00:14:34,740
character in a sequence

431
00:14:32,760 --> 00:14:36,540
so that's what that does and the shape

432
00:14:34,740 --> 00:14:40,500
of this or the size of it is of course

433
00:14:36,540 --> 00:14:43,260
32 because our batch size is 32.

434
00:14:40,500 --> 00:14:45,600
so these elements get plugged out and

435
00:14:43,260 --> 00:14:47,220
then their mean and the negative of that

436
00:14:45,600 --> 00:14:49,740
becomes loss

437
00:14:47,220 --> 00:14:52,320
so I always like to work with simpler

438
00:14:49,740 --> 00:14:55,199
examples to understand the numerical

439
00:14:52,320 --> 00:14:58,800
form of derivative what's going on here

440
00:14:55,199 --> 00:15:00,720
is once we've plucked out these examples

441
00:14:58,800 --> 00:15:02,820
um we're taking the mean and then the

442
00:15:00,720 --> 00:15:04,920
negative so the loss basically

443
00:15:02,820 --> 00:15:07,860
I can write it this way is the negative

444
00:15:04,920 --> 00:15:09,360
of say a plus b plus c

445
00:15:07,860 --> 00:15:11,279
and the mean of those three numbers

446
00:15:09,360 --> 00:15:13,199
would be say negative would divide three

447
00:15:11,279 --> 00:15:15,240
that would be how we achieve the mean of

448
00:15:13,199 --> 00:15:16,980
three numbers ABC although we actually

449
00:15:15,240 --> 00:15:20,339
have 32 numbers here

450
00:15:16,980 --> 00:15:22,440
and so what is basically the loss by say

451
00:15:20,339 --> 00:15:24,660
like d a right

452
00:15:22,440 --> 00:15:26,160
well if we simplify this expression

453
00:15:24,660 --> 00:15:28,320
mathematically this is negative one over

454
00:15:26,160 --> 00:15:30,380
three of A and negative plus negative

455
00:15:28,320 --> 00:15:33,779
one over three of B

456
00:15:30,380 --> 00:15:35,699
plus negative 1 over 3 of c and so what

457
00:15:33,779 --> 00:15:36,779
is D loss by D A it's just negative one

458
00:15:35,699 --> 00:15:38,519
over three

459
00:15:36,779 --> 00:15:40,699
and so you can see that if we don't just

460
00:15:38,519 --> 00:15:43,800
have a b and c but we have 32 numbers

461
00:15:40,699 --> 00:15:45,240
then D loss by D

462
00:15:43,800 --> 00:15:47,399
um you know every one of those numbers

463
00:15:45,240 --> 00:15:50,820
is going to be one over N More generally

464
00:15:47,399 --> 00:15:53,040
because n is the um the size of the

465
00:15:50,820 --> 00:15:55,800
batch 32 in this case

466
00:15:53,040 --> 00:15:59,820
so D loss by

467
00:15:55,800 --> 00:16:02,100
um D Lock probs is negative 1 over n

468
00:15:59,820 --> 00:16:04,079
in all these places

469
00:16:02,100 --> 00:16:05,940
now what about the other elements inside

470
00:16:04,079 --> 00:16:07,800
lock problems because lock props is

471
00:16:05,940 --> 00:16:11,160
large array you see that lock problems

472
00:16:07,800 --> 00:16:13,740
at shape is 32 by 27. but only 32 of

473
00:16:11,160 --> 00:16:15,959
them participate in the loss calculation

474
00:16:13,740 --> 00:16:18,480
so what's the derivative of all the

475
00:16:15,959 --> 00:16:20,339
other most of the elements that do not

476
00:16:18,480 --> 00:16:22,079
get plucked out here

477
00:16:20,339 --> 00:16:24,300
while their loss intuitively is zero

478
00:16:22,079 --> 00:16:25,800
sorry they're gradient intuitively is

479
00:16:24,300 --> 00:16:27,420
zero and that's because they did not

480
00:16:25,800 --> 00:16:29,639
participate in the loss

481
00:16:27,420 --> 00:16:32,220
so most of these numbers inside this

482
00:16:29,639 --> 00:16:33,600
tensor does not feed into the loss and

483
00:16:32,220 --> 00:16:36,120
so if we were to change these numbers

484
00:16:33,600 --> 00:16:38,759
then the loss doesn't change which is

485
00:16:36,120 --> 00:16:39,839
the equivalent of way of saying that the

486
00:16:38,759 --> 00:16:43,320
derivative of the loss with respect to

487
00:16:39,839 --> 00:16:45,360
them is zero they don't impact it

488
00:16:43,320 --> 00:16:47,519
so here's a way to implement this

489
00:16:45,360 --> 00:16:50,820
derivative then we start out with

490
00:16:47,519 --> 00:16:52,740
torch.zeros of shape 32 by 27 or let's

491
00:16:50,820 --> 00:16:54,420
just say instead of doing this because

492
00:16:52,740 --> 00:16:57,000
we don't want to hard code numbers let's

493
00:16:54,420 --> 00:16:59,040
do torch.zeros like

494
00:16:57,000 --> 00:17:00,660
block probs so basically this is going

495
00:16:59,040 --> 00:17:02,579
to create an array of zeros exactly in

496
00:17:00,660 --> 00:17:05,100
the shape of log probs

497
00:17:02,579 --> 00:17:07,140
and then we need to set the derivative

498
00:17:05,100 --> 00:17:09,959
of negative 1 over n inside exactly

499
00:17:07,140 --> 00:17:12,839
these locations so here's what we can do

500
00:17:09,959 --> 00:17:14,280
the lock props indexed in The Identical

501
00:17:12,839 --> 00:17:16,439
way

502
00:17:14,280 --> 00:17:19,799
will be just set to negative one over

503
00:17:16,439 --> 00:17:22,559
zero divide n

504
00:17:19,799 --> 00:17:25,860
right just like we derived here

505
00:17:22,559 --> 00:17:27,120
so now let me erase all this reasoning

506
00:17:25,860 --> 00:17:29,940
and then this is the candidate

507
00:17:27,120 --> 00:17:31,799
derivative for D log props let's

508
00:17:29,940 --> 00:17:34,260
uncomment the first line and check that

509
00:17:31,799 --> 00:17:39,720
this is correct

510
00:17:34,260 --> 00:17:41,220
okay so CMP ran and let's go back to CMP

511
00:17:39,720 --> 00:17:42,840
and you see that what it's doing is it's

512
00:17:41,220 --> 00:17:46,080
calculating if

513
00:17:42,840 --> 00:17:48,179
the calculated value by us which is DT

514
00:17:46,080 --> 00:17:51,059
is exactly equal to T dot grad as

515
00:17:48,179 --> 00:17:52,679
calculated by pi torch and then this is

516
00:17:51,059 --> 00:17:54,960
making sure that all the elements are

517
00:17:52,679 --> 00:17:57,179
exactly equal and then converting this

518
00:17:54,960 --> 00:17:58,440
to a single Boolean value because we

519
00:17:57,179 --> 00:18:00,059
don't want the Boolean tensor we just

520
00:17:58,440 --> 00:18:02,760
want to Boolean value

521
00:18:00,059 --> 00:18:04,320
and then here we are making sure that

522
00:18:02,760 --> 00:18:06,000
okay if they're not exactly equal maybe

523
00:18:04,320 --> 00:18:07,559
they are approximately equal because of

524
00:18:06,000 --> 00:18:09,120
some floating Point issues but they're

525
00:18:07,559 --> 00:18:10,919
very very close

526
00:18:09,120 --> 00:18:13,020
so here we are using torch.allclose

527
00:18:10,919 --> 00:18:15,299
which has a little bit of a wiggle

528
00:18:13,020 --> 00:18:17,760
available because sometimes you can get

529
00:18:15,299 --> 00:18:19,380
very very close but if you use a

530
00:18:17,760 --> 00:18:22,559
slightly different calculation because a

531
00:18:19,380 --> 00:18:24,419
floating Point arithmetic you can get a

532
00:18:22,559 --> 00:18:25,860
slightly different result so this is

533
00:18:24,419 --> 00:18:27,299
checking if you get an approximately

534
00:18:25,860 --> 00:18:28,799
close result

535
00:18:27,299 --> 00:18:31,620
and then here we are checking the

536
00:18:28,799 --> 00:18:34,200
maximum uh basically the value that has

537
00:18:31,620 --> 00:18:35,880
the highest difference and what is the

538
00:18:34,200 --> 00:18:37,860
difference in the absolute value

539
00:18:35,880 --> 00:18:39,360
difference between those two and so we

540
00:18:37,860 --> 00:18:42,059
are printing whether we have an exact

541
00:18:39,360 --> 00:18:45,000
equality an approximate equality and

542
00:18:42,059 --> 00:18:46,860
what is the largest difference

543
00:18:45,000 --> 00:18:48,360
and so here

544
00:18:46,860 --> 00:18:50,280
we see that we actually have exact

545
00:18:48,360 --> 00:18:52,440
equality and so therefore of course we

546
00:18:50,280 --> 00:18:54,600
also have an approximate equality and

547
00:18:52,440 --> 00:18:57,660
the maximum difference is exactly zero

548
00:18:54,600 --> 00:19:00,860
so basically our d-log props is exactly

549
00:18:57,660 --> 00:19:03,780
equal to what pytors calculated to be

550
00:19:00,860 --> 00:19:06,539
lockprops.grad in its back propagation

551
00:19:03,780 --> 00:19:07,620
so so far we're working pretty well okay

552
00:19:06,539 --> 00:19:08,700
so let's now continue our back

553
00:19:07,620 --> 00:19:10,559
propagation

554
00:19:08,700 --> 00:19:12,480
we have that lock props depends on

555
00:19:10,559 --> 00:19:14,280
probes through a log

556
00:19:12,480 --> 00:19:17,580
so all the elements of probes are being

557
00:19:14,280 --> 00:19:19,740
element wise applied log to

558
00:19:17,580 --> 00:19:22,140
now if we want deep props then then

559
00:19:19,740 --> 00:19:24,539
remember your micrograph training

560
00:19:22,140 --> 00:19:27,780
we have like a log node it takes in

561
00:19:24,539 --> 00:19:30,000
probs and creates log probs and the

562
00:19:27,780 --> 00:19:33,419
props will be the local derivative of

563
00:19:30,000 --> 00:19:34,860
that individual Operation Log times the

564
00:19:33,419 --> 00:19:37,559
derivative loss with respect to its

565
00:19:34,860 --> 00:19:39,600
output which in this case is D log props

566
00:19:37,559 --> 00:19:41,520
so what is the local derivative of this

567
00:19:39,600 --> 00:19:43,799
operation well we are taking log element

568
00:19:41,520 --> 00:19:45,840
wise and we can come here and we can see

569
00:19:43,799 --> 00:19:47,940
well from alpha is your friend that d by

570
00:19:45,840 --> 00:19:48,960
DX of log of x is just simply one of our

571
00:19:47,940 --> 00:19:51,660
X

572
00:19:48,960 --> 00:19:54,900
so therefore in this case X is problems

573
00:19:51,660 --> 00:19:56,940
so we have d by DX is one over X which

574
00:19:54,900 --> 00:19:58,980
is one of our probes and then this is

575
00:19:56,940 --> 00:20:00,179
the local derivative and then times we

576
00:19:58,980 --> 00:20:01,679
want to chain it

577
00:20:00,179 --> 00:20:03,539
so this is chain rule

578
00:20:01,679 --> 00:20:06,240
times do log props

579
00:20:03,539 --> 00:20:08,880
let me uncomment this and let me run the

580
00:20:06,240 --> 00:20:10,320
cell in place and we see that the

581
00:20:08,880 --> 00:20:12,900
derivative of props as we calculated

582
00:20:10,320 --> 00:20:15,720
here is exactly correct

583
00:20:12,900 --> 00:20:18,299
and so notice here how this works probes

584
00:20:15,720 --> 00:20:20,760
that are props is going to be inverted

585
00:20:18,299 --> 00:20:23,100
and then element was multiplied here

586
00:20:20,760 --> 00:20:25,020
so if your probes is very very close to

587
00:20:23,100 --> 00:20:26,100
one that means you are your network is

588
00:20:25,020 --> 00:20:28,200
currently predicting the character

589
00:20:26,100 --> 00:20:30,720
correctly then this will become one over

590
00:20:28,200 --> 00:20:31,620
one and D log probes just gets passed

591
00:20:30,720 --> 00:20:33,059
through

592
00:20:31,620 --> 00:20:35,400
but if your probabilities are

593
00:20:33,059 --> 00:20:37,380
incorrectly assigned so if the correct

594
00:20:35,400 --> 00:20:41,640
character here is getting a very low

595
00:20:37,380 --> 00:20:43,260
probability then 1.0 dividing by it will

596
00:20:41,640 --> 00:20:45,660
boost this

597
00:20:43,260 --> 00:20:46,799
and then multiply by the log props so

598
00:20:45,660 --> 00:20:49,380
basically what this line is doing

599
00:20:46,799 --> 00:20:50,940
intuitively is it's taking the examples

600
00:20:49,380 --> 00:20:52,860
that have a very low probability

601
00:20:50,940 --> 00:20:55,020
currently assigned and it's boosting

602
00:20:52,860 --> 00:20:59,280
their gradient uh you can you can look

603
00:20:55,020 --> 00:21:02,760
at it that way next up is Count some imp

604
00:20:59,280 --> 00:21:05,100
so we want the river of this now let me

605
00:21:02,760 --> 00:21:06,840
just pause here and kind of introduce

606
00:21:05,100 --> 00:21:08,640
What's Happening Here in general because

607
00:21:06,840 --> 00:21:09,840
I know it's a little bit confusing we

608
00:21:08,640 --> 00:21:11,940
have the locusts that come out of the

609
00:21:09,840 --> 00:21:15,000
neural nut here what I'm doing is I'm

610
00:21:11,940 --> 00:21:16,500
finding the maximum in each row and I'm

611
00:21:15,000 --> 00:21:18,240
subtracting it for the purposes of

612
00:21:16,500 --> 00:21:20,520
numerical stability and we talked about

613
00:21:18,240 --> 00:21:22,260
how if you do not do this you run

614
00:21:20,520 --> 00:21:24,539
numerical issues if some of the logits

615
00:21:22,260 --> 00:21:26,340
take on two large values because we end

616
00:21:24,539 --> 00:21:28,260
up exponentiating them

617
00:21:26,340 --> 00:21:30,659
so this is done just for safety

618
00:21:28,260 --> 00:21:32,600
numerically then here's the

619
00:21:30,659 --> 00:21:35,820
exponentiation of all the sort of like

620
00:21:32,600 --> 00:21:38,039
logits to create our accounts and then

621
00:21:35,820 --> 00:21:40,260
we want to take the some of these counts

622
00:21:38,039 --> 00:21:41,640
and normalize so that all of the probes

623
00:21:40,260 --> 00:21:43,559
sum to one

624
00:21:41,640 --> 00:21:46,020
now here instead of using one over count

625
00:21:43,559 --> 00:21:47,640
sum I use uh raised to the power of

626
00:21:46,020 --> 00:21:49,320
negative one mathematically they are

627
00:21:47,640 --> 00:21:50,460
identical I just found that there's

628
00:21:49,320 --> 00:21:52,020
something wrong with the pytorch

629
00:21:50,460 --> 00:21:53,460
implementation of the backward pass of

630
00:21:52,020 --> 00:21:55,980
division

631
00:21:53,460 --> 00:21:58,140
um and it gives like a real result but

632
00:21:55,980 --> 00:21:59,820
that doesn't happen for star star native

633
00:21:58,140 --> 00:22:01,919
one that's why I'm using this formula

634
00:21:59,820 --> 00:22:04,140
instead but basically all that's

635
00:22:01,919 --> 00:22:05,460
happening here is we got the logits

636
00:22:04,140 --> 00:22:07,679
we're going to exponentiate all of them

637
00:22:05,460 --> 00:22:09,720
and want to normalize the counts to

638
00:22:07,679 --> 00:22:12,539
create our probabilities it's just that

639
00:22:09,720 --> 00:22:14,220
it's happening across multiple lines

640
00:22:12,539 --> 00:22:17,220
so now

641
00:22:14,220 --> 00:22:17,220
here

642
00:22:17,700 --> 00:22:21,840
we want to First Take the derivative we

643
00:22:20,700 --> 00:22:24,419
want to back propagate into account

644
00:22:21,840 --> 00:22:28,559
sumiv and then into counts as well

645
00:22:24,419 --> 00:22:29,940
so what should be the count sum M now we

646
00:22:28,559 --> 00:22:32,220
actually have to be careful here because

647
00:22:29,940 --> 00:22:35,940
we have to scrutinize and be careful

648
00:22:32,220 --> 00:22:39,240
with the shapes so counts that shape and

649
00:22:35,940 --> 00:22:40,620
then count some inverse shape

650
00:22:39,240 --> 00:22:43,440
are different

651
00:22:40,620 --> 00:22:47,159
so in particular counts as 32 by 27 but

652
00:22:43,440 --> 00:22:49,140
this count sum m is 32 by 1. and so in

653
00:22:47,159 --> 00:22:52,020
this multiplication here we also have an

654
00:22:49,140 --> 00:22:53,760
implicit broadcasting that pytorch will

655
00:22:52,020 --> 00:22:55,679
do because it needs to take this column

656
00:22:53,760 --> 00:22:58,500
tensor of 32 numbers and replicate it

657
00:22:55,679 --> 00:23:00,419
horizontally 27 times to align these two

658
00:22:58,500 --> 00:23:01,620
tensors so it can do an element twice

659
00:23:00,419 --> 00:23:03,539
multiply

660
00:23:01,620 --> 00:23:06,299
so really what this looks like is the

661
00:23:03,539 --> 00:23:08,280
following using a toy example again

662
00:23:06,299 --> 00:23:10,260
what we really have here is just props

663
00:23:08,280 --> 00:23:11,760
is counts times conservative so it's a C

664
00:23:10,260 --> 00:23:15,059
equals a times B

665
00:23:11,760 --> 00:23:17,580
but a is 3 by 3 and b is just three by

666
00:23:15,059 --> 00:23:19,860
one a column tensor and so pytorch

667
00:23:17,580 --> 00:23:22,440
internally replicated this elements of B

668
00:23:19,860 --> 00:23:24,539
and it did that across all the columns

669
00:23:22,440 --> 00:23:26,400
so for example B1 which is the first

670
00:23:24,539 --> 00:23:27,780
element of B would be replicated here

671
00:23:26,400 --> 00:23:29,280
across all the columns in this

672
00:23:27,780 --> 00:23:31,020
multiplication

673
00:23:29,280 --> 00:23:34,200
and now we're trying to back propagate

674
00:23:31,020 --> 00:23:35,880
through this operation to count some m

675
00:23:34,200 --> 00:23:37,380
so when we're calculating this

676
00:23:35,880 --> 00:23:39,360
derivative

677
00:23:37,380 --> 00:23:41,760
it's important to realize that these two

678
00:23:39,360 --> 00:23:44,039
this looks like a single operation but

679
00:23:41,760 --> 00:23:46,260
actually is two operations applied

680
00:23:44,039 --> 00:23:48,360
sequentially the first operation that

681
00:23:46,260 --> 00:23:52,020
pytorch did is it took this column

682
00:23:48,360 --> 00:23:54,120
tensor and replicated it across all the

683
00:23:52,020 --> 00:23:55,919
um across all the columns basically 27

684
00:23:54,120 --> 00:23:57,539
times so that's the first operation it's

685
00:23:55,919 --> 00:23:59,880
a replication and then the second

686
00:23:57,539 --> 00:24:01,080
operation is the multiplication so let's

687
00:23:59,880 --> 00:24:02,580
first background through the

688
00:24:01,080 --> 00:24:05,640
multiplication

689
00:24:02,580 --> 00:24:08,280
if these two arrays are of the same size

690
00:24:05,640 --> 00:24:11,159
and we just have a and b of both of them

691
00:24:08,280 --> 00:24:12,120
three by three then how do we mult how

692
00:24:11,159 --> 00:24:14,039
do we back propagate through a

693
00:24:12,120 --> 00:24:16,140
multiplication so if we just have

694
00:24:14,039 --> 00:24:19,320
scalars and not tensors then if you have

695
00:24:16,140 --> 00:24:21,539
C equals a times B then what is uh the

696
00:24:19,320 --> 00:24:23,460
order of the of C with respect to B well

697
00:24:21,539 --> 00:24:24,659
it's just a and so that's the local

698
00:24:23,460 --> 00:24:27,059
derivative

699
00:24:24,659 --> 00:24:29,039
so here in our case undoing the

700
00:24:27,059 --> 00:24:30,539
multiplication and back propagating

701
00:24:29,039 --> 00:24:33,000
through just the multiplication itself

702
00:24:30,539 --> 00:24:36,059
which is element wise is going to be the

703
00:24:33,000 --> 00:24:40,260
local derivative which in this case is

704
00:24:36,059 --> 00:24:42,059
simply counts because counts is the a

705
00:24:40,260 --> 00:24:46,140
so this is the local derivative and then

706
00:24:42,059 --> 00:24:48,600
times because the chain rule D props

707
00:24:46,140 --> 00:24:50,760
so this here is the derivative or the

708
00:24:48,600 --> 00:24:52,080
gradient but with respect to replicated

709
00:24:50,760 --> 00:24:54,120
B

710
00:24:52,080 --> 00:24:56,460
but we don't have a replicated B we just

711
00:24:54,120 --> 00:24:59,340
have a single B column so how do we now

712
00:24:56,460 --> 00:25:02,220
back propagate through the replication

713
00:24:59,340 --> 00:25:04,020
and intuitively this B1 is the same

714
00:25:02,220 --> 00:25:04,919
variable and it's just reused multiple

715
00:25:04,020 --> 00:25:07,320
times

716
00:25:04,919 --> 00:25:09,360
and so you can look at it

717
00:25:07,320 --> 00:25:10,980
as being equivalent to a case we've

718
00:25:09,360 --> 00:25:12,720
encountered in micrograd

719
00:25:10,980 --> 00:25:14,940
and so here I'm just pulling out a

720
00:25:12,720 --> 00:25:17,760
random graph we used in micrograd we had

721
00:25:14,940 --> 00:25:19,860
an example where a single node

722
00:25:17,760 --> 00:25:22,740
has its output feeding into two branches

723
00:25:19,860 --> 00:25:25,020
of basically the graph until the last

724
00:25:22,740 --> 00:25:26,279
function and we're talking about how the

725
00:25:25,020 --> 00:25:29,039
correct thing to do in the backward pass

726
00:25:26,279 --> 00:25:31,860
is we need to sum all the gradients that

727
00:25:29,039 --> 00:25:33,419
arrive at any one node so across these

728
00:25:31,860 --> 00:25:34,799
different branches the gradients would

729
00:25:33,419 --> 00:25:37,320
sum

730
00:25:34,799 --> 00:25:39,779
so if a node is used multiple times the

731
00:25:37,320 --> 00:25:41,460
gradients for all of its uses sum during

732
00:25:39,779 --> 00:25:44,039
back propagation

733
00:25:41,460 --> 00:25:45,659
so here B1 is used multiple times in all

734
00:25:44,039 --> 00:25:48,380
these columns and therefore the right

735
00:25:45,659 --> 00:25:51,480
thing to do here is to sum

736
00:25:48,380 --> 00:25:52,740
horizontally across all the rows so I'm

737
00:25:51,480 --> 00:25:55,799
going to sum in

738
00:25:52,740 --> 00:25:58,320
Dimension one but we want to retain this

739
00:25:55,799 --> 00:26:00,299
Dimension so that the uh so that counts

740
00:25:58,320 --> 00:26:02,400
some end and its gradient are going to

741
00:26:00,299 --> 00:26:04,860
be exactly the same shape so we want to

742
00:26:02,400 --> 00:26:07,140
make sure that we keep them as true so

743
00:26:04,860 --> 00:26:08,940
we don't lose this dimension and this

744
00:26:07,140 --> 00:26:11,460
will make the count sum M be exactly

745
00:26:08,940 --> 00:26:14,760
shape 32 by 1.

746
00:26:11,460 --> 00:26:17,279
so revealing this comparison as well and

747
00:26:14,760 --> 00:26:18,360
running this we see that we get an exact

748
00:26:17,279 --> 00:26:22,260
match

749
00:26:18,360 --> 00:26:24,419
so this derivative is exactly correct

750
00:26:22,260 --> 00:26:26,880
and let me erase

751
00:26:24,419 --> 00:26:29,580
this now let's also back propagate into

752
00:26:26,880 --> 00:26:32,159
counts which is the other variable here

753
00:26:29,580 --> 00:26:33,779
to create probes so from props to count

754
00:26:32,159 --> 00:26:35,400
some INF we just did that let's go into

755
00:26:33,779 --> 00:26:39,380
counts as well

756
00:26:35,400 --> 00:26:39,380
so decounts will be

757
00:26:39,779 --> 00:26:47,520
the chances are a so DC by d a is just B

758
00:26:43,620 --> 00:26:51,240
so therefore it's count summative

759
00:26:47,520 --> 00:26:54,480
um and then times chain rule the props

760
00:26:51,240 --> 00:26:57,900
now councilman is three two by One D

761
00:26:54,480 --> 00:26:59,159
probs is 32 by 27.

762
00:26:57,900 --> 00:27:02,340
so

763
00:26:59,159 --> 00:27:04,440
um those will broadcast fine and will

764
00:27:02,340 --> 00:27:06,659
give us decounts there's no additional

765
00:27:04,440 --> 00:27:08,880
summation required here

766
00:27:06,659 --> 00:27:11,100
um there will be a broadcasting that

767
00:27:08,880 --> 00:27:12,720
happens in this multiply here because

768
00:27:11,100 --> 00:27:16,500
count some M needs to be replicated

769
00:27:12,720 --> 00:27:18,659
again to correctly multiply D props but

770
00:27:16,500 --> 00:27:20,880
that's going to give the correct result

771
00:27:18,659 --> 00:27:23,279
so as far as the single operation is

772
00:27:20,880 --> 00:27:25,380
concerned so we back probably go from

773
00:27:23,279 --> 00:27:29,400
props to counts but we can't actually

774
00:27:25,380 --> 00:27:31,440
check the derivative counts uh I have it

775
00:27:29,400 --> 00:27:34,380
much later on and the reason for that is

776
00:27:31,440 --> 00:27:36,179
because count sum in depends on counts

777
00:27:34,380 --> 00:27:38,340
and so there's a second Branch here that

778
00:27:36,179 --> 00:27:40,320
we have to finish because can't summon

779
00:27:38,340 --> 00:27:42,120
back propagates into account sum and

780
00:27:40,320 --> 00:27:44,520
count sum will buy properly into counts

781
00:27:42,120 --> 00:27:46,860
and so counts is a node that is being

782
00:27:44,520 --> 00:27:48,240
used twice it's used right here in two

783
00:27:46,860 --> 00:27:50,340
props and it goes through this other

784
00:27:48,240 --> 00:27:52,620
Branch through count summative

785
00:27:50,340 --> 00:27:54,120
so even though we've calculated the

786
00:27:52,620 --> 00:27:55,740
first contribution of it we still have

787
00:27:54,120 --> 00:27:57,000
to calculate the second contribution of

788
00:27:55,740 --> 00:27:58,620
it later

789
00:27:57,000 --> 00:28:00,600
okay so we're continuing with this

790
00:27:58,620 --> 00:28:02,340
Branch we have the derivative for count

791
00:28:00,600 --> 00:28:05,940
sum if now we want the derivative of

792
00:28:02,340 --> 00:28:07,320
count sum so D count sum equals what is

793
00:28:05,940 --> 00:28:09,720
the local derivative of this operation

794
00:28:07,320 --> 00:28:11,940
so this is basically an element wise one

795
00:28:09,720 --> 00:28:13,620
over counts sum

796
00:28:11,940 --> 00:28:15,419
so count sum raised to the power of

797
00:28:13,620 --> 00:28:17,820
negative one is the same as one over

798
00:28:15,419 --> 00:28:20,279
count sum if we go to all from alpha we

799
00:28:17,820 --> 00:28:23,159
see that x to the negative one D by D by

800
00:28:20,279 --> 00:28:25,980
D by DX of it is basically Negative X to

801
00:28:23,159 --> 00:28:27,539
the negative 2. right one negative one

802
00:28:25,980 --> 00:28:29,220
over squared is the same as Negative X

803
00:28:27,539 --> 00:28:32,640
to the negative two

804
00:28:29,220 --> 00:28:35,640
so D count sum here will be local

805
00:28:32,640 --> 00:28:36,179
derivative is going to be negative

806
00:28:35,640 --> 00:28:39,419
um

807
00:28:36,179 --> 00:28:41,880
counts sum to the negative two that's

808
00:28:39,419 --> 00:28:46,340
the local derivative times chain rule

809
00:28:41,880 --> 00:28:46,340
which is D count sum in

810
00:28:46,740 --> 00:28:51,659
so that's D count sum

811
00:28:49,260 --> 00:28:55,380
let's uncomment this and check that I am

812
00:28:51,659 --> 00:28:58,080
correct okay so we have perfect equality

813
00:28:55,380 --> 00:28:59,760
and there's no sketchiness going on here

814
00:28:58,080 --> 00:29:02,340
with any shapes because these are of the

815
00:28:59,760 --> 00:29:04,140
same shape okay next up we want to back

816
00:29:02,340 --> 00:29:07,320
propagate through this line we have that

817
00:29:04,140 --> 00:29:09,419
count sum it's count.sum along the rows

818
00:29:07,320 --> 00:29:11,760
so I wrote out

819
00:29:09,419 --> 00:29:13,740
um some help here we have to keep in

820
00:29:11,760 --> 00:29:17,279
mind that counts of course is 32 by 27

821
00:29:13,740 --> 00:29:19,919
and count sum is 32 by 1. so in this

822
00:29:17,279 --> 00:29:22,500
back propagation we need to take this

823
00:29:19,919 --> 00:29:24,539
column of derivatives and transform it

824
00:29:22,500 --> 00:29:26,700
into a array of derivatives

825
00:29:24,539 --> 00:29:28,980
two-dimensional array

826
00:29:26,700 --> 00:29:31,020
so what is this operation doing we're

827
00:29:28,980 --> 00:29:32,760
taking in some kind of an input like say

828
00:29:31,020 --> 00:29:36,000
a three by three Matrix a and we are

829
00:29:32,760 --> 00:29:39,899
summing up the rows into a column tells

830
00:29:36,000 --> 00:29:41,880
her B1 b2b3 that is basically this

831
00:29:39,899 --> 00:29:44,279
so now we have the derivatives of the

832
00:29:41,880 --> 00:29:45,120
loss with respect to B all the elements

833
00:29:44,279 --> 00:29:47,039
of B

834
00:29:45,120 --> 00:29:49,740
and now we want to derivative loss with

835
00:29:47,039 --> 00:29:52,799
respect to all these little A's

836
00:29:49,740 --> 00:29:54,120
so how do the B's depend on the ace is

837
00:29:52,799 --> 00:29:56,340
basically what we're after what is the

838
00:29:54,120 --> 00:29:58,200
local derivative of this operation

839
00:29:56,340 --> 00:30:01,620
well we can see here that B1 only

840
00:29:58,200 --> 00:30:03,240
depends on these elements here the

841
00:30:01,620 --> 00:30:06,419
derivative of B1 with respect to all of

842
00:30:03,240 --> 00:30:09,299
these elements down here is zero but for

843
00:30:06,419 --> 00:30:13,140
these elements here like a11 a12 Etc the

844
00:30:09,299 --> 00:30:16,200
local derivative is one right so DB 1 by

845
00:30:13,140 --> 00:30:18,000
D A 1 1 for example is one so it's one

846
00:30:16,200 --> 00:30:19,919
one and one

847
00:30:18,000 --> 00:30:21,480
so when we have the derivative of loss

848
00:30:19,919 --> 00:30:23,520
with respect to B1

849
00:30:21,480 --> 00:30:25,620
did a local derivative of B1 with

850
00:30:23,520 --> 00:30:27,840
respect to these inputs is zeros here

851
00:30:25,620 --> 00:30:29,880
but it's one on these guys

852
00:30:27,840 --> 00:30:32,419
so in the chain rule

853
00:30:29,880 --> 00:30:35,820
we have the local derivative uh times

854
00:30:32,419 --> 00:30:37,860
sort of the derivative of B1 and so

855
00:30:35,820 --> 00:30:39,899
because the local derivative is one on

856
00:30:37,860 --> 00:30:41,700
these three elements the look of them

857
00:30:39,899 --> 00:30:45,600
are multiplying the derivative of B1

858
00:30:41,700 --> 00:30:47,580
will just be the derivative of B1 and so

859
00:30:45,600 --> 00:30:50,399
you can look at it as a router basically

860
00:30:47,580 --> 00:30:52,020
an addition is a router of gradient

861
00:30:50,399 --> 00:30:53,820
whatever gradient comes from above it

862
00:30:52,020 --> 00:30:55,140
just gets routed equally to all the

863
00:30:53,820 --> 00:30:56,100
elements that participate in that

864
00:30:55,140 --> 00:30:58,320
addition

865
00:30:56,100 --> 00:31:00,840
so in this case the derivative of B1

866
00:30:58,320 --> 00:31:03,299
will just flow equally to the derivative

867
00:31:00,840 --> 00:31:05,100
of a11 a12 and a13

868
00:31:03,299 --> 00:31:07,740
. so if we have a derivative of all the

869
00:31:05,100 --> 00:31:10,380
elements of B and in this column tensor

870
00:31:07,740 --> 00:31:11,940
which is D counts sum that we've

871
00:31:10,380 --> 00:31:14,340
calculated just now

872
00:31:11,940 --> 00:31:17,159
we basically see that what that amounts

873
00:31:14,340 --> 00:31:19,320
to is all of these are now flowing to

874
00:31:17,159 --> 00:31:21,000
all these elements of a and they're

875
00:31:19,320 --> 00:31:22,919
doing that horizontally

876
00:31:21,000 --> 00:31:26,039
so basically what we want is we want to

877
00:31:22,919 --> 00:31:28,220
take the decount sum of size 30 by 1 and

878
00:31:26,039 --> 00:31:32,340
we just want to replicate it 27 times

879
00:31:28,220 --> 00:31:33,840
horizontally to create 32 by 27 array

880
00:31:32,340 --> 00:31:35,100
so there's many ways to implement this

881
00:31:33,840 --> 00:31:37,620
operation you could of course just

882
00:31:35,100 --> 00:31:40,380
replicate the tensor but I think maybe

883
00:31:37,620 --> 00:31:43,740
one clean one is that the counts is

884
00:31:40,380 --> 00:31:45,960
simply torch dot once like

885
00:31:43,740 --> 00:31:49,679
so just an two-dimensional arrays of

886
00:31:45,960 --> 00:31:53,640
ones in the shape of counts so 32 by 27

887
00:31:49,679 --> 00:31:56,640
times D counts sum so this way we're

888
00:31:53,640 --> 00:31:58,140
letting the broadcasting here basically

889
00:31:56,640 --> 00:31:59,399
implement the replication you can look

890
00:31:58,140 --> 00:32:02,460
at it that way

891
00:31:59,399 --> 00:32:05,039
but then we have to also be careful

892
00:32:02,460 --> 00:32:08,100
because decounts was already calculated

893
00:32:05,039 --> 00:32:09,539
we calculated earlier here and that was

894
00:32:08,100 --> 00:32:11,159
just the first branch and we're now

895
00:32:09,539 --> 00:32:13,500
finishing the second Branch so we need

896
00:32:11,159 --> 00:32:14,760
to make sure that these gradients add so

897
00:32:13,500 --> 00:32:16,320
plus equals

898
00:32:14,760 --> 00:32:20,520
and then here

899
00:32:16,320 --> 00:32:23,220
um let's comment out the comparison and

900
00:32:20,520 --> 00:32:25,320
let's make sure crossing fingers that we

901
00:32:23,220 --> 00:32:28,020
have the correct result so pytorch

902
00:32:25,320 --> 00:32:29,820
agrees with us on this gradient as well

903
00:32:28,020 --> 00:32:32,940
okay hopefully we're getting a hang of

904
00:32:29,820 --> 00:32:36,659
this now counts as an element-wise X of

905
00:32:32,940 --> 00:32:38,100
Norm legits so now we want D Norm logits

906
00:32:36,659 --> 00:32:40,020
and because it's an element price

907
00:32:38,100 --> 00:32:41,520
operation everything is very simple what

908
00:32:40,020 --> 00:32:45,179
is the local derivative of e to the X

909
00:32:41,520 --> 00:32:47,899
it's famously just e to the x so this is

910
00:32:45,179 --> 00:32:47,899
the local derivative

911
00:32:48,480 --> 00:32:51,659
that is the local derivative now we

912
00:32:50,279 --> 00:32:53,640
already calculated it and it's inside

913
00:32:51,659 --> 00:32:55,320
counts so we may as well potentially

914
00:32:53,640 --> 00:32:56,220
just reuse counts that is the local

915
00:32:55,320 --> 00:33:00,740
derivative

916
00:32:56,220 --> 00:33:00,740
times uh D counts

917
00:33:01,919 --> 00:33:07,020
funny as that looks constant decount is

918
00:33:04,320 --> 00:33:10,080
derivative on the normal objects and now

919
00:33:07,020 --> 00:33:12,380
let's erase this and let's verify and it

920
00:33:10,080 --> 00:33:12,380
looks good

921
00:33:12,419 --> 00:33:17,519
so that's uh normal agents

922
00:33:14,880 --> 00:33:18,899
okay so we are here on this line now the

923
00:33:17,519 --> 00:33:20,519
normal objects

924
00:33:18,899 --> 00:33:22,679
we have that and we're trying to

925
00:33:20,519 --> 00:33:25,380
calculate the logits and deloget Maxes

926
00:33:22,679 --> 00:33:26,760
so back propagating through this line

927
00:33:25,380 --> 00:33:29,100
now we have to be careful here because

928
00:33:26,760 --> 00:33:30,899
the shapes again are not the same and so

929
00:33:29,100 --> 00:33:32,100
there's an implicit broadcasting

930
00:33:30,899 --> 00:33:34,559
Happening Here

931
00:33:32,100 --> 00:33:37,620
so normal jits has this shape 32 by 27

932
00:33:34,559 --> 00:33:40,440
logist does as well but logit Maxis is

933
00:33:37,620 --> 00:33:42,840
only 32 by one so there's a broadcasting

934
00:33:40,440 --> 00:33:45,899
here in the minus

935
00:33:42,840 --> 00:33:48,059
now here I try to sort of write out a

936
00:33:45,899 --> 00:33:50,760
two example again we basically have that

937
00:33:48,059 --> 00:33:52,500
this is our C equals a minus B

938
00:33:50,760 --> 00:33:54,000
and we see that because of the shape

939
00:33:52,500 --> 00:33:55,260
these are three by three but this one is

940
00:33:54,000 --> 00:33:57,720
just a column

941
00:33:55,260 --> 00:34:00,539
and so for example every element of C we

942
00:33:57,720 --> 00:34:01,860
have to look at how it uh came to be and

943
00:34:00,539 --> 00:34:04,860
every element of C is just the

944
00:34:01,860 --> 00:34:08,099
corresponding element of a minus uh

945
00:34:04,860 --> 00:34:10,500
basically that associated b

946
00:34:08,099 --> 00:34:13,080
so it's very clear now that the

947
00:34:10,500 --> 00:34:16,560
derivatives of every one of these c's

948
00:34:13,080 --> 00:34:18,300
with respect to their inputs are one for

949
00:34:16,560 --> 00:34:20,639
the corresponding a

950
00:34:18,300 --> 00:34:22,139
and it's a negative one for the

951
00:34:20,639 --> 00:34:24,000
corresponding B

952
00:34:22,139 --> 00:34:25,020
and so therefore

953
00:34:24,000 --> 00:34:27,800
um

954
00:34:25,020 --> 00:34:30,899
the derivatives on the C will flow

955
00:34:27,800 --> 00:34:33,119
equally to the corresponding Ace and

956
00:34:30,899 --> 00:34:35,040
then also to the corresponding base but

957
00:34:33,119 --> 00:34:36,480
then in addition to that the B's are

958
00:34:35,040 --> 00:34:39,419
broadcast so we'll have to do the

959
00:34:36,480 --> 00:34:41,099
additional sum just like we did before

960
00:34:39,419 --> 00:34:43,500
and of course the derivatives for B's

961
00:34:41,099 --> 00:34:46,260
will undergo a minus because the local

962
00:34:43,500 --> 00:34:50,460
derivative here is uh negative one

963
00:34:46,260 --> 00:34:52,379
so DC three two by D B3 is negative one

964
00:34:50,460 --> 00:34:56,099
so let's just Implement that basically

965
00:34:52,379 --> 00:34:58,500
delugits will be uh exactly copying the

966
00:34:56,099 --> 00:34:59,720
derivative on normal objects

967
00:34:58,500 --> 00:35:03,240
so

968
00:34:59,720 --> 00:35:05,220
delugits equals the norm logits and I'll

969
00:35:03,240 --> 00:35:06,720
do a DOT clone for safety so we're just

970
00:35:05,220 --> 00:35:09,599
making a copy

971
00:35:06,720 --> 00:35:13,920
and then we have that the loaded Maxis

972
00:35:09,599 --> 00:35:15,660
will be the negative of the non-legits

973
00:35:13,920 --> 00:35:17,780
because of the negative sign

974
00:35:15,660 --> 00:35:20,880
and then we have to be careful because

975
00:35:17,780 --> 00:35:23,700
logic Maxis is a column

976
00:35:20,880 --> 00:35:26,820
and so just like we saw before because

977
00:35:23,700 --> 00:35:28,980
we keep replicating the same elements

978
00:35:26,820 --> 00:35:31,079
across all the columns

979
00:35:28,980 --> 00:35:33,000
then in the backward pass because we

980
00:35:31,079 --> 00:35:35,220
keep reusing this these are all just

981
00:35:33,000 --> 00:35:37,380
like separate branches of use of that

982
00:35:35,220 --> 00:35:39,780
one variable and so therefore we have to

983
00:35:37,380 --> 00:35:42,060
do a Sum along one would keep them

984
00:35:39,780 --> 00:35:43,380
equals true so that we don't destroy

985
00:35:42,060 --> 00:35:45,180
this dimension

986
00:35:43,380 --> 00:35:47,400
and then the logic Maxes will be the

987
00:35:45,180 --> 00:35:49,260
same shape now we have to be careful

988
00:35:47,400 --> 00:35:52,680
because this deloaches is not the final

989
00:35:49,260 --> 00:35:54,599
deloaches and that's because not only do

990
00:35:52,680 --> 00:35:56,880
we get gradient signal into logits

991
00:35:54,599 --> 00:35:58,920
through here but the logic Maxes as a

992
00:35:56,880 --> 00:36:01,260
function of logits and that's a second

993
00:35:58,920 --> 00:36:03,359
Branch into logits so this is not yet

994
00:36:01,260 --> 00:36:05,579
our final derivative for logits we will

995
00:36:03,359 --> 00:36:07,560
come back later for the second branch

996
00:36:05,579 --> 00:36:10,320
for now the logic Maxis is the final

997
00:36:07,560 --> 00:36:12,359
derivative so let me uncomment this CMP

998
00:36:10,320 --> 00:36:15,839
here and let's just run this

999
00:36:12,359 --> 00:36:16,560
and logit Maxes hit by torch agrees with

1000
00:36:15,839 --> 00:36:19,320
us

1001
00:36:16,560 --> 00:36:21,060
so that was the derivative into through

1002
00:36:19,320 --> 00:36:22,920
this line

1003
00:36:21,060 --> 00:36:24,420
now before we move on I want to pause

1004
00:36:22,920 --> 00:36:26,099
here briefly and I want to look at these

1005
00:36:24,420 --> 00:36:27,359
logic Maxes and especially their

1006
00:36:26,099 --> 00:36:28,800
gradients

1007
00:36:27,359 --> 00:36:31,140
we've talked previously in the previous

1008
00:36:28,800 --> 00:36:33,119
lecture that the only reason we're doing

1009
00:36:31,140 --> 00:36:34,740
this is for the numerical stability of

1010
00:36:33,119 --> 00:36:37,619
the softmax that we are implementing

1011
00:36:34,740 --> 00:36:39,180
here and we talked about how if you take

1012
00:36:37,619 --> 00:36:41,280
these logents for any one of these

1013
00:36:39,180 --> 00:36:44,160
examples so one row of this logit's

1014
00:36:41,280 --> 00:36:47,280
tensor if you add or subtract any value

1015
00:36:44,160 --> 00:36:49,320
equally to all the elements then the

1016
00:36:47,280 --> 00:36:51,300
value of the probes will be unchanged

1017
00:36:49,320 --> 00:36:53,040
you're not changing soft Max the only

1018
00:36:51,300 --> 00:36:55,680
thing that this is doing is it's making

1019
00:36:53,040 --> 00:36:57,359
sure that X doesn't overflow and the

1020
00:36:55,680 --> 00:36:58,920
reason we're using a Max is because then

1021
00:36:57,359 --> 00:37:01,980
we are guaranteed that each row of

1022
00:36:58,920 --> 00:37:03,720
logits the highest number is zero and so

1023
00:37:01,980 --> 00:37:05,280
this will be safe

1024
00:37:03,720 --> 00:37:06,060
and so

1025
00:37:05,280 --> 00:37:09,300
um

1026
00:37:06,060 --> 00:37:11,579
basically what that has repercussions

1027
00:37:09,300 --> 00:37:13,980
if it is the case that changing logit

1028
00:37:11,579 --> 00:37:15,420
Maxis does not change the props and

1029
00:37:13,980 --> 00:37:17,579
therefore there's not change the loss

1030
00:37:15,420 --> 00:37:20,099
then the gradient on logic masses should

1031
00:37:17,579 --> 00:37:21,780
be zero right because saying those two

1032
00:37:20,099 --> 00:37:23,820
things is the same

1033
00:37:21,780 --> 00:37:25,200
so indeed we hope that this is very very

1034
00:37:23,820 --> 00:37:28,320
small numbers so indeed we hope this is

1035
00:37:25,200 --> 00:37:30,119
zero now because of floating Point uh

1036
00:37:28,320 --> 00:37:31,680
sort of wonkiness

1037
00:37:30,119 --> 00:37:33,960
um this doesn't come out exactly zero

1038
00:37:31,680 --> 00:37:35,579
only in some of the rows it does but we

1039
00:37:33,960 --> 00:37:37,619
get extremely small values like one e

1040
00:37:35,579 --> 00:37:39,720
negative nine or ten and so this is

1041
00:37:37,619 --> 00:37:42,060
telling us that the values of loaded

1042
00:37:39,720 --> 00:37:43,200
Maxes are not impacting the loss as they

1043
00:37:42,060 --> 00:37:44,820
shouldn't

1044
00:37:43,200 --> 00:37:48,000
it feels kind of weird to back propagate

1045
00:37:44,820 --> 00:37:50,400
through this branch honestly because

1046
00:37:48,000 --> 00:37:52,200
if you have any implementation of like f

1047
00:37:50,400 --> 00:37:54,119
dot cross entropy and pytorch and you

1048
00:37:52,200 --> 00:37:54,960
you block together all these elements

1049
00:37:54,119 --> 00:37:57,359
and you're not doing the back

1050
00:37:54,960 --> 00:37:59,280
propagation piece by piece then you

1051
00:37:57,359 --> 00:38:01,200
would probably assume that the

1052
00:37:59,280 --> 00:38:03,960
derivative through here is exactly zero

1053
00:38:01,200 --> 00:38:07,500
uh so you would be sort of

1054
00:38:03,960 --> 00:38:09,599
um skipping this branch because it's

1055
00:38:07,500 --> 00:38:10,920
only done for numerical stability but

1056
00:38:09,599 --> 00:38:13,260
it's interesting to see that even if you

1057
00:38:10,920 --> 00:38:14,820
break up everything into the full atoms

1058
00:38:13,260 --> 00:38:16,020
and you still do the computation as

1059
00:38:14,820 --> 00:38:17,940
you'd like with respect to numerical

1060
00:38:16,020 --> 00:38:20,099
stability uh the correct thing happens

1061
00:38:17,940 --> 00:38:21,720
and you still get a very very small

1062
00:38:20,099 --> 00:38:23,400
gradients here

1063
00:38:21,720 --> 00:38:26,099
um basically reflecting the fact that

1064
00:38:23,400 --> 00:38:27,900
the values of these do not matter with

1065
00:38:26,099 --> 00:38:29,220
respect to the final loss

1066
00:38:27,900 --> 00:38:31,619
okay so let's now continue back

1067
00:38:29,220 --> 00:38:33,300
propagation through this line here we've

1068
00:38:31,619 --> 00:38:35,160
just calculated the logit Maxis and now

1069
00:38:33,300 --> 00:38:36,660
we want to back prop into logits through

1070
00:38:35,160 --> 00:38:38,940
this second branch

1071
00:38:36,660 --> 00:38:41,460
now here of course we took legits and we

1072
00:38:38,940 --> 00:38:43,920
took the max along all the rows and then

1073
00:38:41,460 --> 00:38:47,300
we looked at its values here now the way

1074
00:38:43,920 --> 00:38:49,560
this works is that in pytorch

1075
00:38:47,300 --> 00:38:52,079
this thing here

1076
00:38:49,560 --> 00:38:53,400
the max returns both the values and it

1077
00:38:52,079 --> 00:38:55,680
Returns the indices at which those

1078
00:38:53,400 --> 00:38:57,720
values to count the maximum value

1079
00:38:55,680 --> 00:39:00,180
now in the forward pass we only used

1080
00:38:57,720 --> 00:39:01,500
values because that's all we needed but

1081
00:39:00,180 --> 00:39:04,020
in the backward pass it's extremely

1082
00:39:01,500 --> 00:39:06,599
useful to know about where those maximum

1083
00:39:04,020 --> 00:39:08,640
values occurred and we have the indices

1084
00:39:06,599 --> 00:39:10,200
at which they occurred and this will of

1085
00:39:08,640 --> 00:39:12,240
course helps us to help us do the back

1086
00:39:10,200 --> 00:39:14,700
propagation because what should the

1087
00:39:12,240 --> 00:39:16,260
backward pass be here in this case we

1088
00:39:14,700 --> 00:39:18,540
have the largest tensor which is 32 by

1089
00:39:16,260 --> 00:39:20,460
27 and in each row we find the maximum

1090
00:39:18,540 --> 00:39:24,540
value and then that value gets plucked

1091
00:39:20,460 --> 00:39:27,900
out into loaded Maxis and so intuitively

1092
00:39:24,540 --> 00:39:31,099
um basically the derivative flowing

1093
00:39:27,900 --> 00:39:34,020
through here then should be one

1094
00:39:31,099 --> 00:39:35,520
times the look of derivatives is 1 for

1095
00:39:34,020 --> 00:39:36,359
the appropriate entry that was plucked

1096
00:39:35,520 --> 00:39:39,420
out

1097
00:39:36,359 --> 00:39:40,859
and then times the global derivative of

1098
00:39:39,420 --> 00:39:42,720
the logic axis

1099
00:39:40,859 --> 00:39:44,160
so really what we're doing here if you

1100
00:39:42,720 --> 00:39:46,500
think through it is we need to take the

1101
00:39:44,160 --> 00:39:50,099
deloachet Maxis and we need to scatter

1102
00:39:46,500 --> 00:39:52,619
it to the correct positions in these

1103
00:39:50,099 --> 00:39:53,460
logits from where the maximum values

1104
00:39:52,619 --> 00:39:54,960
came

1105
00:39:53,460 --> 00:39:56,160
and so

1106
00:39:54,960 --> 00:39:58,260
um

1107
00:39:56,160 --> 00:39:59,940
I came up with one line of code sort of

1108
00:39:58,260 --> 00:40:02,280
that does that let me just erase a bunch

1109
00:39:59,940 --> 00:40:03,599
of stuff here so the line of uh you

1110
00:40:02,280 --> 00:40:05,460
could do it kind of very similar to what

1111
00:40:03,599 --> 00:40:07,560
we've done here where we create a zeros

1112
00:40:05,460 --> 00:40:10,320
and then we populate uh the correct

1113
00:40:07,560 --> 00:40:13,079
elements uh so we use the indices here

1114
00:40:10,320 --> 00:40:15,359
and we would set them to be one but you

1115
00:40:13,079 --> 00:40:18,480
can also use one hot

1116
00:40:15,359 --> 00:40:21,000
so F dot one hot and then I'm taking the

1117
00:40:18,480 --> 00:40:24,240
lowest of Max over the First Dimension

1118
00:40:21,000 --> 00:40:27,599
dot indices and I'm telling uh pytorch

1119
00:40:24,240 --> 00:40:29,099
that the dimension of every one of these

1120
00:40:27,599 --> 00:40:29,820
tensors should be

1121
00:40:29,099 --> 00:40:33,839
um

1122
00:40:29,820 --> 00:40:37,619
27 and so what this is going to do

1123
00:40:33,839 --> 00:40:39,540
is okay I apologize this is crazy filthy

1124
00:40:37,619 --> 00:40:41,640
that I am sure of this

1125
00:40:39,540 --> 00:40:44,400
it's really just a an array of where the

1126
00:40:41,640 --> 00:40:45,960
Maxes came from in each row and that

1127
00:40:44,400 --> 00:40:47,880
element is one and the all the other

1128
00:40:45,960 --> 00:40:50,640
elements are zero so it's a one-half

1129
00:40:47,880 --> 00:40:53,040
Vector in each row and these indices are

1130
00:40:50,640 --> 00:40:54,480
now populating a single one in the

1131
00:40:53,040 --> 00:40:56,040
proper place

1132
00:40:54,480 --> 00:40:58,619
and then what I'm doing here is I'm

1133
00:40:56,040 --> 00:41:01,200
multiplying by the logit Maxis and keep

1134
00:40:58,619 --> 00:41:05,700
in mind that this is a column

1135
00:41:01,200 --> 00:41:08,460
of 32 by 1. and so when I'm doing this

1136
00:41:05,700 --> 00:41:10,800
times the logic Maxis the logic Maxes

1137
00:41:08,460 --> 00:41:12,119
will broadcast and that column will you

1138
00:41:10,800 --> 00:41:15,000
know get replicated and in an element

1139
00:41:12,119 --> 00:41:17,339
wise multiply will ensure that each of

1140
00:41:15,000 --> 00:41:19,260
these just gets routed to whichever one

1141
00:41:17,339 --> 00:41:21,240
of these bits is turned on

1142
00:41:19,260 --> 00:41:23,460
and so that's another way to implement

1143
00:41:21,240 --> 00:41:26,700
uh this kind of a this kind of a

1144
00:41:23,460 --> 00:41:28,020
operation and both of these can be used

1145
00:41:26,700 --> 00:41:30,359
I just thought I would show an

1146
00:41:28,020 --> 00:41:31,500
equivalent way to do it and I'm using

1147
00:41:30,359 --> 00:41:33,780
plus equals because we already

1148
00:41:31,500 --> 00:41:35,160
calculated the logits here and this is

1149
00:41:33,780 --> 00:41:37,260
not the second branch

1150
00:41:35,160 --> 00:41:39,420
so let's

1151
00:41:37,260 --> 00:41:40,680
look at logits and make sure that this

1152
00:41:39,420 --> 00:41:42,660
is correct

1153
00:41:40,680 --> 00:41:44,520
and we see that we have exactly the

1154
00:41:42,660 --> 00:41:46,980
correct answer

1155
00:41:44,520 --> 00:41:49,140
next up we want to continue with logits

1156
00:41:46,980 --> 00:41:51,540
here that is an outcome of a matrix

1157
00:41:49,140 --> 00:41:53,099
multiplication and a bias offset in this

1158
00:41:51,540 --> 00:41:56,339
linear layer

1159
00:41:53,099 --> 00:41:58,619
so I've printed out the shapes of all

1160
00:41:56,339 --> 00:42:00,960
these intermediate tensors we see that

1161
00:41:58,619 --> 00:42:01,920
logits is of course 32 by 27 as we've

1162
00:42:00,960 --> 00:42:05,820
just seen

1163
00:42:01,920 --> 00:42:08,339
then the H here is 32 by 64. so these

1164
00:42:05,820 --> 00:42:10,740
are 64 dimensional hidden States and

1165
00:42:08,339 --> 00:42:12,839
then this W Matrix projects those 64

1166
00:42:10,740 --> 00:42:15,839
dimensional vectors into 27 dimensions

1167
00:42:12,839 --> 00:42:18,540
and then there's a 27 dimensional offset

1168
00:42:15,839 --> 00:42:20,339
which is a one-dimensional vector

1169
00:42:18,540 --> 00:42:23,220
now we should note that this plus here

1170
00:42:20,339 --> 00:42:27,540
actually broadcasts because H multiplied

1171
00:42:23,220 --> 00:42:31,140
by by W2 will give us a 32 by 27. and so

1172
00:42:27,540 --> 00:42:32,220
then this plus B2 is a 27 dimensional

1173
00:42:31,140 --> 00:42:33,839
lecture here

1174
00:42:32,220 --> 00:42:35,880
now in the rules of broadcasting what's

1175
00:42:33,839 --> 00:42:37,920
going to happen with this bias Vector is

1176
00:42:35,880 --> 00:42:41,160
that this one-dimensional Vector of 27

1177
00:42:37,920 --> 00:42:43,680
will get aligned with a padded dimension

1178
00:42:41,160 --> 00:42:45,839
of one on the left and it will basically

1179
00:42:43,680 --> 00:42:48,839
become a row vector and then it will get

1180
00:42:45,839 --> 00:42:50,640
replicated vertically 32 times to make

1181
00:42:48,839 --> 00:42:52,920
it 32 by 27 and then there's an

1182
00:42:50,640 --> 00:42:54,599
element-wise multiply

1183
00:42:52,920 --> 00:42:56,460
now

1184
00:42:54,599 --> 00:42:59,220
the question is how do we back propagate

1185
00:42:56,460 --> 00:43:02,099
from logits to the hidden States the

1186
00:42:59,220 --> 00:43:03,839
weight Matrix W2 and the bias B2

1187
00:43:02,099 --> 00:43:07,680
and you might think that we need to go

1188
00:43:03,839 --> 00:43:09,720
to some Matrix calculus and then we have

1189
00:43:07,680 --> 00:43:11,640
to look up the derivative for a matrix

1190
00:43:09,720 --> 00:43:12,900
multiplication but actually you don't

1191
00:43:11,640 --> 00:43:14,640
have to do any of that and you can go

1192
00:43:12,900 --> 00:43:17,040
back to First principles and derive this

1193
00:43:14,640 --> 00:43:18,720
yourself on a piece of paper and

1194
00:43:17,040 --> 00:43:20,760
specifically what I like to do and I

1195
00:43:18,720 --> 00:43:23,280
what I find works well for me is you

1196
00:43:20,760 --> 00:43:25,200
find a specific small example that you

1197
00:43:23,280 --> 00:43:27,060
then fully write out and then in the

1198
00:43:25,200 --> 00:43:28,800
process of analyzing how that individual

1199
00:43:27,060 --> 00:43:30,540
small example works you will understand

1200
00:43:28,800 --> 00:43:32,359
the broader pattern and you'll be able

1201
00:43:30,540 --> 00:43:35,520
to generalize and write out the full

1202
00:43:32,359 --> 00:43:37,380
general formula for what how these

1203
00:43:35,520 --> 00:43:39,240
derivatives flow in an expression like

1204
00:43:37,380 --> 00:43:41,280
this so let's try that out

1205
00:43:39,240 --> 00:43:43,560
so pardon the low budget production here

1206
00:43:41,280 --> 00:43:45,300
but what I've done here is I'm writing

1207
00:43:43,560 --> 00:43:46,920
it out on a piece of paper really what

1208
00:43:45,300 --> 00:43:50,700
we are interested in is we have a

1209
00:43:46,920 --> 00:43:53,220
multiply B plus C and that creates a d

1210
00:43:50,700 --> 00:43:54,839
and we have the derivative of the loss

1211
00:43:53,220 --> 00:43:55,980
with respect to D and we'd like to know

1212
00:43:54,839 --> 00:43:57,900
what the derivative of the losses with

1213
00:43:55,980 --> 00:44:00,000
respect to a b and c

1214
00:43:57,900 --> 00:44:01,380
now these here are little

1215
00:44:00,000 --> 00:44:03,960
two-dimensional examples of a matrix

1216
00:44:01,380 --> 00:44:04,859
multiplication Two by Two Times a two by

1217
00:44:03,960 --> 00:44:07,980
two

1218
00:44:04,859 --> 00:44:10,859
plus a 2 a vector of just two elements

1219
00:44:07,980 --> 00:44:14,220
C1 and C2 gives me a two by two

1220
00:44:10,859 --> 00:44:17,040
now notice here that I have a bias

1221
00:44:14,220 --> 00:44:19,319
Vector here called C and the bisex

1222
00:44:17,040 --> 00:44:21,540
vector is C1 and C2 but as I described

1223
00:44:19,319 --> 00:44:23,280
over here that bias Vector will become a

1224
00:44:21,540 --> 00:44:24,960
row Vector in the broadcasting and will

1225
00:44:23,280 --> 00:44:27,300
replicate vertically so that's what's

1226
00:44:24,960 --> 00:44:29,700
happening here as well C1 C2 is

1227
00:44:27,300 --> 00:44:33,180
replicated vertically and we see how we

1228
00:44:29,700 --> 00:44:35,220
have two rows of C1 C2 as a result

1229
00:44:33,180 --> 00:44:37,740
so now when I say write it out I just

1230
00:44:35,220 --> 00:44:40,079
mean like this basically break up this

1231
00:44:37,740 --> 00:44:41,460
matrix multiplication into the actual

1232
00:44:40,079 --> 00:44:44,099
thing that that's going on under the

1233
00:44:41,460 --> 00:44:46,859
hood so as a result of matrix

1234
00:44:44,099 --> 00:44:48,660
multiplication and how it works d11 is

1235
00:44:46,859 --> 00:44:51,000
the result of a DOT product between the

1236
00:44:48,660 --> 00:44:57,720
first row of a and the First Column of B

1237
00:44:51,000 --> 00:44:59,940
so a11 b11 plus a12 B21 plus C1

1238
00:44:57,720 --> 00:45:02,220
and so on so forth for all the other

1239
00:44:59,940 --> 00:45:03,960
elements of D and once you actually

1240
00:45:02,220 --> 00:45:06,119
write it out it becomes obvious this is

1241
00:45:03,960 --> 00:45:09,060
just a bunch of multipliers and

1242
00:45:06,119 --> 00:45:11,700
um adds and we know from micrograd how

1243
00:45:09,060 --> 00:45:13,020
to differentiate multiplies and adds and

1244
00:45:11,700 --> 00:45:15,420
so this is not scary anymore it's not

1245
00:45:13,020 --> 00:45:17,400
just matrix multiplication it's just uh

1246
00:45:15,420 --> 00:45:20,400
tedious unfortunately but this is

1247
00:45:17,400 --> 00:45:23,400
completely tractable we have DL by D for

1248
00:45:20,400 --> 00:45:25,500
all of these and we want DL by uh all

1249
00:45:23,400 --> 00:45:26,700
these little other variables so how do

1250
00:45:25,500 --> 00:45:29,160
we achieve that and how do we actually

1251
00:45:26,700 --> 00:45:30,780
get the gradients okay so the low budget

1252
00:45:29,160 --> 00:45:32,940
production continues here

1253
00:45:30,780 --> 00:45:34,260
so let's for example derive the

1254
00:45:32,940 --> 00:45:36,060
derivative of the loss with respect to

1255
00:45:34,260 --> 00:45:38,880
a11

1256
00:45:36,060 --> 00:45:40,500
we see here that a11 occurs twice in our

1257
00:45:38,880 --> 00:45:43,260
simple expression right here right here

1258
00:45:40,500 --> 00:45:46,140
and influences d11 and D12

1259
00:45:43,260 --> 00:45:51,300
. so this is so what is DL by d a one

1260
00:45:46,140 --> 00:45:53,819
one well it's DL by d11 times the local

1261
00:45:51,300 --> 00:45:55,619
derivative of d11 which in this case is

1262
00:45:53,819 --> 00:45:57,900
just b11 because that's what's

1263
00:45:55,619 --> 00:46:00,480
multiplying a11 here

1264
00:45:57,900 --> 00:46:02,880
so uh and likewise here the local

1265
00:46:00,480 --> 00:46:05,579
derivative of D12 with respect to a11 is

1266
00:46:02,880 --> 00:46:08,760
just B12 and so B12 well in the chain

1267
00:46:05,579 --> 00:46:11,819
rule therefore multiply the L by d 1 2.

1268
00:46:08,760 --> 00:46:15,060
and then because a11 is used both to

1269
00:46:11,819 --> 00:46:18,240
produce d11 and D12 we need to add up

1270
00:46:15,060 --> 00:46:20,099
the contributions of both of those sort

1271
00:46:18,240 --> 00:46:22,740
of chains that are running in parallel

1272
00:46:20,099 --> 00:46:24,240
and that's why we get a plus just adding

1273
00:46:22,740 --> 00:46:26,220
up those two

1274
00:46:24,240 --> 00:46:29,160
um those two contributions and that

1275
00:46:26,220 --> 00:46:31,079
gives us DL by d a one one we can do the

1276
00:46:29,160 --> 00:46:34,260
exact same analysis for the other one

1277
00:46:31,079 --> 00:46:36,060
for all the other elements of a and when

1278
00:46:34,260 --> 00:46:37,260
you simply write it out it's just super

1279
00:46:36,060 --> 00:46:40,140
simple

1280
00:46:37,260 --> 00:46:42,060
um taking of gradients on you know

1281
00:46:40,140 --> 00:46:44,339
expressions like this

1282
00:46:42,060 --> 00:46:47,040
you find that

1283
00:46:44,339 --> 00:46:49,859
this Matrix DL by D A that we're after

1284
00:46:47,040 --> 00:46:52,500
right if we just arrange all the all of

1285
00:46:49,859 --> 00:46:55,200
them in the same shape as a takes so a

1286
00:46:52,500 --> 00:46:59,960
is just too much Matrix so d l by D A

1287
00:46:55,200 --> 00:47:03,300
here will be also just the same shape

1288
00:46:59,960 --> 00:47:05,099
tester with the derivatives now so deal

1289
00:47:03,300 --> 00:47:06,720
by D a11 Etc

1290
00:47:05,099 --> 00:47:09,720
and we see that actually we can express

1291
00:47:06,720 --> 00:47:10,680
what we've written out here as a matrix

1292
00:47:09,720 --> 00:47:13,560
multiplied

1293
00:47:10,680 --> 00:47:15,540
and so it just so happens that D all by

1294
00:47:13,560 --> 00:47:17,880
that all of these formulas that we've

1295
00:47:15,540 --> 00:47:19,140
derived here by taking gradients can

1296
00:47:17,880 --> 00:47:21,300
actually be expressed as a matrix

1297
00:47:19,140 --> 00:47:22,920
multiplication and in particular we see

1298
00:47:21,300 --> 00:47:25,560
that it is the matrix multiplication of

1299
00:47:22,920 --> 00:47:30,180
these two array matrices

1300
00:47:25,560 --> 00:47:32,940
so it is the um DL by D and then Matrix

1301
00:47:30,180 --> 00:47:37,440
multiplying B but B transpose actually

1302
00:47:32,940 --> 00:47:38,579
so you see that B21 and b12 have changed

1303
00:47:37,440 --> 00:47:41,160
place

1304
00:47:38,579 --> 00:47:45,060
whereas before we had of course b11 B12

1305
00:47:41,160 --> 00:47:47,460
B2 on B22 so you see that this other

1306
00:47:45,060 --> 00:47:49,260
Matrix B is transposed

1307
00:47:47,460 --> 00:47:50,640
and so basically what we have long story

1308
00:47:49,260 --> 00:47:52,859
short just by doing very simple

1309
00:47:50,640 --> 00:47:54,359
reasoning here by breaking up the

1310
00:47:52,859 --> 00:47:58,980
expression in the case of a very simple

1311
00:47:54,359 --> 00:48:02,040
example is that DL by d a is which is

1312
00:47:58,980 --> 00:48:05,660
this is simply equal to DL by DD Matrix

1313
00:48:02,040 --> 00:48:05,660
multiplied with B transpose

1314
00:48:05,819 --> 00:48:10,740
so that is what we have so far now we

1315
00:48:08,400 --> 00:48:13,560
also want the derivative with respect to

1316
00:48:10,740 --> 00:48:15,540
um B and C now

1317
00:48:13,560 --> 00:48:18,359
for B I'm not actually doing the full

1318
00:48:15,540 --> 00:48:20,640
derivation because honestly it's um it's

1319
00:48:18,359 --> 00:48:22,440
not deep it's just uh annoying it's

1320
00:48:20,640 --> 00:48:24,599
exhausting you can actually do this

1321
00:48:22,440 --> 00:48:26,220
analysis yourself you'll also find that

1322
00:48:24,599 --> 00:48:27,839
if you take this these expressions and

1323
00:48:26,220 --> 00:48:30,960
you differentiate with respect to b

1324
00:48:27,839 --> 00:48:33,359
instead of a you will find that DL by DB

1325
00:48:30,960 --> 00:48:35,400
is also a matrix multiplication in this

1326
00:48:33,359 --> 00:48:37,680
case you have to take the Matrix a and

1327
00:48:35,400 --> 00:48:39,660
transpose it and Matrix multiply that

1328
00:48:37,680 --> 00:48:42,420
with bl by DD

1329
00:48:39,660 --> 00:48:46,260
and that's what gives you a deal by DB

1330
00:48:42,420 --> 00:48:47,940
and then here for the offsets C1 and C2

1331
00:48:46,260 --> 00:48:50,579
if you again just differentiate with

1332
00:48:47,940 --> 00:48:52,079
respect to C1 you will find an

1333
00:48:50,579 --> 00:48:55,140
expression like this

1334
00:48:52,079 --> 00:48:57,119
and C2 an expression like this

1335
00:48:55,140 --> 00:48:59,339
and basically you'll find the DL by DC

1336
00:48:57,119 --> 00:49:01,440
is simply because they're just

1337
00:48:59,339 --> 00:49:04,859
offsetting these Expressions you just

1338
00:49:01,440 --> 00:49:07,560
have to take the deal by DD Matrix

1339
00:49:04,859 --> 00:49:11,099
of the derivatives of D and you just

1340
00:49:07,560 --> 00:49:13,440
have to sum across the columns and that

1341
00:49:11,099 --> 00:49:15,839
gives you the derivatives for C

1342
00:49:13,440 --> 00:49:18,300
so long story short

1343
00:49:15,839 --> 00:49:20,040
the backward Paths of a matrix multiply

1344
00:49:18,300 --> 00:49:22,260
is a matrix multiply

1345
00:49:20,040 --> 00:49:25,740
and instead of just like we had D equals

1346
00:49:22,260 --> 00:49:27,300
a times B plus C in the scalar case uh

1347
00:49:25,740 --> 00:49:29,400
we sort of like arrive at something very

1348
00:49:27,300 --> 00:49:31,260
very similar but now uh with a matrix

1349
00:49:29,400 --> 00:49:32,579
multiplication instead of a scalar

1350
00:49:31,260 --> 00:49:36,060
multiplication

1351
00:49:32,579 --> 00:49:37,819
so the derivative of D with respect to a

1352
00:49:36,060 --> 00:49:41,220
is

1353
00:49:37,819 --> 00:49:44,460
DL by DD Matrix multiplied B trespose

1354
00:49:41,220 --> 00:49:46,440
and here it's a transpose multiply deal

1355
00:49:44,460 --> 00:49:49,560
by DD but in both cases it's a matrix

1356
00:49:46,440 --> 00:49:53,160
multiplication with the derivative and

1357
00:49:49,560 --> 00:49:55,859
the other term in the multiplication

1358
00:49:53,160 --> 00:49:58,859
and for C it is a sum

1359
00:49:55,859 --> 00:50:00,240
now I'll tell you a secret I can never

1360
00:49:58,859 --> 00:50:01,619
remember the formulas that we just

1361
00:50:00,240 --> 00:50:03,839
arrived for back proper gain information

1362
00:50:01,619 --> 00:50:05,819
multiplication and I can back propagate

1363
00:50:03,839 --> 00:50:07,440
through these Expressions just fine and

1364
00:50:05,819 --> 00:50:09,000
the reason this works is because the

1365
00:50:07,440 --> 00:50:11,640
dimensions have to work out

1366
00:50:09,000 --> 00:50:13,680
uh so let me give you an example say I

1367
00:50:11,640 --> 00:50:16,980
want to create DH

1368
00:50:13,680 --> 00:50:19,200
then what should the H be number one I

1369
00:50:16,980 --> 00:50:21,359
have to know that the shape of DH must

1370
00:50:19,200 --> 00:50:24,300
be the same as the shape of H

1371
00:50:21,359 --> 00:50:26,280
and the shape of H is 32 by 64. and then

1372
00:50:24,300 --> 00:50:28,800
the other piece of information I know is

1373
00:50:26,280 --> 00:50:32,520
that DH must be some kind of matrix

1374
00:50:28,800 --> 00:50:37,619
multiplication of the logits with W2

1375
00:50:32,520 --> 00:50:40,380
and delojits is 32 by 27 and W2 is a 64

1376
00:50:37,619 --> 00:50:43,980
by 27. there is only a single way to

1377
00:50:40,380 --> 00:50:45,960
make the shape work out in this case and

1378
00:50:43,980 --> 00:50:48,839
it is indeed the correct result in

1379
00:50:45,960 --> 00:50:50,880
particular here H needs to be 32 by 64.

1380
00:50:48,839 --> 00:50:52,980
the only way to achieve that is to take

1381
00:50:50,880 --> 00:50:55,920
a deluges

1382
00:50:52,980 --> 00:50:57,480
and Matrix multiply it with you see how

1383
00:50:55,920 --> 00:50:58,920
I have to take W2 but I have to

1384
00:50:57,480 --> 00:50:59,720
transpose it to make the dimensions work

1385
00:50:58,920 --> 00:51:02,819
out

1386
00:50:59,720 --> 00:51:04,740
so w to transpose and it's the only way

1387
00:51:02,819 --> 00:51:06,660
to make these to Matrix multiply those

1388
00:51:04,740 --> 00:51:08,099
two pieces to make the shapes work out

1389
00:51:06,660 --> 00:51:11,819
and that turns out to be the correct

1390
00:51:08,099 --> 00:51:15,359
formula so if we come here we want DH

1391
00:51:11,819 --> 00:51:18,000
which is d a and we see that d a is DL

1392
00:51:15,359 --> 00:51:21,359
by DD Matrix multiply B transpose

1393
00:51:18,000 --> 00:51:24,000
so that's Delo just multiply and B is W2

1394
00:51:21,359 --> 00:51:26,280
so W2 transpose which is exactly what we

1395
00:51:24,000 --> 00:51:30,079
have here so there's no need to remember

1396
00:51:26,280 --> 00:51:33,480
these formulas similarly now if I want

1397
00:51:30,079 --> 00:51:37,440
dw2 well I know that it must be a matrix

1398
00:51:33,480 --> 00:51:39,300
multiplication of D logits and H

1399
00:51:37,440 --> 00:51:40,680
and maybe there's a few transpose like

1400
00:51:39,300 --> 00:51:42,359
there's one transpose in there as well

1401
00:51:40,680 --> 00:51:44,819
and I don't know which way it is so I

1402
00:51:42,359 --> 00:51:47,280
have to come to W2 and I see that its

1403
00:51:44,819 --> 00:51:49,140
shape is 64 by 27

1404
00:51:47,280 --> 00:51:51,119
and that has to come from some interest

1405
00:51:49,140 --> 00:51:55,859
multiplication of these two

1406
00:51:51,119 --> 00:51:56,819
and so to get a 64 by 27 I need to take

1407
00:51:55,859 --> 00:51:59,280
um

1408
00:51:56,819 --> 00:52:01,740
H I need to transpose it

1409
00:51:59,280 --> 00:52:04,140
and then I need to Matrix multiply it

1410
00:52:01,740 --> 00:52:05,280
um so that will become 64 by 32 and then

1411
00:52:04,140 --> 00:52:07,559
I need to make sure to multiply with the

1412
00:52:05,280 --> 00:52:09,720
32 by 27 and that's going to give me a

1413
00:52:07,559 --> 00:52:11,280
64 by 27. so I need to make sure it's

1414
00:52:09,720 --> 00:52:13,260
multiplied this with the logist that

1415
00:52:11,280 --> 00:52:15,660
shape just like that that's the only way

1416
00:52:13,260 --> 00:52:17,940
to make the dimensions work out and just

1417
00:52:15,660 --> 00:52:19,980
use matrix multiplication and if we come

1418
00:52:17,940 --> 00:52:23,000
here we see that that's exactly what's

1419
00:52:19,980 --> 00:52:25,980
here so a transpose a for us is H

1420
00:52:23,000 --> 00:52:30,359
multiplied with deloaches

1421
00:52:25,980 --> 00:52:33,480
so that's W2 and then db2

1422
00:52:30,359 --> 00:52:35,880
is just the um

1423
00:52:33,480 --> 00:52:37,319
vertical sum and actually in the same

1424
00:52:35,880 --> 00:52:38,940
way there's only one way to make the

1425
00:52:37,319 --> 00:52:40,980
shapes work out I don't have to remember

1426
00:52:38,940 --> 00:52:42,720
that it's a vertical Sum along the zero

1427
00:52:40,980 --> 00:52:45,300
axis because that's the only way that

1428
00:52:42,720 --> 00:52:50,460
this makes sense because B2 shape is 27

1429
00:52:45,300 --> 00:52:54,480
so in order to get a um delugits

1430
00:52:50,460 --> 00:52:56,220
here is 30 by 27 so knowing that it's

1431
00:52:54,480 --> 00:52:58,460
just sum over deloaches in some

1432
00:52:56,220 --> 00:52:58,460
Direction

1433
00:52:59,819 --> 00:53:04,680
that direction must be zero because I

1434
00:53:02,099 --> 00:53:06,000
need to eliminate this Dimension so it's

1435
00:53:04,680 --> 00:53:08,520
this

1436
00:53:06,000 --> 00:53:10,559
so this is so let's kind of like the

1437
00:53:08,520 --> 00:53:13,619
hacky way let me copy paste and delete

1438
00:53:10,559 --> 00:53:14,940
that and let me swing over here and this

1439
00:53:13,619 --> 00:53:17,220
is our backward pass for the linear

1440
00:53:14,940 --> 00:53:19,380
layer uh hopefully

1441
00:53:17,220 --> 00:53:21,740
so now let's uncomment

1442
00:53:19,380 --> 00:53:24,300
these three and we're checking that we

1443
00:53:21,740 --> 00:53:26,880
got all the three derivatives correct

1444
00:53:24,300 --> 00:53:30,359
and run

1445
00:53:26,880 --> 00:53:33,000
and we see that h wh and B2 are all

1446
00:53:30,359 --> 00:53:36,200
exactly correct so we back propagated

1447
00:53:33,000 --> 00:53:36,200
through a linear layer

1448
00:53:36,260 --> 00:53:41,460
now next up we have derivative for the h

1449
00:53:39,359 --> 00:53:43,859
already and we need to back propagate

1450
00:53:41,460 --> 00:53:47,160
through 10h into h preact

1451
00:53:43,859 --> 00:53:48,540
so we want to derive DH preact

1452
00:53:47,160 --> 00:53:50,280
and here we have to back propagate

1453
00:53:48,540 --> 00:53:52,260
through a 10 H and we've already done

1454
00:53:50,280 --> 00:53:54,240
this in micrograd and we remember that

1455
00:53:52,260 --> 00:53:56,700
10h has a very simple backward formula

1456
00:53:54,240 --> 00:53:59,099
now unfortunately if I just put in D by

1457
00:53:56,700 --> 00:54:00,900
DX of 10 h of X into both from alpha it

1458
00:53:59,099 --> 00:54:03,300
lets us down it tells us that it's a

1459
00:54:00,900 --> 00:54:06,420
hyperbolic secant function squared of X

1460
00:54:03,300 --> 00:54:08,280
it's not exactly helpful but luckily

1461
00:54:06,420 --> 00:54:10,740
Google image search does not let us down

1462
00:54:08,280 --> 00:54:12,420
and it gives us the simpler formula and

1463
00:54:10,740 --> 00:54:16,020
in particular if you have that a is

1464
00:54:12,420 --> 00:54:17,940
equal to 10 h of Z then d a by DZ by

1465
00:54:16,020 --> 00:54:21,119
propagating through 10 H is just one

1466
00:54:17,940 --> 00:54:23,640
minus a square and take note that 1

1467
00:54:21,119 --> 00:54:27,059
minus a square a here is the output of

1468
00:54:23,640 --> 00:54:29,819
the 10h not the input to the 10h Z so

1469
00:54:27,059 --> 00:54:31,980
the D A by DZ is here formulated in

1470
00:54:29,819 --> 00:54:34,020
terms of the output of that 10h

1471
00:54:31,980 --> 00:54:35,940
and here also in Google image search we

1472
00:54:34,020 --> 00:54:38,099
have the full derivation if you want to

1473
00:54:35,940 --> 00:54:39,960
actually take the actual definition of

1474
00:54:38,099 --> 00:54:42,540
10h and work through the math to figure

1475
00:54:39,960 --> 00:54:45,300
out 1 minus standard square of Z

1476
00:54:42,540 --> 00:54:49,200
so 1 minus a square is the local

1477
00:54:45,300 --> 00:54:52,319
derivative in our case that is 1 minus

1478
00:54:49,200 --> 00:54:53,579
uh the output of 10 H squared which here

1479
00:54:52,319 --> 00:54:56,220
is H

1480
00:54:53,579 --> 00:54:58,980
so it's h squared and that is the local

1481
00:54:56,220 --> 00:55:00,780
derivative and then times the chain rule

1482
00:54:58,980 --> 00:55:02,460
DH

1483
00:55:00,780 --> 00:55:05,460
so that is going to be our candidate

1484
00:55:02,460 --> 00:55:08,700
implementation so if we come here

1485
00:55:05,460 --> 00:55:09,780
and then uncomment this let's hope for

1486
00:55:08,700 --> 00:55:12,359
the best

1487
00:55:09,780 --> 00:55:15,119
and we have the right answer

1488
00:55:12,359 --> 00:55:17,099
okay next up we have DH preact and we

1489
00:55:15,119 --> 00:55:19,140
want to back propagate into the gain the

1490
00:55:17,099 --> 00:55:21,660
B and raw and the B and bias

1491
00:55:19,140 --> 00:55:23,280
so here this is the bathroom parameters

1492
00:55:21,660 --> 00:55:25,619
being gained in bias inside the bash

1493
00:55:23,280 --> 00:55:28,200
term that take the B and raw that is

1494
00:55:25,619 --> 00:55:29,400
exact unit caution and then scale it and

1495
00:55:28,200 --> 00:55:30,960
shift it

1496
00:55:29,400 --> 00:55:33,780
and these are the parameters of The

1497
00:55:30,960 --> 00:55:35,520
Bachelor now here we have a

1498
00:55:33,780 --> 00:55:36,720
multiplication but it's worth noting

1499
00:55:35,520 --> 00:55:38,700
that this multiply is very very

1500
00:55:36,720 --> 00:55:41,460
different from this Matrix multiply here

1501
00:55:38,700 --> 00:55:43,260
Matrix multiply are DOT products between

1502
00:55:41,460 --> 00:55:45,480
rows and Columns of these matrices

1503
00:55:43,260 --> 00:55:46,920
involved this is an element twice

1504
00:55:45,480 --> 00:55:47,940
multiply so things are quite a bit

1505
00:55:46,920 --> 00:55:49,559
simpler

1506
00:55:47,940 --> 00:55:51,240
now we do have to be careful with some

1507
00:55:49,559 --> 00:55:53,640
of the broadcasting happening in this

1508
00:55:51,240 --> 00:55:58,200
line of code though so you see how BN

1509
00:55:53,640 --> 00:56:02,339
gain and B and bias are 1 by 64. but H

1510
00:55:58,200 --> 00:56:04,020
preact and B and raw are 32 by 64.

1511
00:56:02,339 --> 00:56:05,520
so we have to be careful with that and

1512
00:56:04,020 --> 00:56:06,900
make sure that all the shapes work out

1513
00:56:05,520 --> 00:56:08,520
fine and that the broadcasting is

1514
00:56:06,900 --> 00:56:10,260
correctly back propagated

1515
00:56:08,520 --> 00:56:14,940
so in particular let's start with the B

1516
00:56:10,260 --> 00:56:17,040
and Gain so DB and gain should be

1517
00:56:14,940 --> 00:56:19,200
and here this is again elementorized

1518
00:56:17,040 --> 00:56:21,359
multiply and whenever we have a times b

1519
00:56:19,200 --> 00:56:23,520
equals c we saw that the local

1520
00:56:21,359 --> 00:56:25,859
derivative here is just if this is a the

1521
00:56:23,520 --> 00:56:27,900
local derivative is just the B the other

1522
00:56:25,859 --> 00:56:31,559
one so the local derivative is just B

1523
00:56:27,900 --> 00:56:34,140
and raw and then times chain rule

1524
00:56:31,559 --> 00:56:38,880
so DH preact

1525
00:56:34,140 --> 00:56:40,680
so this is the candidate gradient now

1526
00:56:38,880 --> 00:56:44,700
again we have to be careful because B

1527
00:56:40,680 --> 00:56:48,300
and Gain Is of size 1 by 64. but this

1528
00:56:44,700 --> 00:56:49,619
here would be 32 by 64.

1529
00:56:48,300 --> 00:56:51,300
and so

1530
00:56:49,619 --> 00:56:53,819
um the correct thing to do in this case

1531
00:56:51,300 --> 00:56:55,740
of course is that b and gain here is a

1532
00:56:53,819 --> 00:56:58,040
rule Vector of 64 numbers it gets

1533
00:56:55,740 --> 00:57:00,359
replicated vertically in this operation

1534
00:56:58,040 --> 00:57:03,500
and so therefore the correct thing to do

1535
00:57:00,359 --> 00:57:06,059
is to sum because it's being replicated

1536
00:57:03,500 --> 00:57:07,980
and therefore all the gradients in each

1537
00:57:06,059 --> 00:57:10,260
of the rows that are now flowing

1538
00:57:07,980 --> 00:57:13,260
backwards need to sum up to that same

1539
00:57:10,260 --> 00:57:16,859
tensor DB and Gain so we have to sum

1540
00:57:13,260 --> 00:57:17,819
across all the zero all the examples

1541
00:57:16,859 --> 00:57:19,200
basically

1542
00:57:17,819 --> 00:57:20,400
which is the direction in which this

1543
00:57:19,200 --> 00:57:21,780
gets replicated

1544
00:57:20,400 --> 00:57:23,700
and now we have to be also careful

1545
00:57:21,780 --> 00:57:26,640
because we

1546
00:57:23,700 --> 00:57:29,040
um being gain is of shape 1 by 64. so in

1547
00:57:26,640 --> 00:57:31,980
fact I need to keep them as true

1548
00:57:29,040 --> 00:57:34,380
otherwise I would just get 64.

1549
00:57:31,980 --> 00:57:36,839
now I don't actually really remember why

1550
00:57:34,380 --> 00:57:40,020
the being gain and the BN bias I made

1551
00:57:36,839 --> 00:57:41,280
them be 1 by 64.

1552
00:57:40,020 --> 00:57:44,040
um

1553
00:57:41,280 --> 00:57:45,780
but the biases B1 and B2 I just made

1554
00:57:44,040 --> 00:57:47,700
them be one-dimensional vectors they're

1555
00:57:45,780 --> 00:57:51,300
not two-dimensional tensors so I can't

1556
00:57:47,700 --> 00:57:53,280
recall exactly why I left the gain and

1557
00:57:51,300 --> 00:57:54,660
the bias as two-dimensional but it

1558
00:57:53,280 --> 00:57:55,920
doesn't really matter as long as you are

1559
00:57:54,660 --> 00:57:56,579
consistent and you're keeping it the

1560
00:57:55,920 --> 00:57:58,020
same

1561
00:57:56,579 --> 00:58:01,260
so in this case we want to keep the

1562
00:57:58,020 --> 00:58:05,760
dimension so that the tensor shapes work

1563
00:58:01,260 --> 00:58:09,660
next up we have B and raw so DB and raw

1564
00:58:05,760 --> 00:58:11,540
will be BN gain

1565
00:58:09,660 --> 00:58:15,480
multiplying

1566
00:58:11,540 --> 00:58:17,220
dhreact that's our chain rule now what

1567
00:58:15,480 --> 00:58:18,000
about the

1568
00:58:17,220 --> 00:58:20,160
um

1569
00:58:18,000 --> 00:58:24,480
dimensions of this we have to be careful

1570
00:58:20,160 --> 00:58:27,300
right so DH preact is 32 by 64. B and

1571
00:58:24,480 --> 00:58:29,599
gain is 1 by 64. so it will just get

1572
00:58:27,300 --> 00:58:31,859
replicated and to create this

1573
00:58:29,599 --> 00:58:33,780
multiplication which is the correct

1574
00:58:31,859 --> 00:58:35,819
thing because in a forward pass it also

1575
00:58:33,780 --> 00:58:37,319
gets replicated in just the same way

1576
00:58:35,819 --> 00:58:38,460
so in fact we don't need the brackets

1577
00:58:37,319 --> 00:58:40,920
here we're done

1578
00:58:38,460 --> 00:58:43,380
and the shapes are already correct

1579
00:58:40,920 --> 00:58:46,079
and finally for the bias

1580
00:58:43,380 --> 00:58:47,520
very similar this bias here is very very

1581
00:58:46,079 --> 00:58:49,440
similar to the bias we saw when you

1582
00:58:47,520 --> 00:58:51,960
layer in the linear layer and we see

1583
00:58:49,440 --> 00:58:54,059
that the gradients from each preact will

1584
00:58:51,960 --> 00:58:55,920
simply flow into the biases and add up

1585
00:58:54,059 --> 00:58:57,000
because these are just these are just

1586
00:58:55,920 --> 00:58:59,220
offsets

1587
00:58:57,000 --> 00:59:01,859
and so basically we want this to be DH

1588
00:58:59,220 --> 00:59:04,380
preact but it needs to Sum along the

1589
00:59:01,859 --> 00:59:06,240
right Dimension and in this case similar

1590
00:59:04,380 --> 00:59:09,000
to the gain we need to sum across the

1591
00:59:06,240 --> 00:59:10,440
zeroth dimension the examples because of

1592
00:59:09,000 --> 00:59:11,700
the way that the bias gets replicated

1593
00:59:10,440 --> 00:59:14,460
vertically

1594
00:59:11,700 --> 00:59:15,359
and we also want to have keep them as

1595
00:59:14,460 --> 00:59:17,640
true

1596
00:59:15,359 --> 00:59:20,640
and so this will basically take this and

1597
00:59:17,640 --> 00:59:23,040
sum it up and give us a 1 by 64.

1598
00:59:20,640 --> 00:59:25,319
so this is the candidate implementation

1599
00:59:23,040 --> 00:59:28,980
it makes all the shapes work

1600
00:59:25,319 --> 00:59:32,040
let me bring it up down here and then

1601
00:59:28,980 --> 00:59:33,540
let me uncomment these three lines

1602
00:59:32,040 --> 00:59:36,420
to check that we are getting the correct

1603
00:59:33,540 --> 00:59:38,339
result for all the three tensors and

1604
00:59:36,420 --> 00:59:40,559
indeed we see that all of that got back

1605
00:59:38,339 --> 00:59:42,960
propagated correctly so now we get to

1606
00:59:40,559 --> 00:59:44,640
the batch Norm layer we see how here

1607
00:59:42,960 --> 00:59:46,619
being gay and being bias are the

1608
00:59:44,640 --> 00:59:50,160
parameters so the back propagation ends

1609
00:59:46,619 --> 00:59:51,780
but B and raw now is the output of the

1610
00:59:50,160 --> 00:59:53,460
standardization

1611
00:59:51,780 --> 00:59:54,540
so here what I'm doing of course is I'm

1612
00:59:53,460 --> 00:59:55,980
breaking up the batch form into

1613
00:59:54,540 --> 00:59:57,660
manageable pieces so we can back

1614
00:59:55,980 --> 01:00:00,540
propagate through each line individually

1615
00:59:57,660 --> 01:00:03,720
but basically what's happening is BN

1616
01:00:00,540 --> 01:00:06,960
mean I is the sum

1617
01:00:03,720 --> 01:00:10,140
so this is the B and mean I I apologize

1618
01:00:06,960 --> 01:00:11,460
for the variable naming B and diff is x

1619
01:00:10,140 --> 01:00:15,000
minus mu

1620
01:00:11,460 --> 01:00:16,619
B and div 2 is x minus mu squared here

1621
01:00:15,000 --> 01:00:20,520
inside the variance

1622
01:00:16,619 --> 01:00:22,799
B and VAR is the variance so uh Sigma

1623
01:00:20,520 --> 01:00:25,260
Square this is B and bar and it's

1624
01:00:22,799 --> 01:00:28,559
basically the sum of squares

1625
01:00:25,260 --> 01:00:30,720
so this is the x minus mu squared and

1626
01:00:28,559 --> 01:00:32,099
then the sum now you'll notice one

1627
01:00:30,720 --> 01:00:34,980
departure here

1628
01:00:32,099 --> 01:00:37,859
here it is normalized as 1 over m

1629
01:00:34,980 --> 01:00:39,839
uh which is number of examples here I'm

1630
01:00:37,859 --> 01:00:42,000
normalizing as one over n minus 1

1631
01:00:39,839 --> 01:00:43,619
instead of N and this is deliberate and

1632
01:00:42,000 --> 01:00:45,599
I'll come back to that in a bit when we

1633
01:00:43,619 --> 01:00:47,819
are at this line it is something called

1634
01:00:45,599 --> 01:00:51,079
the bezels correction

1635
01:00:47,819 --> 01:00:53,640
but this is how I want it in our case

1636
01:00:51,079 --> 01:00:56,700
bienvar inv then becomes basically

1637
01:00:53,640 --> 01:00:58,799
bienvar plus Epsilon Epsilon is one

1638
01:00:56,700 --> 01:00:59,940
negative five and then it's one over

1639
01:00:58,799 --> 01:01:02,400
square root

1640
01:00:59,940 --> 01:01:05,640
is the same as raising to the power of

1641
01:01:02,400 --> 01:01:07,799
negative 0.5 right because 0.5 is square

1642
01:01:05,640 --> 01:01:08,880
root and then negative makes it one over

1643
01:01:07,799 --> 01:01:12,240
square root

1644
01:01:08,880 --> 01:01:14,339
so BM Bar M is a one over this uh

1645
01:01:12,240 --> 01:01:16,500
denominator here and then we can see

1646
01:01:14,339 --> 01:01:19,700
that b and raw which is the X hat here

1647
01:01:16,500 --> 01:01:22,260
is equal to the BN diff the numerator

1648
01:01:19,700 --> 01:01:24,780
multiplied by the

1649
01:01:22,260 --> 01:01:27,299
um BN bar in

1650
01:01:24,780 --> 01:01:29,040
and this line here that creates pre-h

1651
01:01:27,299 --> 01:01:31,559
pre-act was the last piece we've already

1652
01:01:29,040 --> 01:01:34,020
back propagated through it

1653
01:01:31,559 --> 01:01:35,880
so now what we want to do is we are here

1654
01:01:34,020 --> 01:01:38,819
and we have B and raw and we have to

1655
01:01:35,880 --> 01:01:40,680
first back propagate into B and diff and

1656
01:01:38,819 --> 01:01:43,500
B and Bar M

1657
01:01:40,680 --> 01:01:45,240
so now we're here and we have DB and raw

1658
01:01:43,500 --> 01:01:46,339
and we need to back propagate through

1659
01:01:45,240 --> 01:01:49,260
this line

1660
01:01:46,339 --> 01:01:53,220
now I've written out the shapes here and

1661
01:01:49,260 --> 01:01:55,440
indeed bien VAR m is a shape 1 by 64. so

1662
01:01:53,220 --> 01:01:57,540
there is a broadcasting happening here

1663
01:01:55,440 --> 01:01:58,799
that we have to be careful with but it

1664
01:01:57,540 --> 01:02:00,420
is just an element-wise simple

1665
01:01:58,799 --> 01:02:02,579
multiplication by now we should be

1666
01:02:00,420 --> 01:02:05,460
pretty comfortable with that to get DB

1667
01:02:02,579 --> 01:02:06,540
and diff we know that this is just B and

1668
01:02:05,460 --> 01:02:08,160
varm

1669
01:02:06,540 --> 01:02:11,299
multiplied with

1670
01:02:08,160 --> 01:02:11,299
DP and raw

1671
01:02:11,579 --> 01:02:17,819
and conversely to get dbmring

1672
01:02:15,000 --> 01:02:21,799
we need to take the end if

1673
01:02:17,819 --> 01:02:21,799
and multiply that by DB and raw

1674
01:02:22,380 --> 01:02:26,520
so this is the candidate but of course

1675
01:02:24,660 --> 01:02:29,760
we need to make sure that broadcasting

1676
01:02:26,520 --> 01:02:31,740
is obeyed so in particular B and VAR M

1677
01:02:29,760 --> 01:02:35,640
multiplying with DB and raw

1678
01:02:31,740 --> 01:02:36,780
will be okay and give us 32 by 64 as we

1679
01:02:35,640 --> 01:02:40,680
expect

1680
01:02:36,780 --> 01:02:42,359
but dbm VAR inv would be taking a 32 by

1681
01:02:40,680 --> 01:02:45,420
64.

1682
01:02:42,359 --> 01:02:49,680
multiplying it by 32 by 64. so this is a

1683
01:02:45,420 --> 01:02:52,740
32 by 64. but of course DB this uh B and

1684
01:02:49,680 --> 01:02:55,619
VAR in is only 1 by 64. so the second

1685
01:02:52,740 --> 01:02:57,839
line here needs a sum across the

1686
01:02:55,619 --> 01:03:00,000
examples and because there's this

1687
01:02:57,839 --> 01:03:02,160
Dimension here we need to make sure that

1688
01:03:00,000 --> 01:03:04,920
keep them is true

1689
01:03:02,160 --> 01:03:07,500
so this is the candidate

1690
01:03:04,920 --> 01:03:09,059
let's erase this and let's swing down

1691
01:03:07,500 --> 01:03:11,579
here

1692
01:03:09,059 --> 01:03:16,200
and implement it and then let's comment

1693
01:03:11,579 --> 01:03:18,839
out dbm barif and DB and diff

1694
01:03:16,200 --> 01:03:22,200
now we'll actually notice that DB and

1695
01:03:18,839 --> 01:03:24,380
diff by the way is going to be incorrect

1696
01:03:22,200 --> 01:03:27,839
so when I run this

1697
01:03:24,380 --> 01:03:30,119
BMR m is correct B and diff is not

1698
01:03:27,839 --> 01:03:34,140
correct and this is actually expected

1699
01:03:30,119 --> 01:03:36,000
because we're not done with b and diff

1700
01:03:34,140 --> 01:03:37,740
so in particular when we slide here we

1701
01:03:36,000 --> 01:03:40,980
see here that b and raw as a function of

1702
01:03:37,740 --> 01:03:42,540
B and diff but actually B and far of is

1703
01:03:40,980 --> 01:03:44,520
a function of B of R which is a function

1704
01:03:42,540 --> 01:03:45,359
of B and df2 which is a function of B

1705
01:03:44,520 --> 01:03:48,780
and diff

1706
01:03:45,359 --> 01:03:50,760
so it comes here so bdn diff

1707
01:03:48,780 --> 01:03:53,400
um these variable names are crazy I'm

1708
01:03:50,760 --> 01:03:55,680
sorry it branches out into two branches

1709
01:03:53,400 --> 01:03:57,000
and we've only done one branch of it we

1710
01:03:55,680 --> 01:03:58,559
have to continue our back propagation

1711
01:03:57,000 --> 01:04:00,119
and eventually come back to B and diff

1712
01:03:58,559 --> 01:04:02,579
and then we'll be able to do a plus

1713
01:04:00,119 --> 01:04:05,160
equals and get the actual card gradient

1714
01:04:02,579 --> 01:04:07,260
for now it is good to verify that CMP

1715
01:04:05,160 --> 01:04:08,579
also works it doesn't just lie to us and

1716
01:04:07,260 --> 01:04:11,400
tell us that everything is always

1717
01:04:08,579 --> 01:04:13,859
correct it can in fact detect when your

1718
01:04:11,400 --> 01:04:15,780
gradient is not correct so it's that's

1719
01:04:13,859 --> 01:04:17,339
good to see as well okay so now we have

1720
01:04:15,780 --> 01:04:18,960
the derivative here and we're trying to

1721
01:04:17,339 --> 01:04:21,180
back propagate through this line

1722
01:04:18,960 --> 01:04:23,400
and because we're raising to a power of

1723
01:04:21,180 --> 01:04:25,260
negative 0.5 I brought up the power rule

1724
01:04:23,400 --> 01:04:28,319
and we see that basically we have that

1725
01:04:25,260 --> 01:04:31,559
the BM bar will now be we bring down the

1726
01:04:28,319 --> 01:04:34,799
exponent so negative 0.5 times

1727
01:04:31,559 --> 01:04:36,660
uh X which is this

1728
01:04:34,799 --> 01:04:39,660
and now raised to the power of negative

1729
01:04:36,660 --> 01:04:42,599
0.5 minus 1 which is negative 1.5

1730
01:04:39,660 --> 01:04:45,240
now we would have to also apply a small

1731
01:04:42,599 --> 01:04:48,000
chain rule here in our head because we

1732
01:04:45,240 --> 01:04:49,799
need to take further the derivative of B

1733
01:04:48,000 --> 01:04:51,900
and VAR with respect to this expression

1734
01:04:49,799 --> 01:04:53,460
here inside the bracket but because this

1735
01:04:51,900 --> 01:04:54,960
is an elementalized operation and

1736
01:04:53,460 --> 01:04:57,720
everything is fairly simple that's just

1737
01:04:54,960 --> 01:05:00,299
one and so there's nothing to do there

1738
01:04:57,720 --> 01:05:01,920
so this is the local derivative and then

1739
01:05:00,299 --> 01:05:04,319
times the global derivative to create

1740
01:05:01,920 --> 01:05:05,520
the chain rule this is just times the BM

1741
01:05:04,319 --> 01:05:08,400
bar have

1742
01:05:05,520 --> 01:05:10,440
so this is our candidate let me bring

1743
01:05:08,400 --> 01:05:14,000
this down

1744
01:05:10,440 --> 01:05:14,000
and uncommon to the check

1745
01:05:14,460 --> 01:05:17,400
and we see that we have the correct

1746
01:05:16,260 --> 01:05:19,140
result

1747
01:05:17,400 --> 01:05:20,880
now before we propagate through the next

1748
01:05:19,140 --> 01:05:22,319
line I want to briefly talk about the

1749
01:05:20,880 --> 01:05:24,599
note here where I'm using the bezels

1750
01:05:22,319 --> 01:05:27,180
correction dividing by n minus 1 instead

1751
01:05:24,599 --> 01:05:29,700
of dividing by n when I normalize here

1752
01:05:27,180 --> 01:05:31,319
the sum of squares

1753
01:05:29,700 --> 01:05:33,480
now you'll notice that this is departure

1754
01:05:31,319 --> 01:05:36,599
from the paper which uses one over n

1755
01:05:33,480 --> 01:05:38,099
instead not one over n minus one their m

1756
01:05:36,599 --> 01:05:39,119
is RN

1757
01:05:38,099 --> 01:05:40,619
and

1758
01:05:39,119 --> 01:05:43,380
um so it turns out that there are two

1759
01:05:40,619 --> 01:05:46,799
ways of estimating variance of an array

1760
01:05:43,380 --> 01:05:49,020
one is the biased estimate which is one

1761
01:05:46,799 --> 01:05:51,299
over n and the other one is the unbiased

1762
01:05:49,020 --> 01:05:54,059
estimate which is one over n minus one

1763
01:05:51,299 --> 01:05:56,400
now confusingly in the paper this is uh

1764
01:05:54,059 --> 01:05:58,799
not very clearly described and also it's

1765
01:05:56,400 --> 01:06:00,420
a detail that kind of matters I think

1766
01:05:58,799 --> 01:06:02,640
um they are using the biased version

1767
01:06:00,420 --> 01:06:04,740
training time but later when they are

1768
01:06:02,640 --> 01:06:06,000
talking about the inference they are

1769
01:06:04,740 --> 01:06:08,520
mentioning that when they do the

1770
01:06:06,000 --> 01:06:10,500
inference they are using the unbiased

1771
01:06:08,520 --> 01:06:12,420
estimate which is the n minus one

1772
01:06:10,500 --> 01:06:12,960
version in

1773
01:06:12,420 --> 01:06:15,540
um

1774
01:06:12,960 --> 01:06:18,359
basically for inference

1775
01:06:15,540 --> 01:06:20,700
and to calibrate the running mean and

1776
01:06:18,359 --> 01:06:22,559
the running variance basically and so

1777
01:06:20,700 --> 01:06:24,480
they they actually introduce a trained

1778
01:06:22,559 --> 01:06:26,339
test mismatch where in training they use

1779
01:06:24,480 --> 01:06:28,559
the biased version and in the in test

1780
01:06:26,339 --> 01:06:30,539
time they use the unbiased version I

1781
01:06:28,559 --> 01:06:32,160
find this extremely confusing you can

1782
01:06:30,539 --> 01:06:35,400
read more about the bezels correction

1783
01:06:32,160 --> 01:06:37,260
and why uh dividing by n minus one gives

1784
01:06:35,400 --> 01:06:39,480
you a better estimate of the variance in

1785
01:06:37,260 --> 01:06:41,000
a case where you have population size or

1786
01:06:39,480 --> 01:06:44,280
samples for the population

1787
01:06:41,000 --> 01:06:46,200
that are very small and that is indeed

1788
01:06:44,280 --> 01:06:48,059
the case for us because we are dealing

1789
01:06:46,200 --> 01:06:50,339
with many patches and these mini matches

1790
01:06:48,059 --> 01:06:52,020
are a small sample of a larger

1791
01:06:50,339 --> 01:06:55,260
population which is the entire training

1792
01:06:52,020 --> 01:06:57,180
set and so it just turns out that if you

1793
01:06:55,260 --> 01:06:58,920
just estimate it using one over n that

1794
01:06:57,180 --> 01:07:00,960
actually almost always underestimates

1795
01:06:58,920 --> 01:07:02,819
the variance and it is a biased

1796
01:07:00,960 --> 01:07:04,619
estimator and it is advised that you use

1797
01:07:02,819 --> 01:07:06,299
the unbiased version and divide by n

1798
01:07:04,619 --> 01:07:08,280
minus one and you can go through this

1799
01:07:06,299 --> 01:07:09,960
article here that I liked that actually

1800
01:07:08,280 --> 01:07:12,240
describes the full reasoning and I'll

1801
01:07:09,960 --> 01:07:13,859
link it in the video description

1802
01:07:12,240 --> 01:07:15,299
now when you calculate the torture

1803
01:07:13,859 --> 01:07:16,500
variance

1804
01:07:15,299 --> 01:07:18,359
you'll notice that they take the

1805
01:07:16,500 --> 01:07:21,180
unbiased flag whether or not you want to

1806
01:07:18,359 --> 01:07:24,480
divide by n or n minus one confusingly

1807
01:07:21,180 --> 01:07:26,940
they do not mention what the default is

1808
01:07:24,480 --> 01:07:29,400
for unbiased but I believe unbiased by

1809
01:07:26,940 --> 01:07:31,319
default is true I'm not sure why the

1810
01:07:29,400 --> 01:07:33,420
docs here don't cite that

1811
01:07:31,319 --> 01:07:35,880
now in The Bachelor

1812
01:07:33,420 --> 01:07:38,339
1D the documentation again is kind of

1813
01:07:35,880 --> 01:07:39,900
wrong and confusing it says that the

1814
01:07:38,339 --> 01:07:41,400
standard deviation is calculated via the

1815
01:07:39,900 --> 01:07:43,140
biased estimator

1816
01:07:41,400 --> 01:07:44,640
but this is actually not exactly right

1817
01:07:43,140 --> 01:07:46,799
and people have pointed out that it is

1818
01:07:44,640 --> 01:07:49,380
not right in a number of issues since

1819
01:07:46,799 --> 01:07:52,020
then because actually the rabbit hole is

1820
01:07:49,380 --> 01:07:54,299
deeper and they follow the paper exactly

1821
01:07:52,020 --> 01:07:56,160
and they use the biased version for

1822
01:07:54,299 --> 01:07:58,319
training but when they're estimating the

1823
01:07:56,160 --> 01:08:00,359
running standard deviation we are using

1824
01:07:58,319 --> 01:08:02,700
the unbiased version so again there's

1825
01:08:00,359 --> 01:08:05,160
the train test mismatch so long story

1826
01:08:02,700 --> 01:08:07,440
short I'm not a fan of trained test

1827
01:08:05,160 --> 01:08:08,539
discrepancies I basically kind of

1828
01:08:07,440 --> 01:08:10,980
consider

1829
01:08:08,539 --> 01:08:13,020
the fact that we use the bias version

1830
01:08:10,980 --> 01:08:14,579
the training time and the unbiased test

1831
01:08:13,020 --> 01:08:16,319
time I basically consider this to be a

1832
01:08:14,579 --> 01:08:18,239
bug and I don't think that there's a

1833
01:08:16,319 --> 01:08:19,920
good reason for that it's not really

1834
01:08:18,239 --> 01:08:22,319
they don't really go into the detail of

1835
01:08:19,920 --> 01:08:24,480
the reasoning behind it in this paper so

1836
01:08:22,319 --> 01:08:26,299
that's why I basically prefer to use the

1837
01:08:24,480 --> 01:08:29,219
bestless correction in my own work

1838
01:08:26,299 --> 01:08:30,540
unfortunately Bastion does not take a

1839
01:08:29,219 --> 01:08:33,179
keyword argument that tells you whether

1840
01:08:30,540 --> 01:08:34,679
or not you want to use the unbiased

1841
01:08:33,179 --> 01:08:36,600
version of the bias version in both

1842
01:08:34,679 --> 01:08:38,699
train and test and so therefore anyone

1843
01:08:36,600 --> 01:08:41,219
using batch normalization basically in

1844
01:08:38,699 --> 01:08:42,000
my view has a bit of a bug in the code

1845
01:08:41,219 --> 01:08:44,460
um

1846
01:08:42,000 --> 01:08:46,799
and this turns out to be much less of a

1847
01:08:44,460 --> 01:08:48,660
problem if your batch mini batch sizes

1848
01:08:46,799 --> 01:08:51,359
are a bit larger but still I just might

1849
01:08:48,660 --> 01:08:53,640
kind of uh unpardable so maybe someone

1850
01:08:51,359 --> 01:08:55,259
can explain why this is okay but for now

1851
01:08:53,640 --> 01:08:57,839
I prefer to use the unbiased version

1852
01:08:55,259 --> 01:09:00,179
consistently both during training and at

1853
01:08:57,839 --> 01:09:01,859
this time and that's why I'm using one

1854
01:09:00,179 --> 01:09:03,060
over n minus one here

1855
01:09:01,859 --> 01:09:05,640
okay so let's now actually back

1856
01:09:03,060 --> 01:09:07,319
propagate through this line

1857
01:09:05,640 --> 01:09:08,640
so

1858
01:09:07,319 --> 01:09:10,500
the first thing that I always like to do

1859
01:09:08,640 --> 01:09:12,900
is I like to scrutinize the shapes first

1860
01:09:10,500 --> 01:09:14,940
so in particular here looking at the

1861
01:09:12,900 --> 01:09:18,960
shapes of what's involved I see that b

1862
01:09:14,940 --> 01:09:21,600
and VAR shape is 1 by 64. so it's a row

1863
01:09:18,960 --> 01:09:22,739
vector and BND if two dot shape is 32 by

1864
01:09:21,600 --> 01:09:25,080
64.

1865
01:09:22,739 --> 01:09:28,859
so clearly here we're doing a sum over

1866
01:09:25,080 --> 01:09:32,699
the zeroth axis to squash the first

1867
01:09:28,859 --> 01:09:35,040
dimension of of the shapes here using a

1868
01:09:32,699 --> 01:09:36,779
sum so that right away actually hints to

1869
01:09:35,040 --> 01:09:38,759
me that there will be some kind of a

1870
01:09:36,779 --> 01:09:40,799
replication or broadcasting in the

1871
01:09:38,759 --> 01:09:42,660
backward pass and maybe you're noticing

1872
01:09:40,799 --> 01:09:45,420
the pattern here but basically anytime

1873
01:09:42,660 --> 01:09:47,759
you have a sum in the forward pass that

1874
01:09:45,420 --> 01:09:49,140
turns into a replication or broadcasting

1875
01:09:47,759 --> 01:09:52,140
in the backward pass along the same

1876
01:09:49,140 --> 01:09:54,840
Dimension and conversely when we have a

1877
01:09:52,140 --> 01:09:57,120
replication or a broadcasting in the

1878
01:09:54,840 --> 01:09:59,940
forward pass that indicates a variable

1879
01:09:57,120 --> 01:10:01,800
reuse and so in the backward pass that

1880
01:09:59,940 --> 01:10:02,820
turns into a sum over the exact same

1881
01:10:01,800 --> 01:10:04,440
dimension

1882
01:10:02,820 --> 01:10:06,060
and so hopefully you're noticing that

1883
01:10:04,440 --> 01:10:07,440
Duality that those two are kind of like

1884
01:10:06,060 --> 01:10:09,120
the opposite of each other in the

1885
01:10:07,440 --> 01:10:11,520
forward and backward pass

1886
01:10:09,120 --> 01:10:12,900
now once we understand the shapes the

1887
01:10:11,520 --> 01:10:15,000
next thing I like to do always is I like

1888
01:10:12,900 --> 01:10:16,679
to look at a toy example in my head to

1889
01:10:15,000 --> 01:10:18,360
sort of just like understand roughly how

1890
01:10:16,679 --> 01:10:19,980
uh the variable the variable

1891
01:10:18,360 --> 01:10:21,420
dependencies go in the mathematical

1892
01:10:19,980 --> 01:10:24,300
formula

1893
01:10:21,420 --> 01:10:26,699
so here we have a two-dimensional array

1894
01:10:24,300 --> 01:10:29,880
of the end of two which we are scaling

1895
01:10:26,699 --> 01:10:32,219
by a constant and then we are summing uh

1896
01:10:29,880 --> 01:10:33,960
vertically over the columns so if we

1897
01:10:32,219 --> 01:10:36,179
have a two by two Matrix a and then we

1898
01:10:33,960 --> 01:10:39,780
sum over the columns and scale we would

1899
01:10:36,179 --> 01:10:42,360
get a row Vector B1 B2 and B1 depends on

1900
01:10:39,780 --> 01:10:45,659
a in this way whereas just sum they're

1901
01:10:42,360 --> 01:10:48,900
scaled of a and B2 in this way where

1902
01:10:45,659 --> 01:10:52,080
it's the second column sump and scale

1903
01:10:48,900 --> 01:10:53,520
and so looking at this basically

1904
01:10:52,080 --> 01:10:55,679
what we want to do now is we have the

1905
01:10:53,520 --> 01:10:58,260
derivatives on B1 and B2 and we want to

1906
01:10:55,679 --> 01:10:59,640
back propagate them into Ace and so it's

1907
01:10:58,260 --> 01:11:01,920
clear that just differentiating in your

1908
01:10:59,640 --> 01:11:05,340
head the local derivative here is one

1909
01:11:01,920 --> 01:11:09,360
over n minus 1 times uh one

1910
01:11:05,340 --> 01:11:11,460
uh for each one of these A's and um

1911
01:11:09,360 --> 01:11:13,920
basically the derivative of B1 has to

1912
01:11:11,460 --> 01:11:16,560
flow through The Columns of a

1913
01:11:13,920 --> 01:11:18,600
scaled by one over n minus one

1914
01:11:16,560 --> 01:11:21,480
and that's roughly What's Happening Here

1915
01:11:18,600 --> 01:11:24,840
so intuitively the derivative flow tells

1916
01:11:21,480 --> 01:11:27,780
us that DB and diff2

1917
01:11:24,840 --> 01:11:29,820
will be the local derivative of this

1918
01:11:27,780 --> 01:11:31,080
operation and there are many ways to do

1919
01:11:29,820 --> 01:11:33,840
this by the way but I like to do

1920
01:11:31,080 --> 01:11:37,320
something like this torch dot once like

1921
01:11:33,840 --> 01:11:39,659
of bndf2 so I'll create a large array

1922
01:11:37,320 --> 01:11:42,179
two-dimensional of ones

1923
01:11:39,659 --> 01:11:44,699
and then I will scale it so 1.0 divided

1924
01:11:42,179 --> 01:11:46,920
by n minus 1.

1925
01:11:44,699 --> 01:11:49,260
so this is a array of

1926
01:11:46,920 --> 01:11:50,699
um one over n minus one and that's sort

1927
01:11:49,260 --> 01:11:53,400
of like the local derivative

1928
01:11:50,699 --> 01:11:58,040
and now for the chain rule I will simply

1929
01:11:53,400 --> 01:11:58,040
just multiply it by dbm bar

1930
01:11:58,380 --> 01:12:02,940
and notice here what's going to happen

1931
01:12:00,000 --> 01:12:06,120
this is 32 by 64 and this is just 1 by

1932
01:12:02,940 --> 01:12:08,460
64. so I'm letting the broadcasting do

1933
01:12:06,120 --> 01:12:11,520
the replication because internally in

1934
01:12:08,460 --> 01:12:13,080
pytorch basically dbnbar which is 1 by

1935
01:12:11,520 --> 01:12:15,420
64 row vector

1936
01:12:13,080 --> 01:12:18,179
well in this multiplication get

1937
01:12:15,420 --> 01:12:19,560
um copied vertically until the two are

1938
01:12:18,179 --> 01:12:22,020
of the same shape and then there will be

1939
01:12:19,560 --> 01:12:23,699
an element wise multiply and so that uh

1940
01:12:22,020 --> 01:12:25,140
so that the broadcasting is basically

1941
01:12:23,699 --> 01:12:27,120
doing the replication

1942
01:12:25,140 --> 01:12:30,060
and I will end up with the derivatives

1943
01:12:27,120 --> 01:12:32,460
of DB and diff2 here

1944
01:12:30,060 --> 01:12:33,960
so this is the candidate solution let's

1945
01:12:32,460 --> 01:12:36,780
bring it down here

1946
01:12:33,960 --> 01:12:39,480
let's uncomment this line where we check

1947
01:12:36,780 --> 01:12:41,400
it and let's hope for the best

1948
01:12:39,480 --> 01:12:43,380
and indeed we see that this is the

1949
01:12:41,400 --> 01:12:45,600
correct formula next up let's

1950
01:12:43,380 --> 01:12:48,000
differentiate here and to be in this

1951
01:12:45,600 --> 01:12:50,940
so here we have that b and diff is

1952
01:12:48,000 --> 01:12:52,739
element y squared to create B and F2

1953
01:12:50,940 --> 01:12:54,600
so this is a relatively simple

1954
01:12:52,739 --> 01:12:56,280
derivative because it's a simple element

1955
01:12:54,600 --> 01:12:59,400
wise operation so it's kind of like the

1956
01:12:56,280 --> 01:13:02,280
scalar case and we have that DB and div

1957
01:12:59,400 --> 01:13:04,679
should be if this is x squared then the

1958
01:13:02,280 --> 01:13:07,620
derivative of this is 2x right so it's

1959
01:13:04,679 --> 01:13:08,880
simply 2 times B and if that's the local

1960
01:13:07,620 --> 01:13:11,219
derivative

1961
01:13:08,880 --> 01:13:13,080
and then times chain Rule and the shape

1962
01:13:11,219 --> 01:13:15,840
of these is the same they are of the

1963
01:13:13,080 --> 01:13:17,699
same shape so times this

1964
01:13:15,840 --> 01:13:20,580
so that's the backward pass for this

1965
01:13:17,699 --> 01:13:22,080
variable let me bring that down here

1966
01:13:20,580 --> 01:13:24,480
and now we have to be careful because we

1967
01:13:22,080 --> 01:13:27,900
already calculated dbm depth right so

1968
01:13:24,480 --> 01:13:30,120
this is just the end of the other uh you

1969
01:13:27,900 --> 01:13:30,900
know other Branch coming back to B and

1970
01:13:30,120 --> 01:13:32,699
diff

1971
01:13:30,900 --> 01:13:34,860
because B and diff was already back

1972
01:13:32,699 --> 01:13:37,860
propagated to way over here

1973
01:13:34,860 --> 01:13:39,719
from being raw so we now completed the

1974
01:13:37,860 --> 01:13:42,540
second branch and so that's why I have

1975
01:13:39,719 --> 01:13:43,980
to do plus equals and if you recall we

1976
01:13:42,540 --> 01:13:46,260
had an incorrect derivative for being

1977
01:13:43,980 --> 01:13:48,900
diff before and I'm hoping that once we

1978
01:13:46,260 --> 01:13:51,620
append this last missing piece we have

1979
01:13:48,900 --> 01:13:55,199
the exact correctness so let's run

1980
01:13:51,620 --> 01:13:57,840
ambient to be in div now actually shows

1981
01:13:55,199 --> 01:14:00,000
the exact correct derivative

1982
01:13:57,840 --> 01:14:01,380
um so that's comforting okay so let's

1983
01:14:00,000 --> 01:14:03,120
now back propagate through this line

1984
01:14:01,380 --> 01:14:04,920
here

1985
01:14:03,120 --> 01:14:07,020
um the first thing we do of course is we

1986
01:14:04,920 --> 01:14:08,820
check the shapes and I wrote them out

1987
01:14:07,020 --> 01:14:12,480
here and basically the shape of this is

1988
01:14:08,820 --> 01:14:15,420
32 by 64. hpbn is the same shape

1989
01:14:12,480 --> 01:14:17,820
but B and mean I is a row Vector 1 by

1990
01:14:15,420 --> 01:14:19,620
64. so this minus here will actually do

1991
01:14:17,820 --> 01:14:21,900
broadcasting and so we have to be

1992
01:14:19,620 --> 01:14:23,760
careful with that and as a hint to us

1993
01:14:21,900 --> 01:14:25,920
again because of The Duality a

1994
01:14:23,760 --> 01:14:27,659
broadcasting and the forward pass means

1995
01:14:25,920 --> 01:14:30,120
a variable reuse and therefore there

1996
01:14:27,659 --> 01:14:31,500
will be a sum in the backward pass

1997
01:14:30,120 --> 01:14:33,120
so let's write out the backward pass

1998
01:14:31,500 --> 01:14:34,320
here now

1999
01:14:33,120 --> 01:14:37,199
um

2000
01:14:34,320 --> 01:14:39,060
back propagate into the hpbn

2001
01:14:37,199 --> 01:14:41,040
because this is these are the same shape

2002
01:14:39,060 --> 01:14:43,140
then the local derivative for each one

2003
01:14:41,040 --> 01:14:45,120
of the elements here is just one for the

2004
01:14:43,140 --> 01:14:47,699
corresponding element in here

2005
01:14:45,120 --> 01:14:50,699
so basically what this means is that the

2006
01:14:47,699 --> 01:14:52,800
gradient just simply copies it's just a

2007
01:14:50,699 --> 01:14:54,900
variable assignment it's quality so I'm

2008
01:14:52,800 --> 01:14:58,739
just going to clone this tensor just for

2009
01:14:54,900 --> 01:15:00,239
safety to create an exact copy of DB and

2010
01:14:58,739 --> 01:15:01,800
div

2011
01:15:00,239 --> 01:15:06,500
and then here to back propagate into

2012
01:15:01,800 --> 01:15:06,500
this one what I'm inclined to do here is

2013
01:15:07,219 --> 01:15:12,300
will basically be

2014
01:15:09,840 --> 01:15:16,380
uh what is the local derivative well

2015
01:15:12,300 --> 01:15:19,320
it's negative torch.1's like

2016
01:15:16,380 --> 01:15:21,739
of the shape of uh B and diff

2017
01:15:19,320 --> 01:15:21,739
right

2018
01:15:22,199 --> 01:15:27,360
and then times

2019
01:15:24,900 --> 01:15:31,260
the um

2020
01:15:27,360 --> 01:15:31,260
the derivative here dbf

2021
01:15:32,880 --> 01:15:37,800
and this here is the back propagation

2022
01:15:34,739 --> 01:15:39,659
for the replicated B and mean I

2023
01:15:37,800 --> 01:15:42,060
so I still have to back propagate

2024
01:15:39,659 --> 01:15:43,800
through the uh replication in the

2025
01:15:42,060 --> 01:15:45,600
broadcasting and I do that by doing a

2026
01:15:43,800 --> 01:15:47,940
sum so I'm going to take this whole

2027
01:15:45,600 --> 01:15:49,739
thing and I'm going to do a sum over the

2028
01:15:47,940 --> 01:15:52,340
zeroth dimension which was the

2029
01:15:49,739 --> 01:15:52,340
replication

2030
01:15:53,520 --> 01:15:57,659
so if you scrutinize this by the way

2031
01:15:55,920 --> 01:16:00,000
you'll notice that this is the same

2032
01:15:57,659 --> 01:16:01,320
shape as that and so what I'm doing uh

2033
01:16:00,000 --> 01:16:03,440
what I'm doing here doesn't actually

2034
01:16:01,320 --> 01:16:06,480
make that much sense because it's just a

2035
01:16:03,440 --> 01:16:10,199
array of ones multiplying DP and diff so

2036
01:16:06,480 --> 01:16:12,179
in fact I can just do this

2037
01:16:10,199 --> 01:16:15,179
um and that is equivalent

2038
01:16:12,179 --> 01:16:18,000
so this is the candidate backward pass

2039
01:16:15,179 --> 01:16:22,500
let me copy it here and then let me

2040
01:16:18,000 --> 01:16:24,060
comment out this one and this one

2041
01:16:22,500 --> 01:16:27,140
enter

2042
01:16:24,060 --> 01:16:27,140
and it's wrong

2043
01:16:27,239 --> 01:16:31,199
damn

2044
01:16:29,460 --> 01:16:33,360
actually sorry this is supposed to be

2045
01:16:31,199 --> 01:16:34,620
wrong and it's supposed to be wrong

2046
01:16:33,360 --> 01:16:36,659
because

2047
01:16:34,620 --> 01:16:39,840
we are back propagating from a b and

2048
01:16:36,659 --> 01:16:43,380
diff into hpbn and but we're not done

2049
01:16:39,840 --> 01:16:44,880
because B and mean I depends on hpbn and

2050
01:16:43,380 --> 01:16:46,260
there will be a second portion of that

2051
01:16:44,880 --> 01:16:48,420
derivative coming from this second

2052
01:16:46,260 --> 01:16:50,040
Branch so we're not done yet and we

2053
01:16:48,420 --> 01:16:50,760
expect it to be incorrect so there you

2054
01:16:50,040 --> 01:16:53,100
go

2055
01:16:50,760 --> 01:16:56,340
uh so let's now back propagate from uh B

2056
01:16:53,100 --> 01:16:57,300
and mean I into hpbn

2057
01:16:56,340 --> 01:16:58,860
um

2058
01:16:57,300 --> 01:17:01,380
and so here again we have to be careful

2059
01:16:58,860 --> 01:17:03,239
because there's a broadcasting along

2060
01:17:01,380 --> 01:17:04,560
um or there's a Sum along the zeroth

2061
01:17:03,239 --> 01:17:06,540
dimension so this will turn into

2062
01:17:04,560 --> 01:17:08,640
broadcasting in the backward pass now

2063
01:17:06,540 --> 01:17:10,320
and I'm going to go a little bit faster

2064
01:17:08,640 --> 01:17:12,420
on this line because it is very similar

2065
01:17:10,320 --> 01:17:14,820
to the line that we had before and

2066
01:17:12,420 --> 01:17:18,540
multiplies in the past in fact

2067
01:17:14,820 --> 01:17:20,060
so the hpbn

2068
01:17:18,540 --> 01:17:22,620
will be

2069
01:17:20,060 --> 01:17:25,440
the gradient will be scaled by 1 over n

2070
01:17:22,620 --> 01:17:27,300
and then basically this gradient here on

2071
01:17:25,440 --> 01:17:30,239
dbn mean I

2072
01:17:27,300 --> 01:17:32,100
is going to be scaled by 1 over n and

2073
01:17:30,239 --> 01:17:35,699
then it's going to flow across all the

2074
01:17:32,100 --> 01:17:38,460
columns and deposit itself into the hpvn

2075
01:17:35,699 --> 01:17:39,600
so what we want is this thing scaled by

2076
01:17:38,460 --> 01:17:43,219
1 over n

2077
01:17:39,600 --> 01:17:43,219
only put the constant up front here

2078
01:17:43,980 --> 01:17:47,520
um

2079
01:17:45,300 --> 01:17:51,480
so scale down the gradient and now we

2080
01:17:47,520 --> 01:17:55,140
need to replicate it across all the um

2081
01:17:51,480 --> 01:18:00,840
across all the rows here so we I like to

2082
01:17:55,140 --> 01:18:03,840
do that by torch.lunslike of basically

2083
01:18:00,840 --> 01:18:05,820
um hpbn

2084
01:18:03,840 --> 01:18:09,300
and I will let the broadcasting do the

2085
01:18:05,820 --> 01:18:11,840
work of replication

2086
01:18:09,300 --> 01:18:11,840
so

2087
01:18:14,940 --> 01:18:21,300
like that

2088
01:18:16,320 --> 01:18:24,739
so this is uh the hppn and hopefully

2089
01:18:21,300 --> 01:18:24,739
we can plus equals that

2090
01:18:27,120 --> 01:18:32,280
so this here is broadcasting

2091
01:18:30,120 --> 01:18:33,540
um and then this is the scaling so this

2092
01:18:32,280 --> 01:18:35,060
should be current

2093
01:18:33,540 --> 01:18:37,140
okay

2094
01:18:35,060 --> 01:18:38,940
so that completes the back propagation

2095
01:18:37,140 --> 01:18:40,560
of the bathroom layer and we are now

2096
01:18:38,940 --> 01:18:43,260
here let's back propagate through the

2097
01:18:40,560 --> 01:18:44,340
linear layer one here now because

2098
01:18:43,260 --> 01:18:46,980
everything is getting a little

2099
01:18:44,340 --> 01:18:48,360
vertically crazy I copy pasted the line

2100
01:18:46,980 --> 01:18:50,159
here and let's just back properly

2101
01:18:48,360 --> 01:18:52,199
through this one line

2102
01:18:50,159 --> 01:18:56,280
so first of course we inspect the shapes

2103
01:18:52,199 --> 01:18:58,860
and we see that this is 32 by 64. MCAT

2104
01:18:56,280 --> 01:19:04,620
is 32 by 30.

2105
01:18:58,860 --> 01:19:06,659
W1 is 30 30 by 64 and B1 is just 64. so

2106
01:19:04,620 --> 01:19:08,400
as I mentioned back propagating through

2107
01:19:06,659 --> 01:19:11,100
linear layers is fairly easy just by

2108
01:19:08,400 --> 01:19:14,100
matching the shapes so let's do that we

2109
01:19:11,100 --> 01:19:15,600
have that dmcat

2110
01:19:14,100 --> 01:19:18,480
should be

2111
01:19:15,600 --> 01:19:21,659
um some matrix multiplication of dhbn

2112
01:19:18,480 --> 01:19:28,199
with uh W1 and one transpose thrown in

2113
01:19:21,659 --> 01:19:32,159
there so to make uh MCAT be 32 by 30

2114
01:19:28,199 --> 01:19:36,300
I need to take dhpn

2115
01:19:32,159 --> 01:19:38,780
32 by 64 and multiply it by w1.

2116
01:19:36,300 --> 01:19:38,780
transpose

2117
01:19:39,900 --> 01:19:45,600
to get the only one I need to end up

2118
01:19:43,380 --> 01:19:48,780
with 30 by 64.

2119
01:19:45,600 --> 01:19:51,300
so to get that I need to take uh MCAT

2120
01:19:48,780 --> 01:19:53,340
transpose

2121
01:19:51,300 --> 01:19:57,300
and multiply that by

2122
01:19:53,340 --> 01:19:57,300
uh dhpion

2123
01:19:58,380 --> 01:20:04,800
and finally to get DB1

2124
01:20:01,500 --> 01:20:06,960
this is a addition and we saw that

2125
01:20:04,800 --> 01:20:09,560
basically I need to just sum the

2126
01:20:06,960 --> 01:20:12,060
elements in dhpbn along some Dimension

2127
01:20:09,560 --> 01:20:14,219
and to make the dimensions work out I

2128
01:20:12,060 --> 01:20:17,520
need to Sum along the zeroth axis here

2129
01:20:14,219 --> 01:20:19,080
to eliminate this Dimension and we do

2130
01:20:17,520 --> 01:20:21,060
not keep dims

2131
01:20:19,080 --> 01:20:23,219
uh so that we want to just get a single

2132
01:20:21,060 --> 01:20:27,360
one-dimensional lecture of 64.

2133
01:20:23,219 --> 01:20:29,540
so these are the claimed derivatives

2134
01:20:27,360 --> 01:20:32,520
let me put that here and let me

2135
01:20:29,540 --> 01:20:34,080
uncomment three lines and cross our

2136
01:20:32,520 --> 01:20:36,060
fingers

2137
01:20:34,080 --> 01:20:37,980
everything is great okay so we now

2138
01:20:36,060 --> 01:20:39,780
continue almost there we have the

2139
01:20:37,980 --> 01:20:41,580
derivative of MCAT and we want to

2140
01:20:39,780 --> 01:20:43,199
derivative we want to back propagate

2141
01:20:41,580 --> 01:20:46,260
into m

2142
01:20:43,199 --> 01:20:48,659
so I again copied this line over here

2143
01:20:46,260 --> 01:20:51,300
so this is the forward pass and then

2144
01:20:48,659 --> 01:20:53,400
this is the shapes so remember that the

2145
01:20:51,300 --> 01:20:57,120
shape here was 32 by 30 and the original

2146
01:20:53,400 --> 01:20:58,620
shape of M plus 32 by 3 by 10. so this

2147
01:20:57,120 --> 01:21:01,280
layer in the forward pass as you recall

2148
01:20:58,620 --> 01:21:04,199
did the concatenation of these three

2149
01:21:01,280 --> 01:21:06,719
10-dimensional character vectors

2150
01:21:04,199 --> 01:21:08,520
and so now we just want to undo that

2151
01:21:06,719 --> 01:21:11,040
so this is actually relatively

2152
01:21:08,520 --> 01:21:12,600
straightforward operation because uh the

2153
01:21:11,040 --> 01:21:15,120
backward pass of the what is the view

2154
01:21:12,600 --> 01:21:17,400
view is just a representation of the

2155
01:21:15,120 --> 01:21:18,960
array it's just a logical form of how

2156
01:21:17,400 --> 01:21:21,900
you interpret the array so let's just

2157
01:21:18,960 --> 01:21:25,620
reinterpret it to be what it was before

2158
01:21:21,900 --> 01:21:29,460
so in other words the end is not uh 32

2159
01:21:25,620 --> 01:21:34,199
by 30. it is basically dmcat

2160
01:21:29,460 --> 01:21:37,199
but if you view it as the original shape

2161
01:21:34,199 --> 01:21:39,540
so just m dot shape

2162
01:21:37,199 --> 01:21:40,500
uh you can you can pass in tuples into

2163
01:21:39,540 --> 01:21:44,540
view

2164
01:21:40,500 --> 01:21:44,540
and so this should just be okay

2165
01:21:44,640 --> 01:21:49,980
we just re-represent that view and then

2166
01:21:47,100 --> 01:21:51,060
we uncomment this line here and

2167
01:21:49,980 --> 01:21:55,140
hopefully

2168
01:21:51,060 --> 01:21:56,159
yeah so the derivative of M is correct

2169
01:21:55,140 --> 01:21:57,360
so in this case we just have to

2170
01:21:56,159 --> 01:21:59,880
re-represent the shape of those

2171
01:21:57,360 --> 01:22:01,739
derivatives into the original View

2172
01:21:59,880 --> 01:22:02,940
so now we are at the final line and the

2173
01:22:01,739 --> 01:22:05,540
only thing that's left to back propagate

2174
01:22:02,940 --> 01:22:09,480
through is this indexing operation here

2175
01:22:05,540 --> 01:22:11,580
MSC at xB so as I did before I copy

2176
01:22:09,480 --> 01:22:12,840
pasted this line here and let's look at

2177
01:22:11,580 --> 01:22:15,239
the shapes of everything that's involved

2178
01:22:12,840 --> 01:22:19,620
and remind ourselves how this worked

2179
01:22:15,239 --> 01:22:22,260
so m.shape was 32 by 3 by 10.

2180
01:22:19,620 --> 01:22:24,480
it says 32 examples and then we have

2181
01:22:22,260 --> 01:22:26,640
three characters each one of them has a

2182
01:22:24,480 --> 01:22:28,920
10 dimensional embedding

2183
01:22:26,640 --> 01:22:31,380
and this was achieved by taking the

2184
01:22:28,920 --> 01:22:32,640
lookup table C which have 27 possible

2185
01:22:31,380 --> 01:22:34,860
characters

2186
01:22:32,640 --> 01:22:35,900
each of them 10 dimensional and we

2187
01:22:34,860 --> 01:22:39,000
looked up

2188
01:22:35,900 --> 01:22:41,040
at the rows that were specified inside

2189
01:22:39,000 --> 01:22:43,739
this tensor xB

2190
01:22:41,040 --> 01:22:45,659
so XB is 32 by 3 and it's basically

2191
01:22:43,739 --> 01:22:49,199
giving us for each example the Identity

2192
01:22:45,659 --> 01:22:50,820
or the index of which character is part

2193
01:22:49,199 --> 01:22:52,920
of that example

2194
01:22:50,820 --> 01:22:57,120
and so here I'm showing the first five

2195
01:22:52,920 --> 01:22:58,800
rows of three of this tensor xB

2196
01:22:57,120 --> 01:23:00,659
and so we can see that for example here

2197
01:22:58,800 --> 01:23:02,880
it was the first example in this batch

2198
01:23:00,659 --> 01:23:04,199
is that the first character and the

2199
01:23:02,880 --> 01:23:06,300
first character and the fourth character

2200
01:23:04,199 --> 01:23:08,400
comes into the neural net

2201
01:23:06,300 --> 01:23:10,140
and then we want to predict the next

2202
01:23:08,400 --> 01:23:12,300
character in a sequence after the

2203
01:23:10,140 --> 01:23:14,580
character is one one four

2204
01:23:12,300 --> 01:23:18,000
so basically What's Happening Here is

2205
01:23:14,580 --> 01:23:19,560
there are integers inside XB and each

2206
01:23:18,000 --> 01:23:22,500
one of these integers is specifying

2207
01:23:19,560 --> 01:23:25,560
which row of C we want to pluck out

2208
01:23:22,500 --> 01:23:28,679
right and then we arrange those rows

2209
01:23:25,560 --> 01:23:30,900
that we've plucked out into 32 by 3 by

2210
01:23:28,679 --> 01:23:33,659
10 tensor and we just package them in we

2211
01:23:30,900 --> 01:23:35,100
just package them into the sensor

2212
01:23:33,659 --> 01:23:36,300
and now what's happening is that we have

2213
01:23:35,100 --> 01:23:39,000
D amp

2214
01:23:36,300 --> 01:23:41,640
so for every one of these uh basically

2215
01:23:39,000 --> 01:23:42,960
plucked out rows we have their gradients

2216
01:23:41,640 --> 01:23:45,239
now

2217
01:23:42,960 --> 01:23:48,060
but they're arranged inside this 32 by 3

2218
01:23:45,239 --> 01:23:49,760
by 10 tensor so all we have to do now is

2219
01:23:48,060 --> 01:23:52,260
we just need to Route this gradient

2220
01:23:49,760 --> 01:23:54,960
backwards through this assignment so we

2221
01:23:52,260 --> 01:23:56,100
need to find which row of C that every

2222
01:23:54,960 --> 01:23:59,040
one of these

2223
01:23:56,100 --> 01:24:03,120
um 10 dimensional embeddings come from

2224
01:23:59,040 --> 01:24:06,060
and then we need to deposit them into DC

2225
01:24:03,120 --> 01:24:08,460
so we just need to undo the indexing and

2226
01:24:06,060 --> 01:24:10,140
of course if any of these rows of C was

2227
01:24:08,460 --> 01:24:11,820
used multiple times which almost

2228
01:24:10,140 --> 01:24:13,800
certainly is the case like the row one

2229
01:24:11,820 --> 01:24:15,600
and one was used multiple times then we

2230
01:24:13,800 --> 01:24:18,060
have to remember that the gradients that

2231
01:24:15,600 --> 01:24:19,739
arrive there have to add

2232
01:24:18,060 --> 01:24:21,360
so for each occurrence we have to have

2233
01:24:19,739 --> 01:24:23,520
an addition

2234
01:24:21,360 --> 01:24:24,840
so let's now write this out and I don't

2235
01:24:23,520 --> 01:24:26,460
actually know if like a much better way

2236
01:24:24,840 --> 01:24:28,020
to do this than a for Loop unfortunately

2237
01:24:26,460 --> 01:24:29,760
in Python

2238
01:24:28,020 --> 01:24:32,219
um so maybe someone can come up with a

2239
01:24:29,760 --> 01:24:34,679
vectorized efficient operation but for

2240
01:24:32,219 --> 01:24:37,140
now let's just use for loops so let me

2241
01:24:34,679 --> 01:24:40,980
create a torch.zeros like

2242
01:24:37,140 --> 01:24:43,140
C to initialize uh just uh 27 by 10

2243
01:24:40,980 --> 01:24:46,980
tensor of all zeros

2244
01:24:43,140 --> 01:24:49,440
and then honestly 4K in range XB dot

2245
01:24:46,980 --> 01:24:51,060
shape at zero

2246
01:24:49,440 --> 01:24:53,219
maybe someone has a better way to do

2247
01:24:51,060 --> 01:24:55,739
this but for J and range

2248
01:24:53,219 --> 01:24:58,320
be that shape at one

2249
01:24:55,739 --> 01:25:01,800
this is going to iterate over all the

2250
01:24:58,320 --> 01:25:03,179
um all the elements of XB all these

2251
01:25:01,800 --> 01:25:05,340
integers

2252
01:25:03,179 --> 01:25:06,840
and then let's get the index at this

2253
01:25:05,340 --> 01:25:11,640
position

2254
01:25:06,840 --> 01:25:14,340
so the index is basically x b at KJ

2255
01:25:11,640 --> 01:25:16,020
so that an example of that like is 11 or

2256
01:25:14,340 --> 01:25:19,440
14 and so on

2257
01:25:16,020 --> 01:25:23,719
and now in the forward pass we took

2258
01:25:19,440 --> 01:25:23,719
and we basically took um

2259
01:25:24,239 --> 01:25:30,719
the row of C at index and we deposited

2260
01:25:27,960 --> 01:25:32,400
it into M at K of J

2261
01:25:30,719 --> 01:25:34,140
that's what happened that's where they

2262
01:25:32,400 --> 01:25:36,560
are packaged so now we need to go

2263
01:25:34,140 --> 01:25:39,900
backwards and we just need to route

2264
01:25:36,560 --> 01:25:42,659
DM at the position KJ

2265
01:25:39,900 --> 01:25:44,760
we now have these derivatives

2266
01:25:42,659 --> 01:25:45,840
for each position and it's 10

2267
01:25:44,760 --> 01:25:47,280
dimensional

2268
01:25:45,840 --> 01:25:49,140
and you just need to go into the correct

2269
01:25:47,280 --> 01:25:54,480
row of C

2270
01:25:49,140 --> 01:25:55,679
so DC rather at IX is this but plus

2271
01:25:54,480 --> 01:25:56,880
equals

2272
01:25:55,679 --> 01:25:58,980
because there could be multiple

2273
01:25:56,880 --> 01:26:00,960
occurrences uh like the same row could

2274
01:25:58,980 --> 01:26:04,560
have been used many many times and so

2275
01:26:00,960 --> 01:26:06,179
all of those derivatives will just go

2276
01:26:04,560 --> 01:26:07,980
backwards through the indexing and they

2277
01:26:06,179 --> 01:26:12,540
will add

2278
01:26:07,980 --> 01:26:12,540
so this is my candidate solution

2279
01:26:12,659 --> 01:26:16,159
let's copy it here

2280
01:26:16,380 --> 01:26:20,940
let's uncomment this and cross our

2281
01:26:19,380 --> 01:26:21,980
fingers

2282
01:26:20,940 --> 01:26:24,300
hey

2283
01:26:21,980 --> 01:26:25,679
so that's it we've back propagated

2284
01:26:24,300 --> 01:26:28,020
through

2285
01:26:25,679 --> 01:26:31,080
this entire Beast

2286
01:26:28,020 --> 01:26:33,600
so there we go totally makes sense

2287
01:26:31,080 --> 01:26:34,980
so now we come to exercise two it

2288
01:26:33,600 --> 01:26:36,780
basically turns out that in this first

2289
01:26:34,980 --> 01:26:39,000
exercise we were doing way too much work

2290
01:26:36,780 --> 01:26:40,739
uh we were back propagating way too much

2291
01:26:39,000 --> 01:26:42,239
and it was all good practice and so on

2292
01:26:40,739 --> 01:26:44,340
but it's not what you would do in

2293
01:26:42,239 --> 01:26:47,159
practice and the reason for that is for

2294
01:26:44,340 --> 01:26:49,260
example here I separated out this loss

2295
01:26:47,159 --> 01:26:51,719
calculation over multiple lines and I

2296
01:26:49,260 --> 01:26:53,340
broke it up all all to like its smallest

2297
01:26:51,719 --> 01:26:55,199
atomic pieces and we back propagated

2298
01:26:53,340 --> 01:26:56,760
through all of those individually

2299
01:26:55,199 --> 01:26:58,380
but it turns out that if you just look

2300
01:26:56,760 --> 01:27:00,000
at the mathematical expression for the

2301
01:26:58,380 --> 01:27:02,159
loss

2302
01:27:00,000 --> 01:27:04,320
um then actually you can do the

2303
01:27:02,159 --> 01:27:06,480
differentiation on pen and paper and a

2304
01:27:04,320 --> 01:27:07,679
lot of terms cancel and simplify and the

2305
01:27:06,480 --> 01:27:10,080
mathematical expression you end up with

2306
01:27:07,679 --> 01:27:11,639
can be significantly shorter and easier

2307
01:27:10,080 --> 01:27:12,719
to implement than back propagating

2308
01:27:11,639 --> 01:27:13,980
through all the little pieces of

2309
01:27:12,719 --> 01:27:16,139
everything you've done

2310
01:27:13,980 --> 01:27:18,420
so before we had this complicated

2311
01:27:16,139 --> 01:27:19,260
forward paths going from logits to the

2312
01:27:18,420 --> 01:27:21,120
loss

2313
01:27:19,260 --> 01:27:22,980
but in pytorch everything can just be

2314
01:27:21,120 --> 01:27:24,719
glued together into a single call at

2315
01:27:22,980 --> 01:27:26,400
that cross entropy you just pass in

2316
01:27:24,719 --> 01:27:28,980
logits and the labels and you get the

2317
01:27:26,400 --> 01:27:31,320
exact same loss as I verify here so our

2318
01:27:28,980 --> 01:27:33,480
previous loss and the fast loss coming

2319
01:27:31,320 --> 01:27:36,719
from the chunk of operations as a single

2320
01:27:33,480 --> 01:27:38,580
mathematical expression is the same but

2321
01:27:36,719 --> 01:27:40,560
it's much much faster in a forward pass

2322
01:27:38,580 --> 01:27:42,480
it's also much much faster in backward

2323
01:27:40,560 --> 01:27:43,739
pass and the reason for that is if you

2324
01:27:42,480 --> 01:27:45,540
just look at the mathematical form of

2325
01:27:43,739 --> 01:27:46,860
this and differentiate again you will

2326
01:27:45,540 --> 01:27:48,960
end up with a very small and short

2327
01:27:46,860 --> 01:27:51,600
expression so that's what we want to do

2328
01:27:48,960 --> 01:27:54,420
here we want to in a single operation or

2329
01:27:51,600 --> 01:27:56,460
in a single go or like very quickly go

2330
01:27:54,420 --> 01:27:59,159
directly to delojits

2331
01:27:56,460 --> 01:28:02,400
and we need to implement the logits as a

2332
01:27:59,159 --> 01:28:04,199
function of logits and yb's

2333
01:28:02,400 --> 01:28:06,780
but it will be significantly shorter

2334
01:28:04,199 --> 01:28:08,699
than whatever we did here where to get

2335
01:28:06,780 --> 01:28:10,020
to deluggets we had to go all the way

2336
01:28:08,699 --> 01:28:12,480
here

2337
01:28:10,020 --> 01:28:13,860
so all of this work can be skipped in a

2338
01:28:12,480 --> 01:28:16,320
much much simpler mathematical

2339
01:28:13,860 --> 01:28:18,659
expression that you can Implement here

2340
01:28:16,320 --> 01:28:21,840
so you can give it a shot yourself

2341
01:28:18,659 --> 01:28:23,520
basically look at what exactly is the

2342
01:28:21,840 --> 01:28:26,280
mathematical expression of loss and

2343
01:28:23,520 --> 01:28:29,699
differentiate with respect to the logits

2344
01:28:26,280 --> 01:28:31,980
so let me show you a hint you can of

2345
01:28:29,699 --> 01:28:33,540
course try it fully yourself but if not

2346
01:28:31,980 --> 01:28:36,440
I can give you some hint of how to get

2347
01:28:33,540 --> 01:28:36,440
started mathematically

2348
01:28:36,600 --> 01:28:41,040
so basically What's Happening Here is we

2349
01:28:38,699 --> 01:28:42,060
have logits then there's a softmax that

2350
01:28:41,040 --> 01:28:44,880
takes the logits and gives you

2351
01:28:42,060 --> 01:28:46,739
probabilities then we are using the

2352
01:28:44,880 --> 01:28:50,040
identity of the correct next character

2353
01:28:46,739 --> 01:28:51,780
to pluck out a row of probabilities take

2354
01:28:50,040 --> 01:28:54,060
the negative log of it to get our

2355
01:28:51,780 --> 01:28:56,880
negative block probability and then we

2356
01:28:54,060 --> 01:28:58,139
average up all the log probabilities or

2357
01:28:56,880 --> 01:28:59,820
negative block probabilities to get our

2358
01:28:58,139 --> 01:29:01,620
loss

2359
01:28:59,820 --> 01:29:04,020
so basically what we have is for a

2360
01:29:01,620 --> 01:29:06,060
single individual example rather we have

2361
01:29:04,020 --> 01:29:09,420
that loss is equal to negative log

2362
01:29:06,060 --> 01:29:11,100
probability uh where P here is kind of

2363
01:29:09,420 --> 01:29:14,159
like thought of as a vector of all the

2364
01:29:11,100 --> 01:29:16,500
probabilities so at the Y position where

2365
01:29:14,159 --> 01:29:19,620
Y is the label

2366
01:29:16,500 --> 01:29:23,460
and we have that P here of course is the

2367
01:29:19,620 --> 01:29:25,380
softmax so the ith component of P of

2368
01:29:23,460 --> 01:29:28,320
this probability Vector is just the

2369
01:29:25,380 --> 01:29:31,920
softmax function so raising all the

2370
01:29:28,320 --> 01:29:34,320
logits uh basically to the power of E

2371
01:29:31,920 --> 01:29:35,400
and normalizing so everything comes to

2372
01:29:34,320 --> 01:29:38,340
1.

2373
01:29:35,400 --> 01:29:40,260
now if you write out P of Y here you can

2374
01:29:38,340 --> 01:29:41,340
just write out the soft Max and then

2375
01:29:40,260 --> 01:29:43,080
basically what we're interested in is

2376
01:29:41,340 --> 01:29:47,820
we're interested in the derivative of

2377
01:29:43,080 --> 01:29:51,060
the loss with respect to the I logit

2378
01:29:47,820 --> 01:29:52,739
and so basically it's a d by DLI of this

2379
01:29:51,060 --> 01:29:54,300
expression here

2380
01:29:52,739 --> 01:29:56,580
where we have L indexed with the

2381
01:29:54,300 --> 01:29:58,860
specific label Y and on the bottom we

2382
01:29:56,580 --> 01:30:00,960
have a sum over J of e to the L J and

2383
01:29:58,860 --> 01:30:02,639
the negative block of all that so

2384
01:30:00,960 --> 01:30:04,139
potentially give it a shot pen and paper

2385
01:30:02,639 --> 01:30:07,199
and see if you can actually derive the

2386
01:30:04,139 --> 01:30:09,540
expression for the loss by DLI and then

2387
01:30:07,199 --> 01:30:11,699
we're going to implement it here okay so

2388
01:30:09,540 --> 01:30:13,380
I'm going to give away the result here

2389
01:30:11,699 --> 01:30:17,400
so this is some of the math I did to

2390
01:30:13,380 --> 01:30:19,020
derive the gradients analytically and so

2391
01:30:17,400 --> 01:30:20,520
we see here that I'm just applying the

2392
01:30:19,020 --> 01:30:22,139
rules of calculus from your first or

2393
01:30:20,520 --> 01:30:24,659
second year of bachelor's degree if you

2394
01:30:22,139 --> 01:30:26,340
took it and we see that the expression

2395
01:30:24,659 --> 01:30:27,960
is actually simplify quite a bit you

2396
01:30:26,340 --> 01:30:30,179
have to separate out the analysis in the

2397
01:30:27,960 --> 01:30:32,159
case where the ith index that you're

2398
01:30:30,179 --> 01:30:34,260
interested in inside logits is either

2399
01:30:32,159 --> 01:30:35,699
equal to the label or it's not equal to

2400
01:30:34,260 --> 01:30:37,560
the label and then the expression

2401
01:30:35,699 --> 01:30:39,719
simplify and cancel in a slightly

2402
01:30:37,560 --> 01:30:41,159
different way and what we end up with is

2403
01:30:39,719 --> 01:30:43,020
something very very simple

2404
01:30:41,159 --> 01:30:46,219
and we either end up with basically

2405
01:30:43,020 --> 01:30:49,380
pirai where p is again this Vector of

2406
01:30:46,219 --> 01:30:51,600
probabilities after a soft Max or P at I

2407
01:30:49,380 --> 01:30:53,699
minus 1 where we just simply subtract a

2408
01:30:51,600 --> 01:30:56,340
one but in any case we just need to

2409
01:30:53,699 --> 01:30:58,199
calculate the soft Max p e and then in

2410
01:30:56,340 --> 01:31:00,480
the correct Dimension we need to

2411
01:30:58,199 --> 01:31:03,000
subtract one and that's the gradient the

2412
01:31:00,480 --> 01:31:04,679
form that it takes analytically so let's

2413
01:31:03,000 --> 01:31:06,000
implement this basically and we have to

2414
01:31:04,679 --> 01:31:08,219
keep in mind that this is only done for

2415
01:31:06,000 --> 01:31:09,780
a single example but here we are working

2416
01:31:08,219 --> 01:31:12,360
with batches of examples

2417
01:31:09,780 --> 01:31:14,820
so we have to be careful of that and

2418
01:31:12,360 --> 01:31:17,100
then the loss for a batch is the average

2419
01:31:14,820 --> 01:31:18,480
loss over all the examples so in other

2420
01:31:17,100 --> 01:31:20,639
words is the example for all the

2421
01:31:18,480 --> 01:31:22,739
individual examples is the loss for each

2422
01:31:20,639 --> 01:31:24,659
individual example summed up and then

2423
01:31:22,739 --> 01:31:26,280
divided by n and we have to back

2424
01:31:24,659 --> 01:31:28,020
propagate through that as well and be

2425
01:31:26,280 --> 01:31:30,600
careful with it

2426
01:31:28,020 --> 01:31:32,699
so deluggets is going to be of that soft

2427
01:31:30,600 --> 01:31:35,040
Max

2428
01:31:32,699 --> 01:31:36,900
uh pytorch has a softmax function that

2429
01:31:35,040 --> 01:31:39,420
you can call and we want to apply the

2430
01:31:36,900 --> 01:31:42,600
softmax on the logits and we want to go

2431
01:31:39,420 --> 01:31:44,040
in the dimension that is one so

2432
01:31:42,600 --> 01:31:47,460
basically we want to do the softmax

2433
01:31:44,040 --> 01:31:49,380
along the rows of these logits

2434
01:31:47,460 --> 01:31:52,920
then at the correct positions we need to

2435
01:31:49,380 --> 01:31:54,480
subtract a 1. so delugits at iterating

2436
01:31:52,920 --> 01:31:57,000
over all the rows

2437
01:31:54,480 --> 01:32:00,000
and indexing into the columns

2438
01:31:57,000 --> 01:32:03,060
provided by the correct labels inside YB

2439
01:32:00,000 --> 01:32:05,520
we need to subtract one

2440
01:32:03,060 --> 01:32:07,139
and then finally it's the average loss

2441
01:32:05,520 --> 01:32:09,480
that is the loss and in the average

2442
01:32:07,139 --> 01:32:12,000
there's a one over n of all the losses

2443
01:32:09,480 --> 01:32:14,400
added up and so we need to also

2444
01:32:12,000 --> 01:32:16,500
propagate through that division

2445
01:32:14,400 --> 01:32:19,860
so the gradient has to be scaled down by

2446
01:32:16,500 --> 01:32:22,199
by n as well because of the mean

2447
01:32:19,860 --> 01:32:24,780
but this otherwise should be the result

2448
01:32:22,199 --> 01:32:26,880
so now if we verify this

2449
01:32:24,780 --> 01:32:30,000
we see that we don't get an exact match

2450
01:32:26,880 --> 01:32:33,600
but at the same time the maximum

2451
01:32:30,000 --> 01:32:37,320
difference from logits from pytorch and

2452
01:32:33,600 --> 01:32:39,060
RD logits here is uh on the order of 5e

2453
01:32:37,320 --> 01:32:41,580
negative 9. so it's a tiny tiny number

2454
01:32:39,060 --> 01:32:44,159
so because of floating point wantiness

2455
01:32:41,580 --> 01:32:47,520
we don't get the exact bitwise result

2456
01:32:44,159 --> 01:32:49,080
but we basically get the correct answer

2457
01:32:47,520 --> 01:32:51,060
approximately

2458
01:32:49,080 --> 01:32:52,440
now I'd like to pause here briefly

2459
01:32:51,060 --> 01:32:54,480
before we move on to the next exercise

2460
01:32:52,440 --> 01:32:56,760
because I'd like us to get an intuitive

2461
01:32:54,480 --> 01:32:58,280
sense of what the logits is because it

2462
01:32:56,760 --> 01:33:00,600
has a beautiful and very simple

2463
01:32:58,280 --> 01:33:03,300
explanation honestly

2464
01:33:00,600 --> 01:33:05,219
um so here I'm taking the logits and I'm

2465
01:33:03,300 --> 01:33:07,380
visualizing it and we can see that we

2466
01:33:05,219 --> 01:33:08,460
have a batch of 32 examples of 27

2467
01:33:07,380 --> 01:33:10,800
characters

2468
01:33:08,460 --> 01:33:13,860
and what is the logits intuitively right

2469
01:33:10,800 --> 01:33:15,540
the logits is the probabilities that the

2470
01:33:13,860 --> 01:33:17,639
properties Matrix in the forward pass

2471
01:33:15,540 --> 01:33:19,080
but then here these black squares are

2472
01:33:17,639 --> 01:33:21,719
the positions of the correct indices

2473
01:33:19,080 --> 01:33:24,540
where we subtracted a one

2474
01:33:21,719 --> 01:33:27,840
and so uh what is this doing right these

2475
01:33:24,540 --> 01:33:31,739
are the derivatives on the logits and so

2476
01:33:27,840 --> 01:33:33,300
let's look at just the first row here

2477
01:33:31,739 --> 01:33:34,620
so that's what I'm doing here I'm

2478
01:33:33,300 --> 01:33:36,120
clocking the probabilities of these

2479
01:33:34,620 --> 01:33:38,280
logits and then I'm taking just the

2480
01:33:36,120 --> 01:33:41,340
first row and this is the probability

2481
01:33:38,280 --> 01:33:43,920
row and then the logits of the first row

2482
01:33:41,340 --> 01:33:46,320
and multiplying by n just for us so that

2483
01:33:43,920 --> 01:33:48,540
we don't have the scaling by n in here

2484
01:33:46,320 --> 01:33:50,040
and everything is more interpretable we

2485
01:33:48,540 --> 01:33:52,260
see that it's exactly equal to the

2486
01:33:50,040 --> 01:33:53,639
probability of course but then the

2487
01:33:52,260 --> 01:33:56,400
position of the correct index has a

2488
01:33:53,639 --> 01:33:57,360
minus equals one so minus one on that

2489
01:33:56,400 --> 01:33:59,219
position

2490
01:33:57,360 --> 01:34:01,679
and so notice that

2491
01:33:59,219 --> 01:34:03,420
um if you take Delo Jets at zero and you

2492
01:34:01,679 --> 01:34:06,300
sum it

2493
01:34:03,420 --> 01:34:08,880
it actually sums to zero and so you

2494
01:34:06,300 --> 01:34:12,540
should think of these uh gradients here

2495
01:34:08,880 --> 01:34:15,480
at each cell as like a force

2496
01:34:12,540 --> 01:34:17,820
um we are going to be basically pulling

2497
01:34:15,480 --> 01:34:19,560
down on the probabilities of the

2498
01:34:17,820 --> 01:34:22,679
incorrect characters and we're going to

2499
01:34:19,560 --> 01:34:24,600
be pulling up on the probability at the

2500
01:34:22,679 --> 01:34:29,100
correct index and that's what's

2501
01:34:24,600 --> 01:34:31,679
basically happening in each row and thus

2502
01:34:29,100 --> 01:34:34,679
the amount of push and pull is exactly

2503
01:34:31,679 --> 01:34:36,179
equalized because the sum is zero so the

2504
01:34:34,679 --> 01:34:37,560
amount to which we pull down in the

2505
01:34:36,179 --> 01:34:39,480
probabilities and the demand that we

2506
01:34:37,560 --> 01:34:41,219
push up on the probability of the

2507
01:34:39,480 --> 01:34:43,199
correct character is equal

2508
01:34:41,219 --> 01:34:45,659
so sort of the the repulsion and the

2509
01:34:43,199 --> 01:34:48,199
attraction are equal and think of the

2510
01:34:45,659 --> 01:34:50,340
neural app now as a like a massive uh

2511
01:34:48,199 --> 01:34:52,320
pulley system or something like that

2512
01:34:50,340 --> 01:34:54,360
we're up here on top of the logits and

2513
01:34:52,320 --> 01:34:55,800
we're pulling up we're pulling down the

2514
01:34:54,360 --> 01:34:57,960
properties of Incorrect and pulling up

2515
01:34:55,800 --> 01:34:59,340
the property of the correct and in this

2516
01:34:57,960 --> 01:35:01,320
complicated pulley system because

2517
01:34:59,340 --> 01:35:03,300
everything is mathematically uh just

2518
01:35:01,320 --> 01:35:05,400
determined just think of it as sort of

2519
01:35:03,300 --> 01:35:07,320
like this tension translating to this

2520
01:35:05,400 --> 01:35:09,540
complicating pulling mechanism and then

2521
01:35:07,320 --> 01:35:11,460
eventually we get a tug on the weights

2522
01:35:09,540 --> 01:35:13,500
and the biases and basically in each

2523
01:35:11,460 --> 01:35:15,120
update we just kind of like tug in the

2524
01:35:13,500 --> 01:35:17,280
direction that we like for each of these

2525
01:35:15,120 --> 01:35:19,139
elements and the parameters are slowly

2526
01:35:17,280 --> 01:35:20,639
given in to the tug and that's what

2527
01:35:19,139 --> 01:35:22,080
training in neural net kind of like

2528
01:35:20,639 --> 01:35:24,659
looks like on a high level

2529
01:35:22,080 --> 01:35:26,880
and so I think the the forces of push

2530
01:35:24,659 --> 01:35:29,460
and pull in these gradients are actually

2531
01:35:26,880 --> 01:35:31,020
uh very intuitive here we're pushing and

2532
01:35:29,460 --> 01:35:33,239
pulling on the correct answer and the

2533
01:35:31,020 --> 01:35:34,560
incorrect answers and the amount of

2534
01:35:33,239 --> 01:35:37,139
force that we're applying is actually

2535
01:35:34,560 --> 01:35:39,540
proportional to uh the probabilities

2536
01:35:37,139 --> 01:35:41,219
that came out in the forward pass

2537
01:35:39,540 --> 01:35:43,560
and so for example if our probabilities

2538
01:35:41,219 --> 01:35:45,600
came out exactly correct so they would

2539
01:35:43,560 --> 01:35:48,600
have had zero everywhere except for one

2540
01:35:45,600 --> 01:35:51,300
at the correct uh position then the the

2541
01:35:48,600 --> 01:35:52,920
logits would be all a row of zeros for

2542
01:35:51,300 --> 01:35:55,620
that example there would be no push and

2543
01:35:52,920 --> 01:35:58,139
pull so the amount to which your

2544
01:35:55,620 --> 01:35:59,400
prediction is incorrect is exactly the

2545
01:35:58,139 --> 01:36:01,920
amount by which you're going to get a

2546
01:35:59,400 --> 01:36:04,080
pull or a push in that dimension

2547
01:36:01,920 --> 01:36:05,760
so if you have for example a very

2548
01:36:04,080 --> 01:36:07,320
confidently mispredicted element here

2549
01:36:05,760 --> 01:36:08,280
then

2550
01:36:07,320 --> 01:36:10,620
um what's going to happen is that

2551
01:36:08,280 --> 01:36:12,659
element is going to be pulled down very

2552
01:36:10,620 --> 01:36:14,940
heavily and the correct answer is going

2553
01:36:12,659 --> 01:36:16,620
to be pulled up to the same amount

2554
01:36:14,940 --> 01:36:19,320
and the other characters are not going

2555
01:36:16,620 --> 01:36:21,060
to be influenced too much

2556
01:36:19,320 --> 01:36:23,340
so the amounts to which you mispredict

2557
01:36:21,060 --> 01:36:25,560
is then proportional to the strength of

2558
01:36:23,340 --> 01:36:27,179
the pole and that's happening

2559
01:36:25,560 --> 01:36:29,520
independently in all the dimensions of

2560
01:36:27,179 --> 01:36:30,900
this of this tensor and it's sort of

2561
01:36:29,520 --> 01:36:32,940
very intuitive and varies to think

2562
01:36:30,900 --> 01:36:34,620
through and that's basically the magic

2563
01:36:32,940 --> 01:36:36,600
of the cross-entropy loss and what it's

2564
01:36:34,620 --> 01:36:38,639
doing dynamically in the backward pass

2565
01:36:36,600 --> 01:36:41,159
of the neural net so now we get to

2566
01:36:38,639 --> 01:36:42,540
exercise number three which is a very

2567
01:36:41,159 --> 01:36:43,860
fun exercise

2568
01:36:42,540 --> 01:36:45,480
um depending on your definition of fun

2569
01:36:43,860 --> 01:36:47,219
and we are going to do for batch

2570
01:36:45,480 --> 01:36:49,080
normalization exactly what we did for

2571
01:36:47,219 --> 01:36:51,179
cross entropy loss in exercise number

2572
01:36:49,080 --> 01:36:52,800
two that is we are going to consider it

2573
01:36:51,179 --> 01:36:54,780
as a glued single mathematical

2574
01:36:52,800 --> 01:36:56,580
expression and back propagate through it

2575
01:36:54,780 --> 01:36:58,199
in a very efficient manner because we

2576
01:36:56,580 --> 01:36:59,880
are going to derive a much simpler

2577
01:36:58,199 --> 01:37:01,020
formula for the backward path of batch

2578
01:36:59,880 --> 01:37:02,639
normalization

2579
01:37:01,020 --> 01:37:03,480
and we're going to do that using pen and

2580
01:37:02,639 --> 01:37:05,219
paper

2581
01:37:03,480 --> 01:37:06,780
so previously we've broken up

2582
01:37:05,219 --> 01:37:08,460
bastionalization into all of the little

2583
01:37:06,780 --> 01:37:10,380
intermediate pieces and all the atomic

2584
01:37:08,460 --> 01:37:13,440
operations inside it and then we back

2585
01:37:10,380 --> 01:37:15,780
propagate it through it one by one

2586
01:37:13,440 --> 01:37:18,780
now we just have a single sort of

2587
01:37:15,780 --> 01:37:20,100
forward pass of a batch form and it's

2588
01:37:18,780 --> 01:37:21,780
all glued together

2589
01:37:20,100 --> 01:37:23,040
and we see that we get the exact same

2590
01:37:21,780 --> 01:37:25,260
result as before

2591
01:37:23,040 --> 01:37:27,480
now for the backward pass we'd like to

2592
01:37:25,260 --> 01:37:29,520
also Implement a single formula

2593
01:37:27,480 --> 01:37:30,840
basically for back propagating through

2594
01:37:29,520 --> 01:37:32,219
this entire operation that is the

2595
01:37:30,840 --> 01:37:34,260
bachelorization

2596
01:37:32,219 --> 01:37:37,440
so in the forward pass previously we

2597
01:37:34,260 --> 01:37:39,900
took hpvn the hidden states of the

2598
01:37:37,440 --> 01:37:42,360
pre-batch realization and created H

2599
01:37:39,900 --> 01:37:44,040
preact which is the hidden States just

2600
01:37:42,360 --> 01:37:46,620
before the activation

2601
01:37:44,040 --> 01:37:49,620
in the bachelorization paper each pbn is

2602
01:37:46,620 --> 01:37:51,120
X and each preact is y

2603
01:37:49,620 --> 01:37:54,120
so in the backward pass what we'd like

2604
01:37:51,120 --> 01:37:56,580
to do now is we have DH preact and we'd

2605
01:37:54,120 --> 01:37:57,960
like to produce d h previous

2606
01:37:56,580 --> 01:38:00,060
and we'd like to do that in a very

2607
01:37:57,960 --> 01:38:02,580
efficient manner so that's the name of

2608
01:38:00,060 --> 01:38:05,219
the game calculate the H previan given

2609
01:38:02,580 --> 01:38:07,260
DH preact and for the purposes of this

2610
01:38:05,219 --> 01:38:09,659
exercise we're going to ignore gamma and

2611
01:38:07,260 --> 01:38:11,580
beta and their derivatives because they

2612
01:38:09,659 --> 01:38:14,280
take on a very simple form in a very

2613
01:38:11,580 --> 01:38:18,000
similar way to what we did up above

2614
01:38:14,280 --> 01:38:18,960
so let's calculate this given that right

2615
01:38:18,000 --> 01:38:20,699
here

2616
01:38:18,960 --> 01:38:23,580
so to help you a little bit like I did

2617
01:38:20,699 --> 01:38:26,760
before I started off the implementation

2618
01:38:23,580 --> 01:38:28,020
here on pen and paper and I took two

2619
01:38:26,760 --> 01:38:29,760
sheets of paper to derive the

2620
01:38:28,020 --> 01:38:30,480
mathematical formulas for the backward

2621
01:38:29,760 --> 01:38:33,120
pass

2622
01:38:30,480 --> 01:38:35,400
and basically to set up the problem uh

2623
01:38:33,120 --> 01:38:39,300
just write out the MU Sigma Square

2624
01:38:35,400 --> 01:38:40,679
variance x i hat and Y I exactly as in

2625
01:38:39,300 --> 01:38:41,820
the paper except for the bezel

2626
01:38:40,679 --> 01:38:42,960
correction

2627
01:38:41,820 --> 01:38:44,820
and then

2628
01:38:42,960 --> 01:38:46,080
in a backward pass we have the

2629
01:38:44,820 --> 01:38:48,659
derivative of the loss with respect to

2630
01:38:46,080 --> 01:38:50,639
all the elements of Y and remember that

2631
01:38:48,659 --> 01:38:52,080
Y is a vector there's there's multiple

2632
01:38:50,639 --> 01:38:54,600
numbers here

2633
01:38:52,080 --> 01:38:56,940
so we have all the derivatives with

2634
01:38:54,600 --> 01:38:59,280
respect to all the Y's

2635
01:38:56,940 --> 01:39:01,020
and then there's a demo and a beta and

2636
01:38:59,280 --> 01:39:03,179
this is kind of like the compute graph

2637
01:39:01,020 --> 01:39:06,540
the gamma and the beta there's the X hat

2638
01:39:03,179 --> 01:39:10,500
and then the MU and the sigma squared

2639
01:39:06,540 --> 01:39:13,440
and the X so we have DL by DYI and we

2640
01:39:10,500 --> 01:39:15,239
won't DL by d x i for all the I's in

2641
01:39:13,440 --> 01:39:17,580
these vectors

2642
01:39:15,239 --> 01:39:19,980
so this is the compute graph and you

2643
01:39:17,580 --> 01:39:22,320
have to be careful because I'm trying to

2644
01:39:19,980 --> 01:39:25,440
note here that these are vectors so

2645
01:39:22,320 --> 01:39:29,040
there's many nodes here inside x x hat

2646
01:39:25,440 --> 01:39:30,840
and Y but mu and sigma sorry Sigma

2647
01:39:29,040 --> 01:39:33,300
Square are just individual scalars

2648
01:39:30,840 --> 01:39:34,500
single numbers so you have to be careful

2649
01:39:33,300 --> 01:39:35,880
with that you have to imagine there's

2650
01:39:34,500 --> 01:39:38,460
multiple nodes here or you're going to

2651
01:39:35,880 --> 01:39:40,860
get your math wrong

2652
01:39:38,460 --> 01:39:43,199
um so as an example I would suggest that

2653
01:39:40,860 --> 01:39:44,760
you go in the following order one two

2654
01:39:43,199 --> 01:39:46,860
three four in terms of the back

2655
01:39:44,760 --> 01:39:49,679
propagation so back propagating to X hat

2656
01:39:46,860 --> 01:39:52,080
then into Sigma Square then into mu and

2657
01:39:49,679 --> 01:39:54,060
then into X

2658
01:39:52,080 --> 01:39:55,860
um just like in a topological sort in

2659
01:39:54,060 --> 01:39:57,659
micrograd we would go from right to left

2660
01:39:55,860 --> 01:39:59,760
you're doing the exact same thing except

2661
01:39:57,659 --> 01:40:01,560
you're doing it with symbols and on a

2662
01:39:59,760 --> 01:40:05,280
piece of paper

2663
01:40:01,560 --> 01:40:09,960
so for number one uh I'm not giving away

2664
01:40:05,280 --> 01:40:12,719
too much if you want DL of d x i hat

2665
01:40:09,960 --> 01:40:15,179
then we just take DL by DYI and multiply

2666
01:40:12,719 --> 01:40:17,760
it by gamma because of this expression

2667
01:40:15,179 --> 01:40:21,600
here where any individual Yi is just

2668
01:40:17,760 --> 01:40:23,880
gamma times x i hat plus beta so it

2669
01:40:21,600 --> 01:40:25,380
doesn't help you too much there but this

2670
01:40:23,880 --> 01:40:28,920
gives you basically the derivatives for

2671
01:40:25,380 --> 01:40:31,199
all the X hats and so now try to go

2672
01:40:28,920 --> 01:40:35,340
through this computational graph and

2673
01:40:31,199 --> 01:40:38,100
derive what is DL by D Sigma Square

2674
01:40:35,340 --> 01:40:39,840
and then what is DL by B mu and then one

2675
01:40:38,100 --> 01:40:42,300
is D L by DX

2676
01:40:39,840 --> 01:40:44,100
eventually so give it a go and I'm going

2677
01:40:42,300 --> 01:40:46,800
to be revealing the answer one piece at

2678
01:40:44,100 --> 01:40:48,719
a time okay so to get DL by D Sigma

2679
01:40:46,800 --> 01:40:51,120
Square we have to remember again like I

2680
01:40:48,719 --> 01:40:52,320
mentioned that there are many excess X

2681
01:40:51,120 --> 01:40:54,239
hats here

2682
01:40:52,320 --> 01:40:55,860
and remember that Sigma square is just a

2683
01:40:54,239 --> 01:40:59,580
single individual number here

2684
01:40:55,860 --> 01:41:01,800
so when we look at the expression

2685
01:40:59,580 --> 01:41:03,060
for the L by D Sigma Square

2686
01:41:01,800 --> 01:41:08,100
we have that we have to actually

2687
01:41:03,060 --> 01:41:10,679
consider all the possible paths that um

2688
01:41:08,100 --> 01:41:13,080
we basically have that there's many X

2689
01:41:10,679 --> 01:41:15,420
hats and they all feed off from they all

2690
01:41:13,080 --> 01:41:17,219
depend on Sigma Square so Sigma square

2691
01:41:15,420 --> 01:41:19,020
has a large fan out there's lots of

2692
01:41:17,219 --> 01:41:20,580
arrows coming out from Sigma square into

2693
01:41:19,020 --> 01:41:22,380
all the X hats

2694
01:41:20,580 --> 01:41:24,840
and then there's a back propagating

2695
01:41:22,380 --> 01:41:26,820
signal from each X hat into Sigma square

2696
01:41:24,840 --> 01:41:29,940
and that's why we actually need to sum

2697
01:41:26,820 --> 01:41:30,840
over all those I's from I equal to 1 to

2698
01:41:29,940 --> 01:41:35,580
m

2699
01:41:30,840 --> 01:41:36,960
of the DL by d x i hat which is the

2700
01:41:35,580 --> 01:41:40,080
global gradient

2701
01:41:36,960 --> 01:41:42,119
times the x i Hat by D Sigma Square

2702
01:41:40,080 --> 01:41:44,580
which is the local gradient

2703
01:41:42,119 --> 01:41:46,440
of this operation here

2704
01:41:44,580 --> 01:41:48,420
and then mathematically I'm just working

2705
01:41:46,440 --> 01:41:51,000
it out here and I'm simplifying and you

2706
01:41:48,420 --> 01:41:52,619
get a certain expression for DL by D

2707
01:41:51,000 --> 01:41:53,940
Sigma square and we're going to be using

2708
01:41:52,619 --> 01:41:56,760
this expression when we back propagate

2709
01:41:53,940 --> 01:41:58,080
into mu and then eventually into X so

2710
01:41:56,760 --> 01:42:01,800
now let's continue our back propagation

2711
01:41:58,080 --> 01:42:04,739
into mu so what is D L by D mu now again

2712
01:42:01,800 --> 01:42:07,199
be careful that mu influences X hat and

2713
01:42:04,739 --> 01:42:09,420
X hat is actually lots of values so for

2714
01:42:07,199 --> 01:42:10,800
example if our mini batch size is 32 as

2715
01:42:09,420 --> 01:42:13,440
it is in our example that we were

2716
01:42:10,800 --> 01:42:16,560
working on then this is 32 numbers and

2717
01:42:13,440 --> 01:42:18,060
32 arrows going back to mu and then mu

2718
01:42:16,560 --> 01:42:19,619
going to Sigma square is just a single

2719
01:42:18,060 --> 01:42:22,380
Arrow because Sigma square is a scalar

2720
01:42:19,619 --> 01:42:25,199
so in total there are 33 arrows

2721
01:42:22,380 --> 01:42:27,480
emanating from you and then all of them

2722
01:42:25,199 --> 01:42:29,340
have gradients coming into mu and they

2723
01:42:27,480 --> 01:42:31,739
all need to be summed up

2724
01:42:29,340 --> 01:42:34,380
and so that's why when we look at the

2725
01:42:31,739 --> 01:42:37,440
expression for DL by D mu I am summing

2726
01:42:34,380 --> 01:42:40,320
up over all the gradients of DL by d x i

2727
01:42:37,440 --> 01:42:43,020
hat times the x i Hat by being mu

2728
01:42:40,320 --> 01:42:45,360
uh so that's the that's this arrow and

2729
01:42:43,020 --> 01:42:47,219
that's 32 arrows here and then plus the

2730
01:42:45,360 --> 01:42:49,560
one Arrow from here which is the L by

2731
01:42:47,219 --> 01:42:50,520
the sigma Square Times the sigma squared

2732
01:42:49,560 --> 01:42:52,320
by D mu

2733
01:42:50,520 --> 01:42:54,000
so now we have to work out that

2734
01:42:52,320 --> 01:42:55,260
expression and let me just reveal the

2735
01:42:54,000 --> 01:42:58,139
rest of it

2736
01:42:55,260 --> 01:43:00,060
uh simplifying here is not complicated

2737
01:42:58,139 --> 01:43:01,199
the first term and you just get an

2738
01:43:00,060 --> 01:43:02,400
expression here

2739
01:43:01,199 --> 01:43:03,360
for the second term though there's

2740
01:43:02,400 --> 01:43:04,380
something really interesting that

2741
01:43:03,360 --> 01:43:06,360
happens

2742
01:43:04,380 --> 01:43:08,420
when we look at the sigma squared by D

2743
01:43:06,360 --> 01:43:11,760
mu and we simplify

2744
01:43:08,420 --> 01:43:14,340
at one point if we assume that in a

2745
01:43:11,760 --> 01:43:17,119
special case where mu is actually the

2746
01:43:14,340 --> 01:43:20,340
average of X I's as it is in this case

2747
01:43:17,119 --> 01:43:22,080
then if we plug that in then actually

2748
01:43:20,340 --> 01:43:24,780
the gradient vanishes and becomes

2749
01:43:22,080 --> 01:43:26,460
exactly zero and that makes the entire

2750
01:43:24,780 --> 01:43:29,400
second term cancel

2751
01:43:26,460 --> 01:43:30,659
and so these uh if you just have a

2752
01:43:29,400 --> 01:43:33,000
mathematical expression like this and

2753
01:43:30,659 --> 01:43:35,699
you look at D Sigma Square by D mu you

2754
01:43:33,000 --> 01:43:37,679
would get some mathematical formula for

2755
01:43:35,699 --> 01:43:39,900
how mu impacts Sigma Square

2756
01:43:37,679 --> 01:43:42,000
but if it is the special case that Nu is

2757
01:43:39,900 --> 01:43:43,800
actually equal to the average as it is

2758
01:43:42,000 --> 01:43:45,300
in the case of pastoralization that

2759
01:43:43,800 --> 01:43:48,000
gradient will actually vanish and become

2760
01:43:45,300 --> 01:43:49,280
zero so the whole term cancels and we

2761
01:43:48,000 --> 01:43:52,860
just get a fairly straightforward

2762
01:43:49,280 --> 01:43:54,719
expression here for DL by D mu okay and

2763
01:43:52,860 --> 01:43:57,659
now we get to the craziest part which is

2764
01:43:54,719 --> 01:43:59,100
uh deriving DL by dxi which is

2765
01:43:57,659 --> 01:44:00,960
ultimately what we're after

2766
01:43:59,100 --> 01:44:03,360
now let's count

2767
01:44:00,960 --> 01:44:05,280
first of all how many numbers are there

2768
01:44:03,360 --> 01:44:08,159
inside X as I mentioned there are 32

2769
01:44:05,280 --> 01:44:09,179
numbers there are 32 Little X I's and

2770
01:44:08,159 --> 01:44:11,040
let's count the number of arrows

2771
01:44:09,179 --> 01:44:13,320
emanating from each x i

2772
01:44:11,040 --> 01:44:14,760
there's an arrow going to Mu an arrow

2773
01:44:13,320 --> 01:44:16,619
going to Sigma Square

2774
01:44:14,760 --> 01:44:19,199
and then there's an arrow going to X hat

2775
01:44:16,619 --> 01:44:20,280
but this Arrow here let's scrutinize

2776
01:44:19,199 --> 01:44:23,460
that a little bit

2777
01:44:20,280 --> 01:44:27,300
each x i hat is just a function of x i

2778
01:44:23,460 --> 01:44:29,159
and all the other scalars so x i hat

2779
01:44:27,300 --> 01:44:30,300
only depends on x i and none of the

2780
01:44:29,159 --> 01:44:32,639
other X's

2781
01:44:30,300 --> 01:44:34,739
and so therefore there are actually in

2782
01:44:32,639 --> 01:44:37,199
this single Arrow there are 32 arrows

2783
01:44:34,739 --> 01:44:39,000
but those 32 arrows are going exactly

2784
01:44:37,199 --> 01:44:40,920
parallel they don't interfere and

2785
01:44:39,000 --> 01:44:42,960
they're just going parallel between x

2786
01:44:40,920 --> 01:44:44,460
and x hat you can look at it that way

2787
01:44:42,960 --> 01:44:47,159
and so how many arrows are emanating

2788
01:44:44,460 --> 01:44:50,940
from each x i there are three arrows mu

2789
01:44:47,159 --> 01:44:53,159
Sigma squared and the associated X hat

2790
01:44:50,940 --> 01:44:55,560
and so in back propagation we now need

2791
01:44:53,159 --> 01:44:57,960
to apply the chain rule and we need to

2792
01:44:55,560 --> 01:44:59,820
add up those three contributions

2793
01:44:57,960 --> 01:45:02,100
so here's what that looks like if I just

2794
01:44:59,820 --> 01:45:04,679
write that out

2795
01:45:02,100 --> 01:45:06,780
we have uh we're going through we're

2796
01:45:04,679 --> 01:45:09,179
chaining through mu Sigma square and

2797
01:45:06,780 --> 01:45:10,380
through X hat and those three terms are

2798
01:45:09,179 --> 01:45:13,380
just here

2799
01:45:10,380 --> 01:45:15,480
now we already have three of these we

2800
01:45:13,380 --> 01:45:17,880
have d l by d x i hat

2801
01:45:15,480 --> 01:45:19,920
we have DL by D mu which we derived here

2802
01:45:17,880 --> 01:45:22,440
and we have DL by D Sigma Square which

2803
01:45:19,920 --> 01:45:23,639
we derived here but we need three other

2804
01:45:22,440 --> 01:45:26,820
terms here

2805
01:45:23,639 --> 01:45:28,440
the this one this one and this one so I

2806
01:45:26,820 --> 01:45:29,880
invite you to try to derive them it's

2807
01:45:28,440 --> 01:45:31,560
not that complicated you're just looking

2808
01:45:29,880 --> 01:45:34,980
at these Expressions here and

2809
01:45:31,560 --> 01:45:38,900
differentiating with respect to x i

2810
01:45:34,980 --> 01:45:38,900
so give it a shot but here's the result

2811
01:45:39,420 --> 01:45:42,840
or at least what I got

2812
01:45:41,699 --> 01:45:44,100
um

2813
01:45:42,840 --> 01:45:45,540
yeah I'm just I'm just differentiating

2814
01:45:44,100 --> 01:45:47,100
with respect to x i for all these

2815
01:45:45,540 --> 01:45:48,420
expressions and honestly I don't think

2816
01:45:47,100 --> 01:45:50,520
there's anything too tricky here it's

2817
01:45:48,420 --> 01:45:52,260
basic calculus

2818
01:45:50,520 --> 01:45:53,699
now it gets a little bit more tricky is

2819
01:45:52,260 --> 01:45:55,800
we are now going to plug everything

2820
01:45:53,699 --> 01:45:57,420
together so all of these terms

2821
01:45:55,800 --> 01:45:59,580
multiplied with all of these terms and

2822
01:45:57,420 --> 01:46:01,500
add it up according to this formula and

2823
01:45:59,580 --> 01:46:04,080
that gets a little bit hairy so what

2824
01:46:01,500 --> 01:46:05,280
ends up happening is

2825
01:46:04,080 --> 01:46:08,460
uh

2826
01:46:05,280 --> 01:46:09,900
you get a large expression and the thing

2827
01:46:08,460 --> 01:46:12,480
to be very careful with here of course

2828
01:46:09,900 --> 01:46:15,719
is we are working with a DL by dxi for

2829
01:46:12,480 --> 01:46:17,159
specific I here but when we are plugging

2830
01:46:15,719 --> 01:46:18,840
in some of these terms

2831
01:46:17,159 --> 01:46:19,500
like say

2832
01:46:18,840 --> 01:46:22,260
um

2833
01:46:19,500 --> 01:46:24,780
this term here deal by D signal squared

2834
01:46:22,260 --> 01:46:26,280
you see how the L by D Sigma squared I

2835
01:46:24,780 --> 01:46:29,219
end up with an expression and I'm

2836
01:46:26,280 --> 01:46:31,860
iterating over little I's here but I

2837
01:46:29,219 --> 01:46:33,900
can't use I as the variable when I plug

2838
01:46:31,860 --> 01:46:35,159
in here because this is a different I

2839
01:46:33,900 --> 01:46:37,380
from this eye

2840
01:46:35,159 --> 01:46:39,420
this I here is just a place or like a

2841
01:46:37,380 --> 01:46:41,880
local variable for for a for Loop in

2842
01:46:39,420 --> 01:46:43,920
here so here when I plug that in you

2843
01:46:41,880 --> 01:46:45,659
notice that I rename the I to a j

2844
01:46:43,920 --> 01:46:48,719
because I need to make sure that this J

2845
01:46:45,659 --> 01:46:50,400
is not that this J is not this I this J

2846
01:46:48,719 --> 01:46:53,280
is like like a little local iterator

2847
01:46:50,400 --> 01:46:54,360
over 32 terms and so you have to be

2848
01:46:53,280 --> 01:46:56,100
careful with that when you're plugging

2849
01:46:54,360 --> 01:46:58,440
in the expressions from here to here you

2850
01:46:56,100 --> 01:47:00,119
may have to rename eyes into J's and you

2851
01:46:58,440 --> 01:47:04,199
have to be very careful what is actually

2852
01:47:00,119 --> 01:47:07,199
an I with respect to the L by t x i

2853
01:47:04,199 --> 01:47:08,699
so some of these are J's some of these

2854
01:47:07,199 --> 01:47:11,460
are I's

2855
01:47:08,699 --> 01:47:13,800
and then we simplify this expression

2856
01:47:11,460 --> 01:47:15,840
and I guess like the big thing to notice

2857
01:47:13,800 --> 01:47:16,860
here is a bunch of terms just kind of

2858
01:47:15,840 --> 01:47:18,719
come out to the front and you can

2859
01:47:16,860 --> 01:47:19,920
refactor them there's a sigma squared

2860
01:47:18,719 --> 01:47:21,960
plus Epsilon raised to the power of

2861
01:47:19,920 --> 01:47:23,760
negative three over two uh this Sigma

2862
01:47:21,960 --> 01:47:25,920
squared plus Epsilon can be actually

2863
01:47:23,760 --> 01:47:28,380
separated out into three terms each of

2864
01:47:25,920 --> 01:47:30,300
them are Sigma squared plus Epsilon to

2865
01:47:28,380 --> 01:47:33,659
the negative one over two so the three

2866
01:47:30,300 --> 01:47:35,159
of them multiplied is equal to this and

2867
01:47:33,659 --> 01:47:37,739
then those three terms can go different

2868
01:47:35,159 --> 01:47:39,060
places because of the multiplication so

2869
01:47:37,739 --> 01:47:42,600
one of them actually comes out to the

2870
01:47:39,060 --> 01:47:45,420
front and will end up here outside one

2871
01:47:42,600 --> 01:47:47,699
of them joins up with this term and one

2872
01:47:45,420 --> 01:47:49,080
of them joins up with this other term

2873
01:47:47,699 --> 01:47:51,060
and then when you simplify the

2874
01:47:49,080 --> 01:47:52,380
expression you'll notice that some of

2875
01:47:51,060 --> 01:47:54,000
these terms that are coming out are just

2876
01:47:52,380 --> 01:47:56,219
the x i hats

2877
01:47:54,000 --> 01:47:57,119
so you can simplify just by rewriting

2878
01:47:56,219 --> 01:47:58,980
that

2879
01:47:57,119 --> 01:48:00,360
and what we end up with at the end is a

2880
01:47:58,980 --> 01:48:02,280
fairly simple mathematical expression

2881
01:48:00,360 --> 01:48:05,100
over here that I cannot simplify further

2882
01:48:02,280 --> 01:48:06,960
but basically you'll notice that it only

2883
01:48:05,100 --> 01:48:10,380
uses the stuff we have and it derives

2884
01:48:06,960 --> 01:48:13,020
the thing we need so we have the L by d

2885
01:48:10,380 --> 01:48:15,719
y for all the I's and those are used

2886
01:48:13,020 --> 01:48:17,340
plenty of times here and also in

2887
01:48:15,719 --> 01:48:19,260
addition what we're using is these x i

2888
01:48:17,340 --> 01:48:20,940
hats and XJ hats and they just come from

2889
01:48:19,260 --> 01:48:22,920
the forward pass

2890
01:48:20,940 --> 01:48:25,739
and otherwise this is a simple

2891
01:48:22,920 --> 01:48:27,960
expression and it gives us DL by d x i

2892
01:48:25,739 --> 01:48:29,280
for all the I's and that's ultimately

2893
01:48:27,960 --> 01:48:32,820
what we're interested in

2894
01:48:29,280 --> 01:48:34,980
so that's the end of Bachelor backward

2895
01:48:32,820 --> 01:48:36,840
pass analytically let's now implement

2896
01:48:34,980 --> 01:48:38,460
this final result

2897
01:48:36,840 --> 01:48:41,580
okay so I implemented the expression

2898
01:48:38,460 --> 01:48:43,560
into a single line of code here and you

2899
01:48:41,580 --> 01:48:44,940
can see that the max diff is Tiny so

2900
01:48:43,560 --> 01:48:48,480
this is the correct implementation of

2901
01:48:44,940 --> 01:48:50,639
this formula now I'll just uh

2902
01:48:48,480 --> 01:48:52,380
basically tell you that getting this

2903
01:48:50,639 --> 01:48:54,659
formula here from this mathematical

2904
01:48:52,380 --> 01:48:56,219
expression was not trivial and there's a

2905
01:48:54,659 --> 01:48:58,380
lot going on packed into this one

2906
01:48:56,219 --> 01:49:00,780
formula and this is a whole exercise by

2907
01:48:58,380 --> 01:49:03,060
itself because you have to consider the

2908
01:49:00,780 --> 01:49:05,219
fact that this formula here is just for

2909
01:49:03,060 --> 01:49:07,440
a single neuron and a batch of 32

2910
01:49:05,219 --> 01:49:10,139
examples but what I'm doing here is I'm

2911
01:49:07,440 --> 01:49:11,880
actually we actually have 64 neurons and

2912
01:49:10,139 --> 01:49:14,280
so this expression has to in parallel

2913
01:49:11,880 --> 01:49:16,139
evaluate the bathroom backward pass for

2914
01:49:14,280 --> 01:49:18,420
all of those 64 neurons in parallel

2915
01:49:16,139 --> 01:49:20,280
independently so this has to happen

2916
01:49:18,420 --> 01:49:20,820
basically in every single

2917
01:49:20,280 --> 01:49:24,420
um

2918
01:49:20,820 --> 01:49:26,460
column of the inputs here

2919
01:49:24,420 --> 01:49:28,199
and in addition to that you see how

2920
01:49:26,460 --> 01:49:29,400
there are a bunch of sums here and we

2921
01:49:28,199 --> 01:49:31,619
need to make sure that when I do those

2922
01:49:29,400 --> 01:49:33,360
sums that they broadcast correctly onto

2923
01:49:31,619 --> 01:49:35,280
everything else that's here

2924
01:49:33,360 --> 01:49:36,780
and so getting this expression is just

2925
01:49:35,280 --> 01:49:37,980
like highly non-trivial and I invite you

2926
01:49:36,780 --> 01:49:39,360
to basically look through it and step

2927
01:49:37,980 --> 01:49:43,100
through it and it's a whole exercise to

2928
01:49:39,360 --> 01:49:45,360
make sure that this this checks out but

2929
01:49:43,100 --> 01:49:46,560
once all the shapes are green and once

2930
01:49:45,360 --> 01:49:48,780
you convince yourself that it's correct

2931
01:49:46,560 --> 01:49:50,580
you can also verify that Patrick's gets

2932
01:49:48,780 --> 01:49:51,900
the exact same answer as well and so

2933
01:49:50,580 --> 01:49:53,880
that gives you a lot of peace of mind

2934
01:49:51,900 --> 01:49:55,860
that this mathematical formula is

2935
01:49:53,880 --> 01:49:57,960
correctly implemented here and

2936
01:49:55,860 --> 01:50:00,500
broadcasted correctly and replicated in

2937
01:49:57,960 --> 01:50:03,540
parallel for all of the 64 neurons

2938
01:50:00,500 --> 01:50:05,760
inside this bastrum layer okay and

2939
01:50:03,540 --> 01:50:08,280
finally exercise number four asks you to

2940
01:50:05,760 --> 01:50:10,320
put it all together and uh here we have

2941
01:50:08,280 --> 01:50:11,880
a redefinition of the entire problem so

2942
01:50:10,320 --> 01:50:13,920
you see that we reinitialize the neural

2943
01:50:11,880 --> 01:50:15,960
nut from scratch and everything and then

2944
01:50:13,920 --> 01:50:18,239
here instead of calling loss that

2945
01:50:15,960 --> 01:50:20,100
backward we want to have the manual back

2946
01:50:18,239 --> 01:50:23,100
propagation here as we derived It Up

2947
01:50:20,100 --> 01:50:25,080
Above so go up copy paste all the chunks

2948
01:50:23,100 --> 01:50:26,940
of code that we've already derived put

2949
01:50:25,080 --> 01:50:28,800
them here and drive your own gradients

2950
01:50:26,940 --> 01:50:31,139
and then optimize this neural nut

2951
01:50:28,800 --> 01:50:33,060
basically using your own gradients all

2952
01:50:31,139 --> 01:50:34,980
the way to the calibration of The

2953
01:50:33,060 --> 01:50:36,840
Bachelor and the evaluation of the loss

2954
01:50:34,980 --> 01:50:38,400
and I was able to achieve quite a good

2955
01:50:36,840 --> 01:50:40,560
loss basically the same loss you would

2956
01:50:38,400 --> 01:50:41,940
achieve before and that shouldn't be

2957
01:50:40,560 --> 01:50:44,340
surprising because all we've done is

2958
01:50:41,940 --> 01:50:45,840
we've really gotten to Lost That

2959
01:50:44,340 --> 01:50:46,679
backward and we've pulled out all the

2960
01:50:45,840 --> 01:50:49,139
code

2961
01:50:46,679 --> 01:50:50,460
and inserted it here but those gradients

2962
01:50:49,139 --> 01:50:52,320
are identical and everything is

2963
01:50:50,460 --> 01:50:54,480
identical and the results are identical

2964
01:50:52,320 --> 01:50:56,460
it's just that we have full visibility

2965
01:50:54,480 --> 01:50:58,500
on exactly what goes on under the hood

2966
01:50:56,460 --> 01:51:02,040
I'll plot that backward in this specific

2967
01:50:58,500 --> 01:51:04,320
case and this is all of our code this is

2968
01:51:02,040 --> 01:51:06,119
the full backward pass using basically

2969
01:51:04,320 --> 01:51:08,219
the simplified backward pass for the

2970
01:51:06,119 --> 01:51:10,380
cross entropy loss and the mass

2971
01:51:08,219 --> 01:51:13,199
generalization so back propagating

2972
01:51:10,380 --> 01:51:15,600
through cross entropy the second layer

2973
01:51:13,199 --> 01:51:16,980
the 10 H nonlinearity the batch

2974
01:51:15,600 --> 01:51:19,199
normalization

2975
01:51:16,980 --> 01:51:21,179
uh through the first layer and through

2976
01:51:19,199 --> 01:51:23,219
the embedding and so you see that this

2977
01:51:21,179 --> 01:51:25,139
is only maybe what is this 20 lines of

2978
01:51:23,219 --> 01:51:27,480
code or something like that and that's

2979
01:51:25,139 --> 01:51:30,060
what gives us gradients and now we can

2980
01:51:27,480 --> 01:51:31,980
potentially erase losses backward so the

2981
01:51:30,060 --> 01:51:33,840
way I have the code set up is you should

2982
01:51:31,980 --> 01:51:36,060
be able to run this entire cell once you

2983
01:51:33,840 --> 01:51:37,980
fill this in and this will run for only

2984
01:51:36,060 --> 01:51:39,540
100 iterations and then break

2985
01:51:37,980 --> 01:51:41,100
and it breaks because it gives you an

2986
01:51:39,540 --> 01:51:43,080
opportunity to check your gradients

2987
01:51:41,100 --> 01:51:46,320
against pytorch

2988
01:51:43,080 --> 01:51:48,960
so here our gradients we see are not

2989
01:51:46,320 --> 01:51:51,000
exactly equal they are approximately

2990
01:51:48,960 --> 01:51:52,980
equal and the differences are tiny

2991
01:51:51,000 --> 01:51:54,239
wanting negative 9 or so and I don't

2992
01:51:52,980 --> 01:51:56,040
exactly know where they're coming from

2993
01:51:54,239 --> 01:51:57,300
to be honest

2994
01:51:56,040 --> 01:51:59,460
um so once we have some confidence that

2995
01:51:57,300 --> 01:52:01,320
the gradients are basically correct we

2996
01:51:59,460 --> 01:52:05,760
can take out the gradient tracking

2997
01:52:01,320 --> 01:52:07,739
we can disable this breaking statement

2998
01:52:05,760 --> 01:52:10,619
and then we can

2999
01:52:07,739 --> 01:52:13,199
basically disable lost of backward we

3000
01:52:10,619 --> 01:52:14,460
don't need it anymore it feels amazing

3001
01:52:13,199 --> 01:52:16,679
to say that

3002
01:52:14,460 --> 01:52:18,420
and then here when we are doing the

3003
01:52:16,679 --> 01:52:21,780
update we're not going to use P dot grad

3004
01:52:18,420 --> 01:52:22,920
this is the old way of pytorch we don't

3005
01:52:21,780 --> 01:52:25,440
have that anymore because we're not

3006
01:52:22,920 --> 01:52:27,659
doing backward we are going to use this

3007
01:52:25,440 --> 01:52:29,100
update where we you see that I'm

3008
01:52:27,659 --> 01:52:30,900
iterating over

3009
01:52:29,100 --> 01:52:32,580
I've arranged the grads to be in the

3010
01:52:30,900 --> 01:52:34,679
same order as the parameters and I'm

3011
01:52:32,580 --> 01:52:37,139
zipping them up the gradients and the

3012
01:52:34,679 --> 01:52:38,820
parameters into p and grad and then here

3013
01:52:37,139 --> 01:52:40,860
I'm going to step with just the grad

3014
01:52:38,820 --> 01:52:43,739
that we derived manually

3015
01:52:40,860 --> 01:52:46,560
so the last piece

3016
01:52:43,739 --> 01:52:49,679
um is that none of this now requires

3017
01:52:46,560 --> 01:52:51,420
gradients from pytorch and so one thing

3018
01:52:49,679 --> 01:52:52,320
you can do here

3019
01:52:51,420 --> 01:52:56,219
um

3020
01:52:52,320 --> 01:52:58,139
is you can do with no grad and offset

3021
01:52:56,219 --> 01:52:59,400
this whole code block

3022
01:52:58,139 --> 01:53:00,600
and really what you're saying is you're

3023
01:52:59,400 --> 01:53:02,040
telling Pat George that hey I'm not

3024
01:53:00,600 --> 01:53:03,960
going to call backward on any of this

3025
01:53:02,040 --> 01:53:05,460
and this allows pytorch to be a bit more

3026
01:53:03,960 --> 01:53:07,800
efficient with all of it

3027
01:53:05,460 --> 01:53:09,900
and then we should be able to just uh

3028
01:53:07,800 --> 01:53:11,880
run this

3029
01:53:09,900 --> 01:53:13,980
and

3030
01:53:11,880 --> 01:53:16,860
it's running

3031
01:53:13,980 --> 01:53:18,000
and you see that losses backward is

3032
01:53:16,860 --> 01:53:20,159
commented out

3033
01:53:18,000 --> 01:53:23,520
and we're optimizing

3034
01:53:20,159 --> 01:53:25,320
so we're going to leave this run and uh

3035
01:53:23,520 --> 01:53:27,060
hopefully we get a good result

3036
01:53:25,320 --> 01:53:28,679
okay so I allowed the neural net to

3037
01:53:27,060 --> 01:53:31,199
finish optimization

3038
01:53:28,679 --> 01:53:33,239
then here I calibrate the bachelor

3039
01:53:31,199 --> 01:53:35,940
parameters because I did not keep track

3040
01:53:33,239 --> 01:53:37,260
of the running mean and very variants in

3041
01:53:35,940 --> 01:53:39,840
their training Loop

3042
01:53:37,260 --> 01:53:40,980
then here I ran the loss and you see

3043
01:53:39,840 --> 01:53:42,780
that we actually obtained a pretty good

3044
01:53:40,980 --> 01:53:43,980
loss very similar to what we've achieved

3045
01:53:42,780 --> 01:53:45,480
before

3046
01:53:43,980 --> 01:53:47,639
and then here I'm sampling from the

3047
01:53:45,480 --> 01:53:49,739
model and we see some of the name like

3048
01:53:47,639 --> 01:53:52,320
gibberish that we're sort of used to so

3049
01:53:49,739 --> 01:53:54,780
basically the model worked and samples

3050
01:53:52,320 --> 01:53:56,340
uh pretty decent results compared to

3051
01:53:54,780 --> 01:53:58,440
what we were used to so everything is

3052
01:53:56,340 --> 01:54:00,420
the same but of course the big deal is

3053
01:53:58,440 --> 01:54:02,219
that we did not use lots of backward we

3054
01:54:00,420 --> 01:54:04,320
did not use package Auto grad and we

3055
01:54:02,219 --> 01:54:05,100
estimated our gradients ourselves by

3056
01:54:04,320 --> 01:54:06,659
hand

3057
01:54:05,100 --> 01:54:08,699
and so hopefully you're looking at this

3058
01:54:06,659 --> 01:54:10,139
the backward pass of this neural net and

3059
01:54:08,699 --> 01:54:12,480
you're thinking to yourself actually

3060
01:54:10,139 --> 01:54:13,080
that's not too complicated

3061
01:54:12,480 --> 01:54:15,179
um

3062
01:54:13,080 --> 01:54:17,280
each one of these layers is like three

3063
01:54:15,179 --> 01:54:18,719
lines of code or something like that and

3064
01:54:17,280 --> 01:54:20,639
most of it is fairly straightforward

3065
01:54:18,719 --> 01:54:22,800
potentially with the notable exception

3066
01:54:20,639 --> 01:54:25,320
of the batch normalization backward pass

3067
01:54:22,800 --> 01:54:26,699
otherwise it's pretty good okay and

3068
01:54:25,320 --> 01:54:29,159
that's everything I wanted to cover for

3069
01:54:26,699 --> 01:54:31,080
this lecture so hopefully you found this

3070
01:54:29,159 --> 01:54:33,000
interesting and what I liked about it

3071
01:54:31,080 --> 01:54:34,860
honestly is that it gave us a very nice

3072
01:54:33,000 --> 01:54:36,600
diversity of layers to back propagate

3073
01:54:34,860 --> 01:54:38,340
through and

3074
01:54:36,600 --> 01:54:39,780
um I think it gives a pretty nice and

3075
01:54:38,340 --> 01:54:41,580
comprehensive sense of how these

3076
01:54:39,780 --> 01:54:43,860
backward passes are implemented and how

3077
01:54:41,580 --> 01:54:45,179
they work and you'd be able to derive

3078
01:54:43,860 --> 01:54:46,800
them yourself but of course in practice

3079
01:54:45,179 --> 01:54:49,139
you probably don't want to and you want

3080
01:54:46,800 --> 01:54:51,000
to use the pythonograd but hopefully you

3081
01:54:49,139 --> 01:54:52,679
have some intuition about how gradients

3082
01:54:51,000 --> 01:54:55,139
flow backwards through the neural net

3083
01:54:52,679 --> 01:54:56,520
starting at the loss and how they flow

3084
01:54:55,139 --> 01:54:58,199
through all the variables and all the

3085
01:54:56,520 --> 01:55:00,719
intermediate results

3086
01:54:58,199 --> 01:55:02,280
and if you understood a good chunk of it

3087
01:55:00,719 --> 01:55:03,719
and if you have a sense of that then you

3088
01:55:02,280 --> 01:55:06,060
can count yourself as one of these buff

3089
01:55:03,719 --> 01:55:09,119
doji's on the left instead of the uh

3090
01:55:06,060 --> 01:55:10,560
those on the right here now in the next

3091
01:55:09,119 --> 01:55:13,440
lecture we're actually going to go to

3092
01:55:10,560 --> 01:55:16,260
recurrent neural nuts lstms and all the

3093
01:55:13,440 --> 01:55:17,820
other variants of RNs and we're going to

3094
01:55:16,260 --> 01:55:19,679
start to complexify the architecture and

3095
01:55:17,820 --> 01:55:21,540
start to achieve better uh log

3096
01:55:19,679 --> 01:55:25,280
likelihoods and so I'm really looking

3097
01:55:21,540 --> 01:55:25,280
forward to that and I'll see you then

