1
00:00:00,000 --> 00:00:04,740
hi everyone today we are continuing our

2
00:00:02,520 --> 00:00:05,819
implementation of makemore now in the

3
00:00:04,740 --> 00:00:07,379
last lecture we implemented the

4
00:00:05,819 --> 00:00:09,599
multi-layer perceptron along the lines

5
00:00:07,379 --> 00:00:11,519
of benjiotyle 2003 for character level

6
00:00:09,599 --> 00:00:13,620
language modeling so we followed this

7
00:00:11,519 --> 00:00:15,599
paper took in a few characters in the

8
00:00:13,620 --> 00:00:17,279
past and used an MLP to predict the next

9
00:00:15,599 --> 00:00:18,779
character in a sequence

10
00:00:17,279 --> 00:00:20,640
so what we'd like to do now is we'd like

11
00:00:18,779 --> 00:00:22,439
to move on to more complex and larger

12
00:00:20,640 --> 00:00:24,119
neural networks like recurrent neural

13
00:00:22,439 --> 00:00:26,279
networks and their variations like the

14
00:00:24,119 --> 00:00:28,260
Gru lstm and so on

15
00:00:26,279 --> 00:00:29,820
now before we do that though we have to

16
00:00:28,260 --> 00:00:32,160
stick around the level of multilia

17
00:00:29,820 --> 00:00:33,540
perceptron for a bit longer and I'd like

18
00:00:32,160 --> 00:00:35,219
to do this because I would like us to

19
00:00:33,540 --> 00:00:37,260
have a very good intuitive understanding

20
00:00:35,219 --> 00:00:39,239
of the activations in the neural net

21
00:00:37,260 --> 00:00:41,100
during training and especially the

22
00:00:39,239 --> 00:00:42,899
gradients that are flowing backwards and

23
00:00:41,100 --> 00:00:44,820
how they behave and what they look like

24
00:00:42,899 --> 00:00:46,140
and this is going to be very important

25
00:00:44,820 --> 00:00:48,000
to understand the history of the

26
00:00:46,140 --> 00:00:49,559
development of these architectures

27
00:00:48,000 --> 00:00:51,780
because we'll see that recurrent neural

28
00:00:49,559 --> 00:00:53,280
networks while they are very expressive

29
00:00:51,780 --> 00:00:54,660
in that they are a universal

30
00:00:53,280 --> 00:00:58,199
approximator and can in principle

31
00:00:54,660 --> 00:00:59,460
Implement uh all the algorithms we'll

32
00:00:58,199 --> 00:01:01,140
see that they are not very easily

33
00:00:59,460 --> 00:01:02,460
optimizable with the first order

34
00:01:01,140 --> 00:01:03,899
ingredient-based techniques that we have

35
00:01:02,460 --> 00:01:04,559
available to us and that we use all the

36
00:01:03,899 --> 00:01:06,780
time

37
00:01:04,559 --> 00:01:09,180
and the key to understanding why they

38
00:01:06,780 --> 00:01:10,860
are not optimizable easily is to

39
00:01:09,180 --> 00:01:12,000
understand the the activations and the

40
00:01:10,860 --> 00:01:13,799
gradients and how they behave during

41
00:01:12,000 --> 00:01:16,020
training and we'll see that a lot of the

42
00:01:13,799 --> 00:01:19,560
variants since recurring neural networks

43
00:01:16,020 --> 00:01:21,479
have tried to improve that situation and

44
00:01:19,560 --> 00:01:23,460
so that's the path that we have to take

45
00:01:21,479 --> 00:01:25,799
and let's get started so the starting

46
00:01:23,460 --> 00:01:27,479
code for this lecture is largely the

47
00:01:25,799 --> 00:01:28,259
code from four but I've cleaned it up a

48
00:01:27,479 --> 00:01:31,140
little bit

49
00:01:28,259 --> 00:01:33,840
so you'll see that we are importing all

50
00:01:31,140 --> 00:01:35,159
the torch and matplotlab utilities we're

51
00:01:33,840 --> 00:01:37,680
reading in the words just like before

52
00:01:35,159 --> 00:01:39,780
these are eight example words there's a

53
00:01:37,680 --> 00:01:41,759
total of 32 000 of them here's a

54
00:01:39,780 --> 00:01:44,280
vocabulary of all the lowercase letters

55
00:01:41,759 --> 00:01:46,799
and the special dot token

56
00:01:44,280 --> 00:01:50,579
here we are reading in the data set and

57
00:01:46,799 --> 00:01:53,579
processing it and creating three splits

58
00:01:50,579 --> 00:01:55,740
the train Dev and the test split

59
00:01:53,579 --> 00:01:58,079
now in MLP this is the identical same

60
00:01:55,740 --> 00:01:59,579
MLP except you see that I removed a

61
00:01:58,079 --> 00:02:01,680
bunch of magic numbers that we had here

62
00:01:59,579 --> 00:02:03,119
and instead we have the dimensionality

63
00:02:01,680 --> 00:02:05,399
of the embedding space of the characters

64
00:02:03,119 --> 00:02:06,960
and the number of hidden units in the

65
00:02:05,399 --> 00:02:09,300
hidden layer and so I've pulled them

66
00:02:06,960 --> 00:02:10,679
outside here so that we don't have to go

67
00:02:09,300 --> 00:02:11,580
and change all these magic numbers all

68
00:02:10,679 --> 00:02:13,500
the time

69
00:02:11,580 --> 00:02:16,080
with the same neural net with 11 000

70
00:02:13,500 --> 00:02:18,599
parameters that we optimize now over 200

71
00:02:16,080 --> 00:02:20,940
000 steps with a batch size of 32 and

72
00:02:18,599 --> 00:02:22,739
you'll see that I refactor I refactored

73
00:02:20,940 --> 00:02:24,540
the code here a little bit but there are

74
00:02:22,739 --> 00:02:26,760
no functional changes I just created a

75
00:02:24,540 --> 00:02:29,700
few extra variables a few more comments

76
00:02:26,760 --> 00:02:31,920
and I removed all the magic numbers and

77
00:02:29,700 --> 00:02:34,140
otherwise is the exact same thing

78
00:02:31,920 --> 00:02:36,480
then when we optimize we saw that our

79
00:02:34,140 --> 00:02:39,080
loss looked something like this we saw

80
00:02:36,480 --> 00:02:41,640
that the train and Val loss were about

81
00:02:39,080 --> 00:02:44,459
2.16 and so on

82
00:02:41,640 --> 00:02:46,860
here I refactored the code a little bit

83
00:02:44,459 --> 00:02:48,900
for the evaluation of arbitrary splits

84
00:02:46,860 --> 00:02:50,879
so you pass in a string of which split

85
00:02:48,900 --> 00:02:53,819
you'd like to evaluate and then here

86
00:02:50,879 --> 00:02:55,920
depending on train Val or test I index

87
00:02:53,819 --> 00:02:57,120
in and I get the correct split and then

88
00:02:55,920 --> 00:02:59,220
this is the forward pass of the network

89
00:02:57,120 --> 00:02:59,940
and evaluation of the loss and printing

90
00:02:59,220 --> 00:03:03,720
it

91
00:02:59,940 --> 00:03:05,220
so just making it nicer one thing that

92
00:03:03,720 --> 00:03:07,920
you'll notice here is

93
00:03:05,220 --> 00:03:10,319
I'm using a decorator torch.nograd which

94
00:03:07,920 --> 00:03:12,239
you can also look up and read the

95
00:03:10,319 --> 00:03:14,700
documentation of basically what this

96
00:03:12,239 --> 00:03:17,459
decorator does on top of a function is

97
00:03:14,700 --> 00:03:21,000
that whatever happens in this function

98
00:03:17,459 --> 00:03:23,400
is assumed by torch to never require any

99
00:03:21,000 --> 00:03:25,860
gradients so it will not do any of the

100
00:03:23,400 --> 00:03:27,720
bookkeeping that it does to keep track

101
00:03:25,860 --> 00:03:29,519
of all the gradients in anticipation of

102
00:03:27,720 --> 00:03:31,319
an eventual backward pass

103
00:03:29,519 --> 00:03:33,540
it's almost as if all the tensors that

104
00:03:31,319 --> 00:03:35,700
get created here have a requires grad of

105
00:03:33,540 --> 00:03:36,900
false and so it just makes everything

106
00:03:35,700 --> 00:03:38,819
much more efficient because you're

107
00:03:36,900 --> 00:03:40,860
telling torch that I will not call that

108
00:03:38,819 --> 00:03:42,000
backward on any of this computation and

109
00:03:40,860 --> 00:03:43,620
you don't need to maintain the graph

110
00:03:42,000 --> 00:03:46,200
under the hood

111
00:03:43,620 --> 00:03:49,099
so that's what this does and you can

112
00:03:46,200 --> 00:03:52,920
also use a context manager with

113
00:03:49,099 --> 00:03:54,480
torch.nograd and you can look those up

114
00:03:52,920 --> 00:03:55,620
then here we have the sampling from a

115
00:03:54,480 --> 00:03:57,780
model

116
00:03:55,620 --> 00:03:59,159
um just as before just a poor passive

117
00:03:57,780 --> 00:04:01,560
and neural net getting the distribution

118
00:03:59,159 --> 00:04:03,540
sampling from it adjusting the context

119
00:04:01,560 --> 00:04:05,940
window and repeating until we get the

120
00:04:03,540 --> 00:04:08,340
special and token and we see that we are

121
00:04:05,940 --> 00:04:10,620
starting to get much nicer looking words

122
00:04:08,340 --> 00:04:12,299
sample from the model but still not

123
00:04:10,620 --> 00:04:14,400
amazing and they're still not fully

124
00:04:12,299 --> 00:04:17,579
named like but it's much better than

125
00:04:14,400 --> 00:04:19,739
when we had them with the byground model

126
00:04:17,579 --> 00:04:20,760
so that's our starting point now the

127
00:04:19,739 --> 00:04:22,500
first thing I would like to scrutinize

128
00:04:20,760 --> 00:04:25,139
is the initialization

129
00:04:22,500 --> 00:04:27,060
I can tell that our network is very

130
00:04:25,139 --> 00:04:28,979
improperly configured at initialization

131
00:04:27,060 --> 00:04:30,180
and there's multiple things wrong with

132
00:04:28,979 --> 00:04:31,080
it but let's just start with the first

133
00:04:30,180 --> 00:04:33,180
one

134
00:04:31,080 --> 00:04:34,740
look here on the zeroth iteration the

135
00:04:33,180 --> 00:04:37,560
very first iteration

136
00:04:34,740 --> 00:04:39,419
we are recording a loss of 27 and this

137
00:04:37,560 --> 00:04:41,580
rapidly comes down to roughly one or two

138
00:04:39,419 --> 00:04:43,259
or so I can tell that the initialization

139
00:04:41,580 --> 00:04:44,280
is all messed up because this is way too

140
00:04:43,259 --> 00:04:46,440
high

141
00:04:44,280 --> 00:04:47,699
in training of neural Nets it is almost

142
00:04:46,440 --> 00:04:49,680
always the case that you will have a

143
00:04:47,699 --> 00:04:52,320
rough idea for what loss to expect at

144
00:04:49,680 --> 00:04:54,600
initialization and that just depends on

145
00:04:52,320 --> 00:04:57,360
the loss function and the problem set up

146
00:04:54,600 --> 00:04:58,800
in this case I do not expect 27. I

147
00:04:57,360 --> 00:05:00,479
expect a much lower number and we can

148
00:04:58,800 --> 00:05:02,820
calculate it together

149
00:05:00,479 --> 00:05:06,360
basically at initialization what we'd

150
00:05:02,820 --> 00:05:08,160
like is that there's 27 characters that

151
00:05:06,360 --> 00:05:10,680
could come next for any one training

152
00:05:08,160 --> 00:05:12,120
example at initialization we have no

153
00:05:10,680 --> 00:05:14,460
reason to believe any characterist to be

154
00:05:12,120 --> 00:05:15,720
much more likely than others and so we'd

155
00:05:14,460 --> 00:05:18,419
expect that the probability distribution

156
00:05:15,720 --> 00:05:20,220
that comes out initially is a uniform

157
00:05:18,419 --> 00:05:23,400
distribution assigning about equal

158
00:05:20,220 --> 00:05:25,440
probability to all the 27 characters

159
00:05:23,400 --> 00:05:28,139
so basically what we like is the

160
00:05:25,440 --> 00:05:31,860
probability for any character would be

161
00:05:28,139 --> 00:05:33,240
roughly 1 over 27.

162
00:05:31,860 --> 00:05:35,520
that is the probability we should record

163
00:05:33,240 --> 00:05:37,860
and then the loss is the negative log

164
00:05:35,520 --> 00:05:39,479
probability so let's wrap this in a

165
00:05:37,860 --> 00:05:41,580
tensor

166
00:05:39,479 --> 00:05:44,220
and then that one can take the log of it

167
00:05:41,580 --> 00:05:45,840
and then the negative log probability is

168
00:05:44,220 --> 00:05:49,800
the loss we would expect

169
00:05:45,840 --> 00:05:51,600
which is 3.29 much much lower than 27.

170
00:05:49,800 --> 00:05:53,699
and so what's happening right now is

171
00:05:51,600 --> 00:05:55,380
that at initialization the neural net is

172
00:05:53,699 --> 00:05:57,479
creating probability distributions that

173
00:05:55,380 --> 00:05:59,039
are all messed up some characters are

174
00:05:57,479 --> 00:06:00,479
very confident and some characters are

175
00:05:59,039 --> 00:06:02,160
very not confident

176
00:06:00,479 --> 00:06:04,139
and then basically what's happening is

177
00:06:02,160 --> 00:06:05,100
that the network is very confidently

178
00:06:04,139 --> 00:06:08,940
wrong

179
00:06:05,100 --> 00:06:11,340
and uh that make that's what makes it um

180
00:06:08,940 --> 00:06:12,780
record very high loss so here's a

181
00:06:11,340 --> 00:06:14,820
smaller four-dimensional example of the

182
00:06:12,780 --> 00:06:17,820
issue let's say we only have four

183
00:06:14,820 --> 00:06:19,139
characters and then we have Logics that

184
00:06:17,820 --> 00:06:20,820
come out of the neural land and they are

185
00:06:19,139 --> 00:06:22,919
very very close to zero

186
00:06:20,820 --> 00:06:25,620
then when we take the soft Max of all

187
00:06:22,919 --> 00:06:27,000
zeros we get probabilities there are a

188
00:06:25,620 --> 00:06:30,960
diffuse distribution

189
00:06:27,000 --> 00:06:33,060
so sums to one and is exactly uniform

190
00:06:30,960 --> 00:06:35,759
and then in this case if the label is

191
00:06:33,060 --> 00:06:38,520
say 2 it doesn't actually matter if this

192
00:06:35,759 --> 00:06:40,139
if the label is 2 or 3 or 1 or 0 because

193
00:06:38,520 --> 00:06:41,699
it's a uniform distribution we're

194
00:06:40,139 --> 00:06:44,160
recording the exact same loss in this

195
00:06:41,699 --> 00:06:46,199
case 1.38 so this is the loss we would

196
00:06:44,160 --> 00:06:48,180
expect for a four-dimensional example

197
00:06:46,199 --> 00:06:50,940
and I can see of course that as we start

198
00:06:48,180 --> 00:06:53,039
to manipulate these logits we're going

199
00:06:50,940 --> 00:06:56,039
to be changing the loss here so it could

200
00:06:53,039 --> 00:06:57,840
be that we lock out and by chance this

201
00:06:56,039 --> 00:06:59,639
could be a very high number like you

202
00:06:57,840 --> 00:07:01,080
know five or something like that then in

203
00:06:59,639 --> 00:07:02,220
that case we'll record a very low loss

204
00:07:01,080 --> 00:07:03,960
because we're assigning the correct

205
00:07:02,220 --> 00:07:06,539
probability at the initialization by

206
00:07:03,960 --> 00:07:09,240
chance to the correct label

207
00:07:06,539 --> 00:07:14,340
much more likely it is that some other

208
00:07:09,240 --> 00:07:15,360
dimension will have a high uh logit and

209
00:07:14,340 --> 00:07:17,759
then what will happen is we start to

210
00:07:15,360 --> 00:07:19,259
record much higher loss and what can

211
00:07:17,759 --> 00:07:21,240
come what can happen is basically The

212
00:07:19,259 --> 00:07:23,940
Lodges come out like something like this

213
00:07:21,240 --> 00:07:27,180
you know and they take on Extreme values

214
00:07:23,940 --> 00:07:28,440
and we record really high loss

215
00:07:27,180 --> 00:07:31,020
um

216
00:07:28,440 --> 00:07:34,620
for example if we have torch.randen of

217
00:07:31,020 --> 00:07:37,979
four so these are uniform sorry these

218
00:07:34,620 --> 00:07:40,259
are normally distributed numbers

219
00:07:37,979 --> 00:07:43,500
uh forum

220
00:07:40,259 --> 00:07:45,240
and here we can also print the logits

221
00:07:43,500 --> 00:07:48,660
the probabilities that come out of it

222
00:07:45,240 --> 00:07:51,360
and the loss and so because these logits

223
00:07:48,660 --> 00:07:53,520
are near zero for the most part the loss

224
00:07:51,360 --> 00:07:58,099
that comes out is is okay

225
00:07:53,520 --> 00:07:58,099
but suppose this is like times 10 now

226
00:07:58,740 --> 00:08:02,940
you see how because these are more

227
00:08:00,479 --> 00:08:04,860
extreme values it's very unlikely that

228
00:08:02,940 --> 00:08:07,440
you're going to be guessing the correct

229
00:08:04,860 --> 00:08:09,539
bucket and then you're confidently wrong

230
00:08:07,440 --> 00:08:11,340
and recording very high loss

231
00:08:09,539 --> 00:08:12,599
if your lodges are coming out even more

232
00:08:11,340 --> 00:08:15,660
extreme

233
00:08:12,599 --> 00:08:18,720
you might get extremely insane losses

234
00:08:15,660 --> 00:08:20,400
like infinity even at initialization

235
00:08:18,720 --> 00:08:21,960
um

236
00:08:20,400 --> 00:08:24,660
so basically this is not good and we

237
00:08:21,960 --> 00:08:28,020
want the load just to be roughly zero

238
00:08:24,660 --> 00:08:29,580
um when the network is initialized in

239
00:08:28,020 --> 00:08:31,500
fact the logits can don't have to be

240
00:08:29,580 --> 00:08:34,020
just zero they just have to be equal so

241
00:08:31,500 --> 00:08:35,459
for example if all the objects are one

242
00:08:34,020 --> 00:08:37,620
then because of the normalization inside

243
00:08:35,459 --> 00:08:38,459
the softmax this will actually come out

244
00:08:37,620 --> 00:08:40,200
okay

245
00:08:38,459 --> 00:08:41,760
but by symmetry we don't want it to be

246
00:08:40,200 --> 00:08:43,560
any arbitrary positive or negative

247
00:08:41,760 --> 00:08:45,600
number we just want it to be all zeros

248
00:08:43,560 --> 00:08:47,640
and record the loss that we expect at

249
00:08:45,600 --> 00:08:49,680
initialization so let's Now quickly see

250
00:08:47,640 --> 00:08:51,480
where things go wrong in our example

251
00:08:49,680 --> 00:08:53,880
here we have the initialization

252
00:08:51,480 --> 00:08:55,620
let me reinitialize the neural net and

253
00:08:53,880 --> 00:08:57,360
here let me break after the very first

254
00:08:55,620 --> 00:09:00,120
iteration so we only see the initial

255
00:08:57,360 --> 00:09:02,100
loss which is 27.

256
00:09:00,120 --> 00:09:03,839
so that's way too high and intuitively

257
00:09:02,100 --> 00:09:06,420
now we can expect the variables involved

258
00:09:03,839 --> 00:09:09,540
and we see that the logits here if we

259
00:09:06,420 --> 00:09:11,339
just print some of these

260
00:09:09,540 --> 00:09:12,720
if we just print the first row we see

261
00:09:11,339 --> 00:09:13,740
that the load just take on quite extreme

262
00:09:12,720 --> 00:09:15,779
values

263
00:09:13,740 --> 00:09:18,180
and that's what's creating the fake

264
00:09:15,779 --> 00:09:20,519
confidence and incorrect answers and

265
00:09:18,180 --> 00:09:23,100
makes the loss

266
00:09:20,519 --> 00:09:25,920
get very very high so these Lotus should

267
00:09:23,100 --> 00:09:28,200
be much much closer to zero so now let's

268
00:09:25,920 --> 00:09:30,660
think through how we can achieve logits

269
00:09:28,200 --> 00:09:32,459
coming out of this neural net to be more

270
00:09:30,660 --> 00:09:34,140
closer to zero

271
00:09:32,459 --> 00:09:36,060
you see here that lodges are calculated

272
00:09:34,140 --> 00:09:37,620
as the hidden States multiplied by W2

273
00:09:36,060 --> 00:09:39,180
plus B2

274
00:09:37,620 --> 00:09:42,959
so first of all currently we're

275
00:09:39,180 --> 00:09:44,100
initializing B2 as random values of the

276
00:09:42,959 --> 00:09:46,920
right size

277
00:09:44,100 --> 00:09:48,300
but because we want roughly zero we

278
00:09:46,920 --> 00:09:50,160
don't actually want to be adding a bias

279
00:09:48,300 --> 00:09:52,320
of random numbers so in fact I'm going

280
00:09:50,160 --> 00:09:56,040
to add a times a zero here to make sure

281
00:09:52,320 --> 00:09:57,300
that B2 is just basically zero at the

282
00:09:56,040 --> 00:10:00,600
initialization

283
00:09:57,300 --> 00:10:02,880
and second this is H multiplied by W2 so

284
00:10:00,600 --> 00:10:04,980
if we want logits to be very very small

285
00:10:02,880 --> 00:10:06,959
then we would be multiplying W2 and

286
00:10:04,980 --> 00:10:09,360
making that smaller

287
00:10:06,959 --> 00:10:12,480
so for example if we scale down W2 by

288
00:10:09,360 --> 00:10:14,160
0.1 all the elements then if I

289
00:10:12,480 --> 00:10:16,019
do again just the very first iteration

290
00:10:14,160 --> 00:10:18,300
you see that we are getting much closer

291
00:10:16,019 --> 00:10:22,260
to what we expect so the rough roughly

292
00:10:18,300 --> 00:10:25,560
what we want is about 3.29 this is 4.2

293
00:10:22,260 --> 00:10:27,899
I can make this maybe even smaller

294
00:10:25,560 --> 00:10:31,200
3.32 okay so we're getting closer and

295
00:10:27,899 --> 00:10:33,060
closer now you're probably wondering can

296
00:10:31,200 --> 00:10:34,920
we just set this to zero

297
00:10:33,060 --> 00:10:37,980
then we get of course exactly what we're

298
00:10:34,920 --> 00:10:40,260
looking for at initialization

299
00:10:37,980 --> 00:10:42,779
and the reason I don't usually do this

300
00:10:40,260 --> 00:10:44,760
is because I'm I'm very nervous and I'll

301
00:10:42,779 --> 00:10:47,160
show you in a second why you don't want

302
00:10:44,760 --> 00:10:50,040
to be setting W's or weights of a neural

303
00:10:47,160 --> 00:10:51,660
net exactly to zero um you usually want

304
00:10:50,040 --> 00:10:54,720
it to be small numbers instead of

305
00:10:51,660 --> 00:10:56,519
exactly zero for this output layer in

306
00:10:54,720 --> 00:10:58,500
this specific case I think it would be

307
00:10:56,519 --> 00:11:00,300
fine but I'll show you in a second where

308
00:10:58,500 --> 00:11:02,760
things go wrong very quickly if you do

309
00:11:00,300 --> 00:11:04,800
that so let's just go with 0.01

310
00:11:02,760 --> 00:11:07,740
in that case our loss is close enough

311
00:11:04,800 --> 00:11:10,620
but has some entropy it's not exactly

312
00:11:07,740 --> 00:11:12,060
zero it's got some low entropy and

313
00:11:10,620 --> 00:11:13,500
that's used for symmetry breaking as

314
00:11:12,060 --> 00:11:15,060
we'll see in a second

315
00:11:13,500 --> 00:11:17,220
the logits are now coming out much

316
00:11:15,060 --> 00:11:18,120
closer to zero and everything is well

317
00:11:17,220 --> 00:11:22,380
and good

318
00:11:18,120 --> 00:11:24,839
so if I just erase these and I now take

319
00:11:22,380 --> 00:11:27,120
away the break statement

320
00:11:24,839 --> 00:11:29,600
we can run the optimization with this

321
00:11:27,120 --> 00:11:33,480
new initialization and let's just see

322
00:11:29,600 --> 00:11:35,459
what losses we record okay so I let it

323
00:11:33,480 --> 00:11:37,579
run and you see that we started off good

324
00:11:35,459 --> 00:11:40,620
and then we came down a bit

325
00:11:37,579 --> 00:11:43,019
the plot of the loss now doesn't have

326
00:11:40,620 --> 00:11:45,000
this hockey shape appearance

327
00:11:43,019 --> 00:11:47,040
um because basically what's happening in

328
00:11:45,000 --> 00:11:48,660
the hockey stick the very first few

329
00:11:47,040 --> 00:11:50,820
iterations of the loss what's happening

330
00:11:48,660 --> 00:11:52,560
during the optimization is the

331
00:11:50,820 --> 00:11:54,180
optimization is just squashing down the

332
00:11:52,560 --> 00:11:56,880
logits and then it's rearranging the

333
00:11:54,180 --> 00:11:59,100
logits so basically we took away this

334
00:11:56,880 --> 00:12:00,540
easy part of the loss function where

335
00:11:59,100 --> 00:12:01,680
just the the weights were just being

336
00:12:00,540 --> 00:12:04,260
shrunk down

337
00:12:01,680 --> 00:12:06,060
and so therefore we don't we don't get

338
00:12:04,260 --> 00:12:07,079
these easy gains in the beginning and

339
00:12:06,060 --> 00:12:08,760
we're just getting some of the hard

340
00:12:07,079 --> 00:12:10,140
gains of training the actual neural nut

341
00:12:08,760 --> 00:12:11,399
and so there's no hockey stick

342
00:12:10,140 --> 00:12:13,620
appearance

343
00:12:11,399 --> 00:12:15,540
so good things are happening in that

344
00:12:13,620 --> 00:12:16,860
both number one lawsuit initialization

345
00:12:15,540 --> 00:12:19,740
is what we expect

346
00:12:16,860 --> 00:12:22,019
and the the loss doesn't look like a

347
00:12:19,740 --> 00:12:23,940
hockey stick and this is true for any

348
00:12:22,019 --> 00:12:25,440
neuron that you might train and

349
00:12:23,940 --> 00:12:28,200
something to look out for

350
00:12:25,440 --> 00:12:29,339
and second the last that came out is

351
00:12:28,200 --> 00:12:31,380
actually quite a bit improved

352
00:12:29,339 --> 00:12:33,800
unfortunately I erased what we had here

353
00:12:31,380 --> 00:12:37,500
before I believe this was

354
00:12:33,800 --> 00:12:40,260
2.12 and this is what this was 2.16 so

355
00:12:37,500 --> 00:12:42,060
we get a slightly improved result and

356
00:12:40,260 --> 00:12:43,940
the reason for that is uh because we're

357
00:12:42,060 --> 00:12:45,980
spending more Cycles more time

358
00:12:43,940 --> 00:12:48,720
optimizing the neural net actually

359
00:12:45,980 --> 00:12:50,399
instead of just spending the first

360
00:12:48,720 --> 00:12:53,040
several thousand iterations probably

361
00:12:50,399 --> 00:12:55,139
just squashing down the weights

362
00:12:53,040 --> 00:12:56,700
because they are so way too high in the

363
00:12:55,139 --> 00:12:59,040
beginning in the initialization

364
00:12:56,700 --> 00:13:00,899
so something to look out for and uh

365
00:12:59,040 --> 00:13:02,880
that's number one now let's look at the

366
00:13:00,899 --> 00:13:04,800
second problem let me re-initialize our

367
00:13:02,880 --> 00:13:06,060
neural net and let me reintroduce the

368
00:13:04,800 --> 00:13:08,820
break statement

369
00:13:06,060 --> 00:13:09,959
so we have a reasonable initial loss so

370
00:13:08,820 --> 00:13:11,339
even though everything is looking good

371
00:13:09,959 --> 00:13:13,560
on the level of the loss and we get

372
00:13:11,339 --> 00:13:15,120
something that we expect there's still a

373
00:13:13,560 --> 00:13:17,399
deeper problem working inside this

374
00:13:15,120 --> 00:13:20,279
neural net and its initialization

375
00:13:17,399 --> 00:13:22,860
so the logits are now okay the problem

376
00:13:20,279 --> 00:13:25,620
now is with the values of H

377
00:13:22,860 --> 00:13:27,839
the activations of the Hidden States now

378
00:13:25,620 --> 00:13:30,000
if we just visualize this Vector sorry

379
00:13:27,839 --> 00:13:31,920
this tensor H it's kind of hard to see

380
00:13:30,000 --> 00:13:34,200
but the problem here roughly speaking is

381
00:13:31,920 --> 00:13:35,880
you see how many of the elements are one

382
00:13:34,200 --> 00:13:38,700
or negative one

383
00:13:35,880 --> 00:13:40,860
now recall that torch.nh the 10h

384
00:13:38,700 --> 00:13:42,720
function is a squashing function it

385
00:13:40,860 --> 00:13:44,100
takes arbitrary numbers and it squashes

386
00:13:42,720 --> 00:13:46,139
them into a range of negative one and

387
00:13:44,100 --> 00:13:48,180
one and it does so smoothly

388
00:13:46,139 --> 00:13:50,220
so let's look at the histogram of H to

389
00:13:48,180 --> 00:13:52,260
get a better idea of the distribution of

390
00:13:50,220 --> 00:13:54,899
the values inside this tensor

391
00:13:52,260 --> 00:13:57,959
we can do this first

392
00:13:54,899 --> 00:14:01,079
well we can see that H is 32 examples

393
00:13:57,959 --> 00:14:03,060
and 200 activations in each example we

394
00:14:01,079 --> 00:14:06,240
can view it as negative one to stretch

395
00:14:03,060 --> 00:14:09,480
it out into one large vector

396
00:14:06,240 --> 00:14:12,420
and we can then call to list to convert

397
00:14:09,480 --> 00:14:13,560
this into one large python list of

398
00:14:12,420 --> 00:14:16,079
floats

399
00:14:13,560 --> 00:14:19,500
and then we can pass this into plt.hist

400
00:14:16,079 --> 00:14:22,019
for histogram and we say we want 50 bins

401
00:14:19,500 --> 00:14:24,240
and a semicolon to suppress a bunch of

402
00:14:22,019 --> 00:14:26,100
output we don't want

403
00:14:24,240 --> 00:14:28,800
so we see this histogram and we see that

404
00:14:26,100 --> 00:14:31,500
most the values by far take on the value

405
00:14:28,800 --> 00:14:33,000
of negative one and one so this 10h is

406
00:14:31,500 --> 00:14:35,760
very very active

407
00:14:33,000 --> 00:14:37,740
and we can also look at

408
00:14:35,760 --> 00:14:39,540
basically why that is

409
00:14:37,740 --> 00:14:42,560
we can look at the pre-activations that

410
00:14:39,540 --> 00:14:42,560
feed into the 10h

411
00:14:42,660 --> 00:14:46,860
and we can see that the distribution of

412
00:14:44,459 --> 00:14:48,600
the pre-activations are is very very

413
00:14:46,860 --> 00:14:51,360
broad these take numbers between

414
00:14:48,600 --> 00:14:53,279
negative 15 and 15 and that's why in the

415
00:14:51,360 --> 00:14:54,600
torture 10h everything is being squashed

416
00:14:53,279 --> 00:14:56,459
and capped to be in the range of

417
00:14:54,600 --> 00:14:58,980
negative one and one and lots of numbers

418
00:14:56,459 --> 00:15:01,079
here take on very extreme values

419
00:14:58,980 --> 00:15:02,880
now if you are new to neural networks

420
00:15:01,079 --> 00:15:04,860
you might not actually see this as an

421
00:15:02,880 --> 00:15:06,839
issue but if you're well burst in the

422
00:15:04,860 --> 00:15:08,639
dark arts back propagation and then have

423
00:15:06,839 --> 00:15:10,560
an intuitive sense of how these

424
00:15:08,639 --> 00:15:12,420
gradients flow through a neural net you

425
00:15:10,560 --> 00:15:14,880
are looking at your distribution of 10h

426
00:15:12,420 --> 00:15:17,100
activations here and you are sweating

427
00:15:14,880 --> 00:15:18,300
so let me show you why we have to keep

428
00:15:17,100 --> 00:15:20,399
in mind that during that propagation

429
00:15:18,300 --> 00:15:22,199
just like we saw in micrograd we are

430
00:15:20,399 --> 00:15:23,399
doing backward pass starting at the loss

431
00:15:22,199 --> 00:15:25,680
and flowing through the network

432
00:15:23,399 --> 00:15:28,560
backwards in particular we're going to

433
00:15:25,680 --> 00:15:31,139
back propagate through this tors.10h

434
00:15:28,560 --> 00:15:33,600
and this layer here is made up of 200

435
00:15:31,139 --> 00:15:36,180
neurons for each one of these examples

436
00:15:33,600 --> 00:15:38,579
and it implements an element twice 10 H

437
00:15:36,180 --> 00:15:39,720
so let's look at what happens in 10h in

438
00:15:38,579 --> 00:15:41,480
the backward pass

439
00:15:39,720 --> 00:15:44,100
we can actually go back to our previous

440
00:15:41,480 --> 00:15:46,740
micrograd code in the very first lecture

441
00:15:44,100 --> 00:15:49,560
and see how we implement the 10h

442
00:15:46,740 --> 00:15:51,660
we saw that the input here was X and

443
00:15:49,560 --> 00:15:52,320
then we calculate T which is the 10h of

444
00:15:51,660 --> 00:15:54,480
x

445
00:15:52,320 --> 00:15:56,699
so that's T and T is between negative 1

446
00:15:54,480 --> 00:15:58,079
and 1. it's the output of the 10h and

447
00:15:56,699 --> 00:15:59,940
then in the backward pass how do we back

448
00:15:58,079 --> 00:16:02,160
propagate through a 10 h

449
00:15:59,940 --> 00:16:04,380
we take out that grad

450
00:16:02,160 --> 00:16:06,360
and then we multiply it this is the

451
00:16:04,380 --> 00:16:09,000
chain rule with the local gradient which

452
00:16:06,360 --> 00:16:10,980
took the form of 1 minus t squared

453
00:16:09,000 --> 00:16:13,320
so what happens if the outputs of your

454
00:16:10,980 --> 00:16:15,660
10h are very close to negative one or

455
00:16:13,320 --> 00:16:18,240
one if you plug in t equals one here

456
00:16:15,660 --> 00:16:21,060
you're going to get a zero multiplying

457
00:16:18,240 --> 00:16:23,579
out that grad no matter what up.grad is

458
00:16:21,060 --> 00:16:25,199
we are killing the gradient and we're

459
00:16:23,579 --> 00:16:27,360
stopping effectively the back

460
00:16:25,199 --> 00:16:29,519
propagation through this 10h unit

461
00:16:27,360 --> 00:16:31,620
similarly when T is negative one this

462
00:16:29,519 --> 00:16:32,940
will again become zero and out that grad

463
00:16:31,620 --> 00:16:34,800
just stops

464
00:16:32,940 --> 00:16:37,440
and intuitively this makes sense because

465
00:16:34,800 --> 00:16:40,079
this is a 10 H neuron

466
00:16:37,440 --> 00:16:42,000
and what's happening is if its output is

467
00:16:40,079 --> 00:16:43,800
very close to one then we are in the

468
00:16:42,000 --> 00:16:48,959
tail of this 10 h

469
00:16:43,800 --> 00:16:51,060
and so changing basically the input

470
00:16:48,959 --> 00:16:53,759
is not going to impact the output of the

471
00:16:51,060 --> 00:16:56,100
10h too much because it's it's so it's

472
00:16:53,759 --> 00:16:58,019
in a flat region of the 10h and so

473
00:16:56,100 --> 00:17:02,160
therefore there's no impact on the loss

474
00:16:58,019 --> 00:17:04,260
and so indeed the the weights and the

475
00:17:02,160 --> 00:17:06,360
biases along with this tan H neuron do

476
00:17:04,260 --> 00:17:07,799
not impact the loss because the output

477
00:17:06,360 --> 00:17:09,299
of the standard unit is in the flat

478
00:17:07,799 --> 00:17:11,100
region in the 10h and there's no

479
00:17:09,299 --> 00:17:13,380
influence we can we can be changing them

480
00:17:11,100 --> 00:17:15,059
whatever we want however we want and the

481
00:17:13,380 --> 00:17:17,459
loss is not impacted that's that's

482
00:17:15,059 --> 00:17:19,380
another way to justify that indeed the

483
00:17:17,459 --> 00:17:20,760
gradient would be basically zero it

484
00:17:19,380 --> 00:17:24,299
vanishes

485
00:17:20,760 --> 00:17:28,620
indeed when T equals zero

486
00:17:24,299 --> 00:17:31,820
we get 1 times at that grad so when the

487
00:17:28,620 --> 00:17:34,860
10h takes on exactly value of zero then

488
00:17:31,820 --> 00:17:36,660
out.grad is just passed through

489
00:17:34,860 --> 00:17:39,600
so basically what this is doing right is

490
00:17:36,660 --> 00:17:42,240
if T is equal to zero then this the 10h

491
00:17:39,600 --> 00:17:45,120
unit is uh sort of inactive

492
00:17:42,240 --> 00:17:47,520
and uh gradient just passes through but

493
00:17:45,120 --> 00:17:49,440
the more you are in the flat tails the

494
00:17:47,520 --> 00:17:51,179
more the gradient is squashed

495
00:17:49,440 --> 00:17:53,460
so in fact you'll see that the the

496
00:17:51,179 --> 00:17:55,740
gradient flowing through 10 H can only

497
00:17:53,460 --> 00:17:59,640
ever decrease in the amount that it

498
00:17:55,740 --> 00:18:00,860
decreases is proportional through a

499
00:17:59,640 --> 00:18:03,059
square here

500
00:18:00,860 --> 00:18:04,919
depending on how far you are in the flat

501
00:18:03,059 --> 00:18:06,600
tails of this 10 age

502
00:18:04,919 --> 00:18:10,080
and so that's kind of what's Happening

503
00:18:06,600 --> 00:18:13,320
Here and through this the concern here

504
00:18:10,080 --> 00:18:14,820
is that if all of these outputs H are in

505
00:18:13,320 --> 00:18:16,860
the flat regions of negative one and one

506
00:18:14,820 --> 00:18:18,000
then the gradients that are flowing

507
00:18:16,860 --> 00:18:20,940
through the network will just get

508
00:18:18,000 --> 00:18:23,700
destroyed at this layer

509
00:18:20,940 --> 00:18:25,679
now there is some redeeming quality here

510
00:18:23,700 --> 00:18:27,660
and that we can actually get a sense of

511
00:18:25,679 --> 00:18:29,760
the problem here as follows

512
00:18:27,660 --> 00:18:31,320
I've wrote some code here and basically

513
00:18:29,760 --> 00:18:34,320
what we want to do here is we want to

514
00:18:31,320 --> 00:18:37,620
take a look at H take the absolute value

515
00:18:34,320 --> 00:18:42,240
and see how often it is in the in a flat

516
00:18:37,620 --> 00:18:44,760
region so say greater than 0.99

517
00:18:42,240 --> 00:18:47,460
and what you get is the following and

518
00:18:44,760 --> 00:18:49,740
this is a Boolean tensor so uh in the

519
00:18:47,460 --> 00:18:52,440
Boolean tensor you get a white if this

520
00:18:49,740 --> 00:18:53,820
is true and a black and this is false

521
00:18:52,440 --> 00:18:56,400
and so basically what we have here is

522
00:18:53,820 --> 00:18:59,280
the 32 examples and then 200 hidden

523
00:18:56,400 --> 00:19:01,799
neurons and we see that a lot of this is

524
00:18:59,280 --> 00:19:05,220
white and what that's telling us is that

525
00:19:01,799 --> 00:19:08,880
all these 10h neurons were very very

526
00:19:05,220 --> 00:19:12,539
active and uh they're in a flat tail

527
00:19:08,880 --> 00:19:13,919
and so in all these cases uh the back

528
00:19:12,539 --> 00:19:16,320
the backward gradient would get

529
00:19:13,919 --> 00:19:19,500
destroyed

530
00:19:16,320 --> 00:19:21,600
now we would be in a lot of trouble if

531
00:19:19,500 --> 00:19:24,419
for ever for any one of these 200

532
00:19:21,600 --> 00:19:26,940
neurons if it was the case that the

533
00:19:24,419 --> 00:19:28,440
entire column is white because in that

534
00:19:26,940 --> 00:19:30,299
case we have what's called a dead neuron

535
00:19:28,440 --> 00:19:31,500
and this could be a tannage neuron where

536
00:19:30,299 --> 00:19:33,780
the initialization of the weights and

537
00:19:31,500 --> 00:19:37,740
the biases could be such that no single

538
00:19:33,780 --> 00:19:40,380
example ever activates this 10h in the

539
00:19:37,740 --> 00:19:43,440
sort of active part of the 10h if all

540
00:19:40,380 --> 00:19:45,419
the examples land in the tail then this

541
00:19:43,440 --> 00:19:46,559
neuron will never learn it is a dead

542
00:19:45,419 --> 00:19:49,020
neuron

543
00:19:46,559 --> 00:19:51,480
and so just scrutinizing this and

544
00:19:49,020 --> 00:19:55,740
looking for Columns of completely white

545
00:19:51,480 --> 00:19:57,780
we see that this is not the case so I

546
00:19:55,740 --> 00:19:59,340
don't see a single neuron that is all of

547
00:19:57,780 --> 00:20:01,140
uh you know white

548
00:19:59,340 --> 00:20:03,840
and so therefore it is the case that for

549
00:20:01,140 --> 00:20:06,000
every one of these 10h neurons

550
00:20:03,840 --> 00:20:09,299
we do have some examples that activate

551
00:20:06,000 --> 00:20:10,860
them in the active part of the 10h and

552
00:20:09,299 --> 00:20:13,200
so some gradients will flow through and

553
00:20:10,860 --> 00:20:14,880
this neuron will learn and the neuron

554
00:20:13,200 --> 00:20:16,320
will change and it will move and it will

555
00:20:14,880 --> 00:20:18,360
do something

556
00:20:16,320 --> 00:20:20,580
but you can sometimes get yourself in

557
00:20:18,360 --> 00:20:23,160
cases where you have dead neurons and

558
00:20:20,580 --> 00:20:25,440
the way this manifests is that for 10

559
00:20:23,160 --> 00:20:27,059
inch neuron this would be when no matter

560
00:20:25,440 --> 00:20:28,980
what inputs you plug in from your data

561
00:20:27,059 --> 00:20:30,960
set this 10 inch neuron always fires

562
00:20:28,980 --> 00:20:32,720
completely one or completely negative

563
00:20:30,960 --> 00:20:34,980
one and then it will just not learn

564
00:20:32,720 --> 00:20:36,419
because all the gradients will be just

565
00:20:34,980 --> 00:20:38,280
zeroed out

566
00:20:36,419 --> 00:20:40,080
uh this is true not just percentage but

567
00:20:38,280 --> 00:20:41,700
for a lot of other non-linearities that

568
00:20:40,080 --> 00:20:44,100
people use in neural networks so we

569
00:20:41,700 --> 00:20:46,020
certainly use 10h a lot but sigmoid will

570
00:20:44,100 --> 00:20:48,419
have the exact same issue because it is

571
00:20:46,020 --> 00:20:51,059
a squashing neuron and so the same will

572
00:20:48,419 --> 00:20:54,179
be true for sigmoid but

573
00:20:51,059 --> 00:20:55,860
um but um you know

574
00:20:54,179 --> 00:20:57,900
um basically the same will actually

575
00:20:55,860 --> 00:21:00,539
applied to sigmoid the same will also

576
00:20:57,900 --> 00:21:03,419
reply to a relu so relu has a completely

577
00:21:00,539 --> 00:21:05,580
flat region here below zero

578
00:21:03,419 --> 00:21:07,140
so if you have a relative neuron then it

579
00:21:05,580 --> 00:21:10,020
is a pass-through

580
00:21:07,140 --> 00:21:11,700
um if it is positive and if it's if the

581
00:21:10,020 --> 00:21:13,980
pre-activation is negative it will just

582
00:21:11,700 --> 00:21:15,780
shut it off since the region here is

583
00:21:13,980 --> 00:21:18,720
completely flat then during back

584
00:21:15,780 --> 00:21:20,400
propagation uh this would be exactly

585
00:21:18,720 --> 00:21:22,260
zeroing out the gradient

586
00:21:20,400 --> 00:21:23,880
um like all of the gradient would be set

587
00:21:22,260 --> 00:21:25,559
exactly to zero instead of just like a

588
00:21:23,880 --> 00:21:28,380
very very small number depending on how

589
00:21:25,559 --> 00:21:30,299
positive or negative T is

590
00:21:28,380 --> 00:21:33,600
and so you can get for example a dead

591
00:21:30,299 --> 00:21:35,220
relu neuron and a dead relinuron would

592
00:21:33,600 --> 00:21:37,860
basically look like

593
00:21:35,220 --> 00:21:41,039
basically what it is is if a neuron with

594
00:21:37,860 --> 00:21:43,320
a relu nonlinearity never activates

595
00:21:41,039 --> 00:21:45,539
so for any examples that you plug in in

596
00:21:43,320 --> 00:21:47,820
the data set it never turns on it's

597
00:21:45,539 --> 00:21:49,740
always in this flat region then this

598
00:21:47,820 --> 00:21:52,320
reli neuron is a dead neuron it's

599
00:21:49,740 --> 00:21:53,700
weights and bias will never learn they

600
00:21:52,320 --> 00:21:55,620
will never get a gradient because the

601
00:21:53,700 --> 00:21:56,940
neuron never activated

602
00:21:55,620 --> 00:21:58,919
and this can sometimes happen at

603
00:21:56,940 --> 00:22:00,419
initialization because the weights and

604
00:21:58,919 --> 00:22:01,919
the biases just make it so that by

605
00:22:00,419 --> 00:22:02,700
chance some neurons are just forever

606
00:22:01,919 --> 00:22:03,960
dead

607
00:22:02,700 --> 00:22:06,240
but it can also happen during

608
00:22:03,960 --> 00:22:07,320
optimization if you have like a too high

609
00:22:06,240 --> 00:22:09,179
of the learning rate for example

610
00:22:07,320 --> 00:22:10,980
sometimes you have these neurons that

611
00:22:09,179 --> 00:22:13,620
gets too much of a gradient and they get

612
00:22:10,980 --> 00:22:16,080
knocked out off the data manifold

613
00:22:13,620 --> 00:22:18,059
and what happens is that from then on no

614
00:22:16,080 --> 00:22:19,860
example ever activates this neuron so

615
00:22:18,059 --> 00:22:21,419
this neuron remains that forever so it's

616
00:22:19,860 --> 00:22:23,760
kind of like a permanent brain damage in

617
00:22:21,419 --> 00:22:25,620
a in a mind of a network

618
00:22:23,760 --> 00:22:26,940
and so sometimes what can happen is if

619
00:22:25,620 --> 00:22:28,500
your learning rate is very high for

620
00:22:26,940 --> 00:22:30,659
example and you have a neural net with

621
00:22:28,500 --> 00:22:33,000
regular neurons you train the neural net

622
00:22:30,659 --> 00:22:34,980
and you get some last loss but then

623
00:22:33,000 --> 00:22:37,700
actually what you do is you go through

624
00:22:34,980 --> 00:22:40,559
the entire training set and you forward

625
00:22:37,700 --> 00:22:42,659
your examples and you can find neurons

626
00:22:40,559 --> 00:22:44,760
that never activate they are dead

627
00:22:42,659 --> 00:22:46,740
neurons in your network and so those

628
00:22:44,760 --> 00:22:47,820
neurons will will never turn on and

629
00:22:46,740 --> 00:22:49,440
usually what happens is that during

630
00:22:47,820 --> 00:22:51,360
training these relative neurons are

631
00:22:49,440 --> 00:22:53,340
changing moving Etc and then because of

632
00:22:51,360 --> 00:22:54,419
a high gradient somewhere by chance they

633
00:22:53,340 --> 00:22:56,700
get knocked off

634
00:22:54,419 --> 00:22:58,620
and then nothing ever activates them and

635
00:22:56,700 --> 00:23:00,240
from then on they are just dead

636
00:22:58,620 --> 00:23:01,860
uh so that's kind of like a permanent

637
00:23:00,240 --> 00:23:02,940
brain damage that can happen to some of

638
00:23:01,860 --> 00:23:05,220
these neurons

639
00:23:02,940 --> 00:23:06,900
these other nonlinearities like leaky

640
00:23:05,220 --> 00:23:08,400
relu will not suffer from this issue as

641
00:23:06,900 --> 00:23:11,580
much because you can see that it doesn't

642
00:23:08,400 --> 00:23:12,780
have flat tails you almost always get

643
00:23:11,580 --> 00:23:15,799
gradients

644
00:23:12,780 --> 00:23:18,000
and elu is also fairly frequently used

645
00:23:15,799 --> 00:23:20,159
it also might suffer from this issue

646
00:23:18,000 --> 00:23:22,380
because it has flat parts

647
00:23:20,159 --> 00:23:24,480
so that's just something to be aware of

648
00:23:22,380 --> 00:23:27,480
and something to be concerned about and

649
00:23:24,480 --> 00:23:29,640
in this case we have way too many

650
00:23:27,480 --> 00:23:32,460
um activations H that take on Extreme

651
00:23:29,640 --> 00:23:34,799
values and because there's no column of

652
00:23:32,460 --> 00:23:36,360
white I think we will be okay and indeed

653
00:23:34,799 --> 00:23:38,520
the network optimizes and gives us a

654
00:23:36,360 --> 00:23:40,140
pretty decent loss but it's just not

655
00:23:38,520 --> 00:23:41,700
optimal and this is not something you

656
00:23:40,140 --> 00:23:44,280
want especially during initialization

657
00:23:41,700 --> 00:23:46,860
and so basically what's happening is

658
00:23:44,280 --> 00:23:48,480
that this H pre-activation that's

659
00:23:46,860 --> 00:23:50,820
flowing to 10h

660
00:23:48,480 --> 00:23:53,159
it's it's too extreme it's too large

661
00:23:50,820 --> 00:23:55,080
it's creating very

662
00:23:53,159 --> 00:23:57,059
um it's creating a distribution that is

663
00:23:55,080 --> 00:23:58,799
too saturated in both sides of the 10h

664
00:23:57,059 --> 00:24:01,440
and it's not something you want because

665
00:23:58,799 --> 00:24:04,500
it means that there's less training uh

666
00:24:01,440 --> 00:24:06,720
for these neurons because they update

667
00:24:04,500 --> 00:24:09,840
um less frequently so how do we fix this

668
00:24:06,720 --> 00:24:13,380
well HP activation is

669
00:24:09,840 --> 00:24:15,539
MCAT which comes from C so these are

670
00:24:13,380 --> 00:24:18,900
uniform gaussian but then it's

671
00:24:15,539 --> 00:24:20,880
multiplied by W1 plus B1 and H preact is

672
00:24:18,900 --> 00:24:23,520
too far off from zero and that's causing

673
00:24:20,880 --> 00:24:25,440
the issue so we want this reactivation

674
00:24:23,520 --> 00:24:27,299
to be closer to zero very similar to

675
00:24:25,440 --> 00:24:30,059
what we have with lodges

676
00:24:27,299 --> 00:24:31,260
so here we want actually something very

677
00:24:30,059 --> 00:24:34,620
very similar

678
00:24:31,260 --> 00:24:36,120
now it's okay to set the biases to very

679
00:24:34,620 --> 00:24:37,620
small number we can either multiply it

680
00:24:36,120 --> 00:24:39,240
by zero zero one to get like a little

681
00:24:37,620 --> 00:24:41,460
bit of entropy

682
00:24:39,240 --> 00:24:43,380
um I sometimes like to do that

683
00:24:41,460 --> 00:24:44,940
um just so that

684
00:24:43,380 --> 00:24:47,059
there's like a little bit of variation

685
00:24:44,940 --> 00:24:49,799
and diversity in the original

686
00:24:47,059 --> 00:24:51,419
initialization of these 10h neurons and

687
00:24:49,799 --> 00:24:53,580
I find in practice that that can help

688
00:24:51,419 --> 00:24:55,740
optimization a little bit

689
00:24:53,580 --> 00:24:57,360
and then the weights we can also just

690
00:24:55,740 --> 00:24:59,100
like squash so let's multiply everything

691
00:24:57,360 --> 00:25:01,500
by 0.1

692
00:24:59,100 --> 00:25:04,260
let's rerun the first batch

693
00:25:01,500 --> 00:25:06,960
and now let's look at this and well

694
00:25:04,260 --> 00:25:08,760
first let's look here

695
00:25:06,960 --> 00:25:10,860
you see now because we multiply doubly

696
00:25:08,760 --> 00:25:12,419
by 0.1 we have a much better histogram

697
00:25:10,860 --> 00:25:15,179
and that's because the pre-activations

698
00:25:12,419 --> 00:25:18,480
are now between negative 1.5 and 1.5 and

699
00:25:15,179 --> 00:25:20,820
this we expect much much less white

700
00:25:18,480 --> 00:25:23,340
okay there's no white

701
00:25:20,820 --> 00:25:26,700
so basically that's because there are no

702
00:25:23,340 --> 00:25:28,679
neurons that's saturated above 0.99 in

703
00:25:26,700 --> 00:25:30,840
either direction this is actually a

704
00:25:28,679 --> 00:25:31,679
pretty decent place to be

705
00:25:30,840 --> 00:25:36,440
um

706
00:25:31,679 --> 00:25:36,440
maybe we can go up a little bit

707
00:25:36,659 --> 00:25:42,120
it's very much am I changing W1 here so

708
00:25:39,480 --> 00:25:45,000
maybe we can go to point two

709
00:25:42,120 --> 00:25:47,220
okay so maybe something like this is is

710
00:25:45,000 --> 00:25:49,500
a nice distribution so maybe this is

711
00:25:47,220 --> 00:25:51,419
what our initialization should be so let

712
00:25:49,500 --> 00:25:53,279
me now erase

713
00:25:51,419 --> 00:25:56,059
these

714
00:25:53,279 --> 00:25:59,100
and let me starting with initialization

715
00:25:56,059 --> 00:26:02,820
let me run the full optimization without

716
00:25:59,100 --> 00:26:05,159
the break and uh let's see what we got

717
00:26:02,820 --> 00:26:06,960
okay so the optimization finished and I

718
00:26:05,159 --> 00:26:09,059
Rebrand the loss and this is the result

719
00:26:06,960 --> 00:26:10,740
that we get and then just as a reminder

720
00:26:09,059 --> 00:26:12,360
I put down all the losses that we saw

721
00:26:10,740 --> 00:26:14,520
previously in this lecture

722
00:26:12,360 --> 00:26:16,080
so we see that we actually do get an

723
00:26:14,520 --> 00:26:18,120
improvement here and just as a reminder

724
00:26:16,080 --> 00:26:20,820
we started off with a validation loss of

725
00:26:18,120 --> 00:26:22,860
2.17 when we started by fixing the

726
00:26:20,820 --> 00:26:25,440
softmax being confidently wrong we came

727
00:26:22,860 --> 00:26:27,360
down to 2.13 and by fixing the 10h layer

728
00:26:25,440 --> 00:26:28,799
being way too saturated we came down to

729
00:26:27,360 --> 00:26:30,240
2.10

730
00:26:28,799 --> 00:26:31,559
and the reason this is happening of

731
00:26:30,240 --> 00:26:33,179
course is because our initialization is

732
00:26:31,559 --> 00:26:35,820
better and so we're spending more time

733
00:26:33,179 --> 00:26:36,539
doing productive training instead of

734
00:26:35,820 --> 00:26:38,580
um

735
00:26:36,539 --> 00:26:41,340
not very productive training because our

736
00:26:38,580 --> 00:26:43,440
gradients are set to zero and we have to

737
00:26:41,340 --> 00:26:45,000
learn very simple things like the

738
00:26:43,440 --> 00:26:46,620
overconfidence of the softmax in the

739
00:26:45,000 --> 00:26:48,900
beginning and we're spending Cycles just

740
00:26:46,620 --> 00:26:51,900
like squashing down the weight Matrix

741
00:26:48,900 --> 00:26:53,760
so this is illustrating

742
00:26:51,900 --> 00:26:56,580
um basically initialization and its

743
00:26:53,760 --> 00:26:58,320
impacts on performance just by being

744
00:26:56,580 --> 00:26:59,520
aware of the internals of these neural

745
00:26:58,320 --> 00:27:02,340
Nets and their activations their

746
00:26:59,520 --> 00:27:03,960
gradients now we're working with a very

747
00:27:02,340 --> 00:27:06,659
small Network this is just one layer

748
00:27:03,960 --> 00:27:08,460
multiplayer perceptron so because the

749
00:27:06,659 --> 00:27:10,559
network is so shallow the optimization

750
00:27:08,460 --> 00:27:12,299
problem is actually quite easy and very

751
00:27:10,559 --> 00:27:13,919
forgiving so even though our

752
00:27:12,299 --> 00:27:16,200
initialization was terrible the network

753
00:27:13,919 --> 00:27:18,900
still learned eventually it just got a

754
00:27:16,200 --> 00:27:20,539
bit worse result this is not the case in

755
00:27:18,900 --> 00:27:23,039
general though once we actually start

756
00:27:20,539 --> 00:27:26,039
working with much deeper networks that

757
00:27:23,039 --> 00:27:28,860
have say 50 layers things can get much

758
00:27:26,039 --> 00:27:30,299
more complicated and these problems

759
00:27:28,860 --> 00:27:33,120
Stack Up

760
00:27:30,299 --> 00:27:34,559
and so you can actually get into a place

761
00:27:33,120 --> 00:27:36,000
where the network is basically not

762
00:27:34,559 --> 00:27:38,700
training at all if your initialization

763
00:27:36,000 --> 00:27:39,960
is bad enough and the deeper your

764
00:27:38,700 --> 00:27:42,059
network is and the more complex it is

765
00:27:39,960 --> 00:27:44,460
the less forgiving it is to some of

766
00:27:42,059 --> 00:27:45,960
these errors and so

767
00:27:44,460 --> 00:27:48,360
um something that we definitely be aware

768
00:27:45,960 --> 00:27:50,279
of and uh something to scrutinize

769
00:27:48,360 --> 00:27:52,679
something to plot and something to be

770
00:27:50,279 --> 00:27:54,659
careful with and um

771
00:27:52,679 --> 00:27:56,940
yeah okay so that's great that that

772
00:27:54,659 --> 00:27:58,740
worked for us but what we have here now

773
00:27:56,940 --> 00:28:00,360
is all these metric numbers like point

774
00:27:58,740 --> 00:28:02,400
two like where do I come up with this

775
00:28:00,360 --> 00:28:03,779
and how am I supposed to set these if I

776
00:28:02,400 --> 00:28:05,100
have a large neural left with lots and

777
00:28:03,779 --> 00:28:07,200
lots of layers

778
00:28:05,100 --> 00:28:08,940
and so obviously no one does this by

779
00:28:07,200 --> 00:28:11,880
hand there's actually some relatively

780
00:28:08,940 --> 00:28:13,140
principled ways of setting these scales

781
00:28:11,880 --> 00:28:13,980
um that I would like to introduce to you

782
00:28:13,140 --> 00:28:15,960
now

783
00:28:13,980 --> 00:28:17,700
so let me paste some code here that I

784
00:28:15,960 --> 00:28:19,440
prepared just to motivate the discussion

785
00:28:17,700 --> 00:28:21,539
of this

786
00:28:19,440 --> 00:28:24,299
so what I'm doing here is we have some

787
00:28:21,539 --> 00:28:27,419
random input here x that is drawn from a

788
00:28:24,299 --> 00:28:29,460
gaussian and there's 1000 examples that

789
00:28:27,419 --> 00:28:31,919
are 10 dimensional and then we have a

790
00:28:29,460 --> 00:28:33,659
weight and layer here that is also

791
00:28:31,919 --> 00:28:34,620
initialized using gaussian just like we

792
00:28:33,659 --> 00:28:37,320
did here

793
00:28:34,620 --> 00:28:39,720
and we these neurons in the hidden layer

794
00:28:37,320 --> 00:28:42,299
look at 10 inputs and there are 200

795
00:28:39,720 --> 00:28:44,400
neurons in this hidden layer and then we

796
00:28:42,299 --> 00:28:46,440
have here just like here

797
00:28:44,400 --> 00:28:47,880
um in this case the multiplication X

798
00:28:46,440 --> 00:28:50,700
multiplied by W to get the

799
00:28:47,880 --> 00:28:52,799
pre-activations of these neurons

800
00:28:50,700 --> 00:28:54,900
and basically the analysis here looks at

801
00:28:52,799 --> 00:28:56,760
okay suppose these are uniform gaussian

802
00:28:54,900 --> 00:29:00,179
and these weights are uniform gaussian

803
00:28:56,760 --> 00:29:03,120
if I do x times W and we forget for now

804
00:29:00,179 --> 00:29:04,860
the bias and the non-linearity

805
00:29:03,120 --> 00:29:06,900
then what is the mean and the standard

806
00:29:04,860 --> 00:29:09,539
deviation of these gaussians

807
00:29:06,900 --> 00:29:11,340
so in the beginning here the input is uh

808
00:29:09,539 --> 00:29:13,320
just a normal gaussian distribution mean

809
00:29:11,340 --> 00:29:15,179
zero and the standard deviation is one

810
00:29:13,320 --> 00:29:18,419
and the standard deviation again is just

811
00:29:15,179 --> 00:29:20,279
a measure of a spread of discussion

812
00:29:18,419 --> 00:29:24,120
but then once we multiply here and we

813
00:29:20,279 --> 00:29:25,919
look at the histogram of Y we see that

814
00:29:24,120 --> 00:29:28,140
the mean of course stays the same it's

815
00:29:25,919 --> 00:29:29,820
about zero because this is a symmetric

816
00:29:28,140 --> 00:29:31,919
operation but we see here that the

817
00:29:29,820 --> 00:29:34,140
standard deviation has expanded to three

818
00:29:31,919 --> 00:29:36,840
so the input standard deviation was one

819
00:29:34,140 --> 00:29:38,039
but now we've grown to three and so what

820
00:29:36,840 --> 00:29:40,919
you're seeing in the histogram is that

821
00:29:38,039 --> 00:29:42,419
this gaussian is expanding

822
00:29:40,919 --> 00:29:45,000
and so

823
00:29:42,419 --> 00:29:47,039
um we're expanding this gaussian from

824
00:29:45,000 --> 00:29:48,419
the input and we don't want that we want

825
00:29:47,039 --> 00:29:51,240
most of the neural Nets to have

826
00:29:48,419 --> 00:29:53,220
relatively similar activations so unit

827
00:29:51,240 --> 00:29:55,260
gaussian roughly throughout the neural

828
00:29:53,220 --> 00:29:59,399
net and so the question is how do we

829
00:29:55,260 --> 00:30:02,460
scale these wfs to preserve the um to

830
00:29:59,399 --> 00:30:03,600
preserve this distribution to remain a

831
00:30:02,460 --> 00:30:06,840
gaussian

832
00:30:03,600 --> 00:30:09,360
and so intuitively if I multiply here uh

833
00:30:06,840 --> 00:30:11,700
these elements of w by a large number

834
00:30:09,360 --> 00:30:14,820
let's say by five

835
00:30:11,700 --> 00:30:17,159
then this gaussian grows and grows in

836
00:30:14,820 --> 00:30:19,200
standard deviation so now we're at 15.

837
00:30:17,159 --> 00:30:21,360
so basically these numbers here in the

838
00:30:19,200 --> 00:30:22,500
output y take on more and more extreme

839
00:30:21,360 --> 00:30:24,799
values

840
00:30:22,500 --> 00:30:27,960
but if we scale it down well I say 0.2

841
00:30:24,799 --> 00:30:30,840
then conversely this gaussian is getting

842
00:30:27,960 --> 00:30:32,279
smaller and smaller and it's shrinking

843
00:30:30,840 --> 00:30:34,860
and you can see that the standard

844
00:30:32,279 --> 00:30:37,620
deviation is 0.6 and so the question is

845
00:30:34,860 --> 00:30:39,840
what do I multiply by here to exactly

846
00:30:37,620 --> 00:30:40,860
preserve the standard deviation to be

847
00:30:39,840 --> 00:30:42,299
one

848
00:30:40,860 --> 00:30:43,860
and it turns out that the correct answer

849
00:30:42,299 --> 00:30:47,340
mathematically when you work out through

850
00:30:43,860 --> 00:30:50,100
the variance of this multiplication here

851
00:30:47,340 --> 00:30:53,580
is that you are supposed to divide by

852
00:30:50,100 --> 00:30:56,640
the square root of the fan in the fan in

853
00:30:53,580 --> 00:30:58,799
is the basically the uh number of input

854
00:30:56,640 --> 00:31:01,559
elements here 10. so we are supposed to

855
00:30:58,799 --> 00:31:03,059
divide by 10 square root and this is one

856
00:31:01,559 --> 00:31:05,640
way to do the square root you raise it

857
00:31:03,059 --> 00:31:07,140
to a power of 0.5 that's the same as

858
00:31:05,640 --> 00:31:10,020
doing a square root

859
00:31:07,140 --> 00:31:12,299
so when you divide by the square root of

860
00:31:10,020 --> 00:31:15,360
10 then we see that

861
00:31:12,299 --> 00:31:17,880
the output gaussian it has exactly

862
00:31:15,360 --> 00:31:19,740
standard deviation of one now

863
00:31:17,880 --> 00:31:22,260
unsurprisingly a number of papers have

864
00:31:19,740 --> 00:31:24,480
looked into how but to best initialize

865
00:31:22,260 --> 00:31:25,799
neural networks and in the case of

866
00:31:24,480 --> 00:31:27,480
multiplayer perceptions we can have

867
00:31:25,799 --> 00:31:29,940
fairly deep networks that have these

868
00:31:27,480 --> 00:31:31,440
nonlinearities in between and we want to

869
00:31:29,940 --> 00:31:33,059
make sure that the activations are well

870
00:31:31,440 --> 00:31:35,039
behaved and they don't expand to

871
00:31:33,059 --> 00:31:36,659
infinity or Shrink all the way to zero

872
00:31:35,039 --> 00:31:38,159
and the question is how do we initialize

873
00:31:36,659 --> 00:31:39,840
the weights so that these activations

874
00:31:38,159 --> 00:31:40,860
take on reasonable values throughout the

875
00:31:39,840 --> 00:31:43,080
network

876
00:31:40,860 --> 00:31:44,460
now one paper that has stuck this in

877
00:31:43,080 --> 00:31:46,380
quite a bit of detail that is often

878
00:31:44,460 --> 00:31:48,120
referenced is this paper by coming here

879
00:31:46,380 --> 00:31:50,279
at all called the delving deep into

880
00:31:48,120 --> 00:31:52,020
rectifiers now in this case they

881
00:31:50,279 --> 00:31:54,659
actually study convolutional neural

882
00:31:52,020 --> 00:31:57,120
networks and they studied especially the

883
00:31:54,659 --> 00:31:58,860
relu nonlinearity and the p-valued

884
00:31:57,120 --> 00:32:00,960
nonlinearity instead of a 10 H

885
00:31:58,860 --> 00:32:02,760
nonlinearity but the analysis is very

886
00:32:00,960 --> 00:32:05,520
similar and

887
00:32:02,760 --> 00:32:08,100
um basically what happens here is for

888
00:32:05,520 --> 00:32:10,260
them the the relation that they care

889
00:32:08,100 --> 00:32:13,020
about quite a bit here is a squashing

890
00:32:10,260 --> 00:32:16,320
function where all the negative numbers

891
00:32:13,020 --> 00:32:18,000
are simply clamped to zero so the

892
00:32:16,320 --> 00:32:20,460
positive numbers are passed through but

893
00:32:18,000 --> 00:32:22,440
everything negative is just set to zero

894
00:32:20,460 --> 00:32:24,059
and because uh you are basically

895
00:32:22,440 --> 00:32:26,580
throwing away half of the distribution

896
00:32:24,059 --> 00:32:28,200
they find in their analysis of the

897
00:32:26,580 --> 00:32:29,820
forward activations in the neural net

898
00:32:28,200 --> 00:32:31,980
that you have to compensate for that

899
00:32:29,820 --> 00:32:33,720
with a gain

900
00:32:31,980 --> 00:32:36,299
and so here

901
00:32:33,720 --> 00:32:37,740
they find that basically when they

902
00:32:36,299 --> 00:32:39,659
initialize their weights they have to do

903
00:32:37,740 --> 00:32:41,640
it with a zero mean gaussian whose

904
00:32:39,659 --> 00:32:43,320
standard deviation is square root of 2

905
00:32:41,640 --> 00:32:45,360
over the fan in

906
00:32:43,320 --> 00:32:47,820
what we have here is we are initializing

907
00:32:45,360 --> 00:32:48,840
a concussion with the square root of

908
00:32:47,820 --> 00:32:52,020
fanin

909
00:32:48,840 --> 00:32:54,360
this NL here is the Fanon so what we

910
00:32:52,020 --> 00:32:55,320
have is square root of one over the fan

911
00:32:54,360 --> 00:32:57,960
in

912
00:32:55,320 --> 00:32:59,940
because we have the division here

913
00:32:57,960 --> 00:33:01,980
now they have to add this factor of 2

914
00:32:59,940 --> 00:33:04,140
because of the relu which basically

915
00:33:01,980 --> 00:33:06,179
discards half of the distribution and

916
00:33:04,140 --> 00:33:07,860
clamps it at zero and so that's where

917
00:33:06,179 --> 00:33:10,200
you get an initial Factor

918
00:33:07,860 --> 00:33:12,779
now in addition to that this paper also

919
00:33:10,200 --> 00:33:14,279
studies not just the uh sort of behavior

920
00:33:12,779 --> 00:33:16,440
of the activations in the forward pass

921
00:33:14,279 --> 00:33:18,419
of the neural net but it also studies

922
00:33:16,440 --> 00:33:20,220
the back propagation and we have to make

923
00:33:18,419 --> 00:33:22,380
sure that the gradients also are well

924
00:33:20,220 --> 00:33:24,000
behaved and so

925
00:33:22,380 --> 00:33:25,679
um because ultimately they end up

926
00:33:24,000 --> 00:33:27,840
updating our parameters

927
00:33:25,679 --> 00:33:29,340
and what they find here through a lot of

928
00:33:27,840 --> 00:33:30,720
the analysis that I invite you to read

929
00:33:29,340 --> 00:33:33,720
through but it's not exactly

930
00:33:30,720 --> 00:33:35,640
approachable what they find is basically

931
00:33:33,720 --> 00:33:37,559
if you properly initialize the forward

932
00:33:35,640 --> 00:33:40,260
pass the backward pass is also

933
00:33:37,559 --> 00:33:42,480
approximately initialized up to a

934
00:33:40,260 --> 00:33:45,480
constant factor that has to do with the

935
00:33:42,480 --> 00:33:48,779
size of the number of hidden neurons in

936
00:33:45,480 --> 00:33:50,760
an early and uh late layer

937
00:33:48,779 --> 00:33:52,320
and uh but basically they find

938
00:33:50,760 --> 00:33:53,940
empirically that this is not a choice

939
00:33:52,320 --> 00:33:56,760
that matters too much

940
00:33:53,940 --> 00:33:59,059
now this timing initialization is also

941
00:33:56,760 --> 00:34:01,080
implemented in pytorch so if you go to

942
00:33:59,059 --> 00:34:02,399
torch.nn.net documentation you'll find

943
00:34:01,080 --> 00:34:04,200
timing normal

944
00:34:02,399 --> 00:34:05,940
and in my opinion this is probably the

945
00:34:04,200 --> 00:34:07,380
most common way of initializing neural

946
00:34:05,940 --> 00:34:09,119
networks now

947
00:34:07,380 --> 00:34:12,240
and it takes a few keyword arguments

948
00:34:09,119 --> 00:34:14,159
here so number one it wants to know the

949
00:34:12,240 --> 00:34:15,480
mode would you like to normalize the

950
00:34:14,159 --> 00:34:18,020
activations or would you like to

951
00:34:15,480 --> 00:34:20,879
normalize the gradients to to be always

952
00:34:18,020 --> 00:34:23,339
gaussian with zero mean and a unit or

953
00:34:20,879 --> 00:34:24,599
one standard deviation and because they

954
00:34:23,339 --> 00:34:26,159
find the paper that this doesn't matter

955
00:34:24,599 --> 00:34:28,619
too much most of the people just leave

956
00:34:26,159 --> 00:34:30,359
it as the default which is Fan in and

957
00:34:28,619 --> 00:34:32,399
then second passing the nonlinearity

958
00:34:30,359 --> 00:34:34,740
that you are using because depending on

959
00:34:32,399 --> 00:34:36,599
the nonlinearity we need to calculate a

960
00:34:34,740 --> 00:34:39,720
slightly different gain and so if your

961
00:34:36,599 --> 00:34:41,700
nonlinearity is just linear so there's

962
00:34:39,720 --> 00:34:44,040
no nonlinearity then the gain here will

963
00:34:41,700 --> 00:34:46,260
be one and we have the exact same uh

964
00:34:44,040 --> 00:34:47,580
kind of formula that we've got here

965
00:34:46,260 --> 00:34:48,780
but if the nonlinearity is something

966
00:34:47,580 --> 00:34:50,940
else we're going to get a slightly

967
00:34:48,780 --> 00:34:52,020
different gain and so if we come up here

968
00:34:50,940 --> 00:34:53,700
to the top

969
00:34:52,020 --> 00:34:56,099
we see that for example in the case of

970
00:34:53,700 --> 00:34:57,540
relu this gain is a square root of 2.

971
00:34:56,099 --> 00:35:00,800
and the reason it's a square root

972
00:34:57,540 --> 00:35:00,800
because in this paper

973
00:35:02,820 --> 00:35:07,980
you see how the two is inside of the

974
00:35:05,580 --> 00:35:08,940
square root so the gain is a square root

975
00:35:07,980 --> 00:35:12,300
of 2.

976
00:35:08,940 --> 00:35:14,339
in the case of linear or identity we

977
00:35:12,300 --> 00:35:16,380
just get a gain of one in the case of

978
00:35:14,339 --> 00:35:18,780
10h which is what we're using here the

979
00:35:16,380 --> 00:35:21,240
advised gain is a 5 over 3.

980
00:35:18,780 --> 00:35:23,579
and intuitively why do we need a gain on

981
00:35:21,240 --> 00:35:25,920
top of the initialization is because 10h

982
00:35:23,579 --> 00:35:28,320
just like relu is a contractive

983
00:35:25,920 --> 00:35:29,820
transformation so what that means is

984
00:35:28,320 --> 00:35:31,920
you're taking the output distribution

985
00:35:29,820 --> 00:35:33,839
from this matrix multiplication and then

986
00:35:31,920 --> 00:35:35,460
you are squashing it in some way now

987
00:35:33,839 --> 00:35:38,220
relu squashes it by taking everything

988
00:35:35,460 --> 00:35:39,720
below zero and clamping it to zero tan H

989
00:35:38,220 --> 00:35:41,280
also squashes it because it's a

990
00:35:39,720 --> 00:35:42,740
contractual operation it will take the

991
00:35:41,280 --> 00:35:45,540
Tails and it will

992
00:35:42,740 --> 00:35:48,240
squeeze them in and so in order to fight

993
00:35:45,540 --> 00:35:49,560
the squeezing in we need to boost the

994
00:35:48,240 --> 00:35:51,660
weights a little bit so that we

995
00:35:49,560 --> 00:35:53,400
renormalize everything back to standard

996
00:35:51,660 --> 00:35:55,320
unit standard deviation

997
00:35:53,400 --> 00:35:56,460
so that's why there's a little bit of a

998
00:35:55,320 --> 00:35:58,200
gain that comes out

999
00:35:56,460 --> 00:35:59,520
now I'm skipping through this section A

1000
00:35:58,200 --> 00:36:01,380
little bit quickly and I'm doing that

1001
00:35:59,520 --> 00:36:03,060
actually intentionally and the reason

1002
00:36:01,380 --> 00:36:04,920
for that is because

1003
00:36:03,060 --> 00:36:07,079
about seven years ago when this paper

1004
00:36:04,920 --> 00:36:08,640
was written you had to actually be

1005
00:36:07,079 --> 00:36:10,800
extremely careful with the activations

1006
00:36:08,640 --> 00:36:12,540
and ingredients and their ranges and

1007
00:36:10,800 --> 00:36:13,920
their histograms and you have to be very

1008
00:36:12,540 --> 00:36:15,359
careful with the precise setting of

1009
00:36:13,920 --> 00:36:17,400
gains and the scrutinizing of the

1010
00:36:15,359 --> 00:36:19,140
nonlinearities used and so on and

1011
00:36:17,400 --> 00:36:21,480
everything was very finicky and very

1012
00:36:19,140 --> 00:36:23,160
fragile and very properly arranged for

1013
00:36:21,480 --> 00:36:24,780
the neural not to train especially if

1014
00:36:23,160 --> 00:36:25,859
your neural network was very deep

1015
00:36:24,780 --> 00:36:27,060
but there are a number of modern

1016
00:36:25,859 --> 00:36:28,859
innovations that have made everything

1017
00:36:27,060 --> 00:36:30,599
significantly more stable and more

1018
00:36:28,859 --> 00:36:32,280
well-behaved and has become less

1019
00:36:30,599 --> 00:36:33,780
important to initialize these networks

1020
00:36:32,280 --> 00:36:35,520
exactly right

1021
00:36:33,780 --> 00:36:37,619
and some of those modern Innovations for

1022
00:36:35,520 --> 00:36:40,140
example are residual connections which

1023
00:36:37,619 --> 00:36:43,260
we will cover in the future the use of a

1024
00:36:40,140 --> 00:36:45,300
number of normalization layers like for

1025
00:36:43,260 --> 00:36:47,280
example batch normalization layer

1026
00:36:45,300 --> 00:36:48,540
normalization group normalization we're

1027
00:36:47,280 --> 00:36:50,640
going to go into a lot of these as well

1028
00:36:48,540 --> 00:36:52,740
and number three much better optimizers

1029
00:36:50,640 --> 00:36:54,480
not just stochastic gradient descent the

1030
00:36:52,740 --> 00:36:56,400
simple Optimizer we're basically using

1031
00:36:54,480 --> 00:36:58,740
here but a slightly more complex

1032
00:36:56,400 --> 00:37:00,540
optimizers like RMS prop and especially

1033
00:36:58,740 --> 00:37:02,880
Adam and so all of these modern

1034
00:37:00,540 --> 00:37:04,200
Innovations make it less important for

1035
00:37:02,880 --> 00:37:06,480
you to precisely calibrate the

1036
00:37:04,200 --> 00:37:08,640
initialization of the neural net all

1037
00:37:06,480 --> 00:37:10,619
that being said in practice uh what

1038
00:37:08,640 --> 00:37:12,480
should we do in practice when I

1039
00:37:10,619 --> 00:37:14,460
initialize these neural Nets I basically

1040
00:37:12,480 --> 00:37:18,060
just normalize my weights by the square

1041
00:37:14,460 --> 00:37:20,700
root of the fan in uh so basically uh

1042
00:37:18,060 --> 00:37:22,440
roughly what we did here is what I do

1043
00:37:20,700 --> 00:37:27,300
now if we want to be exactly accurate

1044
00:37:22,440 --> 00:37:30,060
here we and go by init of coming normal

1045
00:37:27,300 --> 00:37:31,500
this is how a good implemented we want

1046
00:37:30,060 --> 00:37:33,960
to set the standard deviation to be

1047
00:37:31,500 --> 00:37:37,380
gained over the square root of fan n

1048
00:37:33,960 --> 00:37:39,839
right so to set the standard deviation

1049
00:37:37,380 --> 00:37:41,160
of our weights we will proceed as

1050
00:37:39,839 --> 00:37:43,200
follows

1051
00:37:41,160 --> 00:37:45,000
basically when we have a torch that

1052
00:37:43,200 --> 00:37:46,619
renin and let's say I just create a

1053
00:37:45,000 --> 00:37:47,940
thousand numbers we can look at the

1054
00:37:46,619 --> 00:37:49,859
standard deviation of this and of course

1055
00:37:47,940 --> 00:37:51,240
that's one that's the amount of spread

1056
00:37:49,859 --> 00:37:52,380
let's make this a bit bigger so it's

1057
00:37:51,240 --> 00:37:55,320
closer to one

1058
00:37:52,380 --> 00:37:58,020
so that's the spread of the gaussian of

1059
00:37:55,320 --> 00:37:59,940
zero mean and unit standard deviation

1060
00:37:58,020 --> 00:38:02,280
now basically when you take these and

1061
00:37:59,940 --> 00:38:04,200
you multiply by say 0.2

1062
00:38:02,280 --> 00:38:06,000
that basically scales down the gaussian

1063
00:38:04,200 --> 00:38:08,280
and that makes its standard deviation

1064
00:38:06,000 --> 00:38:09,780
0.2 so basically the number that you

1065
00:38:08,280 --> 00:38:12,119
multiply by here ends up being the

1066
00:38:09,780 --> 00:38:15,960
standard deviation of this caution

1067
00:38:12,119 --> 00:38:19,200
so here this is a standard deviation 0.2

1068
00:38:15,960 --> 00:38:20,579
gaussian here when we sample rw1

1069
00:38:19,200 --> 00:38:23,579
but we want to set the standard

1070
00:38:20,579 --> 00:38:26,040
deviation to gain over square root of

1071
00:38:23,579 --> 00:38:28,520
fan mode which is valid

1072
00:38:26,040 --> 00:38:34,320
so in other words we want to multiply by

1073
00:38:28,520 --> 00:38:36,599
gain which for 10 H is 5 over 3.

1074
00:38:34,320 --> 00:38:39,420
5 over 3 is the gain

1075
00:38:36,599 --> 00:38:41,480
and then times

1076
00:38:39,420 --> 00:38:41,480
um

1077
00:38:44,280 --> 00:38:52,140
I guess sorry divide

1078
00:38:47,400 --> 00:38:54,180
uh square root of the fan in and in this

1079
00:38:52,140 --> 00:38:56,280
example here the fan in was 10 and I

1080
00:38:54,180 --> 00:38:58,740
just noticed that actually here the fan

1081
00:38:56,280 --> 00:39:00,780
in for W1 is actually an embed times

1082
00:38:58,740 --> 00:39:02,880
block size which as you all recall is

1083
00:39:00,780 --> 00:39:04,320
actually 30 and that's because each

1084
00:39:02,880 --> 00:39:05,700
character is 10 dimensional but then we

1085
00:39:04,320 --> 00:39:07,800
have three of them and we concatenate

1086
00:39:05,700 --> 00:39:09,540
them so actually the fan in here was 30

1087
00:39:07,800 --> 00:39:13,560
and I should have used 30 here probably

1088
00:39:09,540 --> 00:39:15,480
but basically we want 30 square root so

1089
00:39:13,560 --> 00:39:17,400
this is the number this is what our

1090
00:39:15,480 --> 00:39:19,440
standard deviation we want to be and

1091
00:39:17,400 --> 00:39:21,540
this number turns out to be 0.3

1092
00:39:19,440 --> 00:39:22,740
whereas here just by fiddling with it

1093
00:39:21,540 --> 00:39:24,780
and looking at the distribution and

1094
00:39:22,740 --> 00:39:25,859
making sure it looks okay we came up

1095
00:39:24,780 --> 00:39:27,660
with 0.2

1096
00:39:25,859 --> 00:39:29,579
and so instead what we want to do here

1097
00:39:27,660 --> 00:39:31,680
is we want to make the standard

1098
00:39:29,579 --> 00:39:33,240
deviation B

1099
00:39:31,680 --> 00:39:36,900
um

1100
00:39:33,240 --> 00:39:39,060
5 over 3 which is our gain divide

1101
00:39:36,900 --> 00:39:42,359
this amount

1102
00:39:39,060 --> 00:39:44,640
times 0.2 square root and these brackets

1103
00:39:42,359 --> 00:39:46,560
here are not that necessary but I'll

1104
00:39:44,640 --> 00:39:48,119
just put them here for clarity this is

1105
00:39:46,560 --> 00:39:51,000
basically what we want this is the

1106
00:39:48,119 --> 00:39:53,220
chiming in it in our case for a 10h

1107
00:39:51,000 --> 00:39:55,500
nonlinearity and this is how we would

1108
00:39:53,220 --> 00:39:58,619
initialize the neural net and so we're

1109
00:39:55,500 --> 00:40:00,900
multiplying by 0.3 instead of

1110
00:39:58,619 --> 00:40:02,940
multiplying by 0.2

1111
00:40:00,900 --> 00:40:05,700
and so we can

1112
00:40:02,940 --> 00:40:07,380
we can initialize this way and then we

1113
00:40:05,700 --> 00:40:08,040
can train the neural net and see what we

1114
00:40:07,380 --> 00:40:09,780
got

1115
00:40:08,040 --> 00:40:12,480
okay so I trained the neural net and we

1116
00:40:09,780 --> 00:40:13,920
end up in roughly the same spot so

1117
00:40:12,480 --> 00:40:16,920
looking at the validation loss we now

1118
00:40:13,920 --> 00:40:18,599
get 2.10 and previously we also had 2.10

1119
00:40:16,920 --> 00:40:20,099
and there's a little bit of a difference

1120
00:40:18,599 --> 00:40:21,420
but that's just the randomness of the

1121
00:40:20,099 --> 00:40:23,160
process I suspect

1122
00:40:21,420 --> 00:40:25,740
but the big deal of course is we get to

1123
00:40:23,160 --> 00:40:29,640
the same spot but we did not have to

1124
00:40:25,740 --> 00:40:31,440
introduce any magic numbers that we got

1125
00:40:29,640 --> 00:40:33,240
from just looking at histograms and

1126
00:40:31,440 --> 00:40:35,640
guessing checking we have something that

1127
00:40:33,240 --> 00:40:38,040
is semi-principled and will scale us to

1128
00:40:35,640 --> 00:40:40,500
much bigger networks and uh something

1129
00:40:38,040 --> 00:40:41,880
that we can sort of use as a guide so I

1130
00:40:40,500 --> 00:40:43,980
mentioned that the precise setting of

1131
00:40:41,880 --> 00:40:45,240
these initializations is not as

1132
00:40:43,980 --> 00:40:47,040
important today due to some Modern

1133
00:40:45,240 --> 00:40:48,240
Innovations and I think now is a pretty

1134
00:40:47,040 --> 00:40:49,920
good time to introduce one of those

1135
00:40:48,240 --> 00:40:51,119
modern Innovations and that is best

1136
00:40:49,920 --> 00:40:54,180
normalization

1137
00:40:51,119 --> 00:40:56,820
so batch normalization came out in 2015

1138
00:40:54,180 --> 00:40:58,920
from a team at Google and it was an

1139
00:40:56,820 --> 00:41:00,599
extremely impactful paper because it

1140
00:40:58,920 --> 00:41:03,900
made it possible to train very deep

1141
00:41:00,599 --> 00:41:05,640
neural Nets quite reliably and uh it

1142
00:41:03,900 --> 00:41:06,780
basically just worked so here's what

1143
00:41:05,640 --> 00:41:08,640
nationalization does and what's

1144
00:41:06,780 --> 00:41:09,780
implemented

1145
00:41:08,640 --> 00:41:12,660
um

1146
00:41:09,780 --> 00:41:14,880
basically we have these hidden States HP

1147
00:41:12,660 --> 00:41:17,240
act right and we were talking about how

1148
00:41:14,880 --> 00:41:20,099
we don't want these uh these

1149
00:41:17,240 --> 00:41:23,160
pre-activation states to be way too

1150
00:41:20,099 --> 00:41:24,839
small because then the 10h is not doing

1151
00:41:23,160 --> 00:41:26,400
anything but we don't want them to be

1152
00:41:24,839 --> 00:41:27,420
too large because then the 10h is

1153
00:41:26,400 --> 00:41:29,280
saturated

1154
00:41:27,420 --> 00:41:31,980
in fact we want them to be roughly

1155
00:41:29,280 --> 00:41:34,680
roughly gaussian so zero mean and a unit

1156
00:41:31,980 --> 00:41:36,000
or one standard deviation at least at

1157
00:41:34,680 --> 00:41:38,220
initialization

1158
00:41:36,000 --> 00:41:40,079
so the Insight from The Bachelor

1159
00:41:38,220 --> 00:41:42,240
normalization paper is okay you have

1160
00:41:40,079 --> 00:41:44,640
these hidden States and you'd like them

1161
00:41:42,240 --> 00:41:46,980
to be roughly gaussian then why not take

1162
00:41:44,640 --> 00:41:49,680
the hidden States and just normalize

1163
00:41:46,980 --> 00:41:51,240
them to be gaussian and it sounds kind

1164
00:41:49,680 --> 00:41:55,020
of crazy but you can just do that

1165
00:41:51,240 --> 00:41:57,240
because uh standardizing hidden States

1166
00:41:55,020 --> 00:41:58,680
so that their unit caution is a

1167
00:41:57,240 --> 00:42:00,480
perfectly differentiable operation as

1168
00:41:58,680 --> 00:42:02,520
we'll soon see and so that was kind of

1169
00:42:00,480 --> 00:42:04,260
like the big Insight in this paper and

1170
00:42:02,520 --> 00:42:05,820
when I first read it my mind was blown

1171
00:42:04,260 --> 00:42:07,560
because you can just normalize these

1172
00:42:05,820 --> 00:42:10,440
hidden States and if you'd like unit

1173
00:42:07,560 --> 00:42:12,359
gaussian States in your network at least

1174
00:42:10,440 --> 00:42:15,480
initialization you can just normalize

1175
00:42:12,359 --> 00:42:17,520
them to be in gaussian so let's see how

1176
00:42:15,480 --> 00:42:19,200
that works so we're going to scroll to

1177
00:42:17,520 --> 00:42:21,359
our pre-activations here just before

1178
00:42:19,200 --> 00:42:22,920
they enter into the 10 age

1179
00:42:21,359 --> 00:42:24,480
now the idea again is remember we're

1180
00:42:22,920 --> 00:42:26,579
trying to make these roughly gaussian

1181
00:42:24,480 --> 00:42:29,220
and that's because if these are way too

1182
00:42:26,579 --> 00:42:31,680
small numbers then the 10h here is kind

1183
00:42:29,220 --> 00:42:34,440
of connective but if these are very

1184
00:42:31,680 --> 00:42:36,660
large numbers then the 10h is way too

1185
00:42:34,440 --> 00:42:39,060
saturated and grade is in the flow so

1186
00:42:36,660 --> 00:42:41,520
we'd like this to be roughly caution

1187
00:42:39,060 --> 00:42:43,560
so the Insight in bathroomization again

1188
00:42:41,520 --> 00:42:46,800
is that we can just standardize these

1189
00:42:43,560 --> 00:42:49,800
activations so they are exactly gaussian

1190
00:42:46,800 --> 00:42:53,220
so here hpact

1191
00:42:49,800 --> 00:42:55,980
has a shape of 32 by 200 32 examples by

1192
00:42:53,220 --> 00:42:57,480
200 neurons in the hidden layer

1193
00:42:55,980 --> 00:42:59,700
so basically what we can do is we can

1194
00:42:57,480 --> 00:43:00,619
take hpact and we can just calculate the

1195
00:42:59,700 --> 00:43:03,480
mean

1196
00:43:00,619 --> 00:43:05,280
and the mean we want to calculate across

1197
00:43:03,480 --> 00:43:08,339
the zeroth dimension

1198
00:43:05,280 --> 00:43:11,640
and we want to also keep them as true so

1199
00:43:08,339 --> 00:43:13,500
that we can easily broadcast this

1200
00:43:11,640 --> 00:43:16,380
so the shape of this

1201
00:43:13,500 --> 00:43:19,440
is 1 by 200 in other words we are doing

1202
00:43:16,380 --> 00:43:20,700
the mean over all the uh elements in the

1203
00:43:19,440 --> 00:43:22,920
batch

1204
00:43:20,700 --> 00:43:26,900
and similarly we can calculate the

1205
00:43:22,920 --> 00:43:26,900
standard deviation of these activations

1206
00:43:26,940 --> 00:43:32,160
and that will also be one by 200.

1207
00:43:29,339 --> 00:43:35,460
now in this paper they have the

1208
00:43:32,160 --> 00:43:37,260
uh sort of prescription here and see

1209
00:43:35,460 --> 00:43:40,980
here we are calculating the mean which

1210
00:43:37,260 --> 00:43:44,339
is just taking the average value

1211
00:43:40,980 --> 00:43:46,079
of any neurons activation and then the

1212
00:43:44,339 --> 00:43:46,819
standard deviation is basically kind of

1213
00:43:46,079 --> 00:43:49,200
like

1214
00:43:46,819 --> 00:43:51,420
this the measure of the spread that

1215
00:43:49,200 --> 00:43:54,300
we've been using which is the distance

1216
00:43:51,420 --> 00:43:58,619
of every one of these values away from

1217
00:43:54,300 --> 00:44:02,040
the mean and that squared and averaged

1218
00:43:58,619 --> 00:44:03,000
that's the that's the variance and then

1219
00:44:02,040 --> 00:44:04,800
if you want to take the standard

1220
00:44:03,000 --> 00:44:07,680
deviation you would square root the

1221
00:44:04,800 --> 00:44:09,359
variance to get the standard deviation

1222
00:44:07,680 --> 00:44:10,920
so these are the two that we're

1223
00:44:09,359 --> 00:44:13,440
calculating and now we're going to

1224
00:44:10,920 --> 00:44:15,359
normalize or standardize these X's by

1225
00:44:13,440 --> 00:44:17,819
subtracting the mean and

1226
00:44:15,359 --> 00:44:20,579
um dividing by the standard deviation

1227
00:44:17,819 --> 00:44:23,099
so basically we're taking Edge preact

1228
00:44:20,579 --> 00:44:25,640
and we subtract

1229
00:44:23,099 --> 00:44:25,640
the mean

1230
00:44:29,460 --> 00:44:34,260
and then we divide by the standard

1231
00:44:32,040 --> 00:44:36,660
deviation

1232
00:44:34,260 --> 00:44:38,339
this is exactly what these two STD and

1233
00:44:36,660 --> 00:44:40,380
mean are calculating

1234
00:44:38,339 --> 00:44:42,480
oops

1235
00:44:40,380 --> 00:44:44,579
sorry this is the mean and this is the

1236
00:44:42,480 --> 00:44:45,900
variance you see how the sigma is the

1237
00:44:44,579 --> 00:44:47,880
standard deviation usually so this is

1238
00:44:45,900 --> 00:44:50,880
Sigma Square which is variance is the

1239
00:44:47,880 --> 00:44:52,500
square of the standard deviation

1240
00:44:50,880 --> 00:44:54,420
so this is how you standardize these

1241
00:44:52,500 --> 00:44:56,640
values and what this will do is that

1242
00:44:54,420 --> 00:44:59,280
every single neuron now and its firing

1243
00:44:56,640 --> 00:45:01,619
rate will be exactly unit gaussian on

1244
00:44:59,280 --> 00:45:02,460
these 32 examples at least of this batch

1245
00:45:01,619 --> 00:45:04,740
that's why it's called batch

1246
00:45:02,460 --> 00:45:06,540
normalization we are normalizing these

1247
00:45:04,740 --> 00:45:09,119
batches

1248
00:45:06,540 --> 00:45:11,099
and then we could in principle train

1249
00:45:09,119 --> 00:45:12,780
this notice that calculating the mean

1250
00:45:11,099 --> 00:45:14,040
and the standard deviation these are

1251
00:45:12,780 --> 00:45:15,839
just mathematical formulas they're

1252
00:45:14,040 --> 00:45:17,400
perfectly differentiable all this is

1253
00:45:15,839 --> 00:45:18,660
perfectly differentiable and we can just

1254
00:45:17,400 --> 00:45:20,160
strain this

1255
00:45:18,660 --> 00:45:23,339
the problem is you actually won't

1256
00:45:20,160 --> 00:45:25,740
achieve a very good result with this and

1257
00:45:23,339 --> 00:45:27,839
the reason for that is

1258
00:45:25,740 --> 00:45:30,720
we want these to be roughly gaussian but

1259
00:45:27,839 --> 00:45:32,760
only at initialization but we don't want

1260
00:45:30,720 --> 00:45:35,339
these to be to be forced to be gaussian

1261
00:45:32,760 --> 00:45:37,800
always we would actually We'll add the

1262
00:45:35,339 --> 00:45:39,660
neural nuts to move this around to

1263
00:45:37,800 --> 00:45:41,940
potentially make it more diffuse to make

1264
00:45:39,660 --> 00:45:43,740
it more sharp to make some 10 H neurons

1265
00:45:41,940 --> 00:45:46,020
maybe mean more trigger more trigger

1266
00:45:43,740 --> 00:45:47,760
happy or less trigger happy so we'd like

1267
00:45:46,020 --> 00:45:49,260
this distribution to move around and

1268
00:45:47,760 --> 00:45:51,180
we'd like the back propagation to tell

1269
00:45:49,260 --> 00:45:54,300
us how that distribution should move

1270
00:45:51,180 --> 00:45:57,240
around and so in addition to this idea

1271
00:45:54,300 --> 00:45:58,920
of standardizing the activations at any

1272
00:45:57,240 --> 00:46:00,960
point in the network

1273
00:45:58,920 --> 00:46:02,819
uh we have to also introduce this

1274
00:46:00,960 --> 00:46:05,760
additional component in the paper

1275
00:46:02,819 --> 00:46:07,020
here describe the scale and shift

1276
00:46:05,760 --> 00:46:09,480
and so basically what we're doing is

1277
00:46:07,020 --> 00:46:11,520
we're taking these normalized inputs and

1278
00:46:09,480 --> 00:46:14,520
we are additionally scaling them by some

1279
00:46:11,520 --> 00:46:17,700
gain and offsetting them by some bias to

1280
00:46:14,520 --> 00:46:19,380
get our final output from this layer

1281
00:46:17,700 --> 00:46:20,339
and so what that amounts to is the

1282
00:46:19,380 --> 00:46:21,900
following

1283
00:46:20,339 --> 00:46:23,880
we are going to allow a batch

1284
00:46:21,900 --> 00:46:27,420
normalization gain

1285
00:46:23,880 --> 00:46:29,880
to be initialized at just a once

1286
00:46:27,420 --> 00:46:32,220
and the ones will be in the shape of 1

1287
00:46:29,880 --> 00:46:35,220
by n hidden

1288
00:46:32,220 --> 00:46:37,680
and then we also will have a b and bias

1289
00:46:35,220 --> 00:46:40,440
which will be torched at zeros

1290
00:46:37,680 --> 00:46:42,119
and it will also be of the shape n by 1

1291
00:46:40,440 --> 00:46:43,500
by and hidden

1292
00:46:42,119 --> 00:46:47,220
and then here

1293
00:46:43,500 --> 00:46:51,119
the B and gain will multiply this

1294
00:46:47,220 --> 00:46:52,920
and the BN bias will offset it here

1295
00:46:51,119 --> 00:46:54,780
so because this is initialized to one

1296
00:46:52,920 --> 00:46:58,200
and this to zero

1297
00:46:54,780 --> 00:47:00,900
at initialization each neuron's firing

1298
00:46:58,200 --> 00:47:02,760
values in this batch will be exactly

1299
00:47:00,900 --> 00:47:04,740
unit gaussian and we'll have nice

1300
00:47:02,760 --> 00:47:07,859
numbers no matter what the distribution

1301
00:47:04,740 --> 00:47:09,960
of the hpact is coming in coming out it

1302
00:47:07,859 --> 00:47:11,400
will be in gaussian for each neuron and

1303
00:47:09,960 --> 00:47:13,619
that's roughly what we want at least at

1304
00:47:11,400 --> 00:47:15,839
initialization

1305
00:47:13,619 --> 00:47:17,700
um and then during optimization we'll be

1306
00:47:15,839 --> 00:47:19,920
able to back propagate to be in game and

1307
00:47:17,700 --> 00:47:22,140
being biased and change them so the

1308
00:47:19,920 --> 00:47:24,240
network is given the full ability to do

1309
00:47:22,140 --> 00:47:25,560
with this whatever it wants uh

1310
00:47:24,240 --> 00:47:28,740
internally

1311
00:47:25,560 --> 00:47:31,619
here we just have to make sure that we

1312
00:47:28,740 --> 00:47:32,880
um include these in the parameters of

1313
00:47:31,619 --> 00:47:35,700
the neural nut because they will be

1314
00:47:32,880 --> 00:47:37,380
trained with back propagation

1315
00:47:35,700 --> 00:47:41,300
so let's initialize this

1316
00:47:37,380 --> 00:47:41,300
and then we should be able to train

1317
00:47:45,599 --> 00:47:49,619
and then we're going to also

1318
00:47:47,640 --> 00:47:51,839
copy this line

1319
00:47:49,619 --> 00:47:54,060
which is the best normalization layer

1320
00:47:51,839 --> 00:47:55,560
here on a single line of code and we're

1321
00:47:54,060 --> 00:47:57,900
going to swing down here and we're also

1322
00:47:55,560 --> 00:48:00,319
going to do the exact same thing at test

1323
00:47:57,900 --> 00:48:00,319
time here

1324
00:48:01,500 --> 00:48:06,900
so similar to training time we're going

1325
00:48:03,720 --> 00:48:08,400
to normalize and then scale and that's

1326
00:48:06,900 --> 00:48:10,680
going to give us our train and

1327
00:48:08,400 --> 00:48:11,880
validation loss

1328
00:48:10,680 --> 00:48:12,960
and we'll see in a second that we're

1329
00:48:11,880 --> 00:48:14,339
actually going to change this a little

1330
00:48:12,960 --> 00:48:15,540
bit but for now I'm going to keep it

1331
00:48:14,339 --> 00:48:16,859
this way

1332
00:48:15,540 --> 00:48:18,960
so I'm just going to wait for this to

1333
00:48:16,859 --> 00:48:20,700
converge okay so I'll add the neural

1334
00:48:18,960 --> 00:48:22,500
nuts to converge here and when we scroll

1335
00:48:20,700 --> 00:48:25,619
down we see that our validation loss

1336
00:48:22,500 --> 00:48:27,540
here is 2.10 roughly which I wrote down

1337
00:48:25,619 --> 00:48:28,800
here and we see that this is actually

1338
00:48:27,540 --> 00:48:31,079
kind of comparable to some of the

1339
00:48:28,800 --> 00:48:33,240
results that we've achieved previously

1340
00:48:31,079 --> 00:48:35,280
now I'm not actually expecting an

1341
00:48:33,240 --> 00:48:36,480
improvement in this case and that's

1342
00:48:35,280 --> 00:48:38,220
because we are dealing with a very

1343
00:48:36,480 --> 00:48:41,460
simple neural nut that has just a single

1344
00:48:38,220 --> 00:48:43,260
hidden layer so in fact in this very

1345
00:48:41,460 --> 00:48:44,819
simple case of just one hidden layer we

1346
00:48:43,260 --> 00:48:47,640
were able to actually calculate what the

1347
00:48:44,819 --> 00:48:49,380
scale of w should be to make these

1348
00:48:47,640 --> 00:48:51,300
pre-activations already have a roughly

1349
00:48:49,380 --> 00:48:53,160
gaussian shape so the best normalization

1350
00:48:51,300 --> 00:48:54,839
is not doing much here

1351
00:48:53,160 --> 00:48:56,880
but you might imagine that once you have

1352
00:48:54,839 --> 00:48:59,460
a much deeper neural nut that has lots

1353
00:48:56,880 --> 00:49:00,720
of different types of operations and

1354
00:48:59,460 --> 00:49:02,400
there's also for example residual

1355
00:49:00,720 --> 00:49:04,800
connections which we'll cover and so on

1356
00:49:02,400 --> 00:49:07,619
it will become basically very very

1357
00:49:04,800 --> 00:49:09,660
difficult to tune those scales of your

1358
00:49:07,619 --> 00:49:11,160
weight matrices such that all the

1359
00:49:09,660 --> 00:49:12,720
activations throughout the neural Nets

1360
00:49:11,160 --> 00:49:14,700
are roughly gaussian

1361
00:49:12,720 --> 00:49:17,280
and so that's going to become very

1362
00:49:14,700 --> 00:49:19,140
quickly intractable but compared to that

1363
00:49:17,280 --> 00:49:20,640
it's going to be much much easier to

1364
00:49:19,140 --> 00:49:22,079
sprinkle batch normalization layers

1365
00:49:20,640 --> 00:49:24,960
throughout the neural net

1366
00:49:22,079 --> 00:49:26,819
so in particular it's common to to look

1367
00:49:24,960 --> 00:49:28,440
at every single linear layer like this

1368
00:49:26,819 --> 00:49:30,720
one this is a linear layer multiplying

1369
00:49:28,440 --> 00:49:32,760
by a weight Matrix and adding the bias

1370
00:49:30,720 --> 00:49:35,460
or for example convolutions which we'll

1371
00:49:32,760 --> 00:49:37,140
cover later and also perform basically a

1372
00:49:35,460 --> 00:49:38,940
multiplication with the weight Matrix

1373
00:49:37,140 --> 00:49:41,819
but in a more spatially structured

1374
00:49:38,940 --> 00:49:43,319
format it's custom it's customary to

1375
00:49:41,819 --> 00:49:46,500
take these linear layer or convolutional

1376
00:49:43,319 --> 00:49:49,440
layer and append a bachelorization layer

1377
00:49:46,500 --> 00:49:50,940
right after it to control the scale of

1378
00:49:49,440 --> 00:49:52,859
these activations at every point in the

1379
00:49:50,940 --> 00:49:54,420
neural net so we'd be adding these

1380
00:49:52,859 --> 00:49:56,520
bathroom layers throughout the neural

1381
00:49:54,420 --> 00:49:58,020
net and then this controls the scale of

1382
00:49:56,520 --> 00:50:00,420
these activations throughout the neural

1383
00:49:58,020 --> 00:50:02,700
net it doesn't require us to do a

1384
00:50:00,420 --> 00:50:04,740
perfect mathematics and care about the

1385
00:50:02,700 --> 00:50:06,660
activation distributions for all these

1386
00:50:04,740 --> 00:50:07,920
different types of neural network Lego

1387
00:50:06,660 --> 00:50:10,020
building blocks that you might want to

1388
00:50:07,920 --> 00:50:12,240
introduce into your neural net and it

1389
00:50:10,020 --> 00:50:14,220
significantly stabilizes uh the training

1390
00:50:12,240 --> 00:50:16,200
and that's why these layers are quite

1391
00:50:14,220 --> 00:50:17,880
popular now the stability offered by

1392
00:50:16,200 --> 00:50:20,579
batch normalization actually comes at a

1393
00:50:17,880 --> 00:50:21,720
terrible cost and that cost is that if

1394
00:50:20,579 --> 00:50:24,480
you think about what's Happening Here

1395
00:50:21,720 --> 00:50:26,339
something something terribly strange and

1396
00:50:24,480 --> 00:50:28,740
unnatural is happening

1397
00:50:26,339 --> 00:50:30,960
it used to be that we have a single

1398
00:50:28,740 --> 00:50:33,240
example feeding into a neural net and

1399
00:50:30,960 --> 00:50:36,119
then we calculate this activations and

1400
00:50:33,240 --> 00:50:38,400
it's logits and this is a deterministic

1401
00:50:36,119 --> 00:50:40,920
sort of process so you arrive at some

1402
00:50:38,400 --> 00:50:43,079
Logics for this example and then because

1403
00:50:40,920 --> 00:50:45,060
of efficiency of training we suddenly

1404
00:50:43,079 --> 00:50:46,740
started to use batches of examples but

1405
00:50:45,060 --> 00:50:48,359
those batches of examples were processed

1406
00:50:46,740 --> 00:50:49,740
independently and it was just an

1407
00:50:48,359 --> 00:50:51,599
efficiency thing

1408
00:50:49,740 --> 00:50:53,099
but now suddenly in bash normalization

1409
00:50:51,599 --> 00:50:55,220
because of the normalization through the

1410
00:50:53,099 --> 00:50:57,540
batch we are coupling these examples

1411
00:50:55,220 --> 00:50:59,460
mathematically and in the forward pass

1412
00:50:57,540 --> 00:51:01,980
and the backward pass of the neural land

1413
00:50:59,460 --> 00:51:04,980
so now the hidden State activations

1414
00:51:01,980 --> 00:51:06,900
hpact and your logits for any one input

1415
00:51:04,980 --> 00:51:09,359
example are not just a function of that

1416
00:51:06,900 --> 00:51:11,280
example and its input but they're also a

1417
00:51:09,359 --> 00:51:13,760
function of all the other examples that

1418
00:51:11,280 --> 00:51:16,140
happen to come for a ride in that batch

1419
00:51:13,760 --> 00:51:17,819
and these examples are sampled randomly

1420
00:51:16,140 --> 00:51:19,200
and so what's happening is for example

1421
00:51:17,819 --> 00:51:21,300
when you look at each preact that's

1422
00:51:19,200 --> 00:51:23,819
going to feed into H the hidden State

1423
00:51:21,300 --> 00:51:25,980
activations for for example for for any

1424
00:51:23,819 --> 00:51:28,319
one of these input examples is going to

1425
00:51:25,980 --> 00:51:30,240
actually change slightly depending on

1426
00:51:28,319 --> 00:51:32,400
what other examples there are in a batch

1427
00:51:30,240 --> 00:51:34,020
and and depending on what other examples

1428
00:51:32,400 --> 00:51:36,720
happen to come for a ride

1429
00:51:34,020 --> 00:51:38,040
H is going to change subtly and it's

1430
00:51:36,720 --> 00:51:40,200
going to like Jitter if you imagine

1431
00:51:38,040 --> 00:51:41,940
sampling different examples because the

1432
00:51:40,200 --> 00:51:43,920
statistics of the mean and the standard

1433
00:51:41,940 --> 00:51:45,900
deviation are going to be impacted

1434
00:51:43,920 --> 00:51:48,540
and so you'll get a Jitter for H and

1435
00:51:45,900 --> 00:51:50,280
you'll get a Jitter for logits

1436
00:51:48,540 --> 00:51:53,400
and you think that this would be a bug

1437
00:51:50,280 --> 00:51:55,619
or something undesirable but in a very

1438
00:51:53,400 --> 00:51:58,559
strange way this actually turns out to

1439
00:51:55,619 --> 00:52:00,599
be good in neural network training and

1440
00:51:58,559 --> 00:52:02,339
as a side effect and the reason for that

1441
00:52:00,599 --> 00:52:04,559
is that you can think of this as kind of

1442
00:52:02,339 --> 00:52:05,880
like a regularizer because what's

1443
00:52:04,559 --> 00:52:07,500
happening is you have your input and you

1444
00:52:05,880 --> 00:52:09,900
get your age and then depending on the

1445
00:52:07,500 --> 00:52:11,339
other examples this is generating a bit

1446
00:52:09,900 --> 00:52:13,559
and so what that does is that it's

1447
00:52:11,339 --> 00:52:15,240
effectively padding out any one of these

1448
00:52:13,559 --> 00:52:17,460
input examples and it's introducing a

1449
00:52:15,240 --> 00:52:18,900
little bit of entropy and

1450
00:52:17,460 --> 00:52:20,700
um because of the padding out it's

1451
00:52:18,900 --> 00:52:22,380
actually kind of like a form of data

1452
00:52:20,700 --> 00:52:24,420
augmentation which we'll cover in the

1453
00:52:22,380 --> 00:52:26,160
future and it's kind of like augmenting

1454
00:52:24,420 --> 00:52:27,900
the input a little bit and it's

1455
00:52:26,160 --> 00:52:30,059
jittering it and that makes it harder

1456
00:52:27,900 --> 00:52:32,400
for the neural nuts to overfit to these

1457
00:52:30,059 --> 00:52:34,200
concrete specific examples so by

1458
00:52:32,400 --> 00:52:36,300
introducing all this noise it actually

1459
00:52:34,200 --> 00:52:38,160
like Pats out the examples and it

1460
00:52:36,300 --> 00:52:39,800
regularizes the neural net and that's

1461
00:52:38,160 --> 00:52:41,760
one of the reasons why

1462
00:52:39,800 --> 00:52:44,160
deceivingly as a second order effect

1463
00:52:41,760 --> 00:52:46,800
this is actually a regularizer and that

1464
00:52:44,160 --> 00:52:48,420
has made it harder for us to remove the

1465
00:52:46,800 --> 00:52:50,160
use of batch normalization

1466
00:52:48,420 --> 00:52:52,740
because basically no one likes this

1467
00:52:50,160 --> 00:52:54,720
property that the the examples in the

1468
00:52:52,740 --> 00:52:57,000
batch are coupled mathematically and in

1469
00:52:54,720 --> 00:52:59,280
the forward pass and at least all kinds

1470
00:52:57,000 --> 00:53:01,559
of like strange results uh we'll go into

1471
00:52:59,280 --> 00:53:03,960
some of that in a second as well

1472
00:53:01,559 --> 00:53:05,940
um and it leads to a lot of bugs and

1473
00:53:03,960 --> 00:53:09,140
um and so on and so no one likes this

1474
00:53:05,940 --> 00:53:11,339
property uh and so people have tried to

1475
00:53:09,140 --> 00:53:12,900
deprecate the use of astronomization and

1476
00:53:11,339 --> 00:53:14,460
move to other normalization techniques

1477
00:53:12,900 --> 00:53:16,680
that do not couple the examples of a

1478
00:53:14,460 --> 00:53:18,420
batch examples are layer normalization

1479
00:53:16,680 --> 00:53:20,640
instance normalization group

1480
00:53:18,420 --> 00:53:23,819
normalization and so on and we'll cover

1481
00:53:20,640 --> 00:53:25,859
we'll cover some of these later

1482
00:53:23,819 --> 00:53:27,660
um but basically long story short bash

1483
00:53:25,859 --> 00:53:29,579
formalization was the first kind of

1484
00:53:27,660 --> 00:53:31,920
normalization layer to be introduced it

1485
00:53:29,579 --> 00:53:34,559
worked extremely well it happens to have

1486
00:53:31,920 --> 00:53:35,819
this regularizing effect it stabilized

1487
00:53:34,559 --> 00:53:37,800
training

1488
00:53:35,819 --> 00:53:39,119
and people have been trying to remove it

1489
00:53:37,800 --> 00:53:41,700
and move to some of the other

1490
00:53:39,119 --> 00:53:44,040
normalization techniques but it's been

1491
00:53:41,700 --> 00:53:45,839
hard because it just works quite well

1492
00:53:44,040 --> 00:53:47,160
and some of the reason that it works

1493
00:53:45,839 --> 00:53:49,079
quite well is again because of this

1494
00:53:47,160 --> 00:53:51,059
regularizing effect and because of the

1495
00:53:49,079 --> 00:53:53,040
because it is quite effective at

1496
00:53:51,059 --> 00:53:54,180
controlling the activations and their

1497
00:53:53,040 --> 00:53:55,920
distributions

1498
00:53:54,180 --> 00:53:58,319
uh so that's kind of like the brief

1499
00:53:55,920 --> 00:54:01,260
story of nationalization and I'd like to

1500
00:53:58,319 --> 00:54:03,660
show you one of the other weird sort of

1501
00:54:01,260 --> 00:54:05,099
outcomes of this coupling

1502
00:54:03,660 --> 00:54:07,559
so here's one of the strange outcomes

1503
00:54:05,099 --> 00:54:09,359
that I only glossed over previously

1504
00:54:07,559 --> 00:54:10,740
when I was evaluating the loss on the

1505
00:54:09,359 --> 00:54:12,480
validation side

1506
00:54:10,740 --> 00:54:15,119
basically once we've trained a neural

1507
00:54:12,480 --> 00:54:16,800
net we'd like to deploy it in some kind

1508
00:54:15,119 --> 00:54:18,839
of a setting and we'd like to be able to

1509
00:54:16,800 --> 00:54:21,300
feed in a single individual example and

1510
00:54:18,839 --> 00:54:23,280
get a prediction out from our neural net

1511
00:54:21,300 --> 00:54:25,260
but how do we do that when our neural

1512
00:54:23,280 --> 00:54:26,880
net now in a forward pass estimates the

1513
00:54:25,260 --> 00:54:28,559
statistics of the mean energy standard

1514
00:54:26,880 --> 00:54:31,079
deviation of a batch the neural net

1515
00:54:28,559 --> 00:54:32,520
expects badges as an input now so how do

1516
00:54:31,079 --> 00:54:34,380
we feed in a single example and get

1517
00:54:32,520 --> 00:54:36,420
sensible results out

1518
00:54:34,380 --> 00:54:38,819
and so the proposal in the batch

1519
00:54:36,420 --> 00:54:40,800
normalization paper is the following

1520
00:54:38,819 --> 00:54:42,660
what we would like to do here is we

1521
00:54:40,800 --> 00:54:47,099
would like to basically have a step

1522
00:54:42,660 --> 00:54:49,500
after training that calculates and sets

1523
00:54:47,099 --> 00:54:52,140
the bathroom mean and standard deviation

1524
00:54:49,500 --> 00:54:54,119
a single time over the training set

1525
00:54:52,140 --> 00:54:56,400
and so I wrote this code here in

1526
00:54:54,119 --> 00:54:57,839
interest of time and we're going to call

1527
00:54:56,400 --> 00:54:59,040
what's called calibrate the Bachelor of

1528
00:54:57,839 --> 00:55:02,579
statistics

1529
00:54:59,040 --> 00:55:04,920
and basically what we do is not no grad

1530
00:55:02,579 --> 00:55:07,079
telling pytorch that none of this we

1531
00:55:04,920 --> 00:55:08,760
will call the dot backward on and it's

1532
00:55:07,079 --> 00:55:11,099
going to be a bit more efficient

1533
00:55:08,760 --> 00:55:12,540
we're going to take the training set get

1534
00:55:11,099 --> 00:55:14,700
the pre-activations for every single

1535
00:55:12,540 --> 00:55:15,900
training example and then one single

1536
00:55:14,700 --> 00:55:18,119
time estimate the mean and standard

1537
00:55:15,900 --> 00:55:19,559
deviation over the entire training set

1538
00:55:18,119 --> 00:55:21,359
and then we're going to get B and mean

1539
00:55:19,559 --> 00:55:23,339
and be in standard deviation and now

1540
00:55:21,359 --> 00:55:25,140
these are fixed numbers as the meaning

1541
00:55:23,339 --> 00:55:27,780
of the entire training set

1542
00:55:25,140 --> 00:55:29,760
and here instead of estimating it

1543
00:55:27,780 --> 00:55:33,000
dynamically

1544
00:55:29,760 --> 00:55:34,200
we are going to instead here use B and

1545
00:55:33,000 --> 00:55:36,059
mean

1546
00:55:34,200 --> 00:55:37,980
and here we're just going to use B and

1547
00:55:36,059 --> 00:55:40,380
standard deviation

1548
00:55:37,980 --> 00:55:42,240
and so at this time we are going to fix

1549
00:55:40,380 --> 00:55:45,420
these clamp them and use them during

1550
00:55:42,240 --> 00:55:47,520
inference and now

1551
00:55:45,420 --> 00:55:48,599
you see that we get basically identical

1552
00:55:47,520 --> 00:55:51,059
result

1553
00:55:48,599 --> 00:55:52,680
but the benefit that we've gained is

1554
00:55:51,059 --> 00:55:54,240
that we can now also forward a single

1555
00:55:52,680 --> 00:55:56,220
example because the mean and standard

1556
00:55:54,240 --> 00:55:57,300
deviation are now fixed uh sort of

1557
00:55:56,220 --> 00:55:59,040
tensors

1558
00:55:57,300 --> 00:56:00,359
that said nobody actually wants to

1559
00:55:59,040 --> 00:56:03,420
estimate this mean and standard

1560
00:56:00,359 --> 00:56:05,040
deviation as a second stage after neural

1561
00:56:03,420 --> 00:56:07,619
network training because everyone is

1562
00:56:05,040 --> 00:56:09,180
lazy and so this batch normalization

1563
00:56:07,619 --> 00:56:11,520
paper actually introduced one more idea

1564
00:56:09,180 --> 00:56:13,260
which is that we can we can estimate the

1565
00:56:11,520 --> 00:56:16,020
mean and standard deviation in a running

1566
00:56:13,260 --> 00:56:18,599
matter running manner during training of

1567
00:56:16,020 --> 00:56:20,460
the neural net and then we can simply

1568
00:56:18,599 --> 00:56:22,140
just have a single stage of training and

1569
00:56:20,460 --> 00:56:23,880
on the side of that training we are

1570
00:56:22,140 --> 00:56:25,380
estimating the running mean and standard

1571
00:56:23,880 --> 00:56:26,579
deviation so let's see what that would

1572
00:56:25,380 --> 00:56:28,920
look like

1573
00:56:26,579 --> 00:56:30,480
let me basically take the mean here that

1574
00:56:28,920 --> 00:56:32,460
we are estimating on the batch and let

1575
00:56:30,480 --> 00:56:34,980
me call this B and mean on the I

1576
00:56:32,460 --> 00:56:39,859
iteration

1577
00:56:34,980 --> 00:56:39,859
um and then here this is B and sdd

1578
00:56:40,380 --> 00:56:45,200
um bnstd at I okay

1579
00:56:46,400 --> 00:56:54,119
uh and the mean comes here and the STD

1580
00:56:51,420 --> 00:56:55,920
comes here so so far I've done nothing

1581
00:56:54,119 --> 00:56:57,420
I've just moved around and I created

1582
00:56:55,920 --> 00:56:59,339
these extra variables for the mean and

1583
00:56:57,420 --> 00:57:01,859
standard deviation and I've put them

1584
00:56:59,339 --> 00:57:03,000
here so so far nothing has changed but

1585
00:57:01,859 --> 00:57:04,859
what we're going to do now is we're

1586
00:57:03,000 --> 00:57:07,140
going to keep a running mean of both of

1587
00:57:04,859 --> 00:57:09,180
these values during training so let me

1588
00:57:07,140 --> 00:57:11,760
swing up here and let me create a BN

1589
00:57:09,180 --> 00:57:15,900
mean underscore running

1590
00:57:11,760 --> 00:57:18,599
and I'm going to initialize it at zeros

1591
00:57:15,900 --> 00:57:22,880
and then be an STD running

1592
00:57:18,599 --> 00:57:22,880
which I'll initialize at once

1593
00:57:23,040 --> 00:57:26,160
because

1594
00:57:24,720 --> 00:57:30,180
in the beginning because of the way we

1595
00:57:26,160 --> 00:57:31,920
initialized W1 and B1 each react will be

1596
00:57:30,180 --> 00:57:33,180
roughly unit gaussian so the mean will

1597
00:57:31,920 --> 00:57:35,400
be roughly zero and the standard

1598
00:57:33,180 --> 00:57:37,020
deviation roughly one so I'm going to

1599
00:57:35,400 --> 00:57:39,359
initialize these that way

1600
00:57:37,020 --> 00:57:41,720
but then here I'm going to update these

1601
00:57:39,359 --> 00:57:44,460
and in pytorch

1602
00:57:41,720 --> 00:57:46,440
these mean and standard deviation that

1603
00:57:44,460 --> 00:57:47,940
are running they're not actually part of

1604
00:57:46,440 --> 00:57:49,200
the gradient based optimization we're

1605
00:57:47,940 --> 00:57:51,300
never going to derive gradients with

1606
00:57:49,200 --> 00:57:53,460
respect to them they're they're updated

1607
00:57:51,300 --> 00:57:55,079
on the side of training

1608
00:57:53,460 --> 00:57:57,240
and so what we're going to do here is

1609
00:57:55,079 --> 00:58:00,420
we're going to say with torch top no

1610
00:57:57,240 --> 00:58:02,579
grad telling pytorch that the update

1611
00:58:00,420 --> 00:58:03,960
here is not supposed to be building out

1612
00:58:02,579 --> 00:58:05,280
a graph because there will be no doubt

1613
00:58:03,960 --> 00:58:07,859
backward

1614
00:58:05,280 --> 00:58:10,160
but this running is basically going to

1615
00:58:07,859 --> 00:58:13,500
be 0.99

1616
00:58:10,160 --> 00:58:18,300
times the current value

1617
00:58:13,500 --> 00:58:20,220
plus 0.001 times the this value

1618
00:58:18,300 --> 00:58:22,920
this new mean

1619
00:58:20,220 --> 00:58:24,359
and in the same way be an STD running

1620
00:58:22,920 --> 00:58:27,920
will be

1621
00:58:24,359 --> 00:58:27,920
mostly what it used to be

1622
00:58:28,500 --> 00:58:32,339
but it will receive a small update in

1623
00:58:30,660 --> 00:58:34,980
the direction of what the current

1624
00:58:32,339 --> 00:58:37,140
standard deviation is

1625
00:58:34,980 --> 00:58:39,839
and as you're seeing here this update is

1626
00:58:37,140 --> 00:58:42,240
outside and on the side of the gradient

1627
00:58:39,839 --> 00:58:44,160
based optimization and it's simply being

1628
00:58:42,240 --> 00:58:46,859
updated not using gradient descent it's

1629
00:58:44,160 --> 00:58:48,319
just being updated using a gen key like

1630
00:58:46,859 --> 00:58:50,280
Smooth

1631
00:58:48,319 --> 00:58:53,160
sort of

1632
00:58:50,280 --> 00:58:55,859
running mean manner

1633
00:58:53,160 --> 00:58:57,540
and so while the network is training and

1634
00:58:55,859 --> 00:58:59,520
these pre-activations are sort of

1635
00:58:57,540 --> 00:59:02,040
changing and shifting around during back

1636
00:58:59,520 --> 00:59:03,900
propagation we are keeping track of the

1637
00:59:02,040 --> 00:59:06,359
typical mean and standard deviation and

1638
00:59:03,900 --> 00:59:08,780
we're estimating them once and when I

1639
00:59:06,359 --> 00:59:08,780
run this

1640
00:59:09,180 --> 00:59:13,020
now I'm keeping track of this in a

1641
00:59:10,859 --> 00:59:14,579
running manner and what we're hoping for

1642
00:59:13,020 --> 00:59:16,079
of course is that the meat being mean

1643
00:59:14,579 --> 00:59:18,359
underscore running and B and mean

1644
00:59:16,079 --> 00:59:20,760
underscore STD are going to be very

1645
00:59:18,359 --> 00:59:22,200
similar to the ones that we calculated

1646
00:59:20,760 --> 00:59:24,000
here before

1647
00:59:22,200 --> 00:59:26,160
and that way we don't need a second

1648
00:59:24,000 --> 00:59:27,900
stage because we've sort of combined the

1649
00:59:26,160 --> 00:59:29,339
two stages and we've put them on the

1650
00:59:27,900 --> 00:59:30,480
side of each other if you want to look

1651
00:59:29,339 --> 00:59:32,339
at it that way

1652
00:59:30,480 --> 00:59:34,500
and this is how this is also implemented

1653
00:59:32,339 --> 00:59:37,020
in the batch normalization layer in pi

1654
00:59:34,500 --> 00:59:39,299
torch so during training

1655
00:59:37,020 --> 00:59:40,680
um the exact same thing will happen and

1656
00:59:39,299 --> 00:59:43,440
then later when you're using inference

1657
00:59:40,680 --> 00:59:46,140
it will use the estimated running mean

1658
00:59:43,440 --> 00:59:47,760
of both the mean estimate deviation of

1659
00:59:46,140 --> 00:59:49,740
those hidden States

1660
00:59:47,760 --> 00:59:51,599
so let's wait for the optimization to

1661
00:59:49,740 --> 00:59:53,040
converge and hopefully the running mean

1662
00:59:51,599 --> 00:59:54,780
and standard deviation are roughly equal

1663
00:59:53,040 --> 00:59:57,599
to these two and then we can simply use

1664
00:59:54,780 --> 00:59:59,640
it here and we don't need this stage of

1665
00:59:57,599 --> 01:00:01,260
explicit calibration at the end okay so

1666
00:59:59,640 --> 01:00:04,200
the optimization finished

1667
01:00:01,260 --> 01:00:06,059
I'll rerun the explicit estimation and

1668
01:00:04,200 --> 01:00:07,680
then the B and mean from the explicit

1669
01:00:06,059 --> 01:00:09,599
estimation is here

1670
01:00:07,680 --> 01:00:10,920
and B and mean from the running

1671
01:00:09,599 --> 01:00:14,040
estimation

1672
01:00:10,920 --> 01:00:16,200
during the during the optimization you

1673
01:00:14,040 --> 01:00:19,440
can see it's very very similar

1674
01:00:16,200 --> 01:00:23,040
it's not identical but it's pretty close

1675
01:00:19,440 --> 01:00:26,280
in the same way be an STD is this and be

1676
01:00:23,040 --> 01:00:28,260
an STD running is this

1677
01:00:26,280 --> 01:00:30,540
as you can see that once again they are

1678
01:00:28,260 --> 01:00:31,740
fairly similar values not identical but

1679
01:00:30,540 --> 01:00:33,780
pretty close

1680
01:00:31,740 --> 01:00:35,819
and so then here instead of being mean

1681
01:00:33,780 --> 01:00:37,980
we can use the B and mean running

1682
01:00:35,819 --> 01:00:39,720
instead of being STD we can use bnstd

1683
01:00:37,980 --> 01:00:42,119
running

1684
01:00:39,720 --> 01:00:44,339
and uh hopefully the validation loss

1685
01:00:42,119 --> 01:00:47,099
will not be impacted too much

1686
01:00:44,339 --> 01:00:49,680
okay so it's basically identical and

1687
01:00:47,099 --> 01:00:51,240
this way we've eliminated the need for

1688
01:00:49,680 --> 01:00:53,400
this explicit stage of calibration

1689
01:00:51,240 --> 01:00:55,319
because we are doing it in line over

1690
01:00:53,400 --> 01:00:57,000
here okay so we're almost done with

1691
01:00:55,319 --> 01:00:58,740
batch normalization there are only two

1692
01:00:57,000 --> 01:01:00,480
more notes that I'd like to make number

1693
01:00:58,740 --> 01:01:02,460
one I've skipped a discussion over what

1694
01:01:00,480 --> 01:01:04,740
is this plus Epsilon doing here this

1695
01:01:02,460 --> 01:01:06,180
Epsilon is usually like some small fixed

1696
01:01:04,740 --> 01:01:08,220
number for example one a negative five

1697
01:01:06,180 --> 01:01:09,780
by default and what it's doing is that

1698
01:01:08,220 --> 01:01:13,140
it's basically preventing a division by

1699
01:01:09,780 --> 01:01:14,280
zero in the case that the variance over

1700
01:01:13,140 --> 01:01:17,640
your batch

1701
01:01:14,280 --> 01:01:19,260
is exactly zero in that case uh here we

1702
01:01:17,640 --> 01:01:21,240
normally have a division by zero but

1703
01:01:19,260 --> 01:01:22,619
because of the plus Epsilon this is

1704
01:01:21,240 --> 01:01:24,359
going to become a small number in the

1705
01:01:22,619 --> 01:01:26,880
denominator instead and things will be

1706
01:01:24,359 --> 01:01:28,680
more well behaved so feel free to also

1707
01:01:26,880 --> 01:01:30,299
add a plus Epsilon here of a very small

1708
01:01:28,680 --> 01:01:32,040
number it doesn't actually substantially

1709
01:01:30,299 --> 01:01:33,540
change the result I'm going to skip it

1710
01:01:32,040 --> 01:01:34,920
in our case just because this is

1711
01:01:33,540 --> 01:01:37,200
unlikely to happen in our very simple

1712
01:01:34,920 --> 01:01:38,819
example here and the second thing I want

1713
01:01:37,200 --> 01:01:41,640
you to notice is that we're being

1714
01:01:38,819 --> 01:01:43,380
wasteful here and it's very subtle but

1715
01:01:41,640 --> 01:01:45,359
right here where we are adding the bias

1716
01:01:43,380 --> 01:01:47,640
into hpact

1717
01:01:45,359 --> 01:01:49,980
these biases now are actually useless

1718
01:01:47,640 --> 01:01:52,740
because we're adding them to the hpact

1719
01:01:49,980 --> 01:01:54,780
but then we are calculating the mean

1720
01:01:52,740 --> 01:01:57,480
for every one of these neurons and

1721
01:01:54,780 --> 01:01:59,819
subtracting it so whatever bias you add

1722
01:01:57,480 --> 01:02:00,660
here is going to get subtracted right

1723
01:01:59,819 --> 01:02:02,460
here

1724
01:02:00,660 --> 01:02:03,960
and so these biases are not doing

1725
01:02:02,460 --> 01:02:05,940
anything in fact they're being

1726
01:02:03,960 --> 01:02:07,740
subtracted out and they don't impact the

1727
01:02:05,940 --> 01:02:09,540
rest of the calculation so if you look

1728
01:02:07,740 --> 01:02:11,520
at b1.grad it's actually going to be

1729
01:02:09,540 --> 01:02:13,680
zero because it's being subtracted out

1730
01:02:11,520 --> 01:02:14,819
and doesn't actually have any effect

1731
01:02:13,680 --> 01:02:16,740
and so whenever you're using batch

1732
01:02:14,819 --> 01:02:18,900
normalization layers then if you have

1733
01:02:16,740 --> 01:02:21,000
any weight layers before like a linear

1734
01:02:18,900 --> 01:02:23,040
or a comma or something like that you're

1735
01:02:21,000 --> 01:02:25,980
better off coming here and just like not

1736
01:02:23,040 --> 01:02:28,980
using bias so you don't want to use bias

1737
01:02:25,980 --> 01:02:31,500
and then here you don't want to add it

1738
01:02:28,980 --> 01:02:33,359
because it's that spurious instead we

1739
01:02:31,500 --> 01:02:36,059
have this vast normalization bias here

1740
01:02:33,359 --> 01:02:38,099
and that bastionalization bias is now in

1741
01:02:36,059 --> 01:02:40,680
charge of the biasing of this

1742
01:02:38,099 --> 01:02:42,059
distribution instead of this B1 that we

1743
01:02:40,680 --> 01:02:44,700
had here originally

1744
01:02:42,059 --> 01:02:46,440
and so basically the rationalization

1745
01:02:44,700 --> 01:02:48,780
layer has its own bias and there's no

1746
01:02:46,440 --> 01:02:50,520
need to have a bias in the layer before

1747
01:02:48,780 --> 01:02:51,900
it because that bias is going to be

1748
01:02:50,520 --> 01:02:53,579
extracted up anyway

1749
01:02:51,900 --> 01:02:55,200
so that's the other small detail to be

1750
01:02:53,579 --> 01:02:57,480
careful with sometimes it's not going to

1751
01:02:55,200 --> 01:02:59,460
do anything catastrophic this B1 will

1752
01:02:57,480 --> 01:03:01,619
just be useless it will never get any

1753
01:02:59,460 --> 01:03:03,480
gradient it will not learn it will stay

1754
01:03:01,619 --> 01:03:06,059
constant and it's just wasteful but it

1755
01:03:03,480 --> 01:03:08,339
doesn't actually really impact anything

1756
01:03:06,059 --> 01:03:10,319
otherwise okay so I rearranged the code

1757
01:03:08,339 --> 01:03:11,819
a little bit with comments and I just

1758
01:03:10,319 --> 01:03:13,619
wanted to give a very quick summary of

1759
01:03:11,819 --> 01:03:15,660
the bachelorization layer

1760
01:03:13,619 --> 01:03:18,540
we are using batch normalization to

1761
01:03:15,660 --> 01:03:19,559
control the statistics of activations in

1762
01:03:18,540 --> 01:03:20,940
the neural net

1763
01:03:19,559 --> 01:03:22,920
it is common to sprinkle batch

1764
01:03:20,940 --> 01:03:25,140
normalization layer across the neural

1765
01:03:22,920 --> 01:03:27,960
net and usually we will place it after

1766
01:03:25,140 --> 01:03:29,880
layers that have multiplications like

1767
01:03:27,960 --> 01:03:31,500
for example a linear layer or a

1768
01:03:29,880 --> 01:03:33,059
convolutional layer which we may cover

1769
01:03:31,500 --> 01:03:35,119
in the future

1770
01:03:33,059 --> 01:03:39,240
now the batch normalization internally

1771
01:03:35,119 --> 01:03:40,440
has parameters for the gain and the bias

1772
01:03:39,240 --> 01:03:41,640
and these are trained using back

1773
01:03:40,440 --> 01:03:45,420
propagation

1774
01:03:41,640 --> 01:03:47,339
it also has two buffers the buffers are

1775
01:03:45,420 --> 01:03:49,500
the mean and the standard deviation the

1776
01:03:47,339 --> 01:03:50,819
running mean and the running mean of the

1777
01:03:49,500 --> 01:03:52,380
standard deviation

1778
01:03:50,819 --> 01:03:54,480
and these are not trained using back

1779
01:03:52,380 --> 01:03:57,180
propagation these are trained using this

1780
01:03:54,480 --> 01:03:58,859
janky update of kind of like a running

1781
01:03:57,180 --> 01:04:00,059
mean update

1782
01:03:58,859 --> 01:04:01,140
so

1783
01:04:00,059 --> 01:04:03,359
um

1784
01:04:01,140 --> 01:04:05,940
these are sort of the parameters and the

1785
01:04:03,359 --> 01:04:07,859
buffers of bashram layer and then really

1786
01:04:05,940 --> 01:04:09,359
what it's doing is it's calculating the

1787
01:04:07,859 --> 01:04:11,579
mean and the standard deviation of the

1788
01:04:09,359 --> 01:04:12,839
activations uh that are feeding into the

1789
01:04:11,579 --> 01:04:14,760
bathroom layer

1790
01:04:12,839 --> 01:04:17,099
over that batch

1791
01:04:14,760 --> 01:04:19,740
then it's centering that batch to be

1792
01:04:17,099 --> 01:04:22,559
unit gaussian and then it's offsetting

1793
01:04:19,740 --> 01:04:24,000
and scaling it by the Learned bias and

1794
01:04:22,559 --> 01:04:25,980
Gain

1795
01:04:24,000 --> 01:04:27,420
and then on top of that it's keeping

1796
01:04:25,980 --> 01:04:28,859
track of the mean and standard deviation

1797
01:04:27,420 --> 01:04:31,260
of the inputs

1798
01:04:28,859 --> 01:04:32,640
and it's maintaining this running mean

1799
01:04:31,260 --> 01:04:34,619
and standard deviation

1800
01:04:32,640 --> 01:04:36,900
and this will later be used at inference

1801
01:04:34,619 --> 01:04:38,819
so that we don't have to re-estimate the

1802
01:04:36,900 --> 01:04:40,799
meanest standard deviation all the time

1803
01:04:38,819 --> 01:04:42,960
and in addition that allows us to

1804
01:04:40,799 --> 01:04:44,280
basically forward individual examples at

1805
01:04:42,960 --> 01:04:45,720
test time

1806
01:04:44,280 --> 01:04:48,180
so that's the batch normalization layer

1807
01:04:45,720 --> 01:04:49,559
it's a fairly complicated layer

1808
01:04:48,180 --> 01:04:51,599
um but this is what it's doing

1809
01:04:49,559 --> 01:04:53,220
internally now I wanted to show you a

1810
01:04:51,599 --> 01:04:56,339
little bit of a real example

1811
01:04:53,220 --> 01:04:58,740
so you can search resnet which is a

1812
01:04:56,339 --> 01:05:00,359
residual neural network and these are

1813
01:04:58,740 --> 01:05:01,980
context of neural networks used for

1814
01:05:00,359 --> 01:05:04,140
image classification

1815
01:05:01,980 --> 01:05:06,420
and of course we haven't come dresnets

1816
01:05:04,140 --> 01:05:08,520
in detail so I'm not going to explain

1817
01:05:06,420 --> 01:05:11,339
all the pieces of it but for now just

1818
01:05:08,520 --> 01:05:13,020
note that the image feeds into a resnet

1819
01:05:11,339 --> 01:05:15,540
on the top here and there's many many

1820
01:05:13,020 --> 01:05:17,220
layers with repeating structure all the

1821
01:05:15,540 --> 01:05:18,240
way to predictions of what's inside that

1822
01:05:17,220 --> 01:05:20,040
image

1823
01:05:18,240 --> 01:05:21,960
this repeating structure is made up of

1824
01:05:20,040 --> 01:05:24,059
these blocks and these blocks are just

1825
01:05:21,960 --> 01:05:25,500
sequentially stacked up in this deep

1826
01:05:24,059 --> 01:05:28,619
neural network

1827
01:05:25,500 --> 01:05:30,359
now the code for this the block

1828
01:05:28,619 --> 01:05:33,180
basically that's used and repeated

1829
01:05:30,359 --> 01:05:36,000
sequentially in series is called this

1830
01:05:33,180 --> 01:05:37,920
bottleneck block bottleneck block

1831
01:05:36,000 --> 01:05:40,020
and there's a lot here this is all

1832
01:05:37,920 --> 01:05:41,460
pytorch and of course we haven't covered

1833
01:05:40,020 --> 01:05:43,079
all of it but I want to point out some

1834
01:05:41,460 --> 01:05:45,119
small pieces of it

1835
01:05:43,079 --> 01:05:47,099
here in the init is where we initialize

1836
01:05:45,119 --> 01:05:48,480
the neural net so this coded block here

1837
01:05:47,099 --> 01:05:49,980
is basically the kind of stuff we're

1838
01:05:48,480 --> 01:05:50,940
doing here we're initializing all the

1839
01:05:49,980 --> 01:05:53,220
layers

1840
01:05:50,940 --> 01:05:54,780
and in the forward we are specifying how

1841
01:05:53,220 --> 01:05:57,420
the neural lot acts once you actually

1842
01:05:54,780 --> 01:06:01,160
have the input so this code here is

1843
01:05:57,420 --> 01:06:01,160
along the lines of what we're doing here

1844
01:06:01,559 --> 01:06:06,900
and now these blocks are replicated and

1845
01:06:04,260 --> 01:06:08,819
stacked up serially and that's what a

1846
01:06:06,900 --> 01:06:12,200
residual Network would be

1847
01:06:08,819 --> 01:06:14,819
and so notice what's happening here com1

1848
01:06:12,200 --> 01:06:16,680
these are convolutional layers

1849
01:06:14,819 --> 01:06:19,380
and these convolutional layers basically

1850
01:06:16,680 --> 01:06:22,559
they're the same thing as a linear layer

1851
01:06:19,380 --> 01:06:24,180
except convolutional layers don't apply

1852
01:06:22,559 --> 01:06:25,799
um convolutional layers are used for

1853
01:06:24,180 --> 01:06:27,900
images and so they have spatial

1854
01:06:25,799 --> 01:06:30,359
structure and basically this linear

1855
01:06:27,900 --> 01:06:31,680
multiplication and bias offset are done

1856
01:06:30,359 --> 01:06:33,900
on patches

1857
01:06:31,680 --> 01:06:36,240
instead of a math instead of the full

1858
01:06:33,900 --> 01:06:38,579
input so because these images have

1859
01:06:36,240 --> 01:06:41,460
structure spatial structure convolutions

1860
01:06:38,579 --> 01:06:43,799
just basically do WX plus b but they do

1861
01:06:41,460 --> 01:06:46,619
it on overlapping patches of the input

1862
01:06:43,799 --> 01:06:48,660
but otherwise it's WX plus b

1863
01:06:46,619 --> 01:06:50,099
then we have the norm layer which by

1864
01:06:48,660 --> 01:06:52,140
default here is initialized to be a

1865
01:06:50,099 --> 01:06:54,119
batch Norm in 2D so two-dimensional

1866
01:06:52,140 --> 01:06:56,039
batch normalization layer

1867
01:06:54,119 --> 01:06:59,460
and then we have a nonlinearity like

1868
01:06:56,039 --> 01:07:02,520
relu so instead of uh here they use relu

1869
01:06:59,460 --> 01:07:04,500
we are using 10h in this case

1870
01:07:02,520 --> 01:07:06,420
but both both are just nonlinearities

1871
01:07:04,500 --> 01:07:08,400
and you can just use them relatively

1872
01:07:06,420 --> 01:07:10,500
interchangeably from very deep networks

1873
01:07:08,400 --> 01:07:11,640
relu is typically empirically work a bit

1874
01:07:10,500 --> 01:07:13,740
better

1875
01:07:11,640 --> 01:07:15,420
so see the motif that's being repeated

1876
01:07:13,740 --> 01:07:17,579
here we have convolution batch

1877
01:07:15,420 --> 01:07:19,920
normalization convolution patch

1878
01:07:17,579 --> 01:07:21,480
normalization early Etc and then here

1879
01:07:19,920 --> 01:07:22,920
this is residual connection that we

1880
01:07:21,480 --> 01:07:24,359
haven't covered yet

1881
01:07:22,920 --> 01:07:27,299
but basically that's the exact same

1882
01:07:24,359 --> 01:07:29,460
pattern we have here we have a weight

1883
01:07:27,299 --> 01:07:32,940
layer like a convolution or like a

1884
01:07:29,460 --> 01:07:35,940
linear layer batch normalization and

1885
01:07:32,940 --> 01:07:37,980
then 10h which is a nonlinearity but

1886
01:07:35,940 --> 01:07:40,140
basically a weight layer a normalization

1887
01:07:37,980 --> 01:07:41,880
layer and a nonlinearity and that's the

1888
01:07:40,140 --> 01:07:43,500
motif that you would be stacking up when

1889
01:07:41,880 --> 01:07:46,200
you create these deep neural networks

1890
01:07:43,500 --> 01:07:47,520
exactly as it's done here and one more

1891
01:07:46,200 --> 01:07:49,619
thing I'd like you to notice is that

1892
01:07:47,520 --> 01:07:52,559
here when they are initializing the comp

1893
01:07:49,619 --> 01:07:54,359
layers like comp one by one the depth

1894
01:07:52,559 --> 01:07:56,880
for that is right here

1895
01:07:54,359 --> 01:07:58,260
and so it's initializing an nn.cap2d

1896
01:07:56,880 --> 01:08:00,000
which is a convolutional layer in

1897
01:07:58,260 --> 01:08:01,440
pytorch and there's a bunch of keyword

1898
01:08:00,000 --> 01:08:03,660
arguments here that I'm not going to

1899
01:08:01,440 --> 01:08:05,880
explain yet but you see how there's bias

1900
01:08:03,660 --> 01:08:08,460
equals false the bicycles fall is

1901
01:08:05,880 --> 01:08:11,220
exactly for the same reason as bias is

1902
01:08:08,460 --> 01:08:13,559
not used in our case the CRI race to use

1903
01:08:11,220 --> 01:08:15,420
a bias and these are bias is spurious

1904
01:08:13,559 --> 01:08:17,339
because after this weight layer there's

1905
01:08:15,420 --> 01:08:19,440
a bachelorization and the bachelor

1906
01:08:17,339 --> 01:08:21,359
normalization subtracts that bias and

1907
01:08:19,440 --> 01:08:22,920
then has its own bias so there's no need

1908
01:08:21,359 --> 01:08:24,719
to introduce these spurious parameters

1909
01:08:22,920 --> 01:08:25,679
it wouldn't hurt performance it's just

1910
01:08:24,719 --> 01:08:28,380
useless

1911
01:08:25,679 --> 01:08:30,480
and so because they have this motif of

1912
01:08:28,380 --> 01:08:32,040
calf pasture and relu they don't need to

1913
01:08:30,480 --> 01:08:33,480
buy us here because there's a bias

1914
01:08:32,040 --> 01:08:34,620
inside here

1915
01:08:33,480 --> 01:08:36,480
so

1916
01:08:34,620 --> 01:08:39,179
by the way this example here is very

1917
01:08:36,480 --> 01:08:42,239
easy to find just do resnet pie torch

1918
01:08:39,179 --> 01:08:43,980
and uh it's this example here so this is

1919
01:08:42,239 --> 01:08:46,739
kind of like the stock implementation of

1920
01:08:43,980 --> 01:08:48,779
a residual neural network in pytorch and

1921
01:08:46,739 --> 01:08:50,580
you can find that here but of course I

1922
01:08:48,779 --> 01:08:52,440
haven't covered many of these parts yet

1923
01:08:50,580 --> 01:08:54,900
and I would also like to briefly descend

1924
01:08:52,440 --> 01:08:56,580
into the definitions of these pytorch

1925
01:08:54,900 --> 01:08:58,380
layers and the parameters that they take

1926
01:08:56,580 --> 01:09:00,960
now instead of a convolutional layer

1927
01:08:58,380 --> 01:09:02,460
we're going to look at a linear layer

1928
01:09:00,960 --> 01:09:04,319
uh because that's the one that we're

1929
01:09:02,460 --> 01:09:06,600
using here this is a linear layer and I

1930
01:09:04,319 --> 01:09:07,980
haven't covered convolutions yet but as

1931
01:09:06,600 --> 01:09:11,219
I mentioned convolutions are basically

1932
01:09:07,980 --> 01:09:14,339
linear layers except on patches

1933
01:09:11,219 --> 01:09:16,199
so a linear layer performs a w x plus b

1934
01:09:14,339 --> 01:09:18,179
except here they're calling the W A

1935
01:09:16,199 --> 01:09:20,400
transpose

1936
01:09:18,179 --> 01:09:22,319
um since is WX plus b very much

1937
01:09:20,400 --> 01:09:24,420
like we did here to initialize this

1938
01:09:22,319 --> 01:09:25,560
layer you need to know the fan in the

1939
01:09:24,420 --> 01:09:28,140
fan out

1940
01:09:25,560 --> 01:09:31,199
and that's so that they can initialize

1941
01:09:28,140 --> 01:09:33,719
this W this is the fan in and the fan

1942
01:09:31,199 --> 01:09:35,400
out so they know how how big the weight

1943
01:09:33,719 --> 01:09:36,960
Matrix should be

1944
01:09:35,400 --> 01:09:39,660
you need to also pass in whether you

1945
01:09:36,960 --> 01:09:41,580
whether or not you want a bias and if

1946
01:09:39,660 --> 01:09:44,100
you set it to false then no bias will be

1947
01:09:41,580 --> 01:09:46,140
inside this layer

1948
01:09:44,100 --> 01:09:48,060
um and you may want to do that exactly

1949
01:09:46,140 --> 01:09:49,799
like in our case if your layer is

1950
01:09:48,060 --> 01:09:51,719
followed by a normalization layer such

1951
01:09:49,799 --> 01:09:53,339
as Bachelor

1952
01:09:51,719 --> 01:09:54,480
so this allows you to basically disable

1953
01:09:53,339 --> 01:09:56,040
bias

1954
01:09:54,480 --> 01:09:58,020
now in terms of the initialization if we

1955
01:09:56,040 --> 01:10:00,660
swing down here this is reporting the

1956
01:09:58,020 --> 01:10:03,480
variables used inside this linear layer

1957
01:10:00,660 --> 01:10:06,120
and our linear layer here has two

1958
01:10:03,480 --> 01:10:07,500
parameters the weight and the bias in

1959
01:10:06,120 --> 01:10:08,460
the same way they have a weight and a

1960
01:10:07,500 --> 01:10:09,960
bias

1961
01:10:08,460 --> 01:10:12,540
and they're talking about how they

1962
01:10:09,960 --> 01:10:14,880
initialize it by default so by default

1963
01:10:12,540 --> 01:10:16,320
python initialize your weights by taking

1964
01:10:14,880 --> 01:10:19,620
the fan in

1965
01:10:16,320 --> 01:10:20,760
and then doing one over Fannin square

1966
01:10:19,620 --> 01:10:22,560
root

1967
01:10:20,760 --> 01:10:24,600
and then instead of a normal

1968
01:10:22,560 --> 01:10:25,739
distribution they are using a uniform

1969
01:10:24,600 --> 01:10:28,320
distribution

1970
01:10:25,739 --> 01:10:30,000
so it's very much the same thing but

1971
01:10:28,320 --> 01:10:31,679
they are using a one instead of five

1972
01:10:30,000 --> 01:10:34,020
over three so there's no gain being

1973
01:10:31,679 --> 01:10:36,600
calculated here the gain is just one but

1974
01:10:34,020 --> 01:10:38,760
otherwise it's exactly one over the

1975
01:10:36,600 --> 01:10:40,320
square root of fan in exactly as we have

1976
01:10:38,760 --> 01:10:43,320
here

1977
01:10:40,320 --> 01:10:46,020
so 1 over the square root of K is the is

1978
01:10:43,320 --> 01:10:47,159
the scale of the weights but when they

1979
01:10:46,020 --> 01:10:49,080
are drawing the numbers they're not

1980
01:10:47,159 --> 01:10:51,179
using a gaussian by default they're

1981
01:10:49,080 --> 01:10:53,400
using a uniform distribution by default

1982
01:10:51,179 --> 01:10:56,100
and so they draw uniformly from negative

1983
01:10:53,400 --> 01:10:58,080
square root of K to square root of K

1984
01:10:56,100 --> 01:11:00,840
but it's the exact same thing and the

1985
01:10:58,080 --> 01:11:03,540
same motivation from for with respect to

1986
01:11:00,840 --> 01:11:05,159
what we've seen in this lecture and the

1987
01:11:03,540 --> 01:11:07,199
reason they're doing this is if you have

1988
01:11:05,159 --> 01:11:09,900
a roughly gaussian input this will

1989
01:11:07,199 --> 01:11:12,540
ensure that out of this layer you will

1990
01:11:09,900 --> 01:11:14,940
have a roughly gaussian output and you

1991
01:11:12,540 --> 01:11:17,699
you basically achieve that by scaling

1992
01:11:14,940 --> 01:11:19,860
the weights by 100 square root of fan in

1993
01:11:17,699 --> 01:11:22,199
so that's what this is doing

1994
01:11:19,860 --> 01:11:23,880
and then the second thing is the battery

1995
01:11:22,199 --> 01:11:26,159
normalization layer so let's look at

1996
01:11:23,880 --> 01:11:27,900
what that looks like in pytorch

1997
01:11:26,159 --> 01:11:29,640
so here we have a one-dimensional mesh

1998
01:11:27,900 --> 01:11:30,780
normalization layer exactly as we are

1999
01:11:29,640 --> 01:11:31,860
using here

2000
01:11:30,780 --> 01:11:33,540
and there are a number of keyword

2001
01:11:31,860 --> 01:11:34,679
arguments going into it as well

2002
01:11:33,540 --> 01:11:37,739
so we need to know the number of

2003
01:11:34,679 --> 01:11:39,179
features uh for us that is 200 and that

2004
01:11:37,739 --> 01:11:42,179
is needed so that we can initialize

2005
01:11:39,179 --> 01:11:45,060
these parameters here the gain the bias

2006
01:11:42,179 --> 01:11:46,980
and the buffers for the running mean and

2007
01:11:45,060 --> 01:11:48,540
standard deviation

2008
01:11:46,980 --> 01:11:51,179
then they need to know the value of

2009
01:11:48,540 --> 01:11:52,800
Epsilon here and by default this is one

2010
01:11:51,179 --> 01:11:54,960
negative five you don't typically change

2011
01:11:52,800 --> 01:11:57,600
this too much then they need to know the

2012
01:11:54,960 --> 01:12:00,480
momentum and the momentum here as they

2013
01:11:57,600 --> 01:12:01,800
explain is basically used for these uh

2014
01:12:00,480 --> 01:12:02,760
running mean and running standard

2015
01:12:01,800 --> 01:12:04,679
deviation

2016
01:12:02,760 --> 01:12:06,780
so by default the momentum here is 0.1

2017
01:12:04,679 --> 01:12:09,420
the momentum we are using here in this

2018
01:12:06,780 --> 01:12:12,300
example is 0.001

2019
01:12:09,420 --> 01:12:14,280
and basically rough you may want to

2020
01:12:12,300 --> 01:12:16,199
change this sometimes and roughly

2021
01:12:14,280 --> 01:12:17,159
speaking if you have a very large batch

2022
01:12:16,199 --> 01:12:19,199
size

2023
01:12:17,159 --> 01:12:20,219
then typically what you'll see is that

2024
01:12:19,199 --> 01:12:21,420
when you estimate the mean and the

2025
01:12:20,219 --> 01:12:23,159
standard deviation

2026
01:12:21,420 --> 01:12:24,780
for every single batch size if it's

2027
01:12:23,159 --> 01:12:25,980
large enough you're going to get roughly

2028
01:12:24,780 --> 01:12:28,920
the same result

2029
01:12:25,980 --> 01:12:30,960
and so therefore you can use slightly

2030
01:12:28,920 --> 01:12:34,980
higher momentum like 0.1

2031
01:12:30,960 --> 01:12:36,360
but for a batch size as small as 32 the

2032
01:12:34,980 --> 01:12:37,739
mean understand deviation here might

2033
01:12:36,360 --> 01:12:39,540
take on slightly different numbers

2034
01:12:37,739 --> 01:12:41,159
because there's only 32 examples we are

2035
01:12:39,540 --> 01:12:43,140
using to estimate the mean of standard

2036
01:12:41,159 --> 01:12:45,840
deviation so the value is changing

2037
01:12:43,140 --> 01:12:48,000
around a lot and if your momentum is 0.1

2038
01:12:45,840 --> 01:12:49,560
that that might not be good enough for

2039
01:12:48,000 --> 01:12:52,560
this value to settle

2040
01:12:49,560 --> 01:12:53,940
and converge to the actual mean and

2041
01:12:52,560 --> 01:12:55,080
standard deviation over the entire

2042
01:12:53,940 --> 01:12:56,699
training set

2043
01:12:55,080 --> 01:12:58,860
and so basically if your batch size is

2044
01:12:56,699 --> 01:13:00,600
very small momentum of 0.1 is

2045
01:12:58,860 --> 01:13:02,760
potentially dangerous and it might make

2046
01:13:00,600 --> 01:13:04,679
it so that the running mean and standard

2047
01:13:02,760 --> 01:13:06,060
deviation is thrashing too much during

2048
01:13:04,679 --> 01:13:09,000
training and it's not actually

2049
01:13:06,060 --> 01:13:11,159
converging properly

2050
01:13:09,000 --> 01:13:13,440
uh Alpha and equals true determines

2051
01:13:11,159 --> 01:13:16,679
whether dispatch normalization layer has

2052
01:13:13,440 --> 01:13:19,620
these learnable affine parameters the uh

2053
01:13:16,679 --> 01:13:21,360
the gain and the bias and this is almost

2054
01:13:19,620 --> 01:13:23,280
always kept it true I'm not actually

2055
01:13:21,360 --> 01:13:24,900
sure why you would want to change this

2056
01:13:23,280 --> 01:13:26,400
to false

2057
01:13:24,900 --> 01:13:28,620
um

2058
01:13:26,400 --> 01:13:30,480
then track running stats is determining

2059
01:13:28,620 --> 01:13:32,640
whether or not bachelorization layer of

2060
01:13:30,480 --> 01:13:35,699
pytorch will be doing this

2061
01:13:32,640 --> 01:13:38,400
and one reason you may you may want to

2062
01:13:35,699 --> 01:13:40,320
skip the running stats is because you

2063
01:13:38,400 --> 01:13:43,620
may want to for example estimate them at

2064
01:13:40,320 --> 01:13:44,640
the end as a stage two like this and in

2065
01:13:43,620 --> 01:13:46,020
that case you don't want the batch

2066
01:13:44,640 --> 01:13:47,340
normalization layer to be doing all this

2067
01:13:46,020 --> 01:13:48,719
extra compute that you're not going to

2068
01:13:47,340 --> 01:13:51,239
use

2069
01:13:48,719 --> 01:13:52,620
and finally we need to know which device

2070
01:13:51,239 --> 01:13:55,500
we're going to run this batch

2071
01:13:52,620 --> 01:13:57,179
normalization on a CPU or a GPU and what

2072
01:13:55,500 --> 01:13:58,500
the data type should be uh half

2073
01:13:57,179 --> 01:14:00,960
Precision single Precision double

2074
01:13:58,500 --> 01:14:02,580
precision and so on

2075
01:14:00,960 --> 01:14:04,260
so that's the batch normalization layer

2076
01:14:02,580 --> 01:14:06,540
otherwise the link to the paper is the

2077
01:14:04,260 --> 01:14:08,820
same formula we've implement it and

2078
01:14:06,540 --> 01:14:10,800
everything is the same exactly as we've

2079
01:14:08,820 --> 01:14:12,120
done here

2080
01:14:10,800 --> 01:14:14,400
okay so that's everything that I wanted

2081
01:14:12,120 --> 01:14:15,780
to cover for this lecture really what I

2082
01:14:14,400 --> 01:14:17,640
wanted to talk about is the importance

2083
01:14:15,780 --> 01:14:19,980
of understanding the activations and the

2084
01:14:17,640 --> 01:14:21,719
gradients and their statistics in neural

2085
01:14:19,980 --> 01:14:23,100
networks and this becomes increasingly

2086
01:14:21,719 --> 01:14:25,739
important especially as you make your

2087
01:14:23,100 --> 01:14:27,420
neural networks bigger larger and deeper

2088
01:14:25,739 --> 01:14:29,580
we looked at the distributions basically

2089
01:14:27,420 --> 01:14:31,679
at the output layer and we saw that if

2090
01:14:29,580 --> 01:14:33,960
you have two confident mispredictions

2091
01:14:31,679 --> 01:14:35,880
because the activations are too messed

2092
01:14:33,960 --> 01:14:38,460
up at the last layer you can end up with

2093
01:14:35,880 --> 01:14:40,080
these hockey stick losses and if you fix

2094
01:14:38,460 --> 01:14:42,000
this you get a better loss at the end of

2095
01:14:40,080 --> 01:14:43,860
training because your training is not

2096
01:14:42,000 --> 01:14:45,360
doing wasteful work

2097
01:14:43,860 --> 01:14:46,980
then we also saw that we need to control

2098
01:14:45,360 --> 01:14:49,980
the activations we don't want them to

2099
01:14:46,980 --> 01:14:52,020
you know squash to zero or explode to

2100
01:14:49,980 --> 01:14:53,880
infinity and because that you can run

2101
01:14:52,020 --> 01:14:56,159
into a lot of trouble with all of these

2102
01:14:53,880 --> 01:14:57,239
non-linearities in these neural nuts and

2103
01:14:56,159 --> 01:14:58,860
basically you want everything to be

2104
01:14:57,239 --> 01:15:00,060
fairly homogeneous throughout the neural

2105
01:14:58,860 --> 01:15:02,400
net you want roughly gaussian

2106
01:15:00,060 --> 01:15:04,739
activations throughout the neural net

2107
01:15:02,400 --> 01:15:07,440
let me talk about okay if we would

2108
01:15:04,739 --> 01:15:09,540
roughly gaussian activations how do we

2109
01:15:07,440 --> 01:15:11,100
scale these weight matrices and biases

2110
01:15:09,540 --> 01:15:13,620
during initialization of the neural net

2111
01:15:11,100 --> 01:15:17,040
so that we don't get um you know so

2112
01:15:13,620 --> 01:15:19,140
everything is as controlled as possible

2113
01:15:17,040 --> 01:15:21,420
um so that gave us a large boost in

2114
01:15:19,140 --> 01:15:24,780
Improvement and then I talked about how

2115
01:15:21,420 --> 01:15:26,820
that strategy is not actually uh

2116
01:15:24,780 --> 01:15:29,940
possible for much much deeper neural

2117
01:15:26,820 --> 01:15:31,440
Nets because when you have much deeper

2118
01:15:29,940 --> 01:15:33,840
neural nuts with lots of different types

2119
01:15:31,440 --> 01:15:36,179
of layers it becomes really really hard

2120
01:15:33,840 --> 01:15:37,860
to precisely set the weights and the

2121
01:15:36,179 --> 01:15:39,659
biases in such a way that the

2122
01:15:37,860 --> 01:15:41,219
activations are roughly uniform

2123
01:15:39,659 --> 01:15:43,260
throughout the neural net

2124
01:15:41,219 --> 01:15:45,179
so then I introduced the notion of the

2125
01:15:43,260 --> 01:15:47,400
normalization layer now there are many

2126
01:15:45,179 --> 01:15:49,380
normalization layers that people use in

2127
01:15:47,400 --> 01:15:51,239
practice batch normalization layer

2128
01:15:49,380 --> 01:15:53,400
normalization constant normalization

2129
01:15:51,239 --> 01:15:54,900
group normalization we haven't covered

2130
01:15:53,400 --> 01:15:57,000
most of them but I've introduced the

2131
01:15:54,900 --> 01:15:58,739
first one and also the one that I

2132
01:15:57,000 --> 01:16:00,600
believe came out first and that's called

2133
01:15:58,739 --> 01:16:02,340
batch normalization

2134
01:16:00,600 --> 01:16:04,620
and we saw how batch normalization works

2135
01:16:02,340 --> 01:16:07,140
this is a layer that you can sprinkle

2136
01:16:04,620 --> 01:16:09,000
throughout your deep neural nut and the

2137
01:16:07,140 --> 01:16:11,219
basic idea is if you want roughly

2138
01:16:09,000 --> 01:16:13,980
gaussian activations well then take your

2139
01:16:11,219 --> 01:16:17,159
activations and take the mean understand

2140
01:16:13,980 --> 01:16:18,719
deviation and Center your data and you

2141
01:16:17,159 --> 01:16:21,300
can do that because the centering

2142
01:16:18,719 --> 01:16:23,219
operation is differentiable

2143
01:16:21,300 --> 01:16:25,500
but and on top of that we actually had

2144
01:16:23,219 --> 01:16:26,520
to add a lot of bells and whistles and

2145
01:16:25,500 --> 01:16:27,960
that gave you a sense of the

2146
01:16:26,520 --> 01:16:30,060
complexities of the patch normalization

2147
01:16:27,960 --> 01:16:31,860
layer because now we're centering the

2148
01:16:30,060 --> 01:16:34,140
data that's great but suddenly we need

2149
01:16:31,860 --> 01:16:35,640
the gain and the bias and now those are

2150
01:16:34,140 --> 01:16:37,620
trainable

2151
01:16:35,640 --> 01:16:39,360
and then because we are coupling all the

2152
01:16:37,620 --> 01:16:40,620
training examples now suddenly the

2153
01:16:39,360 --> 01:16:43,380
question is how do you do the inference

2154
01:16:40,620 --> 01:16:46,560
or to do to do the inference we need to

2155
01:16:43,380 --> 01:16:49,560
now estimate these mean and standard

2156
01:16:46,560 --> 01:16:52,199
deviation once or the entire training

2157
01:16:49,560 --> 01:16:54,120
set and then use those at inference but

2158
01:16:52,199 --> 01:16:56,100
then no one likes to do stage two so

2159
01:16:54,120 --> 01:16:57,239
instead we fold everything into the

2160
01:16:56,100 --> 01:16:59,699
batch normalization layer during

2161
01:16:57,239 --> 01:17:01,140
training and try to estimate these in

2162
01:16:59,699 --> 01:17:02,580
the running manner so that everything is

2163
01:17:01,140 --> 01:17:03,900
a bit simpler

2164
01:17:02,580 --> 01:17:05,699
and that gives us the batch

2165
01:17:03,900 --> 01:17:08,640
normalization layer

2166
01:17:05,699 --> 01:17:12,120
um and as I mentioned no one likes this

2167
01:17:08,640 --> 01:17:14,640
layer it causes a huge amount of bugs

2168
01:17:12,120 --> 01:17:17,820
um and intuitively it's because it is

2169
01:17:14,640 --> 01:17:21,600
coupling examples in the forward pass of

2170
01:17:17,820 --> 01:17:24,060
a neural net and I've shot myself in the

2171
01:17:21,600 --> 01:17:25,980
foot with this layer over and over again

2172
01:17:24,060 --> 01:17:28,020
in my life and I don't want you to

2173
01:17:25,980 --> 01:17:30,239
suffer the same

2174
01:17:28,020 --> 01:17:32,760
uh so basically try to avoid it as much

2175
01:17:30,239 --> 01:17:34,199
as possible uh some of the other

2176
01:17:32,760 --> 01:17:35,820
alternatives to these layers are for

2177
01:17:34,199 --> 01:17:37,800
example group normalization or layer

2178
01:17:35,820 --> 01:17:40,440
normalization and those have become more

2179
01:17:37,800 --> 01:17:43,500
common uh in more recent deep learning

2180
01:17:40,440 --> 01:17:45,000
but we haven't covered those yet but

2181
01:17:43,500 --> 01:17:46,860
definitely batch normalization was very

2182
01:17:45,000 --> 01:17:49,860
influential at the time when it came out

2183
01:17:46,860 --> 01:17:51,360
in roughly 2015 because it was kind of

2184
01:17:49,860 --> 01:17:55,679
the first time that you could train

2185
01:17:51,360 --> 01:17:56,940
reliably uh much deeper neural nuts and

2186
01:17:55,679 --> 01:17:59,340
fundamentally the reason for that is

2187
01:17:56,940 --> 01:18:01,199
because this layer was very effective at

2188
01:17:59,340 --> 01:18:03,060
controlling the statistics of the

2189
01:18:01,199 --> 01:18:06,960
activations in a neural net

2190
01:18:03,060 --> 01:18:08,699
so that's the story so far and um that's

2191
01:18:06,960 --> 01:18:10,080
all I wanted to cover and in the future

2192
01:18:08,699 --> 01:18:13,380
lectures hopefully we can start going

2193
01:18:10,080 --> 01:18:14,880
into recurring neural Nets and recurring

2194
01:18:13,380 --> 01:18:17,580
neural Nets as we'll see are just very

2195
01:18:14,880 --> 01:18:19,860
very deep networks because you uh you

2196
01:18:17,580 --> 01:18:22,199
unroll the loop and uh when you actually

2197
01:18:19,860 --> 01:18:23,460
optimize these neurons and that's where

2198
01:18:22,199 --> 01:18:24,480
a lot of this

2199
01:18:23,460 --> 01:18:26,280
um

2200
01:18:24,480 --> 01:18:28,739
analysis around the activation

2201
01:18:26,280 --> 01:18:30,659
statistics and all these normalization

2202
01:18:28,739 --> 01:18:33,360
layers will become very very important

2203
01:18:30,659 --> 01:18:35,159
for a good performance so we'll see that

2204
01:18:33,360 --> 01:18:37,560
next time

2205
01:18:35,159 --> 01:18:40,020
okay so I lied I would like us to do one

2206
01:18:37,560 --> 01:18:41,940
more summary here as a bonus and I think

2207
01:18:40,020 --> 01:18:43,380
it's useful as to have one more summary

2208
01:18:41,940 --> 01:18:45,179
of everything I've presented in this

2209
01:18:43,380 --> 01:18:47,159
lecture but also I would like us to

2210
01:18:45,179 --> 01:18:48,600
start by tortifying our code a little

2211
01:18:47,159 --> 01:18:50,699
bit so it looks much more like what you

2212
01:18:48,600 --> 01:18:52,500
would encounter in pi torch so you'll

2213
01:18:50,699 --> 01:18:57,360
see that I will structure our code into

2214
01:18:52,500 --> 01:18:59,820
these modules like a linear module and a

2215
01:18:57,360 --> 01:19:01,920
bachelor module and I'm putting the code

2216
01:18:59,820 --> 01:19:03,420
inside these modules so that we can

2217
01:19:01,920 --> 01:19:05,100
construct neural networks very much like

2218
01:19:03,420 --> 01:19:07,199
we would construct them in pytorch and I

2219
01:19:05,100 --> 01:19:08,640
will go through this in detail so we'll

2220
01:19:07,199 --> 01:19:11,340
create our neural net

2221
01:19:08,640 --> 01:19:12,480
then we will do the optimization loop as

2222
01:19:11,340 --> 01:19:13,980
we did before

2223
01:19:12,480 --> 01:19:15,179
and then the one more thing that I want

2224
01:19:13,980 --> 01:19:16,800
to do here is I want to look at the

2225
01:19:15,179 --> 01:19:18,780
activation statistics both in the

2226
01:19:16,800 --> 01:19:21,060
forward pass and in the backward pass

2227
01:19:18,780 --> 01:19:22,920
and then here we have the evaluation and

2228
01:19:21,060 --> 01:19:24,960
sampling just like before

2229
01:19:22,920 --> 01:19:26,820
so let me rewind all the way up here and

2230
01:19:24,960 --> 01:19:28,860
go a little bit slower

2231
01:19:26,820 --> 01:19:31,620
so here I'm creating a linear layer

2232
01:19:28,860 --> 01:19:33,239
you'll notice that torch.nn has lots of

2233
01:19:31,620 --> 01:19:35,300
different types of layers and one of

2234
01:19:33,239 --> 01:19:37,440
those layers is the linear layer

2235
01:19:35,300 --> 01:19:39,120
linear takes a number of input features

2236
01:19:37,440 --> 01:19:41,159
output features whether or not we should

2237
01:19:39,120 --> 01:19:43,260
have bias and then the device that we

2238
01:19:41,159 --> 01:19:46,140
want to place this layer on and the data

2239
01:19:43,260 --> 01:19:47,940
type so I will omit these two but

2240
01:19:46,140 --> 01:19:50,219
otherwise we have the exact same thing

2241
01:19:47,940 --> 01:19:53,640
we have the fan in which is number of

2242
01:19:50,219 --> 01:19:55,620
inputs fan out the number of outputs and

2243
01:19:53,640 --> 01:19:57,360
whether or not we want to use a bias and

2244
01:19:55,620 --> 01:19:59,760
internally inside this layer there's a

2245
01:19:57,360 --> 01:20:02,159
weight and a bias if you like it

2246
01:19:59,760 --> 01:20:05,340
it is typical to initialize the weight

2247
01:20:02,159 --> 01:20:06,900
using say random numbers drawn from a

2248
01:20:05,340 --> 01:20:09,420
gaussian and then here's the coming

2249
01:20:06,900 --> 01:20:11,460
initialization that we've discussed

2250
01:20:09,420 --> 01:20:13,260
already in this lecture and that's a

2251
01:20:11,460 --> 01:20:15,540
good default and also the default that I

2252
01:20:13,260 --> 01:20:18,300
believe python trees is and by default

2253
01:20:15,540 --> 01:20:21,179
the bias is usually initialized to zeros

2254
01:20:18,300 --> 01:20:23,520
now when you call this module this will

2255
01:20:21,179 --> 01:20:24,780
basically calculate W Times X plus b if

2256
01:20:23,520 --> 01:20:26,040
you have NB

2257
01:20:24,780 --> 01:20:28,020
and then when you also call that

2258
01:20:26,040 --> 01:20:31,020
parameters on this module it will return

2259
01:20:28,020 --> 01:20:32,100
the tensors that are the parameters of

2260
01:20:31,020 --> 01:20:33,960
this layer

2261
01:20:32,100 --> 01:20:38,400
now next we have the bachelorization

2262
01:20:33,960 --> 01:20:41,340
layer so I've written that here and this

2263
01:20:38,400 --> 01:20:44,400
is very similar to Pi torch NN dot bash

2264
01:20:41,340 --> 01:20:47,100
Norm 1D layer as shown here

2265
01:20:44,400 --> 01:20:49,560
so I'm kind of taking these three

2266
01:20:47,100 --> 01:20:51,120
parameters here the dimensionality the

2267
01:20:49,560 --> 01:20:53,580
Epsilon that we'll use in the division

2268
01:20:51,120 --> 01:20:55,320
and the momentum that we will use in

2269
01:20:53,580 --> 01:20:58,260
keeping track of these running stats the

2270
01:20:55,320 --> 01:20:59,880
running mean and the running variance

2271
01:20:58,260 --> 01:21:01,679
um now pack torch actually takes quite a

2272
01:20:59,880 --> 01:21:03,719
few more things but I'm assuming some of

2273
01:21:01,679 --> 01:21:05,460
their settings so for us I find will be

2274
01:21:03,719 --> 01:21:07,920
true that means that we will be using a

2275
01:21:05,460 --> 01:21:09,840
gamma and beta after the normalization

2276
01:21:07,920 --> 01:21:11,159
the track running stats will be true so

2277
01:21:09,840 --> 01:21:13,199
we will be keeping track of the running

2278
01:21:11,159 --> 01:21:14,460
mean and the running variance in the in

2279
01:21:13,199 --> 01:21:17,520
the pattern

2280
01:21:14,460 --> 01:21:20,100
our device by default is the CPU and the

2281
01:21:17,520 --> 01:21:22,199
data type by default is a float

2282
01:21:20,100 --> 01:21:24,780
float32.

2283
01:21:22,199 --> 01:21:26,340
so those are the defaults otherwise we

2284
01:21:24,780 --> 01:21:28,320
are taking all the same parameters in

2285
01:21:26,340 --> 01:21:29,820
this bathroom layer so first I'm just

2286
01:21:28,320 --> 01:21:31,860
saving them

2287
01:21:29,820 --> 01:21:33,840
now here's something new there's a DOT

2288
01:21:31,860 --> 01:21:35,340
training which by default is true in

2289
01:21:33,840 --> 01:21:37,500
packtorch and then modules also have

2290
01:21:35,340 --> 01:21:40,320
this attribute that training and that's

2291
01:21:37,500 --> 01:21:42,179
because many modules and batch Norm is

2292
01:21:40,320 --> 01:21:44,159
included in that have a different

2293
01:21:42,179 --> 01:21:45,840
behavior of whether you are training

2294
01:21:44,159 --> 01:21:47,880
your own lot and or whether you are

2295
01:21:45,840 --> 01:21:49,800
running it in an evaluation mode and

2296
01:21:47,880 --> 01:21:51,480
calculating your evaluation loss or

2297
01:21:49,800 --> 01:21:52,739
using it for inference on some test

2298
01:21:51,480 --> 01:21:54,840
examples

2299
01:21:52,739 --> 01:21:56,400
and masterm is an example of this

2300
01:21:54,840 --> 01:21:57,600
because when we are training we are

2301
01:21:56,400 --> 01:21:58,800
going to be using the mean and the

2302
01:21:57,600 --> 01:22:01,860
variance estimated from the current

2303
01:21:58,800 --> 01:22:03,900
batch but during inference we are using

2304
01:22:01,860 --> 01:22:06,480
the running mean and running variants

2305
01:22:03,900 --> 01:22:08,520
and so also if we are training we are

2306
01:22:06,480 --> 01:22:10,020
updating mean and variants but if we are

2307
01:22:08,520 --> 01:22:11,640
testing then these are not being updated

2308
01:22:10,020 --> 01:22:13,679
they're kept fixed

2309
01:22:11,640 --> 01:22:16,199
and so this flag is necessary and by

2310
01:22:13,679 --> 01:22:18,300
default true just like impact torch

2311
01:22:16,199 --> 01:22:21,659
now the parameters investment 1D are the

2312
01:22:18,300 --> 01:22:23,219
gamma and the beta here

2313
01:22:21,659 --> 01:22:26,219
and then the running mean and running

2314
01:22:23,219 --> 01:22:29,520
variants are called buffers in pytorch

2315
01:22:26,219 --> 01:22:31,860
nomenclature and these buffers are

2316
01:22:29,520 --> 01:22:34,620
trained using exponential moving average

2317
01:22:31,860 --> 01:22:36,120
here explicitly and they are not part of

2318
01:22:34,620 --> 01:22:37,860
the back propagation and stochastic

2319
01:22:36,120 --> 01:22:40,199
gradient descent so they are not sort of

2320
01:22:37,860 --> 01:22:41,940
like parameters of this layer and that's

2321
01:22:40,199 --> 01:22:44,100
why when we calculate when we have a

2322
01:22:41,940 --> 01:22:46,080
parameters here we only return gamma and

2323
01:22:44,100 --> 01:22:47,760
beta we do not return the mean and the

2324
01:22:46,080 --> 01:22:51,900
variance this is trained sort of like

2325
01:22:47,760 --> 01:22:54,480
internally here every forward pass using

2326
01:22:51,900 --> 01:22:56,820
exponential moving average

2327
01:22:54,480 --> 01:22:59,400
so that's the initialization

2328
01:22:56,820 --> 01:23:01,320
now in a forward pass if we are training

2329
01:22:59,400 --> 01:23:04,080
then we use the mean and the variance

2330
01:23:01,320 --> 01:23:05,760
estimated by the batch let me plot the

2331
01:23:04,080 --> 01:23:08,280
paper here

2332
01:23:05,760 --> 01:23:11,219
we calculate the mean and the variance

2333
01:23:08,280 --> 01:23:13,020
now up above I was estimating the

2334
01:23:11,219 --> 01:23:15,300
standard deviation and keeping track of

2335
01:23:13,020 --> 01:23:16,920
the standard deviation here in the

2336
01:23:15,300 --> 01:23:18,900
running standard deviation instead of

2337
01:23:16,920 --> 01:23:21,360
running variance but let's follow the

2338
01:23:18,900 --> 01:23:23,219
paper exactly here they calculate the

2339
01:23:21,360 --> 01:23:25,080
variance which is the standard deviation

2340
01:23:23,219 --> 01:23:27,239
squared and that's what's kept track of

2341
01:23:25,080 --> 01:23:29,340
in the running variance instead of a

2342
01:23:27,239 --> 01:23:31,320
running standard deviation

2343
01:23:29,340 --> 01:23:33,300
uh but those two would be very very

2344
01:23:31,320 --> 01:23:35,280
similar I believe

2345
01:23:33,300 --> 01:23:38,880
if we are not training then we use

2346
01:23:35,280 --> 01:23:41,219
running mean in various we normalize

2347
01:23:38,880 --> 01:23:43,199
and then here I'm calculating the output

2348
01:23:41,219 --> 01:23:45,360
of this layer and I'm also assigning it

2349
01:23:43,199 --> 01:23:47,699
to an attribute called dot out

2350
01:23:45,360 --> 01:23:50,580
now dot out is something that I'm using

2351
01:23:47,699 --> 01:23:52,020
in our modules here this is not what you

2352
01:23:50,580 --> 01:23:54,420
would find in pytorch we are slightly

2353
01:23:52,020 --> 01:23:56,840
deviating from it I'm creating a DOT out

2354
01:23:54,420 --> 01:23:59,219
because I would like to very easily

2355
01:23:56,840 --> 01:24:00,900
maintain all those variables so that we

2356
01:23:59,219 --> 01:24:03,360
can create statistics of them and plot

2357
01:24:00,900 --> 01:24:05,219
them but Pi torch and modules will not

2358
01:24:03,360 --> 01:24:06,840
have a data attribute

2359
01:24:05,219 --> 01:24:08,760
then finally here we are updating the

2360
01:24:06,840 --> 01:24:10,140
buffers using again as I mentioned

2361
01:24:08,760 --> 01:24:12,600
exponential moving average

2362
01:24:10,140 --> 01:24:14,760
uh provide given the provided momentum

2363
01:24:12,600 --> 01:24:16,440
and importantly you'll notice that I'm

2364
01:24:14,760 --> 01:24:18,960
using the torstart no grad context

2365
01:24:16,440 --> 01:24:20,940
manager and I'm doing this because if we

2366
01:24:18,960 --> 01:24:22,679
don't use this then pytorch will start

2367
01:24:20,940 --> 01:24:25,260
building out an entire computational

2368
01:24:22,679 --> 01:24:26,699
graph out of these tensors because it is

2369
01:24:25,260 --> 01:24:28,800
expecting that we will eventually call

2370
01:24:26,699 --> 01:24:30,300
it that backward but we are never going

2371
01:24:28,800 --> 01:24:31,739
to be calling that backward on anything

2372
01:24:30,300 --> 01:24:33,960
that includes running mean and running

2373
01:24:31,739 --> 01:24:36,060
variance so that's why we need to use

2374
01:24:33,960 --> 01:24:38,760
this contact manager so that we are not

2375
01:24:36,060 --> 01:24:41,280
sort of maintaining them using all this

2376
01:24:38,760 --> 01:24:42,600
additional memory so this will make it

2377
01:24:41,280 --> 01:24:44,400
more efficient and it's just telling

2378
01:24:42,600 --> 01:24:45,840
factors that will only know backward we

2379
01:24:44,400 --> 01:24:47,699
just have a bunch of tensors we want to

2380
01:24:45,840 --> 01:24:50,219
update them that's it

2381
01:24:47,699 --> 01:24:52,380
and then we return

2382
01:24:50,219 --> 01:24:54,679
okay now scrolling down we have the 10h

2383
01:24:52,380 --> 01:24:58,080
layer this is very very similar to

2384
01:24:54,679 --> 01:25:00,020
torch.10h and it doesn't do too much it

2385
01:24:58,080 --> 01:25:03,540
just calculates 10h as you might expect

2386
01:25:00,020 --> 01:25:05,159
so that's torch.nh and there's no

2387
01:25:03,540 --> 01:25:07,320
parameters in this layer

2388
01:25:05,159 --> 01:25:09,120
but because these are layers

2389
01:25:07,320 --> 01:25:11,640
um it now becomes very easy to sort of

2390
01:25:09,120 --> 01:25:12,860
like stack them up into basically just a

2391
01:25:11,640 --> 01:25:15,659
list

2392
01:25:12,860 --> 01:25:17,280
and we can do all the initializations

2393
01:25:15,659 --> 01:25:19,860
that we're used to so we have the

2394
01:25:17,280 --> 01:25:20,880
initial sort of embedding Matrix we have

2395
01:25:19,860 --> 01:25:22,140
our layers and we can call them

2396
01:25:20,880 --> 01:25:24,659
sequentially

2397
01:25:22,140 --> 01:25:26,640
and then again with Trump shot no grad

2398
01:25:24,659 --> 01:25:28,500
there's some initializations here so we

2399
01:25:26,640 --> 01:25:30,840
want to make the output softmax a bit

2400
01:25:28,500 --> 01:25:32,820
less confident like we saw and in

2401
01:25:30,840 --> 01:25:35,100
addition to that because we are using a

2402
01:25:32,820 --> 01:25:37,020
six layer multi-layer perceptron here so

2403
01:25:35,100 --> 01:25:38,820
you see how I'm stacking linear 10 age

2404
01:25:37,020 --> 01:25:41,640
linear 10h Etc

2405
01:25:38,820 --> 01:25:42,840
I'm going to be using the game here and

2406
01:25:41,640 --> 01:25:45,060
I'm going to play with this in a second

2407
01:25:42,840 --> 01:25:47,219
so you'll see how when we change this

2408
01:25:45,060 --> 01:25:49,140
what happens to the statistics

2409
01:25:47,219 --> 01:25:51,420
finally the primers are basically the

2410
01:25:49,140 --> 01:25:53,340
embedding Matrix and all the parameters

2411
01:25:51,420 --> 01:25:55,679
in all the layers and notice here I'm

2412
01:25:53,340 --> 01:25:57,540
using a double list comprehension if you

2413
01:25:55,679 --> 01:25:59,820
want to call it that but for every layer

2414
01:25:57,540 --> 01:26:01,320
in layers and for every parameter in

2415
01:25:59,820 --> 01:26:03,780
each of those layers we are just

2416
01:26:01,320 --> 01:26:04,920
stacking up all those piece all those

2417
01:26:03,780 --> 01:26:09,420
parameters

2418
01:26:04,920 --> 01:26:11,040
now in total we have 46 000 parameters

2419
01:26:09,420 --> 01:26:14,239
and I'm telling by George that all of

2420
01:26:11,040 --> 01:26:14,239
them require gradient

2421
01:26:15,960 --> 01:26:21,420
then here we have everything here we are

2422
01:26:19,199 --> 01:26:23,699
actually mostly used to we are sampling

2423
01:26:21,420 --> 01:26:25,199
batch we are doing forward pass the

2424
01:26:23,699 --> 01:26:27,360
forward pass now is just a linear

2425
01:26:25,199 --> 01:26:29,340
application of all the layers in order

2426
01:26:27,360 --> 01:26:30,719
followed by the cross entropy

2427
01:26:29,340 --> 01:26:32,580
and then in the backward path you'll

2428
01:26:30,719 --> 01:26:34,500
notice that for every single layer I now

2429
01:26:32,580 --> 01:26:36,239
iterate over all the outputs and I'm

2430
01:26:34,500 --> 01:26:37,320
telling pytorch to retain the gradient

2431
01:26:36,239 --> 01:26:40,139
of them

2432
01:26:37,320 --> 01:26:42,179
and then here we are already used to all

2433
01:26:40,139 --> 01:26:44,340
the all the gradients set To None do the

2434
01:26:42,179 --> 01:26:46,800
backward to fill in the gradients do an

2435
01:26:44,340 --> 01:26:49,560
update using the caskaranian scent and

2436
01:26:46,800 --> 01:26:51,480
then track some statistics and then I am

2437
01:26:49,560 --> 01:26:53,760
going to break after a single iteration

2438
01:26:51,480 --> 01:26:56,219
now here in this cell in this diagram

2439
01:26:53,760 --> 01:26:57,600
I'm visualizing the histogram the

2440
01:26:56,219 --> 01:26:59,760
histograms of the forward pass

2441
01:26:57,600 --> 01:27:01,800
activations and I'm specifically doing

2442
01:26:59,760 --> 01:27:04,739
it at the 10 inch layers

2443
01:27:01,800 --> 01:27:06,239
so iterating over all the layers except

2444
01:27:04,739 --> 01:27:10,020
for the very last one which is basically

2445
01:27:06,239 --> 01:27:12,239
just the soft Max layer

2446
01:27:10,020 --> 01:27:13,800
um if it is a 10 inch layer and I'm

2447
01:27:12,239 --> 01:27:15,480
using a 10 inch layer just because they

2448
01:27:13,800 --> 01:27:17,280
have a finite output negative one to one

2449
01:27:15,480 --> 01:27:19,199
and so it's very easy to visualize here

2450
01:27:17,280 --> 01:27:21,659
so you see negative one to one and it's

2451
01:27:19,199 --> 01:27:24,239
a finite range and easy to work with

2452
01:27:21,659 --> 01:27:26,820
I take the out tensor from that layer

2453
01:27:24,239 --> 01:27:28,679
into T and then I'm calculating the mean

2454
01:27:26,820 --> 01:27:30,540
the standard deviation and the percent

2455
01:27:28,679 --> 01:27:31,739
saturation of t

2456
01:27:30,540 --> 01:27:33,900
and the way I Define the percent

2457
01:27:31,739 --> 01:27:36,239
saturation is that t dot absolute value

2458
01:27:33,900 --> 01:27:38,940
is greater than 0.97 so that means we

2459
01:27:36,239 --> 01:27:40,260
are here at the Tails of the 10h and

2460
01:27:38,940 --> 01:27:41,880
remember that when we are in the Tails

2461
01:27:40,260 --> 01:27:44,040
of the 10h that will actually stop

2462
01:27:41,880 --> 01:27:45,420
gradients so we don't want this to be

2463
01:27:44,040 --> 01:27:46,860
too high

2464
01:27:45,420 --> 01:27:49,440
now

2465
01:27:46,860 --> 01:27:51,480
here I'm calling torch.histogram and

2466
01:27:49,440 --> 01:27:52,739
then I am plotting this histogram so

2467
01:27:51,480 --> 01:27:54,300
basically what this is doing is that

2468
01:27:52,739 --> 01:27:55,860
every different type of layer and they

2469
01:27:54,300 --> 01:27:57,840
all have a different color we are

2470
01:27:55,860 --> 01:28:00,840
looking at how many

2471
01:27:57,840 --> 01:28:04,139
um values in these tensors take on any

2472
01:28:00,840 --> 01:28:06,120
of the values Below on this axis here

2473
01:28:04,139 --> 01:28:09,360
so the first layer is fairly saturated

2474
01:28:06,120 --> 01:28:11,520
here at 20 so you can see that it's got

2475
01:28:09,360 --> 01:28:13,620
Tails here but then everything sort of

2476
01:28:11,520 --> 01:28:15,239
stabilizes and if we had more layers

2477
01:28:13,620 --> 01:28:16,500
here it would actually just stabilize at

2478
01:28:15,239 --> 01:28:19,440
around the standard deviation of about

2479
01:28:16,500 --> 01:28:20,580
0.65 and the saturation would be roughly

2480
01:28:19,440 --> 01:28:22,620
five percent

2481
01:28:20,580 --> 01:28:24,719
and the reason that this stabilizes and

2482
01:28:22,620 --> 01:28:27,600
gives us a nice distribution here is

2483
01:28:24,719 --> 01:28:31,800
because gain is set to 5 over 3.

2484
01:28:27,600 --> 01:28:33,960
now here this gain you see that by

2485
01:28:31,800 --> 01:28:35,940
default we initialize with one over

2486
01:28:33,960 --> 01:28:37,739
square root of fan in but then here

2487
01:28:35,940 --> 01:28:39,300
during initialization I come in and I

2488
01:28:37,739 --> 01:28:42,300
iterate all the layers and if it's a

2489
01:28:39,300 --> 01:28:45,360
linear layer I boost that by the gain

2490
01:28:42,300 --> 01:28:48,780
now we saw that one so basically if we

2491
01:28:45,360 --> 01:28:52,080
just do not use a gain then what happens

2492
01:28:48,780 --> 01:28:54,840
if I redraw this you will see that

2493
01:28:52,080 --> 01:28:57,420
the standard deviation is shrinking and

2494
01:28:54,840 --> 01:28:58,560
the saturation is coming to zero and

2495
01:28:57,420 --> 01:29:01,500
basically what's happening is the first

2496
01:28:58,560 --> 01:29:03,060
layer is you know pretty decent but then

2497
01:29:01,500 --> 01:29:05,280
further layers are just kind of like

2498
01:29:03,060 --> 01:29:06,840
shrinking down to zero and it's

2499
01:29:05,280 --> 01:29:10,020
happening slowly but it's shrinking to

2500
01:29:06,840 --> 01:29:12,179
zero and the reason for that is when you

2501
01:29:10,020 --> 01:29:16,199
just have a sandwich of linear layers

2502
01:29:12,179 --> 01:29:18,840
alone then a then initializing our

2503
01:29:16,199 --> 01:29:20,760
weights in this manner we saw previously

2504
01:29:18,840 --> 01:29:22,020
would have conserved the standard

2505
01:29:20,760 --> 01:29:24,060
deviation of one

2506
01:29:22,020 --> 01:29:26,460
but because we have this interspersed

2507
01:29:24,060 --> 01:29:28,860
10h layers in there

2508
01:29:26,460 --> 01:29:30,600
the Stanley layers are squashing

2509
01:29:28,860 --> 01:29:32,639
functions and so they take your

2510
01:29:30,600 --> 01:29:36,000
distribution and they slightly squash it

2511
01:29:32,639 --> 01:29:39,440
and so some gain is necessary to keep

2512
01:29:36,000 --> 01:29:42,659
expanding it to fight the squashing

2513
01:29:39,440 --> 01:29:44,639
so it just turns out that 5 over 3 is a

2514
01:29:42,659 --> 01:29:47,639
good value so if we have something too

2515
01:29:44,639 --> 01:29:49,739
small like one we saw that things will

2516
01:29:47,639 --> 01:29:52,320
come towards zero but if it's something

2517
01:29:49,739 --> 01:29:54,300
too high let's do two

2518
01:29:52,320 --> 01:29:56,300
then here we see that

2519
01:29:54,300 --> 01:29:56,300
um

2520
01:29:56,520 --> 01:30:00,120
well let me do something a bit more

2521
01:29:58,440 --> 01:30:02,400
extreme because so it's a bit more

2522
01:30:00,120 --> 01:30:04,020
visible let's try three

2523
01:30:02,400 --> 01:30:06,179
okay so we see here that the saturations

2524
01:30:04,020 --> 01:30:08,699
are trying to be way too large

2525
01:30:06,179 --> 01:30:10,860
okay so three would create way too

2526
01:30:08,699 --> 01:30:14,159
saturated activations

2527
01:30:10,860 --> 01:30:16,980
so five over three is a good setting for

2528
01:30:14,159 --> 01:30:19,320
a sandwich of linear layers with 10 inch

2529
01:30:16,980 --> 01:30:21,480
activations and it roughly stabilizes

2530
01:30:19,320 --> 01:30:22,440
the standard deviation at a reasonable

2531
01:30:21,480 --> 01:30:24,659
point

2532
01:30:22,440 --> 01:30:27,719
now honestly I have no idea where five

2533
01:30:24,659 --> 01:30:28,320
over three came from in pytorch when we

2534
01:30:27,719 --> 01:30:31,320
were looking at the coming

2535
01:30:28,320 --> 01:30:34,080
initialization I see empirically that it

2536
01:30:31,320 --> 01:30:35,760
stabilizes this sandwich of linear n10h

2537
01:30:34,080 --> 01:30:38,040
and that the saturation is in a good

2538
01:30:35,760 --> 01:30:40,020
range but I don't actually know this

2539
01:30:38,040 --> 01:30:42,120
came out of some math formula I tried

2540
01:30:40,020 --> 01:30:44,100
searching briefly for where this comes

2541
01:30:42,120 --> 01:30:46,139
from but I wasn't able to find anything

2542
01:30:44,100 --> 01:30:47,639
but certainly we see that empirically

2543
01:30:46,139 --> 01:30:49,739
these are very nice ranges our

2544
01:30:47,639 --> 01:30:52,440
saturation is roughly five percent which

2545
01:30:49,739 --> 01:30:54,739
is a pretty good number and uh this is a

2546
01:30:52,440 --> 01:30:57,120
good setting of The gain in this context

2547
01:30:54,739 --> 01:30:59,520
similarly we can do the exact same thing

2548
01:30:57,120 --> 01:31:02,040
with the gradients so here is a very

2549
01:30:59,520 --> 01:31:03,360
same Loop if it's a 10h but instead of

2550
01:31:02,040 --> 01:31:05,460
taking the layered that out I'm taking

2551
01:31:03,360 --> 01:31:07,620
the grad and then I'm also showing the

2552
01:31:05,460 --> 01:31:09,840
mean on the standard deviation and I'm

2553
01:31:07,620 --> 01:31:11,219
plotting the histogram of these values

2554
01:31:09,840 --> 01:31:14,100
and so you'll see that the gradient

2555
01:31:11,219 --> 01:31:15,179
distribution is fairly reasonable and in

2556
01:31:14,100 --> 01:31:16,860
particular what we're looking for is

2557
01:31:15,179 --> 01:31:19,380
that all the different layers in this

2558
01:31:16,860 --> 01:31:21,900
sandwich has roughly the same gradient

2559
01:31:19,380 --> 01:31:24,239
things are not shrinking or exploding

2560
01:31:21,900 --> 01:31:25,800
so we can for example come here and we

2561
01:31:24,239 --> 01:31:30,320
can take a look at what happens if this

2562
01:31:25,800 --> 01:31:30,320
gain was way too small so this was 0.5

2563
01:31:30,420 --> 01:31:33,360
then you see the

2564
01:31:32,100 --> 01:31:35,340
first of all the activations are

2565
01:31:33,360 --> 01:31:36,960
shrinking to zero but also the gradients

2566
01:31:35,340 --> 01:31:39,000
are doing something weird the gradient

2567
01:31:36,960 --> 01:31:41,040
started out here and then now they're

2568
01:31:39,000 --> 01:31:43,980
like expanding out

2569
01:31:41,040 --> 01:31:46,380
and similarly if we for example have a

2570
01:31:43,980 --> 01:31:48,360
too high again so like three

2571
01:31:46,380 --> 01:31:50,040
then we see that also the gradients have

2572
01:31:48,360 --> 01:31:52,320
there's some asymmetry going on where as

2573
01:31:50,040 --> 01:31:54,480
you go into deeper and deeper layers the

2574
01:31:52,320 --> 01:31:56,040
activations are also changing and so

2575
01:31:54,480 --> 01:31:58,320
that's not what we want and in this case

2576
01:31:56,040 --> 01:32:00,600
we saw that without the use of Bachelor

2577
01:31:58,320 --> 01:32:02,880
as we are going through right now we

2578
01:32:00,600 --> 01:32:05,760
have to very carefully set those gains

2579
01:32:02,880 --> 01:32:07,800
to get nice activations in both the

2580
01:32:05,760 --> 01:32:09,120
forward pass and the backward pass now

2581
01:32:07,800 --> 01:32:11,219
before we move on to pasture

2582
01:32:09,120 --> 01:32:12,600
normalization I would also like to take

2583
01:32:11,219 --> 01:32:15,239
a look at what happens when we have no

2584
01:32:12,600 --> 01:32:16,679
10h units here so erasing all the 10

2585
01:32:15,239 --> 01:32:19,860
inch nonlinearities

2586
01:32:16,679 --> 01:32:22,260
but keeping the gain at 5 over 3. we now

2587
01:32:19,860 --> 01:32:23,100
have just a giant linear sandwich so

2588
01:32:22,260 --> 01:32:24,179
let's see what happens to the

2589
01:32:23,100 --> 01:32:26,760
activations

2590
01:32:24,179 --> 01:32:28,620
as we saw before the correct gain here

2591
01:32:26,760 --> 01:32:30,560
is one that is the standard deviation

2592
01:32:28,620 --> 01:32:34,440
preserving gain so

2593
01:32:30,560 --> 01:32:36,900
1.667 is too high and so what's going to

2594
01:32:34,440 --> 01:32:39,480
happen now is the following

2595
01:32:36,900 --> 01:32:41,580
I have to change this to be linear so we

2596
01:32:39,480 --> 01:32:42,840
are because there's no more 10 inch

2597
01:32:41,580 --> 01:32:46,020
layers

2598
01:32:42,840 --> 01:32:48,360
and let me change this to linear as well

2599
01:32:46,020 --> 01:32:50,760
so what we're seeing is

2600
01:32:48,360 --> 01:32:54,480
um the activations started out on the

2601
01:32:50,760 --> 01:32:55,980
blue and have by layer 4 become very

2602
01:32:54,480 --> 01:32:57,659
diffuse so what's happening to the

2603
01:32:55,980 --> 01:33:00,480
activations is this

2604
01:32:57,659 --> 01:33:03,060
and with the gradients on the top layer

2605
01:33:00,480 --> 01:33:05,880
the activation the gradient statistics

2606
01:33:03,060 --> 01:33:08,040
are the purple and then they diminish as

2607
01:33:05,880 --> 01:33:09,900
you go down deeper in the layers and so

2608
01:33:08,040 --> 01:33:11,520
basically you have an asymmetry like in

2609
01:33:09,900 --> 01:33:12,840
the neural net and you might imagine

2610
01:33:11,520 --> 01:33:14,340
that if you have very deep neural

2611
01:33:12,840 --> 01:33:17,460
networks say like 50 layers or something

2612
01:33:14,340 --> 01:33:20,699
like that this just this is not a good

2613
01:33:17,460 --> 01:33:22,800
place to be so that's why before bash

2614
01:33:20,699 --> 01:33:26,159
normalization this was incredibly tricky

2615
01:33:22,800 --> 01:33:27,719
to to set in particular if this is too

2616
01:33:26,159 --> 01:33:29,400
large of a game this happens and if it's

2617
01:33:27,719 --> 01:33:32,340
too little it can gain

2618
01:33:29,400 --> 01:33:36,060
then this happens also the opposite of

2619
01:33:32,340 --> 01:33:40,139
that basically happens here we have a um

2620
01:33:36,060 --> 01:33:42,300
shrinking and a diffusion depending on

2621
01:33:40,139 --> 01:33:43,920
which direction you look at it from

2622
01:33:42,300 --> 01:33:45,300
and so certainly this is not what you

2623
01:33:43,920 --> 01:33:48,420
want and in this case the correct

2624
01:33:45,300 --> 01:33:49,800
setting of The gain is exactly one

2625
01:33:48,420 --> 01:33:51,719
just like we're doing at initialization

2626
01:33:49,800 --> 01:33:54,420
and then we see that

2627
01:33:51,719 --> 01:33:57,000
the statistics for the forward and the

2628
01:33:54,420 --> 01:33:59,340
backward pass are well behaved and so

2629
01:33:57,000 --> 01:34:02,460
the reason I want to show you this is

2630
01:33:59,340 --> 01:34:04,199
the basically like getting neuralness to

2631
01:34:02,460 --> 01:34:06,000
train before these normalization layers

2632
01:34:04,199 --> 01:34:07,739
and before the use of advanced

2633
01:34:06,000 --> 01:34:09,780
optimizers like atom which we still have

2634
01:34:07,739 --> 01:34:12,300
to cover and residual connections and so

2635
01:34:09,780 --> 01:34:15,179
on training neurons basically look like

2636
01:34:12,300 --> 01:34:16,440
this it's like a total Balancing Act you

2637
01:34:15,179 --> 01:34:18,840
have to make sure that everything is

2638
01:34:16,440 --> 01:34:19,920
precisely orchestrated and you have to

2639
01:34:18,840 --> 01:34:21,900
care about the activations and the

2640
01:34:19,920 --> 01:34:24,000
gradients and their statistics and then

2641
01:34:21,900 --> 01:34:25,380
maybe you can train something but it was

2642
01:34:24,000 --> 01:34:26,940
basically impossible to train very deep

2643
01:34:25,380 --> 01:34:29,159
networks and this is fundamentally the

2644
01:34:26,940 --> 01:34:32,100
reason for that you'd have to be very

2645
01:34:29,159 --> 01:34:34,980
very careful with your initialization

2646
01:34:32,100 --> 01:34:36,120
um the other point here is you might be

2647
01:34:34,980 --> 01:34:38,280
asking yourself by the way I'm not sure

2648
01:34:36,120 --> 01:34:41,699
if I covered this why do we need these

2649
01:34:38,280 --> 01:34:43,440
10h layers at all why do we include them

2650
01:34:41,699 --> 01:34:45,179
and then have to worry about the gain

2651
01:34:43,440 --> 01:34:46,739
and uh the reason for that of course is

2652
01:34:45,179 --> 01:34:47,639
that if you just have a stack of linear

2653
01:34:46,739 --> 01:34:49,739
layers

2654
01:34:47,639 --> 01:34:53,219
then certainly we're getting very easily

2655
01:34:49,739 --> 01:34:54,900
nice activations and so on but this is

2656
01:34:53,219 --> 01:34:56,699
just a massive linear sandwich and it

2657
01:34:54,900 --> 01:34:58,260
turns out that it collapses to a single

2658
01:34:56,699 --> 01:35:00,719
linear layer in terms of its

2659
01:34:58,260 --> 01:35:02,520
representation power so if you were to

2660
01:35:00,719 --> 01:35:03,900
plot the output as a function of the

2661
01:35:02,520 --> 01:35:05,520
input you're just getting a linear

2662
01:35:03,900 --> 01:35:07,320
function no matter how many linear

2663
01:35:05,520 --> 01:35:09,540
layers you stack up you still just end

2664
01:35:07,320 --> 01:35:12,420
up with a linear transformation all the

2665
01:35:09,540 --> 01:35:15,360
W X Plus B's just collapse into a large

2666
01:35:12,420 --> 01:35:17,400
WX plus b with slightly different W's as

2667
01:35:15,360 --> 01:35:18,960
likely different B

2668
01:35:17,400 --> 01:35:20,639
um but interestingly even though the

2669
01:35:18,960 --> 01:35:23,280
forward pass collapses to just a linear

2670
01:35:20,639 --> 01:35:26,340
layer because of back propagation and

2671
01:35:23,280 --> 01:35:28,380
the Dynamics of the backward pass the

2672
01:35:26,340 --> 01:35:30,480
optimization is really is not identical

2673
01:35:28,380 --> 01:35:31,500
you actually end up with all kinds of

2674
01:35:30,480 --> 01:35:32,219
interesting

2675
01:35:31,500 --> 01:35:35,280
um

2676
01:35:32,219 --> 01:35:36,780
Dynamics in the backward pass because of

2677
01:35:35,280 --> 01:35:39,420
the uh the way the chain rule is

2678
01:35:36,780 --> 01:35:42,000
calculating it and so optimizing a

2679
01:35:39,420 --> 01:35:44,100
linear layer by itself and optimizing a

2680
01:35:42,000 --> 01:35:45,360
sandwich of 10 millionaire layers in

2681
01:35:44,100 --> 01:35:47,040
both cases those are just a linear

2682
01:35:45,360 --> 01:35:48,239
transformation in the forward pass but

2683
01:35:47,040 --> 01:35:50,340
the training Dynamics would be different

2684
01:35:48,239 --> 01:35:53,400
and there's entire papers that analyze

2685
01:35:50,340 --> 01:35:55,800
in fact like infinitely layered linear

2686
01:35:53,400 --> 01:35:58,380
layers and so on and so there's a lot of

2687
01:35:55,800 --> 01:35:59,820
things too that you can play with there

2688
01:35:58,380 --> 01:36:02,100
uh but basically the technical

2689
01:35:59,820 --> 01:36:03,540
linearities allow us to

2690
01:36:02,100 --> 01:36:07,940
um

2691
01:36:03,540 --> 01:36:11,699
turn this sandwich from just a linear

2692
01:36:07,940 --> 01:36:14,219
function into a neural network that can

2693
01:36:11,699 --> 01:36:15,600
in principle approximate any arbitrary

2694
01:36:14,219 --> 01:36:17,400
function

2695
01:36:15,600 --> 01:36:20,760
okay so now I've reset the code to use

2696
01:36:17,400 --> 01:36:22,920
the linear 10h sandwich like before and

2697
01:36:20,760 --> 01:36:25,260
I reset everything so the gains five

2698
01:36:22,920 --> 01:36:27,239
over three we can run a single step of

2699
01:36:25,260 --> 01:36:28,679
optimization and we can look at the

2700
01:36:27,239 --> 01:36:30,480
activation statistics of the forward

2701
01:36:28,679 --> 01:36:32,400
pass and the backward pass

2702
01:36:30,480 --> 01:36:33,840
but I've added one more plot here that I

2703
01:36:32,400 --> 01:36:35,100
think is really important to look at

2704
01:36:33,840 --> 01:36:37,260
when you're training your neural nuts

2705
01:36:35,100 --> 01:36:38,760
and to consider and ultimately what

2706
01:36:37,260 --> 01:36:40,560
we're doing is we're updating the

2707
01:36:38,760 --> 01:36:42,900
parameters of the neural net so we care

2708
01:36:40,560 --> 01:36:44,400
about the parameters and their values

2709
01:36:42,900 --> 01:36:45,780
and their gradients

2710
01:36:44,400 --> 01:36:47,159
so here what I'm doing is I'm actually

2711
01:36:45,780 --> 01:36:49,500
iterating over all the parameters

2712
01:36:47,159 --> 01:36:51,960
available and then I'm only

2713
01:36:49,500 --> 01:36:53,100
um restricting it to the two-dimensional

2714
01:36:51,960 --> 01:36:55,080
parameters which are basically the

2715
01:36:53,100 --> 01:36:57,900
weights of these linear layers and I'm

2716
01:36:55,080 --> 01:37:00,239
skipping the biases and I'm skipping the

2717
01:36:57,900 --> 01:37:02,340
um Gammas and the betas in the bathroom

2718
01:37:00,239 --> 01:37:04,020
just for Simplicity

2719
01:37:02,340 --> 01:37:05,520
but you can also take a look at those as

2720
01:37:04,020 --> 01:37:08,880
well but what's happening with the

2721
01:37:05,520 --> 01:37:10,500
weights is um instructive by itself

2722
01:37:08,880 --> 01:37:13,500
so here we have all the different

2723
01:37:10,500 --> 01:37:15,480
weights their shapes so this is the

2724
01:37:13,500 --> 01:37:16,980
embedding layer the first linear layer

2725
01:37:15,480 --> 01:37:18,780
all the way to the very last linear

2726
01:37:16,980 --> 01:37:20,280
layer and then we have the mean the

2727
01:37:18,780 --> 01:37:21,900
standard deviation of all these

2728
01:37:20,280 --> 01:37:23,639
parameters

2729
01:37:21,900 --> 01:37:25,080
the histogram and you can see that it

2730
01:37:23,639 --> 01:37:26,940
actually doesn't look that amazing so

2731
01:37:25,080 --> 01:37:28,320
there's some trouble in Paradise even

2732
01:37:26,940 --> 01:37:30,060
though these gradients looked okay

2733
01:37:28,320 --> 01:37:32,520
there's something weird going on here

2734
01:37:30,060 --> 01:37:34,920
I'll get to that in a second and the

2735
01:37:32,520 --> 01:37:37,199
last thing here is the gradient to data

2736
01:37:34,920 --> 01:37:39,360
ratio so sometimes I like to visualize

2737
01:37:37,199 --> 01:37:42,060
this as well because what this gives you

2738
01:37:39,360 --> 01:37:44,340
a sense of is what is the scale of the

2739
01:37:42,060 --> 01:37:46,560
gradient compared to the scale of the

2740
01:37:44,340 --> 01:37:48,480
actual values and this is important

2741
01:37:46,560 --> 01:37:49,940
because we're going to end up taking a

2742
01:37:48,480 --> 01:37:52,020
step update

2743
01:37:49,940 --> 01:37:54,719
that is the learning rate times the

2744
01:37:52,020 --> 01:37:56,940
gradient onto the data and so the

2745
01:37:54,719 --> 01:37:58,139
gradient has two large of magnitude if

2746
01:37:56,940 --> 01:38:00,360
the numbers in there are too large

2747
01:37:58,139 --> 01:38:01,620
compared to the numbers in data then

2748
01:38:00,360 --> 01:38:04,199
you'd be in trouble

2749
01:38:01,620 --> 01:38:06,840
but in this case the gradient to data is

2750
01:38:04,199 --> 01:38:09,719
our loan numbers so the values inside

2751
01:38:06,840 --> 01:38:12,719
grad are 1000 times smaller than the

2752
01:38:09,719 --> 01:38:13,739
values inside data in these weights most

2753
01:38:12,719 --> 01:38:16,199
of them

2754
01:38:13,739 --> 01:38:18,120
now notably that is not true about the

2755
01:38:16,199 --> 01:38:19,800
last layer and so the last layer

2756
01:38:18,120 --> 01:38:21,480
actually here the output layer is a bit

2757
01:38:19,800 --> 01:38:23,159
of a troublemaker in the way that this

2758
01:38:21,480 --> 01:38:25,199
is currently arranged because you can

2759
01:38:23,159 --> 01:38:29,100
see that the um

2760
01:38:25,199 --> 01:38:31,260
the last layer here in pink takes on

2761
01:38:29,100 --> 01:38:34,260
values that are much larger than some of

2762
01:38:31,260 --> 01:38:36,659
the values inside

2763
01:38:34,260 --> 01:38:38,159
um inside the neural net so the standard

2764
01:38:36,659 --> 01:38:40,080
deviations are roughly 1 and negative

2765
01:38:38,159 --> 01:38:42,780
three throughout except for the last

2766
01:38:40,080 --> 01:38:44,820
last layer which actually has roughly

2767
01:38:42,780 --> 01:38:47,219
one e negative two a standard deviation

2768
01:38:44,820 --> 01:38:49,739
of gradients and so the gradients on the

2769
01:38:47,219 --> 01:38:52,920
last layer are currently about 100 times

2770
01:38:49,739 --> 01:38:55,860
greater sorry 10 times greater than all

2771
01:38:52,920 --> 01:38:58,320
the other weights inside the neural nut

2772
01:38:55,860 --> 01:39:00,060
and so that's problematic because in the

2773
01:38:58,320 --> 01:39:02,280
simple stochastically in the sun setup

2774
01:39:00,060 --> 01:39:04,440
you would be training this last layer

2775
01:39:02,280 --> 01:39:05,760
about 10 times faster than you would be

2776
01:39:04,440 --> 01:39:07,020
training the other layers at

2777
01:39:05,760 --> 01:39:09,179
initialization

2778
01:39:07,020 --> 01:39:10,920
now this actually like kind of fixes

2779
01:39:09,179 --> 01:39:12,780
itself a little bit if you train for a

2780
01:39:10,920 --> 01:39:16,139
bit longer so for example if I greater

2781
01:39:12,780 --> 01:39:18,179
than 1000 only then do a break

2782
01:39:16,139 --> 01:39:21,420
let me reinitialize and then let me do

2783
01:39:18,179 --> 01:39:24,000
it 1000 steps and after 1000 steps we

2784
01:39:21,420 --> 01:39:26,159
can look at the forward pass

2785
01:39:24,000 --> 01:39:28,500
okay so you see how the neurons are a

2786
01:39:26,159 --> 01:39:30,600
bit are saturating a bit and we can also

2787
01:39:28,500 --> 01:39:32,760
look at the backward pass but otherwise

2788
01:39:30,600 --> 01:39:34,260
they look good they're about equal and

2789
01:39:32,760 --> 01:39:36,060
there's no shrinking to zero or

2790
01:39:34,260 --> 01:39:38,340
exploding to infinities

2791
01:39:36,060 --> 01:39:40,380
and you can see that here in the weights

2792
01:39:38,340 --> 01:39:43,139
things are also stabilizing a little bit

2793
01:39:40,380 --> 01:39:44,940
so the Tails of the last pink layer are

2794
01:39:43,139 --> 01:39:46,260
actually coming down coming in during

2795
01:39:44,940 --> 01:39:47,940
the optimization

2796
01:39:46,260 --> 01:39:50,100
but certainly this is like a little bit

2797
01:39:47,940 --> 01:39:51,360
of troubling especially if you are using

2798
01:39:50,100 --> 01:39:53,280
a very simple update rule like

2799
01:39:51,360 --> 01:39:55,139
stochastic gradient descent instead of a

2800
01:39:53,280 --> 01:39:56,760
modern Optimizer like Adam

2801
01:39:55,139 --> 01:39:57,960
now I'd like to show you one more plot

2802
01:39:56,760 --> 01:40:00,600
that I usually look at when I train

2803
01:39:57,960 --> 01:40:02,280
neural networks and basically the

2804
01:40:00,600 --> 01:40:04,320
gradient to data ratio is not actually

2805
01:40:02,280 --> 01:40:05,639
that informative because what matters at

2806
01:40:04,320 --> 01:40:08,340
the end is not the gradient to date

2807
01:40:05,639 --> 01:40:09,960
ratio but the update to the data ratio

2808
01:40:08,340 --> 01:40:11,580
because that is the amount by which we

2809
01:40:09,960 --> 01:40:12,840
will actually change the data in these

2810
01:40:11,580 --> 01:40:15,179
tensors

2811
01:40:12,840 --> 01:40:17,400
so coming up here what I'd like to do is

2812
01:40:15,179 --> 01:40:19,739
I'd like to introduce a new update to

2813
01:40:17,400 --> 01:40:21,000
data ratio

2814
01:40:19,739 --> 01:40:23,040
it's going to be less than we're going

2815
01:40:21,000 --> 01:40:24,719
to build it out every single iteration

2816
01:40:23,040 --> 01:40:25,679
and here I'd like to keep track of

2817
01:40:24,719 --> 01:40:27,600
basically

2818
01:40:25,679 --> 01:40:30,060
the ratio

2819
01:40:27,600 --> 01:40:31,440
every single iteration

2820
01:40:30,060 --> 01:40:34,440
so

2821
01:40:31,440 --> 01:40:36,420
without any gradients I'm comparing the

2822
01:40:34,440 --> 01:40:38,820
update which is learning rate times the

2823
01:40:36,420 --> 01:40:40,139
times the gradient

2824
01:40:38,820 --> 01:40:42,179
that is the update that we're going to

2825
01:40:40,139 --> 01:40:43,800
apply to every parameter

2826
01:40:42,179 --> 01:40:45,300
social Mediterranean or all the

2827
01:40:43,800 --> 01:40:46,620
parameters and then I'm taking the

2828
01:40:45,300 --> 01:40:49,080
basically standard deviation of the

2829
01:40:46,620 --> 01:40:53,460
update we're going to apply and divide

2830
01:40:49,080 --> 01:40:54,840
it by the actual content the data of

2831
01:40:53,460 --> 01:40:56,040
that parameter and its standard

2832
01:40:54,840 --> 01:40:58,620
deviation

2833
01:40:56,040 --> 01:41:01,139
so this is the ratio of basically how

2834
01:40:58,620 --> 01:41:02,280
great are the updates to the values in

2835
01:41:01,139 --> 01:41:03,780
these tensors

2836
01:41:02,280 --> 01:41:06,540
then we're going to take a log of it and

2837
01:41:03,780 --> 01:41:07,380
actually I'd like to take a log 10.

2838
01:41:06,540 --> 01:41:10,620
um

2839
01:41:07,380 --> 01:41:11,580
just so it's a nicer visualization so

2840
01:41:10,620 --> 01:41:14,659
we're going to be basically looking at

2841
01:41:11,580 --> 01:41:17,699
the exponents of the

2842
01:41:14,659 --> 01:41:19,980
of this division here and then that item

2843
01:41:17,699 --> 01:41:21,239
to pop out the float and we're going to

2844
01:41:19,980 --> 01:41:23,040
be keeping track of this for all the

2845
01:41:21,239 --> 01:41:24,179
parameters and adding it to this UD

2846
01:41:23,040 --> 01:41:26,280
tensor

2847
01:41:24,179 --> 01:41:27,540
so now let me reinitialize and run a

2848
01:41:26,280 --> 01:41:31,020
thousand iterations

2849
01:41:27,540 --> 01:41:33,179
we can look at the activations the

2850
01:41:31,020 --> 01:41:35,280
gradients and the parameter gradients as

2851
01:41:33,179 --> 01:41:37,440
we did before but now I have one more

2852
01:41:35,280 --> 01:41:38,880
plot here to introduce

2853
01:41:37,440 --> 01:41:40,860
now what's happening here is where every

2854
01:41:38,880 --> 01:41:42,719
interval go to parameters and I'm

2855
01:41:40,860 --> 01:41:44,639
constraining it again like I did here to

2856
01:41:42,719 --> 01:41:46,679
just the weights

2857
01:41:44,639 --> 01:41:48,540
so the number of dimensions in these

2858
01:41:46,679 --> 01:41:52,440
sensors is two and then I'm basically

2859
01:41:48,540 --> 01:41:54,420
plotting all of these update ratios over

2860
01:41:52,440 --> 01:41:56,580
time

2861
01:41:54,420 --> 01:41:58,320
so when I plot this

2862
01:41:56,580 --> 01:41:59,699
I plot those ratios and you can see that

2863
01:41:58,320 --> 01:42:01,560
they evolve over time during

2864
01:41:59,699 --> 01:42:03,239
initialization to take on certain values

2865
01:42:01,560 --> 01:42:04,739
and then these updates sort of like

2866
01:42:03,239 --> 01:42:05,760
start stabilizing usually during

2867
01:42:04,739 --> 01:42:07,260
training

2868
01:42:05,760 --> 01:42:08,639
then the other thing that I'm plotting

2869
01:42:07,260 --> 01:42:10,739
here is I'm plotting here like an

2870
01:42:08,639 --> 01:42:13,260
approximate value that is a Rough Guide

2871
01:42:10,739 --> 01:42:14,639
for what it roughly should be and it

2872
01:42:13,260 --> 01:42:15,360
should be like roughly one in negative

2873
01:42:14,639 --> 01:42:17,639
three

2874
01:42:15,360 --> 01:42:20,159
and so that means that basically there's

2875
01:42:17,639 --> 01:42:22,139
some values in this tensor

2876
01:42:20,159 --> 01:42:23,460
um and they take on certain values and

2877
01:42:22,139 --> 01:42:26,639
the updates to them at every single

2878
01:42:23,460 --> 01:42:29,400
iteration are no more than roughly 1 000

2879
01:42:26,639 --> 01:42:32,460
of the actual like magnitude in those

2880
01:42:29,400 --> 01:42:34,739
tensors uh if this was much larger like

2881
01:42:32,460 --> 01:42:35,400
for example if this was

2882
01:42:34,739 --> 01:42:37,260
um

2883
01:42:35,400 --> 01:42:38,940
if the log of this was like say negative

2884
01:42:37,260 --> 01:42:40,860
one this is actually updating those

2885
01:42:38,940 --> 01:42:42,060
values quite a lot they're undergoing a

2886
01:42:40,860 --> 01:42:44,460
lot of change

2887
01:42:42,060 --> 01:42:46,980
but the reason that the final rate the

2888
01:42:44,460 --> 01:42:49,080
final layer here is an outlier is

2889
01:42:46,980 --> 01:42:52,380
because this layer was artificially

2890
01:42:49,080 --> 01:42:54,360
shrunk down to keep the soft max income

2891
01:42:52,380 --> 01:42:56,159
unconfident

2892
01:42:54,360 --> 01:42:57,719
so here

2893
01:42:56,159 --> 01:42:58,980
you see how we multiply The Weight by

2894
01:42:57,719 --> 01:43:00,960
point one

2895
01:42:58,980 --> 01:43:03,960
uh in the initialization to make the

2896
01:43:00,960 --> 01:43:06,900
last layer prediction less confident

2897
01:43:03,960 --> 01:43:08,940
that made that artificially made the

2898
01:43:06,900 --> 01:43:10,920
values inside that tensor way too low

2899
01:43:08,940 --> 01:43:12,900
and that's why we're getting temporarily

2900
01:43:10,920 --> 01:43:15,780
a very high ratio but you see that that

2901
01:43:12,900 --> 01:43:17,880
stabilizes over time once that weight

2902
01:43:15,780 --> 01:43:19,380
starts to learn starts to learn

2903
01:43:17,880 --> 01:43:21,900
but basically I like to look at the

2904
01:43:19,380 --> 01:43:23,940
evolution of this update ratio for all

2905
01:43:21,900 --> 01:43:26,880
my parameters usually and I like to make

2906
01:43:23,940 --> 01:43:29,280
sure that it's not too much above

2907
01:43:26,880 --> 01:43:31,980
wanting negative three roughly

2908
01:43:29,280 --> 01:43:32,940
uh so around negative three on this log

2909
01:43:31,980 --> 01:43:34,560
plot

2910
01:43:32,940 --> 01:43:35,940
if it's below negative three usually

2911
01:43:34,560 --> 01:43:37,320
that means that the parameters are not

2912
01:43:35,940 --> 01:43:38,880
training fast enough

2913
01:43:37,320 --> 01:43:41,340
so if our learning rate was very low

2914
01:43:38,880 --> 01:43:43,920
let's do that experiment

2915
01:43:41,340 --> 01:43:46,139
let's initialize and then let's actually

2916
01:43:43,920 --> 01:43:47,520
do a learning rate of say y negative

2917
01:43:46,139 --> 01:43:49,500
three here

2918
01:43:47,520 --> 01:43:53,119
so 0.001

2919
01:43:49,500 --> 01:43:53,119
if you're learning rate is way too low

2920
01:43:53,699 --> 01:43:59,280
this plot will typically reveal it

2921
01:43:56,400 --> 01:44:01,560
so you see how all of these updates are

2922
01:43:59,280 --> 01:44:05,659
way too small so the size of the update

2923
01:44:01,560 --> 01:44:08,940
is basically uh 10 000 times

2924
01:44:05,659 --> 01:44:10,980
in magnitude to the size of the numbers

2925
01:44:08,940 --> 01:44:12,780
in that tensor in the first place so

2926
01:44:10,980 --> 01:44:14,460
this is a symptom of training way too

2927
01:44:12,780 --> 01:44:16,320
slow

2928
01:44:14,460 --> 01:44:17,639
so this is another way to sometimes set

2929
01:44:16,320 --> 01:44:19,440
the learning rate and to get a sense of

2930
01:44:17,639 --> 01:44:20,639
what that learning rate should be and

2931
01:44:19,440 --> 01:44:23,600
ultimately this is something that you

2932
01:44:20,639 --> 01:44:23,600
would keep track of

2933
01:44:24,900 --> 01:44:29,940
if anything the learning rate here is a

2934
01:44:28,020 --> 01:44:31,440
little bit on the higher side because

2935
01:44:29,940 --> 01:44:33,600
you see that

2936
01:44:31,440 --> 01:44:34,679
um we're above the black line of

2937
01:44:33,600 --> 01:44:38,100
negative three we're somewhere around

2938
01:44:34,679 --> 01:44:39,600
negative 2.5 it's like okay and uh but

2939
01:44:38,100 --> 01:44:41,219
everything is like somewhat stabilizing

2940
01:44:39,600 --> 01:44:42,540
and so this looks like a pretty decent

2941
01:44:41,219 --> 01:44:44,520
setting of of

2942
01:44:42,540 --> 01:44:46,080
um learning rates and so on but this is

2943
01:44:44,520 --> 01:44:47,820
something to look at and when things are

2944
01:44:46,080 --> 01:44:50,340
miscalibrated you will you will see very

2945
01:44:47,820 --> 01:44:51,840
quickly so for example

2946
01:44:50,340 --> 01:44:53,820
everything looks pretty well behaved

2947
01:44:51,840 --> 01:44:55,320
right but just as a comparison when

2948
01:44:53,820 --> 01:44:57,420
things are not properly calibrated what

2949
01:44:55,320 --> 01:45:00,659
does that look like let me come up here

2950
01:44:57,420 --> 01:45:01,560
and let's say that for example uh what

2951
01:45:00,659 --> 01:45:04,139
do we do

2952
01:45:01,560 --> 01:45:06,480
let's say that we forgot to apply this

2953
01:45:04,139 --> 01:45:07,860
fan in normalization so the weights

2954
01:45:06,480 --> 01:45:09,420
inside the linear layers are just a

2955
01:45:07,860 --> 01:45:10,560
sample for my gaussian in all those

2956
01:45:09,420 --> 01:45:12,840
stages

2957
01:45:10,560 --> 01:45:14,280
what happens to our how do we notice

2958
01:45:12,840 --> 01:45:16,199
that something's off

2959
01:45:14,280 --> 01:45:18,080
well the activation plot will tell you

2960
01:45:16,199 --> 01:45:20,280
whoa your neurons are way too saturated

2961
01:45:18,080 --> 01:45:21,000
the gradients are going to be all messed

2962
01:45:20,280 --> 01:45:22,739
up

2963
01:45:21,000 --> 01:45:25,500
the histogram for these weights are

2964
01:45:22,739 --> 01:45:27,659
going to be all messed up as well and

2965
01:45:25,500 --> 01:45:29,280
there's a lot of asymmetry and then if

2966
01:45:27,659 --> 01:45:32,100
we look here I suspect it's all going to

2967
01:45:29,280 --> 01:45:34,739
be also pretty messed up so you see

2968
01:45:32,100 --> 01:45:36,900
there's a lot of discrepancy in how fast

2969
01:45:34,739 --> 01:45:39,119
these layers are learning and some of

2970
01:45:36,900 --> 01:45:41,820
them are learning way too fast so

2971
01:45:39,119 --> 01:45:43,320
negative one negative 1.5 those aren't

2972
01:45:41,820 --> 01:45:45,300
very large numbers in terms of this

2973
01:45:43,320 --> 01:45:46,860
ratio again you should be somewhere

2974
01:45:45,300 --> 01:45:50,760
around negative three and not much more

2975
01:45:46,860 --> 01:45:52,080
about that so this is how miscalibration

2976
01:45:50,760 --> 01:45:54,179
so if your neural nuts are going to

2977
01:45:52,080 --> 01:45:55,920
manifest and these kinds of plots here

2978
01:45:54,179 --> 01:45:58,679
are a good way of

2979
01:45:55,920 --> 01:46:01,679
um sort of bringing those

2980
01:45:58,679 --> 01:46:03,659
miscalibrations sort of uh

2981
01:46:01,679 --> 01:46:06,000
to your attention and so you can address

2982
01:46:03,659 --> 01:46:08,639
them okay so so far we've seen that when

2983
01:46:06,000 --> 01:46:10,260
we have this linear 10h sandwich we can

2984
01:46:08,639 --> 01:46:12,360
actually precisely calibrate the gains

2985
01:46:10,260 --> 01:46:14,460
and make the activations the gradients

2986
01:46:12,360 --> 01:46:16,380
and the parameters and the updates all

2987
01:46:14,460 --> 01:46:18,900
look pretty decent but it definitely

2988
01:46:16,380 --> 01:46:21,600
feels a little bit like balancing

2989
01:46:18,900 --> 01:46:23,480
of a pencil on your finger and that's

2990
01:46:21,600 --> 01:46:25,860
because this gain has to be very

2991
01:46:23,480 --> 01:46:26,940
precisely calibrated

2992
01:46:25,860 --> 01:46:28,560
so now let's introduce batch

2993
01:46:26,940 --> 01:46:31,320
normalization layers into the fix into

2994
01:46:28,560 --> 01:46:33,840
the mix and let's let's see how that

2995
01:46:31,320 --> 01:46:35,940
helps fix the problem

2996
01:46:33,840 --> 01:46:37,320
so here

2997
01:46:35,940 --> 01:46:38,219
I'm going to take the bachelor Monday

2998
01:46:37,320 --> 01:46:40,920
class

2999
01:46:38,219 --> 01:46:43,739
and I'm going to start placing it inside

3000
01:46:40,920 --> 01:46:45,540
and as I mentioned before the standard

3001
01:46:43,739 --> 01:46:47,520
typical place you would place it is

3002
01:46:45,540 --> 01:46:49,380
between the linear layer so right after

3003
01:46:47,520 --> 01:46:50,820
it but before the nonlinearity but

3004
01:46:49,380 --> 01:46:53,520
people have definitely played with that

3005
01:46:50,820 --> 01:46:55,500
and uh in fact you can get very similar

3006
01:46:53,520 --> 01:46:57,139
results even if you place it after the

3007
01:46:55,500 --> 01:46:59,100
nonlinearity

3008
01:46:57,139 --> 01:47:00,360
and the other thing that I wanted to

3009
01:46:59,100 --> 01:47:02,460
mention is it's totally fine to also

3010
01:47:00,360 --> 01:47:04,199
place it at the end after the last

3011
01:47:02,460 --> 01:47:07,020
linear layer and before the loss

3012
01:47:04,199 --> 01:47:08,119
function so this is potentially fine as

3013
01:47:07,020 --> 01:47:10,980
well

3014
01:47:08,119 --> 01:47:14,159
and in this case this would be output

3015
01:47:10,980 --> 01:47:15,840
would be world cup size

3016
01:47:14,159 --> 01:47:18,420
um now because the last layer is

3017
01:47:15,840 --> 01:47:19,980
mushroom we would not be changing the

3018
01:47:18,420 --> 01:47:22,980
weight to make the softmax less

3019
01:47:19,980 --> 01:47:24,920
confident we'd be changing the gamma

3020
01:47:22,980 --> 01:47:27,840
because gamma remember in the bathroom

3021
01:47:24,920 --> 01:47:29,280
is the variable that multiplicatively

3022
01:47:27,840 --> 01:47:31,880
interacts with the output of that

3023
01:47:29,280 --> 01:47:31,880
normalization

3024
01:47:32,580 --> 01:47:38,100
so we can initialize this sandwich now

3025
01:47:35,400 --> 01:47:40,440
and we can train and we can see that the

3026
01:47:38,100 --> 01:47:43,080
activations are going to of course look

3027
01:47:40,440 --> 01:47:44,760
very good and they are going to

3028
01:47:43,080 --> 01:47:47,400
necessarily look good because now before

3029
01:47:44,760 --> 01:47:50,400
every single 10 H layer there is a

3030
01:47:47,400 --> 01:47:52,980
normalization in The Bachelor so this is

3031
01:47:50,400 --> 01:47:54,420
unsurprisingly all looks pretty good

3032
01:47:52,980 --> 01:47:57,179
it's going to be standard deviation of

3033
01:47:54,420 --> 01:47:59,159
roughly 0.65 two percent and roughly

3034
01:47:57,179 --> 01:48:00,719
equal standard deviation throughout the

3035
01:47:59,159 --> 01:48:02,580
entire layers so everything looks very

3036
01:48:00,719 --> 01:48:06,119
homogeneous

3037
01:48:02,580 --> 01:48:09,119
the gradients look good the weights look

3038
01:48:06,119 --> 01:48:11,159
good and they're distributions

3039
01:48:09,119 --> 01:48:14,820
and then the updates

3040
01:48:11,159 --> 01:48:16,560
also look pretty reasonable we're going

3041
01:48:14,820 --> 01:48:19,260
above negative three a little bit but

3042
01:48:16,560 --> 01:48:21,119
not by too much so all the parameters

3043
01:48:19,260 --> 01:48:24,119
are training in roughly the same rate

3044
01:48:21,119 --> 01:48:24,119
here

3045
01:48:24,659 --> 01:48:28,679
but now what we've gained is we are

3046
01:48:27,000 --> 01:48:30,800
going to be slightly less

3047
01:48:28,679 --> 01:48:30,800
um

3048
01:48:31,199 --> 01:48:35,580
brittle with respect to the gain of

3049
01:48:33,719 --> 01:48:38,219
these so for example I can make the gain

3050
01:48:35,580 --> 01:48:39,119
be say 0.2 here

3051
01:48:38,219 --> 01:48:41,040
um

3052
01:48:39,119 --> 01:48:42,840
which was much much slower than what we

3053
01:48:41,040 --> 01:48:44,699
had with the 10h

3054
01:48:42,840 --> 01:48:47,159
but as we'll see the activations will

3055
01:48:44,699 --> 01:48:48,659
actually be exactly unaffected and

3056
01:48:47,159 --> 01:48:50,760
that's because of again this explicit

3057
01:48:48,659 --> 01:48:52,560
normalization the gradients are going to

3058
01:48:50,760 --> 01:48:55,139
look okay the weight gradients are going

3059
01:48:52,560 --> 01:48:56,820
to look okay but actually the updates

3060
01:48:55,139 --> 01:48:58,440
will change

3061
01:48:56,820 --> 01:48:59,760
and so

3062
01:48:58,440 --> 01:49:01,560
even though the forward and backward

3063
01:48:59,760 --> 01:49:03,119
pass to a very large extent look okay

3064
01:49:01,560 --> 01:49:05,100
because of the backward pass of the

3065
01:49:03,119 --> 01:49:07,560
batch form and how the scale of the

3066
01:49:05,100 --> 01:49:10,679
incoming activations interacts in the

3067
01:49:07,560 --> 01:49:13,440
basharm and its backward pass this is

3068
01:49:10,679 --> 01:49:15,360
actually changing the um

3069
01:49:13,440 --> 01:49:17,699
the scale of the updates on these

3070
01:49:15,360 --> 01:49:19,619
parameters so the grades and ingredients

3071
01:49:17,699 --> 01:49:21,780
of these weights are affected

3072
01:49:19,619 --> 01:49:24,560
so we still don't get a completely free

3073
01:49:21,780 --> 01:49:27,480
pass to pass an arbitrary weights here

3074
01:49:24,560 --> 01:49:30,179
but it everything else is significantly

3075
01:49:27,480 --> 01:49:33,300
more robust in terms of the forward

3076
01:49:30,179 --> 01:49:34,679
backward and the weight gradients it's

3077
01:49:33,300 --> 01:49:36,600
just that you may have to retune your

3078
01:49:34,679 --> 01:49:39,179
learning rate if you are changing

3079
01:49:36,600 --> 01:49:40,440
sufficiently the the scale of the

3080
01:49:39,179 --> 01:49:43,920
activations that are coming into the

3081
01:49:40,440 --> 01:49:46,199
bachelor's so here for example this we

3082
01:49:43,920 --> 01:49:48,000
changed the gains of these linear layers

3083
01:49:46,199 --> 01:49:51,600
to be greater and we're seeing that the

3084
01:49:48,000 --> 01:49:53,699
updates are coming out lower as a result

3085
01:49:51,600 --> 01:49:55,860
and then finally we can also if we are

3086
01:49:53,699 --> 01:49:58,080
using basharms we don't actually need to

3087
01:49:55,860 --> 01:50:00,119
necessarily let me reset this to one so

3088
01:49:58,080 --> 01:50:01,560
there's no gain we don't necessarily

3089
01:50:00,119 --> 01:50:04,080
even have to

3090
01:50:01,560 --> 01:50:05,639
um normalize by fan in sometimes so if I

3091
01:50:04,080 --> 01:50:08,159
take out the fan in so these are just

3092
01:50:05,639 --> 01:50:09,600
now uh random gaussian

3093
01:50:08,159 --> 01:50:10,800
we'll see that because of batch drum

3094
01:50:09,600 --> 01:50:14,360
this will actually be relatively well

3095
01:50:10,800 --> 01:50:14,360
behaved so

3096
01:50:14,820 --> 01:50:18,480
this this is look of course in the

3097
01:50:16,380 --> 01:50:19,739
forward pass look good the gradients

3098
01:50:18,480 --> 01:50:22,800
look good

3099
01:50:19,739 --> 01:50:25,260
the backward the weight updates look

3100
01:50:22,800 --> 01:50:26,460
okay A little bit of fat tails in some

3101
01:50:25,260 --> 01:50:29,880
of the layers

3102
01:50:26,460 --> 01:50:32,880
and this looks okay as well but as you

3103
01:50:29,880 --> 01:50:34,199
as you can see uh we're significantly

3104
01:50:32,880 --> 01:50:35,639
below negative three so we'd have to

3105
01:50:34,199 --> 01:50:38,400
bump up the learning rate of this

3106
01:50:35,639 --> 01:50:40,139
bachelor so that we are training more

3107
01:50:38,400 --> 01:50:42,420
properly and in particular looking at

3108
01:50:40,139 --> 01:50:44,340
this roughly looks like we have to 10x

3109
01:50:42,420 --> 01:50:46,619
the learning rate to get to about 20

3110
01:50:44,340 --> 01:50:48,540
negative three

3111
01:50:46,619 --> 01:50:51,300
so we come here and we would change this

3112
01:50:48,540 --> 01:50:54,920
to be update of 1.0

3113
01:50:51,300 --> 01:50:54,920
and if when I reinitialize

3114
01:50:59,040 --> 01:51:02,219
then we'll see that everything still of

3115
01:51:00,540 --> 01:51:04,679
course looks good

3116
01:51:02,219 --> 01:51:07,139
and now we are roughly here and we

3117
01:51:04,679 --> 01:51:08,940
expect this to be an okay training run

3118
01:51:07,139 --> 01:51:11,159
so long story short we are significantly

3119
01:51:08,940 --> 01:51:13,020
more robust to the gain of these linear

3120
01:51:11,159 --> 01:51:15,420
layers whether or not we have to apply

3121
01:51:13,020 --> 01:51:18,239
the fan in and then we can change the

3122
01:51:15,420 --> 01:51:19,860
gain but we actually do have to worry a

3123
01:51:18,239 --> 01:51:22,139
little bit about the update

3124
01:51:19,860 --> 01:51:23,460
um scales and making sure that the

3125
01:51:22,139 --> 01:51:26,040
learning rate is properly calibrated

3126
01:51:23,460 --> 01:51:27,420
here but thus the activations of the

3127
01:51:26,040 --> 01:51:29,580
forward backward pass and the updates

3128
01:51:27,420 --> 01:51:31,619
are all are looking significantly more

3129
01:51:29,580 --> 01:51:34,560
well-behaved except for the global scale

3130
01:51:31,619 --> 01:51:37,020
that is potentially being adjusted here

3131
01:51:34,560 --> 01:51:38,520
okay so now let me summarize there are

3132
01:51:37,020 --> 01:51:40,619
three things I was hoping to achieve

3133
01:51:38,520 --> 01:51:42,179
with this section number one I wanted to

3134
01:51:40,619 --> 01:51:43,800
introduce you to batch normalization

3135
01:51:42,179 --> 01:51:45,659
which is one of the first modern

3136
01:51:43,800 --> 01:51:48,000
innovations that we're looking into that

3137
01:51:45,659 --> 01:51:50,520
helped stabilize very deep neural

3138
01:51:48,000 --> 01:51:51,900
networks and their training and I hope

3139
01:51:50,520 --> 01:51:54,900
you understand how the bachelorization

3140
01:51:51,900 --> 01:51:55,980
works and how it would be used in a

3141
01:51:54,900 --> 01:51:58,020
neural network

3142
01:51:55,980 --> 01:52:00,360
number two I was hoping to pie tortify

3143
01:51:58,020 --> 01:52:03,659
some of our code and wrap it up into

3144
01:52:00,360 --> 01:52:07,139
these modules so like linear Bachelor 1D

3145
01:52:03,659 --> 01:52:09,179
10h Etc these are layers or modules and

3146
01:52:07,139 --> 01:52:12,360
they can be stacked up into neural Nets

3147
01:52:09,179 --> 01:52:15,300
like Lego building blocks and these

3148
01:52:12,360 --> 01:52:17,400
layers actually exist in pie torch and

3149
01:52:15,300 --> 01:52:19,679
if you import torch and then you can

3150
01:52:17,400 --> 01:52:21,300
actually the way I've constructed it you

3151
01:52:19,679 --> 01:52:23,639
can simply just use pytorch by

3152
01:52:21,300 --> 01:52:25,080
prepending and then dot to all these

3153
01:52:23,639 --> 01:52:27,420
different layers

3154
01:52:25,080 --> 01:52:29,699
and actually everything will just work

3155
01:52:27,420 --> 01:52:31,860
because the API that I've developed here

3156
01:52:29,699 --> 01:52:34,380
is identical to the API that pytorch

3157
01:52:31,860 --> 01:52:36,719
uses and the implementation also is

3158
01:52:34,380 --> 01:52:38,219
basically as far as I'm aware identical

3159
01:52:36,719 --> 01:52:40,080
to the one in pi torch

3160
01:52:38,219 --> 01:52:41,699
and number three I try to introduce you

3161
01:52:40,080 --> 01:52:43,980
to the diagnostic tools that you would

3162
01:52:41,699 --> 01:52:45,960
use to understand whether your neural

3163
01:52:43,980 --> 01:52:48,420
network is in a good State dynamically

3164
01:52:45,960 --> 01:52:50,280
so we are looking at the statistics and

3165
01:52:48,420 --> 01:52:52,440
histograms and activation of the forward

3166
01:52:50,280 --> 01:52:54,780
pass application activations the

3167
01:52:52,440 --> 01:52:56,159
backward pass gradients and then also

3168
01:52:54,780 --> 01:52:57,540
we're looking at the weights that are

3169
01:52:56,159 --> 01:52:59,340
going to be updated as part of

3170
01:52:57,540 --> 01:53:00,540
stochastic already in ascent and we're

3171
01:52:59,340 --> 01:53:03,060
looking at their means standard

3172
01:53:00,540 --> 01:53:05,940
deviations and also the ratio of

3173
01:53:03,060 --> 01:53:09,060
gradients to data or even better the

3174
01:53:05,940 --> 01:53:10,440
updates to data and we saw that

3175
01:53:09,060 --> 01:53:12,420
typically we don't actually look at it

3176
01:53:10,440 --> 01:53:14,219
as a single snapshot Frozen in time at

3177
01:53:12,420 --> 01:53:16,739
some particular iteration typically

3178
01:53:14,219 --> 01:53:18,420
people look at this as uh over time just

3179
01:53:16,739 --> 01:53:20,100
like I've done here and they look at

3180
01:53:18,420 --> 01:53:21,900
these updated data ratios and they make

3181
01:53:20,100 --> 01:53:23,760
sure everything looks okay and in

3182
01:53:21,900 --> 01:53:26,100
particular I said that

3183
01:53:23,760 --> 01:53:28,320
um running negative 3 or basically

3184
01:53:26,100 --> 01:53:30,780
negative 3 on the log scale is a good

3185
01:53:28,320 --> 01:53:32,880
rough heuristic for what you want this

3186
01:53:30,780 --> 01:53:34,860
ratio to be and if it's way too high

3187
01:53:32,880 --> 01:53:36,840
then probably the learning rate or the

3188
01:53:34,860 --> 01:53:37,980
updates are a little too too big and if

3189
01:53:36,840 --> 01:53:39,659
it's way too small that the learning

3190
01:53:37,980 --> 01:53:41,219
rate is probably too small

3191
01:53:39,659 --> 01:53:43,020
so that's just some of the things that

3192
01:53:41,219 --> 01:53:45,119
you may want to play with when you try

3193
01:53:43,020 --> 01:53:46,739
to get your neural network to work with

3194
01:53:45,119 --> 01:53:48,179
very well

3195
01:53:46,739 --> 01:53:50,639
now there's a number of things I did not

3196
01:53:48,179 --> 01:53:52,260
try to achieve I did not try to beat our

3197
01:53:50,639 --> 01:53:54,300
previous performance as an example by

3198
01:53:52,260 --> 01:53:55,560
introducing the bathroom layer actually

3199
01:53:54,300 --> 01:53:57,420
I did try

3200
01:53:55,560 --> 01:53:58,920
um and I found the new I used the

3201
01:53:57,420 --> 01:54:00,600
learning rate finding mechanism that

3202
01:53:58,920 --> 01:54:02,820
I've described before I tried to train

3203
01:54:00,600 --> 01:54:05,340
the bathroom layer a bachelor neural nut

3204
01:54:02,820 --> 01:54:06,780
and I actually ended up with results

3205
01:54:05,340 --> 01:54:08,100
that are very very similar to what we've

3206
01:54:06,780 --> 01:54:10,139
obtained before

3207
01:54:08,100 --> 01:54:12,719
and that's because our performance now

3208
01:54:10,139 --> 01:54:14,699
is not bottlenecked by the optimization

3209
01:54:12,719 --> 01:54:16,619
which is what bass Norm is helping with

3210
01:54:14,699 --> 01:54:18,840
the performance at this stage is

3211
01:54:16,619 --> 01:54:21,900
bottlenecked by what I suspect is the

3212
01:54:18,840 --> 01:54:23,460
context length of our context

3213
01:54:21,900 --> 01:54:25,020
So currently we are taking three

3214
01:54:23,460 --> 01:54:26,580
characters to predict the fourth one and

3215
01:54:25,020 --> 01:54:27,659
I think we need to go beyond that and we

3216
01:54:26,580 --> 01:54:29,580
need to look at more powerful

3217
01:54:27,659 --> 01:54:31,199
architectures that are like recurrent

3218
01:54:29,580 --> 01:54:32,940
neural networks and Transformers in

3219
01:54:31,199 --> 01:54:34,199
order to further push

3220
01:54:32,940 --> 01:54:36,360
um the log probabilities that we're

3221
01:54:34,199 --> 01:54:39,540
achieving on this data set

3222
01:54:36,360 --> 01:54:41,820
and I also did not try to have a full

3223
01:54:39,540 --> 01:54:43,800
explanation of all of these activations

3224
01:54:41,820 --> 01:54:45,239
the gradients and the backward paths and

3225
01:54:43,800 --> 01:54:46,920
the statistics of all these gradients

3226
01:54:45,239 --> 01:54:48,480
and so you may have found some of the

3227
01:54:46,920 --> 01:54:50,520
parts here unintuitive and maybe you're

3228
01:54:48,480 --> 01:54:53,219
slightly confused about okay if I change

3229
01:54:50,520 --> 01:54:54,960
the gain here how come that we need a

3230
01:54:53,219 --> 01:54:56,340
different learning rate and I didn't go

3231
01:54:54,960 --> 01:54:57,719
into the full detail because you'd have

3232
01:54:56,340 --> 01:54:59,280
to actually look at the backward pass of

3233
01:54:57,719 --> 01:55:00,780
all these different layers and get an

3234
01:54:59,280 --> 01:55:03,360
intuitive understanding of how that

3235
01:55:00,780 --> 01:55:05,460
works and I did not go into that in this

3236
01:55:03,360 --> 01:55:07,199
lecture the purpose really was just to

3237
01:55:05,460 --> 01:55:08,820
introduce you to the diagnostic tools

3238
01:55:07,199 --> 01:55:10,320
and what they look like but there's

3239
01:55:08,820 --> 01:55:12,179
still a lot of work remaining on the

3240
01:55:10,320 --> 01:55:13,739
intuitive level to understand the

3241
01:55:12,179 --> 01:55:16,560
initialization the backward pass and how

3242
01:55:13,739 --> 01:55:18,840
all of that interacts but you shouldn't

3243
01:55:16,560 --> 01:55:21,960
feel too bad because honestly we are

3244
01:55:18,840 --> 01:55:24,000
getting to The Cutting Edge of where the

3245
01:55:21,960 --> 01:55:26,100
field is we certainly haven't I would

3246
01:55:24,000 --> 01:55:28,739
say solved initialization and we haven't

3247
01:55:26,100 --> 01:55:30,300
solved back propagation and these are

3248
01:55:28,739 --> 01:55:31,679
still very much an active area of

3249
01:55:30,300 --> 01:55:32,699
research people are still trying to

3250
01:55:31,679 --> 01:55:34,380
figure out what's the best way to

3251
01:55:32,699 --> 01:55:36,360
initialize these networks what is the

3252
01:55:34,380 --> 01:55:38,520
best update rule to use

3253
01:55:36,360 --> 01:55:40,199
um and so on so none of this is really

3254
01:55:38,520 --> 01:55:43,020
solved and we don't really have all the

3255
01:55:40,199 --> 01:55:45,659
answers to all the uh to you know all

3256
01:55:43,020 --> 01:55:47,100
these cases but at least you know we're

3257
01:55:45,659 --> 01:55:48,960
making progress at least we have some

3258
01:55:47,100 --> 01:55:51,600
tools to tell us whether or not things

3259
01:55:48,960 --> 01:55:52,920
are on the right track for now

3260
01:55:51,600 --> 01:55:54,659
so

3261
01:55:52,920 --> 01:55:56,040
I think we've made positive progress in

3262
01:55:54,659 --> 01:55:59,000
this lecture and I hope you enjoyed that

3263
01:55:56,040 --> 01:55:59,000
and I will see you next time

