1
00:00:00,640 --> 00:00:03,280
hi everyone

2
00:00:01,920 --> 00:00:05,200
today we are continuing our

3
00:00:03,280 --> 00:00:06,879
implementation of makemore

4
00:00:05,200 --> 00:00:08,559
now in the last lecture we implemented

5
00:00:06,879 --> 00:00:10,880
the bigram language model and we

6
00:00:08,559 --> 00:00:12,880
implemented it both using counts and

7
00:00:10,880 --> 00:00:15,599
also using a super simple neural network

8
00:00:12,880 --> 00:00:17,520
that had a single linear layer

9
00:00:15,599 --> 00:00:19,279
now this is the

10
00:00:17,520 --> 00:00:20,400
jupyter notebook that we built out last

11
00:00:19,279 --> 00:00:21,680
lecture

12
00:00:20,400 --> 00:00:23,279
and we saw that the way we approached

13
00:00:21,680 --> 00:00:24,880
this is that we looked at only the

14
00:00:23,279 --> 00:00:26,480
single previous character and we

15
00:00:24,880 --> 00:00:28,240
predicted the distribution for the

16
00:00:26,480 --> 00:00:30,320
character that would go next in the

17
00:00:28,240 --> 00:00:32,239
sequence and we did that by taking

18
00:00:30,320 --> 00:00:33,680
counts and normalizing them into

19
00:00:32,239 --> 00:00:36,719
probabilities

20
00:00:33,680 --> 00:00:38,559
so that each row here sums to one

21
00:00:36,719 --> 00:00:40,399
now this is all well and good if you

22
00:00:38,559 --> 00:00:41,520
only have one character of previous

23
00:00:40,399 --> 00:00:43,840
context

24
00:00:41,520 --> 00:00:45,280
and this works and it's approachable the

25
00:00:43,840 --> 00:00:46,800
problem with this model of course is

26
00:00:45,280 --> 00:00:48,320
that

27
00:00:46,800 --> 00:00:50,079
the predictions from this model are not

28
00:00:48,320 --> 00:00:52,800
very good because you only take one

29
00:00:50,079 --> 00:00:56,079
character of context so the model didn't

30
00:00:52,800 --> 00:00:57,920
produce very name like sounding things

31
00:00:56,079 --> 00:00:59,920
now the problem with this approach

32
00:00:57,920 --> 00:01:01,840
though is that if we are to take more

33
00:00:59,920 --> 00:01:03,680
context into account when predicting the

34
00:01:01,840 --> 00:01:06,080
next character in a sequence things

35
00:01:03,680 --> 00:01:08,320
quickly blow up and this table the size

36
00:01:06,080 --> 00:01:10,000
of this table grows and in fact it grows

37
00:01:08,320 --> 00:01:11,119
exponentially with the length of the

38
00:01:10,000 --> 00:01:12,240
context

39
00:01:11,119 --> 00:01:13,920
because if we only take a single

40
00:01:12,240 --> 00:01:15,840
character at a time that's 27

41
00:01:13,920 --> 00:01:17,759
possibilities of context

42
00:01:15,840 --> 00:01:19,759
but if we take two characters in the

43
00:01:17,759 --> 00:01:21,600
past and try to predict the third one

44
00:01:19,759 --> 00:01:23,759
suddenly the number of rows in this

45
00:01:21,600 --> 00:01:26,640
matrix you can look at it that way

46
00:01:23,759 --> 00:01:28,720
is 27 times 27 so there's 729

47
00:01:26,640 --> 00:01:30,159
possibilities for what could have come

48
00:01:28,720 --> 00:01:31,759
in the context

49
00:01:30,159 --> 00:01:34,400
if we take three characters as the

50
00:01:31,759 --> 00:01:37,520
context suddenly we have

51
00:01:34,400 --> 00:01:40,079
20 000 possibilities of context

52
00:01:37,520 --> 00:01:43,759
and so there's just way too many rows of

53
00:01:40,079 --> 00:01:45,520
this matrix it's way too few counts

54
00:01:43,759 --> 00:01:47,360
for each possibility and the whole thing

55
00:01:45,520 --> 00:01:48,960
just kind of explodes and doesn't work

56
00:01:47,360 --> 00:01:50,479
very well

57
00:01:48,960 --> 00:01:52,399
so that's why today we're going to move

58
00:01:50,479 --> 00:01:53,759
on to this bullet point here and we're

59
00:01:52,399 --> 00:01:56,720
going to implement a multi-layer

60
00:01:53,759 --> 00:01:58,240
perceptron model to predict the next uh

61
00:01:56,720 --> 00:02:00,079
character in a sequence

62
00:01:58,240 --> 00:02:01,840
and this modeling approach that we're

63
00:02:00,079 --> 00:02:04,320
going to adopt follows this paper

64
00:02:01,840 --> 00:02:06,399
benguetal 2003

65
00:02:04,320 --> 00:02:08,080
so i have the paper pulled up here

66
00:02:06,399 --> 00:02:10,000
now this isn't the very first paper that

67
00:02:08,080 --> 00:02:11,599
proposed the use of multiglio

68
00:02:10,000 --> 00:02:13,680
perceptrons or neural networks to

69
00:02:11,599 --> 00:02:15,840
predict the next character or token in a

70
00:02:13,680 --> 00:02:18,000
sequence but it's definitely one that is

71
00:02:15,840 --> 00:02:20,000
uh was very influential around that time

72
00:02:18,000 --> 00:02:22,000
it is very often cited to stand in for

73
00:02:20,000 --> 00:02:23,760
this idea and i think it's a very nice

74
00:02:22,000 --> 00:02:25,840
write-up and so this is the paper that

75
00:02:23,760 --> 00:02:28,400
we're going to first look at and then

76
00:02:25,840 --> 00:02:30,160
implement now this paper has 19 pages so

77
00:02:28,400 --> 00:02:31,680
we don't have time to go into

78
00:02:30,160 --> 00:02:33,200
the full detail of this paper but i

79
00:02:31,680 --> 00:02:34,640
invite you to read it

80
00:02:33,200 --> 00:02:37,040
it's very readable interesting and has a

81
00:02:34,640 --> 00:02:38,400
lot of interesting ideas in it as well

82
00:02:37,040 --> 00:02:40,560
in the introduction they describe the

83
00:02:38,400 --> 00:02:42,319
exact same problem i just described and

84
00:02:40,560 --> 00:02:43,920
then to address it they propose the

85
00:02:42,319 --> 00:02:46,560
following model

86
00:02:43,920 --> 00:02:48,160
now keep in mind that we are building a

87
00:02:46,560 --> 00:02:50,080
character level language model so we're

88
00:02:48,160 --> 00:02:52,640
working on the level of characters in

89
00:02:50,080 --> 00:02:54,800
this paper they have a vocabulary of 17

90
00:02:52,640 --> 00:02:56,959
000 possible words and they instead

91
00:02:54,800 --> 00:02:58,080
build a word level language model but

92
00:02:56,959 --> 00:02:59,440
we're going to still stick with the

93
00:02:58,080 --> 00:03:01,280
characters but we'll take the same

94
00:02:59,440 --> 00:03:03,519
modeling approach

95
00:03:01,280 --> 00:03:05,280
now what they do is basically they

96
00:03:03,519 --> 00:03:07,040
propose to take every one of these words

97
00:03:05,280 --> 00:03:10,080
seventeen thousand words and they're

98
00:03:07,040 --> 00:03:12,720
going to associate to each word a say

99
00:03:10,080 --> 00:03:15,120
thirty dimensional feature vector

100
00:03:12,720 --> 00:03:17,680
so every word is now

101
00:03:15,120 --> 00:03:19,840
embedded into a thirty dimensional space

102
00:03:17,680 --> 00:03:22,480
you can think of it that way so we have

103
00:03:19,840 --> 00:03:23,840
17 000 points or vectors in a 30

104
00:03:22,480 --> 00:03:25,760
dimensional space

105
00:03:23,840 --> 00:03:27,200
and that's um you might imagine that's

106
00:03:25,760 --> 00:03:28,879
very crowded that's a lot of points for

107
00:03:27,200 --> 00:03:30,080
a very small space

108
00:03:28,879 --> 00:03:31,200
now

109
00:03:30,080 --> 00:03:32,640
in the beginning these words are

110
00:03:31,200 --> 00:03:34,720
initialized completely randomly so

111
00:03:32,640 --> 00:03:36,560
they're spread out at random

112
00:03:34,720 --> 00:03:38,560
but then we're going to tune these

113
00:03:36,560 --> 00:03:39,680
embeddings of these words using back

114
00:03:38,560 --> 00:03:41,200
propagation

115
00:03:39,680 --> 00:03:43,200
so during the course of training of this

116
00:03:41,200 --> 00:03:44,720
neural network these points or vectors

117
00:03:43,200 --> 00:03:46,640
are going to basically move around in

118
00:03:44,720 --> 00:03:48,879
this space and you might imagine that

119
00:03:46,640 --> 00:03:50,959
for example words that have very similar

120
00:03:48,879 --> 00:03:52,480
meanings or that are indeed synonyms of

121
00:03:50,959 --> 00:03:54,799
each other might end up in a very

122
00:03:52,480 --> 00:03:56,400
similar part of the space and conversely

123
00:03:54,799 --> 00:03:59,280
words that mean very different things

124
00:03:56,400 --> 00:04:01,200
would go somewhere else in a space

125
00:03:59,280 --> 00:04:03,360
now their modeling approach otherwise is

126
00:04:01,200 --> 00:04:04,799
identical to ours they are using a

127
00:04:03,360 --> 00:04:07,280
multi-layer neural network to predict

128
00:04:04,799 --> 00:04:08,959
the next word given the previous words

129
00:04:07,280 --> 00:04:10,640
and to train the neural network they are

130
00:04:08,959 --> 00:04:12,799
maximizing the log likelihood of the

131
00:04:10,640 --> 00:04:14,720
training data just like we did

132
00:04:12,799 --> 00:04:16,720
so the modeling approach itself is

133
00:04:14,720 --> 00:04:18,799
identical now here they have a concrete

134
00:04:16,720 --> 00:04:20,239
example of this intuition

135
00:04:18,799 --> 00:04:21,840
why does it work

136
00:04:20,239 --> 00:04:23,759
basically suppose that for example you

137
00:04:21,840 --> 00:04:25,600
are trying to predict a dog was running

138
00:04:23,759 --> 00:04:28,080
in a blank

139
00:04:25,600 --> 00:04:29,600
now suppose that the exact phrase a dog

140
00:04:28,080 --> 00:04:31,759
was running in a

141
00:04:29,600 --> 00:04:33,919
has never occurred in a training data

142
00:04:31,759 --> 00:04:35,360
and here you are at sort of test time

143
00:04:33,919 --> 00:04:36,320
later when the model is deployed

144
00:04:35,360 --> 00:04:38,479
somewhere

145
00:04:36,320 --> 00:04:41,360
and it's trying to make a sentence and

146
00:04:38,479 --> 00:04:43,040
it's saying a dog was running in a blank

147
00:04:41,360 --> 00:04:45,360
and because it's never encountered this

148
00:04:43,040 --> 00:04:47,680
exact phrase in the training set you're

149
00:04:45,360 --> 00:04:49,600
out of distribution as we say like you

150
00:04:47,680 --> 00:04:52,240
don't have fundamentally any

151
00:04:49,600 --> 00:04:54,000
reason to suspect

152
00:04:52,240 --> 00:04:55,759
what might come next

153
00:04:54,000 --> 00:04:57,520
but this approach actually allows you to

154
00:04:55,759 --> 00:04:59,280
get around that because maybe you didn't

155
00:04:57,520 --> 00:05:01,360
see the exact phrase a dog was running

156
00:04:59,280 --> 00:05:03,040
in a something but maybe you've seen

157
00:05:01,360 --> 00:05:06,160
similar phrases maybe you've seen the

158
00:05:03,040 --> 00:05:07,680
phrase the dog was running in a blank

159
00:05:06,160 --> 00:05:08,960
and maybe your network has learned that

160
00:05:07,680 --> 00:05:10,960
a and the

161
00:05:08,960 --> 00:05:13,199
are like frequently are interchangeable

162
00:05:10,960 --> 00:05:15,039
with each other and so maybe it took the

163
00:05:13,199 --> 00:05:17,039
embedding for a and the embedding for

164
00:05:15,039 --> 00:05:18,880
the and it actually put them like nearby

165
00:05:17,039 --> 00:05:20,639
each other in the space and so you can

166
00:05:18,880 --> 00:05:22,479
transfer knowledge through that

167
00:05:20,639 --> 00:05:23,280
embedding and you can generalize in that

168
00:05:22,479 --> 00:05:25,039
way

169
00:05:23,280 --> 00:05:26,960
similarly the network could know that

170
00:05:25,039 --> 00:05:28,639
cats and dogs are animals and they

171
00:05:26,960 --> 00:05:30,720
co-occur in lots of very similar

172
00:05:28,639 --> 00:05:32,479
contexts and so even though you haven't

173
00:05:30,720 --> 00:05:34,160
seen this exact phrase

174
00:05:32,479 --> 00:05:35,199
or if you haven't seen exactly walking

175
00:05:34,160 --> 00:05:37,520
or running

176
00:05:35,199 --> 00:05:39,120
you can through the embedding space

177
00:05:37,520 --> 00:05:42,080
transfer knowledge and you can

178
00:05:39,120 --> 00:05:43,600
generalize to novel scenarios

179
00:05:42,080 --> 00:05:44,960
so let's now scroll down to the diagram

180
00:05:43,600 --> 00:05:47,360
of the neural network

181
00:05:44,960 --> 00:05:49,919
they have a nice diagram here

182
00:05:47,360 --> 00:05:51,759
and in this example we are taking three

183
00:05:49,919 --> 00:05:53,199
previous words

184
00:05:51,759 --> 00:05:54,000
and we are trying to predict the fourth

185
00:05:53,199 --> 00:05:55,919
word

186
00:05:54,000 --> 00:05:57,680
in a sequence

187
00:05:55,919 --> 00:06:00,400
now these three previous words as i

188
00:05:57,680 --> 00:06:02,880
mentioned uh we have a vocabulary of 17

189
00:06:00,400 --> 00:06:04,800
000 um possible words

190
00:06:02,880 --> 00:06:05,919
so every one of these

191
00:06:04,800 --> 00:06:09,280
basically basically

192
00:06:05,919 --> 00:06:11,759
are the index of the incoming word

193
00:06:09,280 --> 00:06:16,840
and because there are 17 000 words this

194
00:06:11,759 --> 00:06:16,840
is an integer between 0 and 16999

195
00:06:17,199 --> 00:06:20,880
now there's also a lookup table that

196
00:06:19,360 --> 00:06:23,360
they call c

197
00:06:20,880 --> 00:06:25,919
this lookup table is a matrix that is 17

198
00:06:23,360 --> 00:06:27,280
000 by say 30.

199
00:06:25,919 --> 00:06:29,600
and basically what we're doing here is

200
00:06:27,280 --> 00:06:31,680
we're treating this as a lookup table

201
00:06:29,600 --> 00:06:34,240
and so every index is

202
00:06:31,680 --> 00:06:35,280
plucking out a row of this embedding

203
00:06:34,240 --> 00:06:37,440
matrix

204
00:06:35,280 --> 00:06:39,759
so that each index is converted to the

205
00:06:37,440 --> 00:06:42,880
30 dimensional vector that corresponds

206
00:06:39,759 --> 00:06:45,600
to the embedding vector for that word

207
00:06:42,880 --> 00:06:48,800
so here we have the input layer of 30

208
00:06:45,600 --> 00:06:50,639
neurons for three words making up 90

209
00:06:48,800 --> 00:06:52,160
neurons in total

210
00:06:50,639 --> 00:06:54,880
and here they're saying that this matrix

211
00:06:52,160 --> 00:06:56,240
c is shared across all the words so

212
00:06:54,880 --> 00:06:59,360
we're always indexing into the same

213
00:06:56,240 --> 00:07:00,840
matrix c over and over um

214
00:06:59,360 --> 00:07:03,280
for each one of these

215
00:07:00,840 --> 00:07:05,199
words next up is the hidden layer of

216
00:07:03,280 --> 00:07:07,680
this neural network the size of this

217
00:07:05,199 --> 00:07:09,840
hidden neural layer of this neural net

218
00:07:07,680 --> 00:07:11,199
is a hoppy parameter so we use the word

219
00:07:09,840 --> 00:07:12,800
hyperparameter when it's kind of like a

220
00:07:11,199 --> 00:07:15,280
design choice up to the designer of the

221
00:07:12,800 --> 00:07:16,800
neural net and this can be as large as

222
00:07:15,280 --> 00:07:19,199
you'd like or as small as you'd like so

223
00:07:16,800 --> 00:07:20,639
for example the size could be a hundred

224
00:07:19,199 --> 00:07:23,199
and we are going to go over multiple

225
00:07:20,639 --> 00:07:24,560
choices of the size of this hidden layer

226
00:07:23,199 --> 00:07:26,000
and we're going to evaluate how well

227
00:07:24,560 --> 00:07:28,319
they work

228
00:07:26,000 --> 00:07:30,000
so say there were 100 neurons here all

229
00:07:28,319 --> 00:07:32,960
of them would be fully connected to the

230
00:07:30,000 --> 00:07:35,759
90 words or 90 um

231
00:07:32,960 --> 00:07:37,840
numbers that make up these three words

232
00:07:35,759 --> 00:07:40,080
so this is a fully connected layer

233
00:07:37,840 --> 00:07:42,080
then there's a 10 inch long linearity

234
00:07:40,080 --> 00:07:44,400
and then there's this output layer and

235
00:07:42,080 --> 00:07:45,840
because there are 17 000 possible words

236
00:07:44,400 --> 00:07:48,639
that could come next

237
00:07:45,840 --> 00:07:51,280
this layer has 17 000 neurons

238
00:07:48,639 --> 00:07:54,720
and all of them are fully connected to

239
00:07:51,280 --> 00:07:56,080
all of these neurons in the hidden layer

240
00:07:54,720 --> 00:07:58,560
so there's a lot of parameters here

241
00:07:56,080 --> 00:07:59,840
because there's a lot of words so most

242
00:07:58,560 --> 00:08:01,680
computation is here this is the

243
00:07:59,840 --> 00:08:04,720
expensive layer

244
00:08:01,680 --> 00:08:06,879
now there are 17 000 logits here so on

245
00:08:04,720 --> 00:08:08,479
top of there we have the softmax layer

246
00:08:06,879 --> 00:08:10,840
which we've seen in our previous video

247
00:08:08,479 --> 00:08:12,720
as well so every one of these logits is

248
00:08:10,840 --> 00:08:15,120
exponentiated and then everything is

249
00:08:12,720 --> 00:08:17,280
normalized to sum to 1 so that we have a

250
00:08:15,120 --> 00:08:19,840
nice probability distribution for the

251
00:08:17,280 --> 00:08:21,440
next word in the sequence

252
00:08:19,840 --> 00:08:22,960
now of course during training we

253
00:08:21,440 --> 00:08:25,680
actually have the label we have the

254
00:08:22,960 --> 00:08:26,960
identity of the next word in a sequence

255
00:08:25,680 --> 00:08:30,720
that word

256
00:08:26,960 --> 00:08:32,719
or its index is used to pluck out the

257
00:08:30,720 --> 00:08:34,800
probability of that word

258
00:08:32,719 --> 00:08:37,039
and then we are maximizing the

259
00:08:34,800 --> 00:08:38,640
probability of that word

260
00:08:37,039 --> 00:08:39,760
with respect to the parameters of this

261
00:08:38,640 --> 00:08:41,360
neural net

262
00:08:39,760 --> 00:08:44,080
so the parameters are the weights and

263
00:08:41,360 --> 00:08:45,839
biases of this output layer

264
00:08:44,080 --> 00:08:49,120
the weights and biases of the hidden

265
00:08:45,839 --> 00:08:50,800
layer and the embedding lookup table c

266
00:08:49,120 --> 00:08:52,320
and all of that is optimized using back

267
00:08:50,800 --> 00:08:55,279
propagation

268
00:08:52,320 --> 00:08:57,040
and these uh dashed arrows ignore those

269
00:08:55,279 --> 00:08:58,320
uh that represents a variation of a

270
00:08:57,040 --> 00:08:59,839
neural nut that we are not going to

271
00:08:58,320 --> 00:09:01,200
explore in this video

272
00:08:59,839 --> 00:09:02,480
so that's the setup and now let's

273
00:09:01,200 --> 00:09:04,160
implement it

274
00:09:02,480 --> 00:09:05,519
okay so i started a brand new notebook

275
00:09:04,160 --> 00:09:07,760
for this lecture

276
00:09:05,519 --> 00:09:09,440
we are importing pytorch and we are

277
00:09:07,760 --> 00:09:10,720
importing matplotlib so we can create

278
00:09:09,440 --> 00:09:13,120
figures

279
00:09:10,720 --> 00:09:15,200
then i am reading all the names into a

280
00:09:13,120 --> 00:09:18,080
list of words like i did before and i'm

281
00:09:15,200 --> 00:09:20,320
showing the first eight right here

282
00:09:18,080 --> 00:09:22,800
keep in mind that we have a 32 000 in

283
00:09:20,320 --> 00:09:24,000
total these are just the first eight

284
00:09:22,800 --> 00:09:25,680
and then here i'm building out the

285
00:09:24,000 --> 00:09:28,240
vocabulary of characters and all the

286
00:09:25,680 --> 00:09:31,279
mappings from the characters as strings

287
00:09:28,240 --> 00:09:32,720
to integers and vice versa

288
00:09:31,279 --> 00:09:34,320
now the first thing we want to do is we

289
00:09:32,720 --> 00:09:35,519
want to compile the data set for the

290
00:09:34,320 --> 00:09:37,839
neural network

291
00:09:35,519 --> 00:09:41,120
and i had to rewrite this code um i'll

292
00:09:37,839 --> 00:09:43,040
show you in a second what it looks like

293
00:09:41,120 --> 00:09:44,959
so this is the code that i created for

294
00:09:43,040 --> 00:09:46,640
the dataset creation so let me first run

295
00:09:44,959 --> 00:09:48,480
it and then i'll briefly explain how

296
00:09:46,640 --> 00:09:50,080
this works

297
00:09:48,480 --> 00:09:52,399
so first we're going to define something

298
00:09:50,080 --> 00:09:54,160
called block size and this is basically

299
00:09:52,399 --> 00:09:56,000
the context length of how many

300
00:09:54,160 --> 00:09:58,000
characters do we take to predict the

301
00:09:56,000 --> 00:09:59,920
next one so here in this example we're

302
00:09:58,000 --> 00:10:02,000
taking three characters to predict the

303
00:09:59,920 --> 00:10:04,320
fourth one so we have a block size of

304
00:10:02,000 --> 00:10:06,560
three that's the size of the block that

305
00:10:04,320 --> 00:10:09,920
supports the prediction

306
00:10:06,560 --> 00:10:12,399
then here i'm building out the x and y

307
00:10:09,920 --> 00:10:15,120
the x are the input to the neural net

308
00:10:12,399 --> 00:10:17,519
and the y are the labels for each

309
00:10:15,120 --> 00:10:19,360
example inside x

310
00:10:17,519 --> 00:10:21,279
then i'm airing over the first five

311
00:10:19,360 --> 00:10:23,120
words i'm doing first five just for

312
00:10:21,279 --> 00:10:24,880
efficiency while we are developing all

313
00:10:23,120 --> 00:10:26,800
the code but then later we're going to

314
00:10:24,880 --> 00:10:29,040
come here and erase this so that we use

315
00:10:26,800 --> 00:10:30,839
the entire training set

316
00:10:29,040 --> 00:10:33,920
so here i'm printing the word

317
00:10:30,839 --> 00:10:36,000
emma and here i'm basically showing the

318
00:10:33,920 --> 00:10:37,519
examples that we can generate the five

319
00:10:36,000 --> 00:10:38,399
examples that we can generate out of the

320
00:10:37,519 --> 00:10:41,120
single

321
00:10:38,399 --> 00:10:42,079
um sort of word emma

322
00:10:41,120 --> 00:10:44,079
so

323
00:10:42,079 --> 00:10:45,920
when we are given the context of just uh

324
00:10:44,079 --> 00:10:47,600
dot dot the first character in a

325
00:10:45,920 --> 00:10:50,320
sequence is e

326
00:10:47,600 --> 00:10:53,279
in this context the label is m

327
00:10:50,320 --> 00:10:55,360
when the context is this the label is m

328
00:10:53,279 --> 00:10:56,959
and so forth and so the way i build this

329
00:10:55,360 --> 00:10:59,760
out is first i start with a padded

330
00:10:56,959 --> 00:11:01,920
context of just zero tokens

331
00:10:59,760 --> 00:11:04,800
then i iterate over all the characters i

332
00:11:01,920 --> 00:11:07,200
get the character in the sequence and i

333
00:11:04,800 --> 00:11:09,519
basically build out the array y of this

334
00:11:07,200 --> 00:11:11,839
current character and the array x which

335
00:11:09,519 --> 00:11:14,079
stores the current running context

336
00:11:11,839 --> 00:11:17,279
and then here see i print everything and

337
00:11:14,079 --> 00:11:18,959
here i um crop the context and enter the

338
00:11:17,279 --> 00:11:22,640
new character in a sequence so this is

339
00:11:18,959 --> 00:11:24,160
kind of like a rolling window of context

340
00:11:22,640 --> 00:11:25,760
now we can change the block size here to

341
00:11:24,160 --> 00:11:27,839
for example four

342
00:11:25,760 --> 00:11:30,240
and in that case we'll be predicting the

343
00:11:27,839 --> 00:11:31,760
fifth character given the previous four

344
00:11:30,240 --> 00:11:33,920
or it can be five

345
00:11:31,760 --> 00:11:36,560
and then it would look like this

346
00:11:33,920 --> 00:11:37,600
or it can be say ten

347
00:11:36,560 --> 00:11:39,040
and then it would look something like

348
00:11:37,600 --> 00:11:40,959
this we're taking ten characters to

349
00:11:39,040 --> 00:11:43,200
predict the eleventh one

350
00:11:40,959 --> 00:11:45,519
and we're always padding with dots

351
00:11:43,200 --> 00:11:47,920
so let me bring this back to three

352
00:11:45,519 --> 00:11:50,000
just so that we have what we have here

353
00:11:47,920 --> 00:11:51,600
in the paper

354
00:11:50,000 --> 00:11:53,440
and finally the data set right now looks

355
00:11:51,600 --> 00:11:55,440
as follows

356
00:11:53,440 --> 00:11:57,920
from these five words we have created a

357
00:11:55,440 --> 00:11:59,760
data set of 32 examples

358
00:11:57,920 --> 00:12:02,000
and each input of the neural net is

359
00:11:59,760 --> 00:12:03,680
three integers and we have a label that

360
00:12:02,000 --> 00:12:04,560
is also an integer

361
00:12:03,680 --> 00:12:06,639
y

362
00:12:04,560 --> 00:12:08,800
so x looks like this

363
00:12:06,639 --> 00:12:12,399
these are the individual examples

364
00:12:08,800 --> 00:12:13,680
and then y are the labels

365
00:12:12,399 --> 00:12:15,360
so

366
00:12:13,680 --> 00:12:17,040
given this

367
00:12:15,360 --> 00:12:19,200
let's now write a neural network that

368
00:12:17,040 --> 00:12:21,200
takes these axes and predicts the y's

369
00:12:19,200 --> 00:12:23,120
first let's build the embedding lookup

370
00:12:21,200 --> 00:12:25,279
table c

371
00:12:23,120 --> 00:12:26,880
so we have 27 possible characters and

372
00:12:25,279 --> 00:12:28,320
we're going to embed them in a lower

373
00:12:26,880 --> 00:12:31,360
dimensional space

374
00:12:28,320 --> 00:12:33,920
in the paper they have 17 000 words and

375
00:12:31,360 --> 00:12:38,480
they bet them in uh spaces as small

376
00:12:33,920 --> 00:12:40,720
dimensional as 30. so they cram 17 000

377
00:12:38,480 --> 00:12:43,360
words into 30 dimensional space in our

378
00:12:40,720 --> 00:12:45,200
case we have only 27 possible characters

379
00:12:43,360 --> 00:12:46,800
so let's grab them in something as small

380
00:12:45,200 --> 00:12:48,480
as to start with for example a

381
00:12:46,800 --> 00:12:50,079
two-dimensional space

382
00:12:48,480 --> 00:12:51,200
so this lookup table will be random

383
00:12:50,079 --> 00:12:54,160
numbers

384
00:12:51,200 --> 00:12:56,079
and we'll have 27 rows and we'll have

385
00:12:54,160 --> 00:12:58,480
two columns

386
00:12:56,079 --> 00:13:00,240
right so each 20 each one of 27

387
00:12:58,480 --> 00:13:02,000
characters will have a two-dimensional

388
00:13:00,240 --> 00:13:05,519
embedding

389
00:13:02,000 --> 00:13:07,839
so that's our matrix c of embeddings in

390
00:13:05,519 --> 00:13:10,160
the beginning initialized randomly

391
00:13:07,839 --> 00:13:12,480
now before we embed all of the integers

392
00:13:10,160 --> 00:13:14,160
inside the input x using this lookup

393
00:13:12,480 --> 00:13:15,920
table c

394
00:13:14,160 --> 00:13:19,600
let me actually just try to embed a

395
00:13:15,920 --> 00:13:21,839
single individual integer like say five

396
00:13:19,600 --> 00:13:22,720
so we get a sense of how this works

397
00:13:21,839 --> 00:13:24,240
now

398
00:13:22,720 --> 00:13:26,399
one way this works of course is we can

399
00:13:24,240 --> 00:13:27,839
just take the c and we can index into

400
00:13:26,399 --> 00:13:30,399
row five

401
00:13:27,839 --> 00:13:31,600
and that gives us a vector the fifth row

402
00:13:30,399 --> 00:13:33,040
of c

403
00:13:31,600 --> 00:13:34,720
and um

404
00:13:33,040 --> 00:13:36,079
this is one way to do it

405
00:13:34,720 --> 00:13:38,639
the other way that i presented in the

406
00:13:36,079 --> 00:13:40,959
previous lecture is actually seemingly

407
00:13:38,639 --> 00:13:42,320
different but actually identical so in

408
00:13:40,959 --> 00:13:44,240
the previous lecture what we did is we

409
00:13:42,320 --> 00:13:47,600
took these integers and we used the one

410
00:13:44,240 --> 00:13:48,720
hot encoding to first encode them so f.1

411
00:13:47,600 --> 00:13:51,200
hot

412
00:13:48,720 --> 00:13:52,880
we want to encode integer 5 and we want

413
00:13:51,200 --> 00:13:55,839
to tell it that the number of classes is

414
00:13:52,880 --> 00:13:57,920
27 so that's the 26 dimensional vector

415
00:13:55,839 --> 00:13:59,920
of all zeros except the fifth bit is

416
00:13:57,920 --> 00:14:03,120
turned on

417
00:13:59,920 --> 00:14:04,639
now this actually doesn't work

418
00:14:03,120 --> 00:14:06,000
the reason is that

419
00:14:04,639 --> 00:14:07,920
this input actually must be a doorstop

420
00:14:06,000 --> 00:14:09,199
tensor

421
00:14:07,920 --> 00:14:10,560
and i'm making some of these errors

422
00:14:09,199 --> 00:14:12,720
intentionally just so you get to see

423
00:14:10,560 --> 00:14:14,320
some errors and how to fix them

424
00:14:12,720 --> 00:14:16,560
so this must be a tester not an int

425
00:14:14,320 --> 00:14:18,959
fairly straightforward to fix we get a

426
00:14:16,560 --> 00:14:22,240
one hot vector the fifth dimension is

427
00:14:18,959 --> 00:14:24,639
one and the shape of this is 27.

428
00:14:22,240 --> 00:14:26,480
and now notice that just as i briefly

429
00:14:24,639 --> 00:14:29,120
alluded to in the previous video if we

430
00:14:26,480 --> 00:14:31,839
take this one hot vector and we multiply

431
00:14:29,120 --> 00:14:31,839
it by c

432
00:14:33,760 --> 00:14:37,360
then

433
00:14:35,680 --> 00:14:39,199
what would you expect

434
00:14:37,360 --> 00:14:41,760
well number one

435
00:14:39,199 --> 00:14:43,600
first you'd expect an error

436
00:14:41,760 --> 00:14:45,360
because

437
00:14:43,600 --> 00:14:46,320
expected scalar type long but found

438
00:14:45,360 --> 00:14:48,560
float

439
00:14:46,320 --> 00:14:50,160
so a little bit confusing but

440
00:14:48,560 --> 00:14:52,160
the problem here is that one hot the

441
00:14:50,160 --> 00:14:53,120
data type of it

442
00:14:52,160 --> 00:14:56,560
is

443
00:14:53,120 --> 00:14:59,040
long it's a 64-bit integer but this is a

444
00:14:56,560 --> 00:15:01,600
float tensor and so pytorch doesn't know

445
00:14:59,040 --> 00:15:03,519
how to multiply an int with a float and

446
00:15:01,600 --> 00:15:06,320
that's why we had to explicitly cast

447
00:15:03,519 --> 00:15:09,360
this to a float so that we can multiply

448
00:15:06,320 --> 00:15:11,279
now the output actually here

449
00:15:09,360 --> 00:15:12,720
is identical

450
00:15:11,279 --> 00:15:15,040
and that it's identical because of the

451
00:15:12,720 --> 00:15:17,279
way the matrix multiplication here works

452
00:15:15,040 --> 00:15:19,839
we have the one hot um vector

453
00:15:17,279 --> 00:15:21,680
multiplying columns of c

454
00:15:19,839 --> 00:15:23,519
and because of all the zeros they

455
00:15:21,680 --> 00:15:25,600
actually end up masking out everything

456
00:15:23,519 --> 00:15:27,199
in c except for the fifth row which is

457
00:15:25,600 --> 00:15:28,800
plucked out

458
00:15:27,199 --> 00:15:29,839
and so we actually arrive at the same

459
00:15:28,800 --> 00:15:31,360
result

460
00:15:29,839 --> 00:15:33,519
and that tells you that

461
00:15:31,360 --> 00:15:35,600
here we can interpret this first

462
00:15:33,519 --> 00:15:37,600
piece here this embedding of the integer

463
00:15:35,600 --> 00:15:40,079
we can either think of it as the integer

464
00:15:37,600 --> 00:15:42,079
indexing into a lookup table c but

465
00:15:40,079 --> 00:15:44,399
equivalently we can also think of this

466
00:15:42,079 --> 00:15:46,399
little piece here as a first layer of

467
00:15:44,399 --> 00:15:48,959
this bigger neural net

468
00:15:46,399 --> 00:15:50,720
this layer here has neurons that have no

469
00:15:48,959 --> 00:15:52,800
non-linearity there's no 10h they're

470
00:15:50,720 --> 00:15:55,440
just linear neurons and their weight

471
00:15:52,800 --> 00:15:57,759
matrix is c

472
00:15:55,440 --> 00:15:59,519
and then we are encoding integers into

473
00:15:57,759 --> 00:16:01,440
one hot and feeding those into a neural

474
00:15:59,519 --> 00:16:02,639
net and this first layer basically

475
00:16:01,440 --> 00:16:04,399
embeds them

476
00:16:02,639 --> 00:16:06,160
so those are two equivalent ways of

477
00:16:04,399 --> 00:16:08,240
doing the same thing we're just going to

478
00:16:06,160 --> 00:16:09,279
index because it's much much faster and

479
00:16:08,240 --> 00:16:12,399
we're going to discard this

480
00:16:09,279 --> 00:16:14,160
interpretation of one hot inputs into

481
00:16:12,399 --> 00:16:16,240
neural nets and we're just going to

482
00:16:14,160 --> 00:16:18,320
index integers and create and use

483
00:16:16,240 --> 00:16:20,880
embedding tables now embedding a single

484
00:16:18,320 --> 00:16:23,279
integer like 5 is easy enough we can

485
00:16:20,880 --> 00:16:24,480
simply ask pytorch to retrieve the fifth

486
00:16:23,279 --> 00:16:27,680
row of c

487
00:16:24,480 --> 00:16:30,079
or the row index five of c

488
00:16:27,680 --> 00:16:32,880
but how do we simultaneously embed all

489
00:16:30,079 --> 00:16:34,639
of these 32 by three integers stored in

490
00:16:32,880 --> 00:16:36,320
array x

491
00:16:34,639 --> 00:16:38,160
luckily pytorch indexing is fairly

492
00:16:36,320 --> 00:16:41,120
flexible and quite powerful

493
00:16:38,160 --> 00:16:44,399
so it doesn't just work to

494
00:16:41,120 --> 00:16:46,399
ask for a single element five like this

495
00:16:44,399 --> 00:16:48,480
you can actually index using lists so

496
00:16:46,399 --> 00:16:49,440
for example we can get the rows five six

497
00:16:48,480 --> 00:16:51,839
and seven

498
00:16:49,440 --> 00:16:53,759
and this will just work like this we can

499
00:16:51,839 --> 00:16:55,199
index with a list

500
00:16:53,759 --> 00:16:58,480
it doesn't just have to be a list it can

501
00:16:55,199 --> 00:17:00,560
also be a actually a tensor of integers

502
00:16:58,480 --> 00:17:03,600
and we can index with that

503
00:17:00,560 --> 00:17:05,919
so this is a integer tensor 567 and this

504
00:17:03,600 --> 00:17:08,400
will just work as well

505
00:17:05,919 --> 00:17:10,959
in fact we can also for example repeat

506
00:17:08,400 --> 00:17:12,160
row 7 and retrieve it multiple times

507
00:17:10,959 --> 00:17:14,160
and

508
00:17:12,160 --> 00:17:16,079
that same index will just get embedded

509
00:17:14,160 --> 00:17:17,439
multiple times here

510
00:17:16,079 --> 00:17:18,880
so here we are indexing with a

511
00:17:17,439 --> 00:17:21,439
one-dimensional

512
00:17:18,880 --> 00:17:22,839
tensor of integers but it turns out that

513
00:17:21,439 --> 00:17:25,120
you can also index with

514
00:17:22,839 --> 00:17:27,520
multi-dimensional tensors of integers

515
00:17:25,120 --> 00:17:30,480
here we have a two-dimensional in tensor

516
00:17:27,520 --> 00:17:34,640
of integers so we can simply just do c

517
00:17:30,480 --> 00:17:36,160
at x and this just works

518
00:17:34,640 --> 00:17:37,200
and the shape of this

519
00:17:36,160 --> 00:17:40,160
is

520
00:17:37,200 --> 00:17:41,679
32 by 3 which is the original shape and

521
00:17:40,160 --> 00:17:43,760
now for every one of those 32 by 3

522
00:17:41,679 --> 00:17:44,840
integers we've retrieved the embedding

523
00:17:43,760 --> 00:17:48,559
vector

524
00:17:44,840 --> 00:17:49,760
here so basically we have that as an

525
00:17:48,559 --> 00:17:54,320
example

526
00:17:49,760 --> 00:17:57,200
the 13th or example index 13

527
00:17:54,320 --> 00:17:58,720
the second dimension is the integer 1 as

528
00:17:57,200 --> 00:17:59,679
an example

529
00:17:58,720 --> 00:18:02,799
and so

530
00:17:59,679 --> 00:18:05,840
here if we do c of x which gives us that

531
00:18:02,799 --> 00:18:07,120
array and then we index into 13 by two

532
00:18:05,840 --> 00:18:09,600
of that array

533
00:18:07,120 --> 00:18:10,480
then we we get the embedding

534
00:18:09,600 --> 00:18:12,080
here

535
00:18:10,480 --> 00:18:12,799
and you can verify that

536
00:18:12,080 --> 00:18:15,600
c

537
00:18:12,799 --> 00:18:19,840
at one which is the integer at that

538
00:18:15,600 --> 00:18:21,679
location is indeed equal to this

539
00:18:19,840 --> 00:18:23,520
you see they're equal

540
00:18:21,679 --> 00:18:26,559
so basically long story short pytorch

541
00:18:23,520 --> 00:18:29,440
indexing is awesome and to embed

542
00:18:26,559 --> 00:18:31,280
simultaneously all of the integers in x

543
00:18:29,440 --> 00:18:33,120
we can simply do c of x

544
00:18:31,280 --> 00:18:35,039
and that is our embedding

545
00:18:33,120 --> 00:18:37,120
and that just works

546
00:18:35,039 --> 00:18:38,799
now let's construct this layer here the

547
00:18:37,120 --> 00:18:42,559
hidden layer

548
00:18:38,799 --> 00:18:44,799
so we have that w1 as i'll call it are

549
00:18:42,559 --> 00:18:46,000
these weights which we will initialize

550
00:18:44,799 --> 00:18:48,400
randomly

551
00:18:46,000 --> 00:18:49,600
now the number of inputs to this layer

552
00:18:48,400 --> 00:18:51,280
is going to be

553
00:18:49,600 --> 00:18:52,640
three times two right because we have

554
00:18:51,280 --> 00:18:53,679
two dimensional embeddings and we have

555
00:18:52,640 --> 00:18:56,160
three of them

556
00:18:53,679 --> 00:18:58,400
so the number of inputs is 6

557
00:18:56,160 --> 00:19:00,240
and the number of neurons in this layer

558
00:18:58,400 --> 00:19:02,880
is a variable up to us

559
00:19:00,240 --> 00:19:04,640
let's use 100 neurons as an example

560
00:19:02,880 --> 00:19:06,559
and then biases

561
00:19:04,640 --> 00:19:07,520
will be also initialized randomly as an

562
00:19:06,559 --> 00:19:11,200
example

563
00:19:07,520 --> 00:19:13,360
and let's and we just need 100 of them

564
00:19:11,200 --> 00:19:15,600
now the problem with this is we can't

565
00:19:13,360 --> 00:19:17,520
simply normally we would take the input

566
00:19:15,600 --> 00:19:20,559
in this case that's embedding and we'd

567
00:19:17,520 --> 00:19:22,240
like to multiply it with these weights

568
00:19:20,559 --> 00:19:24,400
and then we would like to add the bias

569
00:19:22,240 --> 00:19:25,919
this is roughly what we want to do

570
00:19:24,400 --> 00:19:27,760
but the problem here is that these

571
00:19:25,919 --> 00:19:29,919
embeddings are stacked up in the

572
00:19:27,760 --> 00:19:31,360
dimensions of this input tensor

573
00:19:29,919 --> 00:19:32,880
so this will not work this matrix

574
00:19:31,360 --> 00:19:35,360
multiplication because this is a shape

575
00:19:32,880 --> 00:19:37,120
32 by 3 by 2 and i can't multiply that

576
00:19:35,360 --> 00:19:40,000
by 6 by 100

577
00:19:37,120 --> 00:19:41,919
so somehow we need to concatenate these

578
00:19:40,000 --> 00:19:43,120
inputs here together so that we can do

579
00:19:41,919 --> 00:19:45,039
something along these lines which

580
00:19:43,120 --> 00:19:47,919
currently does not work

581
00:19:45,039 --> 00:19:50,720
so how do we transform this 32 by 3 by 2

582
00:19:47,919 --> 00:19:51,840
into a 32 by 6 so that we can actually

583
00:19:50,720 --> 00:19:52,799
perform

584
00:19:51,840 --> 00:19:54,640
this

585
00:19:52,799 --> 00:19:56,160
multiplication over here i'd like to

586
00:19:54,640 --> 00:19:57,440
show you that there are usually many

587
00:19:56,160 --> 00:19:59,679
ways of

588
00:19:57,440 --> 00:20:01,840
implementing what you'd like to do in

589
00:19:59,679 --> 00:20:03,760
torch and some of them will be faster

590
00:20:01,840 --> 00:20:06,240
better shorter etc

591
00:20:03,760 --> 00:20:07,919
and that's because torch is a very large

592
00:20:06,240 --> 00:20:09,440
library and it's got lots and lots of

593
00:20:07,919 --> 00:20:11,360
functions so if you just go to the

594
00:20:09,440 --> 00:20:14,160
documentation and click on torch you'll

595
00:20:11,360 --> 00:20:15,280
see that my slider here is very tiny and

596
00:20:14,160 --> 00:20:16,640
that's because there are so many

597
00:20:15,280 --> 00:20:17,760
functions that you can call on these

598
00:20:16,640 --> 00:20:20,080
tensors

599
00:20:17,760 --> 00:20:22,000
to transform them create them multiply

600
00:20:20,080 --> 00:20:24,880
them add them perform all kinds of

601
00:20:22,000 --> 00:20:28,480
different operations on them

602
00:20:24,880 --> 00:20:31,120
and so this is kind of like

603
00:20:28,480 --> 00:20:32,480
the space of possibility if you will

604
00:20:31,120 --> 00:20:34,640
now one of the things that you can do is

605
00:20:32,480 --> 00:20:36,559
if we can control here ctrl f for

606
00:20:34,640 --> 00:20:37,440
concatenate and we see that there's a

607
00:20:36,559 --> 00:20:40,480
function

608
00:20:37,440 --> 00:20:42,000
torque.cat short for concatenate

609
00:20:40,480 --> 00:20:45,120
and this concatenates the given sequence

610
00:20:42,000 --> 00:20:46,720
of tensors in a given dimension

611
00:20:45,120 --> 00:20:49,200
and these sensors must have the same

612
00:20:46,720 --> 00:20:52,240
shape etc so we can use the concatenate

613
00:20:49,200 --> 00:20:56,000
operation to in a naive way concatenate

614
00:20:52,240 --> 00:20:58,799
these three embeddings for each input

615
00:20:56,000 --> 00:21:01,039
so in this case we have m of

616
00:20:58,799 --> 00:21:03,120
amp of the shape and really what we want

617
00:21:01,039 --> 00:21:04,960
to do is we want to retrieve these three

618
00:21:03,120 --> 00:21:08,480
parts and concatenate them

619
00:21:04,960 --> 00:21:10,240
so we want to grab all the examples

620
00:21:08,480 --> 00:21:13,440
we want to grab

621
00:21:10,240 --> 00:21:14,559
first the zeroth

622
00:21:13,440 --> 00:21:16,559
index

623
00:21:14,559 --> 00:21:17,760
and then all of

624
00:21:16,559 --> 00:21:20,400
this

625
00:21:17,760 --> 00:21:24,320
so this plucks out

626
00:21:20,400 --> 00:21:26,400
the 32 by 2 embeddings of just the first

627
00:21:24,320 --> 00:21:28,559
word here

628
00:21:26,400 --> 00:21:30,880
and so basically we want this guy

629
00:21:28,559 --> 00:21:32,559
we want the first dimension and we want

630
00:21:30,880 --> 00:21:34,640
the second dimension

631
00:21:32,559 --> 00:21:36,400
and these are the three pieces

632
00:21:34,640 --> 00:21:38,000
individually

633
00:21:36,400 --> 00:21:40,559
and then we want to treat this as a

634
00:21:38,000 --> 00:21:43,600
sequence and we want to torch that cat

635
00:21:40,559 --> 00:21:45,600
on that sequence so this is the list

636
00:21:43,600 --> 00:21:46,720
tor.cat takes a

637
00:21:45,600 --> 00:21:48,480
sequence

638
00:21:46,720 --> 00:21:51,200
of tensors and then we have to tell it

639
00:21:48,480 --> 00:21:53,600
along which dimension to concatenate

640
00:21:51,200 --> 00:21:55,280
so in this case all these are 32 by 2

641
00:21:53,600 --> 00:21:58,159
and we want to concatenate not across

642
00:21:55,280 --> 00:22:00,000
dimension 0 by the cross dimension one

643
00:21:58,159 --> 00:22:01,919
so passing in one

644
00:22:00,000 --> 00:22:04,240
gives us a result

645
00:22:01,919 --> 00:22:05,360
the shape of this is 32 by 6 exactly as

646
00:22:04,240 --> 00:22:08,320
we'd like

647
00:22:05,360 --> 00:22:10,559
so that basically took 32 and squashed

648
00:22:08,320 --> 00:22:11,520
these by concatenating them into 32 by

649
00:22:10,559 --> 00:22:13,440
6.

650
00:22:11,520 --> 00:22:15,600
now this is kind of ugly because this

651
00:22:13,440 --> 00:22:17,679
code would not generalize if we want to

652
00:22:15,600 --> 00:22:19,679
later change the block size right now we

653
00:22:17,679 --> 00:22:22,240
have three inputs

654
00:22:19,679 --> 00:22:23,520
three words but what if we had five

655
00:22:22,240 --> 00:22:26,080
then here we would have to change the

656
00:22:23,520 --> 00:22:28,159
code because i'm indexing directly well

657
00:22:26,080 --> 00:22:31,679
torch comes to rescue again because that

658
00:22:28,159 --> 00:22:35,120
turns out to be a function called unbind

659
00:22:31,679 --> 00:22:35,120
and it removes a tensor dimension

660
00:22:35,280 --> 00:22:39,039
so it removes the tensor dimension

661
00:22:36,720 --> 00:22:40,400
returns a tuple of all slices along a

662
00:22:39,039 --> 00:22:41,679
given dimension

663
00:22:40,400 --> 00:22:43,840
without it

664
00:22:41,679 --> 00:22:45,679
so this is exactly what we need

665
00:22:43,840 --> 00:22:48,000
and basically when we call torch dot

666
00:22:45,679 --> 00:22:50,320
unbind

667
00:22:48,000 --> 00:22:51,679
torch dot unbind

668
00:22:50,320 --> 00:22:54,720
of m

669
00:22:51,679 --> 00:22:56,480
and pass in dimension

670
00:22:54,720 --> 00:22:59,280
1 index 1

671
00:22:56,480 --> 00:23:01,600
this gives us a list of

672
00:22:59,280 --> 00:23:02,480
a list of tensors exactly equivalent to

673
00:23:01,600 --> 00:23:04,159
this

674
00:23:02,480 --> 00:23:06,400
so running this

675
00:23:04,159 --> 00:23:07,520
gives us a line

676
00:23:06,400 --> 00:23:09,440
3

677
00:23:07,520 --> 00:23:12,559
and it's exactly this list and so we can

678
00:23:09,440 --> 00:23:14,960
call torch.cat on it

679
00:23:12,559 --> 00:23:16,880
and along the first dimension

680
00:23:14,960 --> 00:23:19,120
and this works

681
00:23:16,880 --> 00:23:21,120
and this shape is the same

682
00:23:19,120 --> 00:23:23,360
but now this is uh it doesn't matter if

683
00:23:21,120 --> 00:23:24,799
we have block size 3 or 5 or 10 this

684
00:23:23,360 --> 00:23:26,960
will just work

685
00:23:24,799 --> 00:23:28,960
so this is one way to do it but it turns

686
00:23:26,960 --> 00:23:30,720
out that in this case there's actually a

687
00:23:28,960 --> 00:23:32,880
significantly better and more efficient

688
00:23:30,720 --> 00:23:34,720
way and this gives me an opportunity to

689
00:23:32,880 --> 00:23:36,400
hint at some of the internals of

690
00:23:34,720 --> 00:23:37,919
torch.tensor

691
00:23:36,400 --> 00:23:40,159
so let's create

692
00:23:37,919 --> 00:23:42,559
an array here

693
00:23:40,159 --> 00:23:44,240
of elements from 0 to 17

694
00:23:42,559 --> 00:23:46,480
and the shape of this

695
00:23:44,240 --> 00:23:47,919
is just 18. it's a single picture of 18

696
00:23:46,480 --> 00:23:49,919
numbers

697
00:23:47,919 --> 00:23:52,960
it turns out that we can very quickly

698
00:23:49,919 --> 00:23:54,480
re-represent this as different sized and

699
00:23:52,960 --> 00:23:57,440
dimensional tensors

700
00:23:54,480 --> 00:23:59,280
we do this by calling a view

701
00:23:57,440 --> 00:24:02,159
and we can say that actually this is not

702
00:23:59,280 --> 00:24:05,600
a single vector of 18 this is a two by

703
00:24:02,159 --> 00:24:08,000
nine tensor or alternatively this is a

704
00:24:05,600 --> 00:24:10,159
nine by two tensor

705
00:24:08,000 --> 00:24:11,840
or this is actually a three by three by

706
00:24:10,159 --> 00:24:13,760
two tensor

707
00:24:11,840 --> 00:24:16,480
as long as the total number of elements

708
00:24:13,760 --> 00:24:18,960
here multiply to be the same

709
00:24:16,480 --> 00:24:21,520
this will just work and

710
00:24:18,960 --> 00:24:24,000
in pytorch this operation calling that

711
00:24:21,520 --> 00:24:26,000
view is extremely efficient

712
00:24:24,000 --> 00:24:27,840
and the reason for that is that

713
00:24:26,000 --> 00:24:30,480
in each tensor there's something called

714
00:24:27,840 --> 00:24:32,799
the underlying storage

715
00:24:30,480 --> 00:24:34,880
and the storage is just the numbers

716
00:24:32,799 --> 00:24:37,120
always as a one-dimensional vector and

717
00:24:34,880 --> 00:24:38,640
this is how this tensor is represented

718
00:24:37,120 --> 00:24:41,679
in the computer memory it's always a

719
00:24:38,640 --> 00:24:44,000
one-dimensional vector

720
00:24:41,679 --> 00:24:46,240
but when we call that view we are

721
00:24:44,000 --> 00:24:48,640
manipulating some of attributes of that

722
00:24:46,240 --> 00:24:50,960
tensor that dictate how this

723
00:24:48,640 --> 00:24:53,600
one-dimensional sequence is interpreted

724
00:24:50,960 --> 00:24:55,360
to be an n-dimensional tensor

725
00:24:53,600 --> 00:24:57,360
and so what's happening here is that no

726
00:24:55,360 --> 00:24:59,440
memory is being changed copied moved or

727
00:24:57,360 --> 00:25:00,559
created when we call that view the

728
00:24:59,440 --> 00:25:03,120
storage

729
00:25:00,559 --> 00:25:05,120
is identical but when you call that view

730
00:25:03,120 --> 00:25:07,840
some of the internal

731
00:25:05,120 --> 00:25:09,520
attributes of the view of the sensor are

732
00:25:07,840 --> 00:25:10,559
being manipulated and changed in

733
00:25:09,520 --> 00:25:12,320
particular that's something there's

734
00:25:10,559 --> 00:25:14,159
something called a storage offset

735
00:25:12,320 --> 00:25:16,000
strides and shapes and those are

736
00:25:14,159 --> 00:25:18,559
manipulated so that this one-dimensional

737
00:25:16,000 --> 00:25:20,559
sequence of bytes is seen as different

738
00:25:18,559 --> 00:25:22,720
and dimensional arrays

739
00:25:20,559 --> 00:25:25,279
there's a blog post here from eric

740
00:25:22,720 --> 00:25:27,520
called pi torch internals where he goes

741
00:25:25,279 --> 00:25:29,120
into some of this with respect to tensor

742
00:25:27,520 --> 00:25:30,320
and how the view of the tensor is

743
00:25:29,120 --> 00:25:32,400
represented

744
00:25:30,320 --> 00:25:34,320
and this is really just like a logical

745
00:25:32,400 --> 00:25:35,760
construct of representing the physical

746
00:25:34,320 --> 00:25:38,240
memory

747
00:25:35,760 --> 00:25:40,000
and so this is a pretty good um blog

748
00:25:38,240 --> 00:25:41,840
post that you can go into i might also

749
00:25:40,000 --> 00:25:44,400
create an entire video on the internals

750
00:25:41,840 --> 00:25:46,159
of torch tensor and how this works

751
00:25:44,400 --> 00:25:48,080
for here we just note that this is an

752
00:25:46,159 --> 00:25:50,559
extremely efficient operation

753
00:25:48,080 --> 00:25:53,120
and if i delete this and come back to

754
00:25:50,559 --> 00:25:55,360
our end

755
00:25:53,120 --> 00:25:58,080
we see that the shape of our end is 32

756
00:25:55,360 --> 00:26:00,559
by three by two but we can simply

757
00:25:58,080 --> 00:26:02,960
ask for pytorch to view this instead as

758
00:26:00,559 --> 00:26:05,360
a 32 by six

759
00:26:02,960 --> 00:26:07,600
and the way this gets flattened into a

760
00:26:05,360 --> 00:26:09,360
32 by six array

761
00:26:07,600 --> 00:26:10,880
just happens that

762
00:26:09,360 --> 00:26:12,480
these two

763
00:26:10,880 --> 00:26:14,400
get stacked up

764
00:26:12,480 --> 00:26:15,840
in a single row and so that's basically

765
00:26:14,400 --> 00:26:17,200
the concatenation operation that we're

766
00:26:15,840 --> 00:26:18,640
after

767
00:26:17,200 --> 00:26:20,400
and you can verify that this actually

768
00:26:18,640 --> 00:26:22,159
gives the exact same result as what we

769
00:26:20,400 --> 00:26:23,760
had before

770
00:26:22,159 --> 00:26:25,200
so this is an element y equals and you

771
00:26:23,760 --> 00:26:27,679
can see that all the elements of these

772
00:26:25,200 --> 00:26:30,799
two tensors are the same

773
00:26:27,679 --> 00:26:33,039
and so we get the exact same result

774
00:26:30,799 --> 00:26:34,159
so long story short we can actually just

775
00:26:33,039 --> 00:26:38,760
come here

776
00:26:34,159 --> 00:26:38,760
and if we just view this as a 32x6

777
00:26:38,880 --> 00:26:43,120
instead then this multiplication will

778
00:26:40,799 --> 00:26:44,240
work and give us the hidden states that

779
00:26:43,120 --> 00:26:45,919
we're after

780
00:26:44,240 --> 00:26:48,400
so if this is h

781
00:26:45,919 --> 00:26:51,039
then h shape is now

782
00:26:48,400 --> 00:26:53,440
the 100 dimensional activations for

783
00:26:51,039 --> 00:26:55,600
every one of our 32 examples

784
00:26:53,440 --> 00:26:57,919
and this gives the desired result let me

785
00:26:55,600 --> 00:27:00,480
do two things here number one let's not

786
00:26:57,919 --> 00:27:02,720
use 32 we can for example do something

787
00:27:00,480 --> 00:27:02,720
like

788
00:27:03,000 --> 00:27:07,440
m.shape at 0

789
00:27:05,440 --> 00:27:09,360
so that we don't hard code these numbers

790
00:27:07,440 --> 00:27:10,320
and this would work for any size of this

791
00:27:09,360 --> 00:27:11,919
amp

792
00:27:10,320 --> 00:27:14,159
or alternatively we can also do negative

793
00:27:11,919 --> 00:27:16,720
one when we do negative one pi torch

794
00:27:14,159 --> 00:27:17,919
will infer what this should be

795
00:27:16,720 --> 00:27:20,320
because the number of elements must be

796
00:27:17,919 --> 00:27:21,919
the same and we're saying that this is 6

797
00:27:20,320 --> 00:27:24,799
by church will derive that this must be

798
00:27:21,919 --> 00:27:26,799
32 or whatever else it is if m is of

799
00:27:24,799 --> 00:27:29,520
different size

800
00:27:26,799 --> 00:27:33,039
the other thing is here um

801
00:27:29,520 --> 00:27:35,200
one more thing i'd like to point out is

802
00:27:33,039 --> 00:27:37,600
here when we do the concatenation

803
00:27:35,200 --> 00:27:39,919
this actually is much less efficient

804
00:27:37,600 --> 00:27:41,360
because um this concatenation would

805
00:27:39,919 --> 00:27:43,279
create a whole new tensor with a whole

806
00:27:41,360 --> 00:27:44,880
new storage so new memory is being

807
00:27:43,279 --> 00:27:46,960
created because there's no way to

808
00:27:44,880 --> 00:27:48,480
concatenate tensors just by manipulating

809
00:27:46,960 --> 00:27:50,080
the view attributes

810
00:27:48,480 --> 00:27:52,320
so this is inefficient and creates all

811
00:27:50,080 --> 00:27:55,520
kinds of new memory

812
00:27:52,320 --> 00:27:57,200
uh so let me delete this now

813
00:27:55,520 --> 00:27:59,600
we don't need this

814
00:27:57,200 --> 00:28:01,440
and here to calculate h we want to also

815
00:27:59,600 --> 00:28:02,480
dot 10h

816
00:28:01,440 --> 00:28:04,399
of this

817
00:28:02,480 --> 00:28:07,039
to get our

818
00:28:04,399 --> 00:28:08,240
oops to get our h

819
00:28:07,039 --> 00:28:10,480
so these are now numbers between

820
00:28:08,240 --> 00:28:11,679
negative one and one because of the 10h

821
00:28:10,480 --> 00:28:14,000
and we have

822
00:28:11,679 --> 00:28:15,919
that the shape is 32 by 100

823
00:28:14,000 --> 00:28:17,600
and that is basically this hidden layer

824
00:28:15,919 --> 00:28:20,240
of activations here

825
00:28:17,600 --> 00:28:21,520
for every one of our 32 examples

826
00:28:20,240 --> 00:28:23,039
now there's one more thing i've lost

827
00:28:21,520 --> 00:28:24,399
over that we have to be very careful

828
00:28:23,039 --> 00:28:26,159
with and that this

829
00:28:24,399 --> 00:28:27,520
and that's this plus here

830
00:28:26,159 --> 00:28:30,720
in particular we want to make sure that

831
00:28:27,520 --> 00:28:33,360
the broadcasting will do what we like

832
00:28:30,720 --> 00:28:35,679
the shape of this is 32 by 100 and the

833
00:28:33,360 --> 00:28:37,600
ones shape is 100.

834
00:28:35,679 --> 00:28:40,000
so we see that the addition here will

835
00:28:37,600 --> 00:28:44,240
broadcast these two and in particular we

836
00:28:40,000 --> 00:28:47,200
have 32 by 100 broadcasting to 100.

837
00:28:44,240 --> 00:28:49,279
so broadcasting will align on the right

838
00:28:47,200 --> 00:28:51,919
create a fake dimension here so this

839
00:28:49,279 --> 00:28:54,720
will become a 1 by 100 row vector and

840
00:28:51,919 --> 00:28:57,200
then it will copy vertically

841
00:28:54,720 --> 00:28:58,880
for every one of these rows of 32 and do

842
00:28:57,200 --> 00:29:00,480
an element wise addition

843
00:28:58,880 --> 00:29:02,480
so in this case the correct thing will

844
00:29:00,480 --> 00:29:05,840
be happening because the same bias

845
00:29:02,480 --> 00:29:06,640
vector will be added to all the rows

846
00:29:05,840 --> 00:29:08,960
of

847
00:29:06,640 --> 00:29:11,279
this matrix so that is correct that's

848
00:29:08,960 --> 00:29:12,880
what we'd like and it's always good

849
00:29:11,279 --> 00:29:14,080
practice you just make sure

850
00:29:12,880 --> 00:29:16,159
so that you don't shoot yourself in the

851
00:29:14,080 --> 00:29:17,600
foot and finally let's create the final

852
00:29:16,159 --> 00:29:19,840
layer here

853
00:29:17,600 --> 00:29:22,640
so let's create

854
00:29:19,840 --> 00:29:24,880
w2 and v2

855
00:29:22,640 --> 00:29:26,960
the input now is 100

856
00:29:24,880 --> 00:29:29,760
and the output number of neurons will be

857
00:29:26,960 --> 00:29:31,760
for us 27 because we have 27 possible

858
00:29:29,760 --> 00:29:34,880
characters that come next

859
00:29:31,760 --> 00:29:36,640
so the biases will be 27 as well

860
00:29:34,880 --> 00:29:38,559
so therefore the logits which are the

861
00:29:36,640 --> 00:29:41,440
outputs of this neural net

862
00:29:38,559 --> 00:29:42,399
are going to be um

863
00:29:41,440 --> 00:29:47,120
h

864
00:29:42,399 --> 00:29:50,480
multiplied by w2 plus b2

865
00:29:47,120 --> 00:29:52,240
logistic shape is 32 by 27

866
00:29:50,480 --> 00:29:54,159
and the logits look

867
00:29:52,240 --> 00:29:55,679
good now exactly as we saw in the

868
00:29:54,159 --> 00:29:58,000
previous video we want to take these

869
00:29:55,679 --> 00:30:00,000
logits and we want to first exponentiate

870
00:29:58,000 --> 00:30:01,360
them to get our fake counts

871
00:30:00,000 --> 00:30:02,960
and then we want to normalize them into

872
00:30:01,360 --> 00:30:05,600
a probability

873
00:30:02,960 --> 00:30:06,880
so prob is counts divide

874
00:30:05,600 --> 00:30:10,559
and now

875
00:30:06,880 --> 00:30:12,480
counts dot sum along the first dimension

876
00:30:10,559 --> 00:30:14,399
and keep them as true exactly as in the

877
00:30:12,480 --> 00:30:16,000
previous video

878
00:30:14,399 --> 00:30:20,080
and so

879
00:30:16,000 --> 00:30:23,520
prob that shape now is 32 by 27

880
00:30:20,080 --> 00:30:26,480
and you'll see that every row of prob

881
00:30:23,520 --> 00:30:28,559
sums to one so it's normalized

882
00:30:26,480 --> 00:30:30,240
so that gives us the probabilities now

883
00:30:28,559 --> 00:30:32,240
of course we have the actual letter that

884
00:30:30,240 --> 00:30:34,240
comes next and that comes from this

885
00:30:32,240 --> 00:30:36,320
array y

886
00:30:34,240 --> 00:30:39,039
which we which we created during the

887
00:30:36,320 --> 00:30:40,880
dataset creation so why is this last

888
00:30:39,039 --> 00:30:42,320
piece here which is the identity of the

889
00:30:40,880 --> 00:30:44,799
next character in the sequence that we'd

890
00:30:42,320 --> 00:30:46,640
like to now predict

891
00:30:44,799 --> 00:30:48,480
so what we'd like to do now is just as

892
00:30:46,640 --> 00:30:51,279
in the previous video we'd like to index

893
00:30:48,480 --> 00:30:52,960
into the rows of prob and in each row

894
00:30:51,279 --> 00:30:55,120
we'd like to pluck out the probability

895
00:30:52,960 --> 00:30:56,960
assigned to the correct character

896
00:30:55,120 --> 00:31:00,480
as given here

897
00:30:56,960 --> 00:31:03,600
so first we have torch.range of 32 which

898
00:31:00,480 --> 00:31:05,760
is kind of like a iterator over

899
00:31:03,600 --> 00:31:07,520
numbers from 0 to 31

900
00:31:05,760 --> 00:31:09,039
and then we can index into prob in the

901
00:31:07,520 --> 00:31:10,200
following way

902
00:31:09,039 --> 00:31:12,799
prop in

903
00:31:10,200 --> 00:31:15,679
torch.range of 32 which iterates the

904
00:31:12,799 --> 00:31:19,279
roads and in each row we'd like to grab

905
00:31:15,679 --> 00:31:21,279
this column as given by y

906
00:31:19,279 --> 00:31:23,120
so this gives the current probabilities

907
00:31:21,279 --> 00:31:24,880
as assigned by this neural network with

908
00:31:23,120 --> 00:31:27,760
this setting of its weights

909
00:31:24,880 --> 00:31:29,520
to the correct character in the sequence

910
00:31:27,760 --> 00:31:30,960
and you can see here that this looks

911
00:31:29,520 --> 00:31:32,960
okay for some of these characters like

912
00:31:30,960 --> 00:31:34,480
this is basically 0.2

913
00:31:32,960 --> 00:31:37,640
but it doesn't look very good at all for

914
00:31:34,480 --> 00:31:40,480
many other characters like this is

915
00:31:37,640 --> 00:31:42,080
0.0701 probability and so the network

916
00:31:40,480 --> 00:31:43,679
thinks that some of these are extremely

917
00:31:42,080 --> 00:31:47,120
unlikely but of course we haven't

918
00:31:43,679 --> 00:31:48,960
trained the neural network yet so

919
00:31:47,120 --> 00:31:50,880
this will improve and ideally all of

920
00:31:48,960 --> 00:31:52,320
these numbers here of course are one

921
00:31:50,880 --> 00:31:53,840
because then we are correctly predicting

922
00:31:52,320 --> 00:31:55,519
the next character

923
00:31:53,840 --> 00:31:57,440
now just as in the previous video we

924
00:31:55,519 --> 00:31:59,360
want to take these probabilities we want

925
00:31:57,440 --> 00:32:00,960
to look at the lock probability

926
00:31:59,360 --> 00:32:02,080
and then we want to look at the average

927
00:32:00,960 --> 00:32:04,480
probability

928
00:32:02,080 --> 00:32:07,519
and the negative of it to create the

929
00:32:04,480 --> 00:32:10,240
negative log likelihood loss

930
00:32:07,519 --> 00:32:11,760
so the loss here is 17

931
00:32:10,240 --> 00:32:14,159
and this is the loss that we'd like to

932
00:32:11,760 --> 00:32:16,640
minimize to get the network to predict

933
00:32:14,159 --> 00:32:18,720
the correct character in the sequence

934
00:32:16,640 --> 00:32:20,799
okay so i rewrote everything here and

935
00:32:18,720 --> 00:32:22,960
made it a bit more respectable so here's

936
00:32:20,799 --> 00:32:24,559
our data set here's all the parameters

937
00:32:22,960 --> 00:32:26,320
that we defined

938
00:32:24,559 --> 00:32:27,679
i'm now using a generator to make it

939
00:32:26,320 --> 00:32:29,360
reproducible

940
00:32:27,679 --> 00:32:31,440
i clustered all the parameters into a

941
00:32:29,360 --> 00:32:33,519
single list of parameters so that for

942
00:32:31,440 --> 00:32:34,799
example it's easy to count them and see

943
00:32:33,519 --> 00:32:37,120
that in total we currently have about

944
00:32:34,799 --> 00:32:38,399
3400 parameters

945
00:32:37,120 --> 00:32:39,600
and this is the forward pass as we

946
00:32:38,399 --> 00:32:41,919
developed it

947
00:32:39,600 --> 00:32:44,000
and we arrive at a single number here

948
00:32:41,919 --> 00:32:45,039
the loss that is currently expressing

949
00:32:44,000 --> 00:32:46,960
how well

950
00:32:45,039 --> 00:32:48,720
this neural network works with the

951
00:32:46,960 --> 00:32:50,080
current setting of parameters

952
00:32:48,720 --> 00:32:52,320
now i would like to make it even more

953
00:32:50,080 --> 00:32:54,640
respectable so in particular see these

954
00:32:52,320 --> 00:32:57,440
lines here where we take the logits and

955
00:32:54,640 --> 00:32:59,440
we calculate the loss

956
00:32:57,440 --> 00:33:01,440
we're not actually reinventing the wheel

957
00:32:59,440 --> 00:33:03,360
here this is just um

958
00:33:01,440 --> 00:33:05,679
classification and many people use

959
00:33:03,360 --> 00:33:07,440
classification and that's why there is a

960
00:33:05,679 --> 00:33:09,600
functional.cross entropy function in

961
00:33:07,440 --> 00:33:10,720
pytorch to calculate this much more

962
00:33:09,600 --> 00:33:12,399
efficiently

963
00:33:10,720 --> 00:33:13,360
so we can just simply call f.cross

964
00:33:12,399 --> 00:33:14,880
entropy

965
00:33:13,360 --> 00:33:16,320
and we can pass in the logits and we can

966
00:33:14,880 --> 00:33:18,080
pass in the

967
00:33:16,320 --> 00:33:21,840
array of targets y

968
00:33:18,080 --> 00:33:21,840
and this calculates the exact same loss

969
00:33:22,240 --> 00:33:27,360
so in fact we can simply put this here

970
00:33:25,279 --> 00:33:29,919
and erase these three lines and we're

971
00:33:27,360 --> 00:33:31,440
going to get the exact same result now

972
00:33:29,919 --> 00:33:34,240
there are actually many good reasons to

973
00:33:31,440 --> 00:33:36,240
prefer f.cross entropy over rolling your

974
00:33:34,240 --> 00:33:37,919
own implementation like this i did this

975
00:33:36,240 --> 00:33:40,559
for educational reasons but you'd never

976
00:33:37,919 --> 00:33:42,880
use this in practice why is that

977
00:33:40,559 --> 00:33:44,960
number one when you use f.cross entropy

978
00:33:42,880 --> 00:33:46,880
by torch will not actually create all

979
00:33:44,960 --> 00:33:49,120
these intermediate tensors because these

980
00:33:46,880 --> 00:33:51,360
are all new tensors in memory and all

981
00:33:49,120 --> 00:33:54,000
this is fairly inefficient to run like

982
00:33:51,360 --> 00:33:56,960
this instead pytorch will cluster up all

983
00:33:54,000 --> 00:33:59,519
these operations and very often create

984
00:33:56,960 --> 00:34:01,279
have fused kernels that very efficiently

985
00:33:59,519 --> 00:34:03,120
evaluate these expressions that are sort

986
00:34:01,279 --> 00:34:04,640
of like clustered mathematical

987
00:34:03,120 --> 00:34:06,320
operations

988
00:34:04,640 --> 00:34:08,320
number two the backward pass can be made

989
00:34:06,320 --> 00:34:10,399
much more efficient and not just because

990
00:34:08,320 --> 00:34:12,480
it's a fused kernel but also

991
00:34:10,399 --> 00:34:15,359
analytically and mathematically it's

992
00:34:12,480 --> 00:34:17,440
much it's often a very much simpler

993
00:34:15,359 --> 00:34:19,599
backward pass to implement

994
00:34:17,440 --> 00:34:22,000
we actually sell this with micrograd

995
00:34:19,599 --> 00:34:23,919
you see here when we implemented 10h the

996
00:34:22,000 --> 00:34:25,760
forward pass of this operation to

997
00:34:23,919 --> 00:34:28,240
calculate the 10h was actually a fairly

998
00:34:25,760 --> 00:34:29,359
complicated mathematical expression

999
00:34:28,240 --> 00:34:31,520
but because it's a clustered

1000
00:34:29,359 --> 00:34:33,440
mathematical expression when we did the

1001
00:34:31,520 --> 00:34:35,919
backward pass we didn't individually

1002
00:34:33,440 --> 00:34:38,159
backward through the x and the two times

1003
00:34:35,919 --> 00:34:40,240
and the minus one in division etc we

1004
00:34:38,159 --> 00:34:42,079
just said it's one minus t squared and

1005
00:34:40,240 --> 00:34:43,200
that's a much simpler mathematical

1006
00:34:42,079 --> 00:34:44,720
expression

1007
00:34:43,200 --> 00:34:46,480
and we were able to do this because

1008
00:34:44,720 --> 00:34:48,079
we're able to reuse calculations and

1009
00:34:46,480 --> 00:34:50,240
because we are able to mathematically

1010
00:34:48,079 --> 00:34:52,320
and analytically derive the derivative

1011
00:34:50,240 --> 00:34:54,800
and often that expression simplifies

1012
00:34:52,320 --> 00:34:56,079
mathematically and so there's much less

1013
00:34:54,800 --> 00:34:57,760
to implement

1014
00:34:56,079 --> 00:34:59,520
so not only can can it be made more

1015
00:34:57,760 --> 00:35:01,680
efficient because it runs in a fused

1016
00:34:59,520 --> 00:35:03,280
kernel but also because the expressions

1017
00:35:01,680 --> 00:35:05,920
can take a much simpler form

1018
00:35:03,280 --> 00:35:08,560
mathematically

1019
00:35:05,920 --> 00:35:10,480
so that's number one number two

1020
00:35:08,560 --> 00:35:13,599
under the hood f that cross entropy can

1021
00:35:10,480 --> 00:35:15,440
also be significantly more um

1022
00:35:13,599 --> 00:35:18,640
numerically well behaved let me show you

1023
00:35:15,440 --> 00:35:18,640
an example of how this works

1024
00:35:19,200 --> 00:35:24,000
suppose we have a logits of negative 2 3

1025
00:35:21,520 --> 00:35:25,440
negative 3 0 and 5

1026
00:35:24,000 --> 00:35:27,599
and then we are taking the exponent of

1027
00:35:25,440 --> 00:35:28,320
it and normalizing it to sum to 1.

1028
00:35:27,599 --> 00:35:30,079
so

1029
00:35:28,320 --> 00:35:31,440
when logits take on this values

1030
00:35:30,079 --> 00:35:33,760
everything is well and good and we get a

1031
00:35:31,440 --> 00:35:35,200
nice probability distribution

1032
00:35:33,760 --> 00:35:37,440
now consider what happens when some of

1033
00:35:35,200 --> 00:35:38,960
these logits take on more extreme values

1034
00:35:37,440 --> 00:35:40,720
and that can happen during optimization

1035
00:35:38,960 --> 00:35:42,480
of the neural network

1036
00:35:40,720 --> 00:35:45,200
suppose that some of these numbers grow

1037
00:35:42,480 --> 00:35:47,119
very negative like say negative 100

1038
00:35:45,200 --> 00:35:49,280
then actually everything will come out

1039
00:35:47,119 --> 00:35:50,079
fine we still get the probabilities that

1040
00:35:49,280 --> 00:35:52,320
um

1041
00:35:50,079 --> 00:35:54,400
you know are well behaved and they sum

1042
00:35:52,320 --> 00:35:56,640
to one and everything is great

1043
00:35:54,400 --> 00:35:58,640
but because of the way the x works if

1044
00:35:56,640 --> 00:36:00,720
you have very positive logits let's say

1045
00:35:58,640 --> 00:36:02,240
positive 100 in here

1046
00:36:00,720 --> 00:36:04,480
you actually start to run into trouble

1047
00:36:02,240 --> 00:36:06,320
and we get not a number here

1048
00:36:04,480 --> 00:36:08,720
and the reason for that is that these

1049
00:36:06,320 --> 00:36:08,720
counts

1050
00:36:08,800 --> 00:36:12,480
have an if here

1051
00:36:10,400 --> 00:36:15,440
so if you pass in a very negative number

1052
00:36:12,480 --> 00:36:17,599
to x you just get a very negative sorry

1053
00:36:15,440 --> 00:36:19,839
not negative but very small number very

1054
00:36:17,599 --> 00:36:21,119
very near zero and that's fine

1055
00:36:19,839 --> 00:36:23,680
but if you pass in a very positive

1056
00:36:21,119 --> 00:36:25,520
number suddenly we run out of range in

1057
00:36:23,680 --> 00:36:28,160
our floating point number that

1058
00:36:25,520 --> 00:36:29,760
represents these counts

1059
00:36:28,160 --> 00:36:31,920
so basically we're taking e and we're

1060
00:36:29,760 --> 00:36:34,000
raising it to the power of 100 and that

1061
00:36:31,920 --> 00:36:35,839
gives us if because we run out of

1062
00:36:34,000 --> 00:36:37,839
dynamic range on this floating point

1063
00:36:35,839 --> 00:36:41,119
number that is count

1064
00:36:37,839 --> 00:36:43,839
and so we cannot pass very large logits

1065
00:36:41,119 --> 00:36:45,280
through this expression

1066
00:36:43,839 --> 00:36:47,280
now let me reset these numbers to

1067
00:36:45,280 --> 00:36:49,200
something reasonable

1068
00:36:47,280 --> 00:36:50,240
the way pi torch solved this

1069
00:36:49,200 --> 00:36:52,240
is that

1070
00:36:50,240 --> 00:36:53,520
you see how we have a well-behaved

1071
00:36:52,240 --> 00:36:54,720
result here

1072
00:36:53,520 --> 00:36:56,640
it turns out that because of the

1073
00:36:54,720 --> 00:36:59,599
normalization here you can actually

1074
00:36:56,640 --> 00:37:02,720
offset logits by any arbitrary constant

1075
00:36:59,599 --> 00:37:04,720
value that you want so if i add 1 here

1076
00:37:02,720 --> 00:37:06,320
you actually get the exact same result

1077
00:37:04,720 --> 00:37:08,800
or if i add 2

1078
00:37:06,320 --> 00:37:10,800
or if i subtract three

1079
00:37:08,800 --> 00:37:12,720
any offset will produce the exact same

1080
00:37:10,800 --> 00:37:15,119
probabilities

1081
00:37:12,720 --> 00:37:16,960
so because negative numbers are okay but

1082
00:37:15,119 --> 00:37:19,520
positive numbers can actually overflow

1083
00:37:16,960 --> 00:37:21,599
this x what patrick does is it

1084
00:37:19,520 --> 00:37:23,359
internally calculates the maximum value

1085
00:37:21,599 --> 00:37:25,280
that occurs in the logits and it

1086
00:37:23,359 --> 00:37:26,960
subtracts it so in this case it would

1087
00:37:25,280 --> 00:37:28,640
subtract five

1088
00:37:26,960 --> 00:37:30,640
and so therefore the greatest number in

1089
00:37:28,640 --> 00:37:32,079
logits will become zero and all the

1090
00:37:30,640 --> 00:37:33,119
other numbers will become some negative

1091
00:37:32,079 --> 00:37:35,119
numbers

1092
00:37:33,119 --> 00:37:37,920
and then the result of this is always

1093
00:37:35,119 --> 00:37:39,119
well behaved so even if we have 100 here

1094
00:37:37,920 --> 00:37:41,119
previously

1095
00:37:39,119 --> 00:37:44,320
not good but because pytorch will

1096
00:37:41,119 --> 00:37:46,960
subtract 100 this will work

1097
00:37:44,320 --> 00:37:49,200
and so there's many good reasons to call

1098
00:37:46,960 --> 00:37:50,800
cross-entropy number one the forward

1099
00:37:49,200 --> 00:37:53,040
pass can be much more efficient the

1100
00:37:50,800 --> 00:37:54,960
backward pass can be much more efficient

1101
00:37:53,040 --> 00:37:57,040
and also things can be much more

1102
00:37:54,960 --> 00:37:58,240
numerically well behaved okay so let's

1103
00:37:57,040 --> 00:37:59,280
now set up the training of this neural

1104
00:37:58,240 --> 00:38:02,400
net

1105
00:37:59,280 --> 00:38:02,400
we have the forward pass

1106
00:38:02,480 --> 00:38:06,800
uh we don't need these

1107
00:38:04,640 --> 00:38:09,760
is that we have the losses equal to the

1108
00:38:06,800 --> 00:38:12,079
f.cross entropy that's the forward pass

1109
00:38:09,760 --> 00:38:14,640
then we need the backward pass first we

1110
00:38:12,079 --> 00:38:16,240
want to set the gradients to be zero so

1111
00:38:14,640 --> 00:38:18,079
for p in parameters

1112
00:38:16,240 --> 00:38:19,440
we want to make sure that p dot grad is

1113
00:38:18,079 --> 00:38:20,880
none which is the same as setting it to

1114
00:38:19,440 --> 00:38:23,040
zero in pi torch

1115
00:38:20,880 --> 00:38:24,400
and then lost that backward to populate

1116
00:38:23,040 --> 00:38:25,760
those gradients

1117
00:38:24,400 --> 00:38:28,240
once we have the gradients we can do the

1118
00:38:25,760 --> 00:38:30,000
parameter update so for p in parameters

1119
00:38:28,240 --> 00:38:32,400
we want to take all the

1120
00:38:30,000 --> 00:38:36,480
data and we want to nudge it

1121
00:38:32,400 --> 00:38:36,480
learning rate times p dot grad

1122
00:38:36,880 --> 00:38:42,400
and then we want to repeat this

1123
00:38:39,760 --> 00:38:42,400
a few times

1124
00:38:43,920 --> 00:38:48,320
and let's print the loss here as well

1125
00:38:48,800 --> 00:38:51,920
now this won't suffice and it will

1126
00:38:50,079 --> 00:38:53,920
create an error because we also have to

1127
00:38:51,920 --> 00:38:55,599
go for pn parameters

1128
00:38:53,920 --> 00:38:59,760
and we have to make sure that p dot

1129
00:38:55,599 --> 00:39:03,760
requires grad is set to true in pi torch

1130
00:38:59,760 --> 00:39:05,760
and this should just work

1131
00:39:03,760 --> 00:39:08,079
okay so we started off with loss of 17

1132
00:39:05,760 --> 00:39:10,079
and we're decreasing it

1133
00:39:08,079 --> 00:39:12,880
let's run longer

1134
00:39:10,079 --> 00:39:15,839
and you see how the loss decreases

1135
00:39:12,880 --> 00:39:15,839
a lot here so

1136
00:39:17,200 --> 00:39:21,520
if we just run for a thousand times

1137
00:39:19,599 --> 00:39:22,480
we get a very very low loss and that

1138
00:39:21,520 --> 00:39:25,280
means that we're making very good

1139
00:39:22,480 --> 00:39:27,040
predictions now the reason that this is

1140
00:39:25,280 --> 00:39:29,359
so straightforward right now

1141
00:39:27,040 --> 00:39:32,320
is because we're only um

1142
00:39:29,359 --> 00:39:34,640
overfitting 32 examples

1143
00:39:32,320 --> 00:39:36,400
so we only have 32 examples uh of the

1144
00:39:34,640 --> 00:39:37,920
first five words

1145
00:39:36,400 --> 00:39:40,800
and therefore it's very easy to make

1146
00:39:37,920 --> 00:39:43,119
this neural net fit only these two 32

1147
00:39:40,800 --> 00:39:46,320
examples because we have 3 400

1148
00:39:43,119 --> 00:39:48,160
parameters and only 32 examples so we're

1149
00:39:46,320 --> 00:39:50,160
doing what's called overfitting a single

1150
00:39:48,160 --> 00:39:52,320
batch of the data

1151
00:39:50,160 --> 00:39:53,440
and getting a very low loss and good

1152
00:39:52,320 --> 00:39:55,040
predictions

1153
00:39:53,440 --> 00:39:56,880
um but that's just because we have so

1154
00:39:55,040 --> 00:39:58,000
many parameters for so few examples so

1155
00:39:56,880 --> 00:40:00,079
it's easy to

1156
00:39:58,000 --> 00:40:01,760
uh make this be very low

1157
00:40:00,079 --> 00:40:02,640
now we're not able to achieve exactly

1158
00:40:01,760 --> 00:40:04,480
zero

1159
00:40:02,640 --> 00:40:06,320
and the reason for that is we can for

1160
00:40:04,480 --> 00:40:08,400
example look at logits which are being

1161
00:40:06,320 --> 00:40:11,440
predicted

1162
00:40:08,400 --> 00:40:13,520
and we can look at the max along the

1163
00:40:11,440 --> 00:40:15,520
first dimension

1164
00:40:13,520 --> 00:40:17,520
and in pi torch

1165
00:40:15,520 --> 00:40:20,240
max reports both the actual values that

1166
00:40:17,520 --> 00:40:22,079
take on the maximum number but also the

1167
00:40:20,240 --> 00:40:23,520
indices of piece

1168
00:40:22,079 --> 00:40:26,320
and you'll see that the indices are very

1169
00:40:23,520 --> 00:40:28,640
close to the labels

1170
00:40:26,320 --> 00:40:31,200
but in some cases they differ

1171
00:40:28,640 --> 00:40:33,520
for example in this very first example

1172
00:40:31,200 --> 00:40:35,040
the predicted index is 19 but the label

1173
00:40:33,520 --> 00:40:36,960
is five

1174
00:40:35,040 --> 00:40:40,560
and we're not able to make loss be zero

1175
00:40:36,960 --> 00:40:42,960
and fundamentally that's because here

1176
00:40:40,560 --> 00:40:44,400
the very first or the zeroth index is

1177
00:40:42,960 --> 00:40:46,160
the example where dot dot dot is

1178
00:40:44,400 --> 00:40:48,079
supposed to predict e but you see how

1179
00:40:46,160 --> 00:40:50,480
dot dot dot is also supposed to predict

1180
00:40:48,079 --> 00:40:53,280
an o and dot dot is also supposed to

1181
00:40:50,480 --> 00:40:57,200
predict an i and then s as well and so

1182
00:40:53,280 --> 00:40:59,200
basically e o a or s are all possible

1183
00:40:57,200 --> 00:41:01,040
outcomes in a training set for the exact

1184
00:40:59,200 --> 00:41:03,520
same input so we're not able to

1185
00:41:01,040 --> 00:41:06,480
completely over fit and um

1186
00:41:03,520 --> 00:41:08,880
and make the loss be exactly zero so but

1187
00:41:06,480 --> 00:41:09,839
we're getting very close in the cases

1188
00:41:08,880 --> 00:41:11,760
where

1189
00:41:09,839 --> 00:41:13,520
there's a unique input for a unique

1190
00:41:11,760 --> 00:41:15,680
output in those cases we do what's

1191
00:41:13,520 --> 00:41:19,040
called overfit and we basically get the

1192
00:41:15,680 --> 00:41:21,119
exact same and the exact correct result

1193
00:41:19,040 --> 00:41:22,720
so now all we have to do

1194
00:41:21,119 --> 00:41:24,160
is we just need to make sure that we

1195
00:41:22,720 --> 00:41:25,440
read in the full data set and optimize

1196
00:41:24,160 --> 00:41:27,200
the neural net

1197
00:41:25,440 --> 00:41:29,119
okay so let's swing back up

1198
00:41:27,200 --> 00:41:30,640
where we created the dataset

1199
00:41:29,119 --> 00:41:32,640
and we see that here we only use the

1200
00:41:30,640 --> 00:41:33,599
first five words so let me now erase

1201
00:41:32,640 --> 00:41:35,119
this

1202
00:41:33,599 --> 00:41:38,000
and let me erase the print statements

1203
00:41:35,119 --> 00:41:39,760
otherwise we'd be printing way too much

1204
00:41:38,000 --> 00:41:42,960
and so when we processed the full data

1205
00:41:39,760 --> 00:41:45,200
set of all the words we now had 228 000

1206
00:41:42,960 --> 00:41:47,520
examples instead of just 32.

1207
00:41:45,200 --> 00:41:50,000
so let's now scroll back down

1208
00:41:47,520 --> 00:41:52,319
to this is much larger reinitialize the

1209
00:41:50,000 --> 00:41:54,319
weights the same number of parameters

1210
00:41:52,319 --> 00:41:56,400
they all require gradients

1211
00:41:54,319 --> 00:41:58,640
and then let's push this print out

1212
00:41:56,400 --> 00:41:59,920
lost.item to be here

1213
00:41:58,640 --> 00:42:03,720
and let's just see how the optimization

1214
00:41:59,920 --> 00:42:03,720
goes if we run this

1215
00:42:04,400 --> 00:42:07,760
okay so we started with a fairly high

1216
00:42:05,839 --> 00:42:10,640
loss and then as we're optimizing the

1217
00:42:07,760 --> 00:42:10,640
loss is coming down

1218
00:42:12,000 --> 00:42:15,839
but you'll notice that it takes quite a

1219
00:42:13,599 --> 00:42:17,839
bit of time for every single iteration

1220
00:42:15,839 --> 00:42:19,520
so let's actually address that because

1221
00:42:17,839 --> 00:42:22,560
we're doing way too much work forwarding

1222
00:42:19,520 --> 00:42:24,319
and backwarding 220 000 examples

1223
00:42:22,560 --> 00:42:26,800
in practice what people usually do is

1224
00:42:24,319 --> 00:42:29,599
they perform forward and backward pass

1225
00:42:26,800 --> 00:42:31,680
and update on many batches of the data

1226
00:42:29,599 --> 00:42:33,680
so what we will want to do is we want to

1227
00:42:31,680 --> 00:42:35,680
randomly select some portion of the data

1228
00:42:33,680 --> 00:42:37,520
set and that's a mini batch and then

1229
00:42:35,680 --> 00:42:40,079
only forward backward and update on that

1230
00:42:37,520 --> 00:42:42,000
little mini batch and then

1231
00:42:40,079 --> 00:42:43,599
we iterate on those many batches

1232
00:42:42,000 --> 00:42:45,119
so in pytorch we can for example use

1233
00:42:43,599 --> 00:42:47,599
storage.randint

1234
00:42:45,119 --> 00:42:50,640
we can generate numbers between 0 and 5

1235
00:42:47,599 --> 00:42:50,640
and make 32 of them

1236
00:42:52,160 --> 00:42:56,400
i believe the size has to be a

1237
00:42:54,319 --> 00:42:57,760
tuple

1238
00:42:56,400 --> 00:43:00,560
in my torch

1239
00:42:57,760 --> 00:43:02,560
so we can have a tuple 32 of numbers

1240
00:43:00,560 --> 00:43:05,440
between zero and five but actually we

1241
00:43:02,560 --> 00:43:08,160
want x dot shape of zero here

1242
00:43:05,440 --> 00:43:10,160
and so this creates uh integers that

1243
00:43:08,160 --> 00:43:13,200
index into our data set and there's 32

1244
00:43:10,160 --> 00:43:14,960
of them so if our mini batch size is 32

1245
00:43:13,200 --> 00:43:18,000
then we can come here and we can first

1246
00:43:14,960 --> 00:43:20,240
do a mini batch

1247
00:43:18,000 --> 00:43:22,000
construct

1248
00:43:20,240 --> 00:43:24,240
so in the integers that we want to

1249
00:43:22,000 --> 00:43:25,680
optimize in this

1250
00:43:24,240 --> 00:43:27,680
single iteration

1251
00:43:25,680 --> 00:43:29,920
are in the ix

1252
00:43:27,680 --> 00:43:30,800
and then we want to index into

1253
00:43:29,920 --> 00:43:34,160
x

1254
00:43:30,800 --> 00:43:36,800
with ix to only grab those rows

1255
00:43:34,160 --> 00:43:38,400
so we're only getting 32 rows of x

1256
00:43:36,800 --> 00:43:40,880
and therefore embeddings will again be

1257
00:43:38,400 --> 00:43:43,280
32 by three by two not two hundred

1258
00:43:40,880 --> 00:43:45,280
thousand by three by two

1259
00:43:43,280 --> 00:43:46,960
and then this ix has to be used not just

1260
00:43:45,280 --> 00:43:50,560
to index into x

1261
00:43:46,960 --> 00:43:52,640
but also to index into y

1262
00:43:50,560 --> 00:43:55,440
and now this should be many batches and

1263
00:43:52,640 --> 00:43:57,920
this should be much much faster so

1264
00:43:55,440 --> 00:44:00,079
okay so it's instant almost

1265
00:43:57,920 --> 00:44:01,520
so this way we can run many many

1266
00:44:00,079 --> 00:44:03,839
examples

1267
00:44:01,520 --> 00:44:05,599
nearly instantly and decrease the loss

1268
00:44:03,839 --> 00:44:07,040
much much faster

1269
00:44:05,599 --> 00:44:09,520
now because we're only dealing with mini

1270
00:44:07,040 --> 00:44:11,520
batches the quality of our gradient is

1271
00:44:09,520 --> 00:44:13,440
lower so the direction is not as

1272
00:44:11,520 --> 00:44:14,640
reliable it's not the actual gradient

1273
00:44:13,440 --> 00:44:16,480
direction

1274
00:44:14,640 --> 00:44:18,640
but the gradient direction is good

1275
00:44:16,480 --> 00:44:22,240
enough even when it's estimating on only

1276
00:44:18,640 --> 00:44:24,560
32 examples that it is useful and so

1277
00:44:22,240 --> 00:44:26,720
it's much better to have an approximate

1278
00:44:24,560 --> 00:44:29,040
gradient and just make more steps than

1279
00:44:26,720 --> 00:44:31,359
it is to evaluate the exact gradient and

1280
00:44:29,040 --> 00:44:34,400
take fewer steps so that's why in

1281
00:44:31,359 --> 00:44:37,839
practice uh this works quite well

1282
00:44:34,400 --> 00:44:37,839
so let's now continue the optimization

1283
00:44:38,319 --> 00:44:42,880
let me take out this lost item from here

1284
00:44:41,680 --> 00:44:45,839
and uh

1285
00:44:42,880 --> 00:44:50,079
place it over here at the end

1286
00:44:45,839 --> 00:44:51,520
okay so we're hovering around 2.5 or so

1287
00:44:50,079 --> 00:44:53,359
however this is only the loss for that

1288
00:44:51,520 --> 00:44:54,960
mini batch so let's actually evaluate

1289
00:44:53,359 --> 00:44:56,480
the loss

1290
00:44:54,960 --> 00:44:58,240
here

1291
00:44:56,480 --> 00:45:00,960
for all of x

1292
00:44:58,240 --> 00:45:03,040
and for all of y just so we have a

1293
00:45:00,960 --> 00:45:05,520
full sense of exactly how all the model

1294
00:45:03,040 --> 00:45:07,200
is doing right now

1295
00:45:05,520 --> 00:45:09,119
so right now we're at about 2.7 on the

1296
00:45:07,200 --> 00:45:10,160
entire training set

1297
00:45:09,119 --> 00:45:12,560
so let's

1298
00:45:10,160 --> 00:45:15,280
run the optimization for a while

1299
00:45:12,560 --> 00:45:17,520
okay right 2.6

1300
00:45:15,280 --> 00:45:20,400
2.57

1301
00:45:17,520 --> 00:45:20,400
2.53

1302
00:45:21,920 --> 00:45:25,040
okay

1303
00:45:22,800 --> 00:45:28,640
so one issue of course is we don't know

1304
00:45:25,040 --> 00:45:30,800
if we're stepping too slow or too fast

1305
00:45:28,640 --> 00:45:33,200
so this point one i just guessed it

1306
00:45:30,800 --> 00:45:34,880
so one question is how do you determine

1307
00:45:33,200 --> 00:45:37,359
this learning rate

1308
00:45:34,880 --> 00:45:39,440
and how do we gain confidence that we're

1309
00:45:37,359 --> 00:45:41,200
stepping in the right

1310
00:45:39,440 --> 00:45:43,520
sort of speed so i'll show you one way

1311
00:45:41,200 --> 00:45:46,079
to determine a reasonable learning rate

1312
00:45:43,520 --> 00:45:47,520
it works as follows let's reset our

1313
00:45:46,079 --> 00:45:49,760
parameters

1314
00:45:47,520 --> 00:45:51,040
to the initial

1315
00:45:49,760 --> 00:45:52,720
settings

1316
00:45:51,040 --> 00:45:54,960
and now let's

1317
00:45:52,720 --> 00:45:58,720
print in every step

1318
00:45:54,960 --> 00:46:00,960
but let's only do 10 steps or so

1319
00:45:58,720 --> 00:46:02,720
or maybe maybe 100 steps

1320
00:46:00,960 --> 00:46:03,839
we want to find like a very reasonable

1321
00:46:02,720 --> 00:46:05,839
set

1322
00:46:03,839 --> 00:46:07,920
search range if you will so for example

1323
00:46:05,839 --> 00:46:09,839
if this is like very low

1324
00:46:07,920 --> 00:46:11,359
then

1325
00:46:09,839 --> 00:46:13,280
we see that the loss is barely

1326
00:46:11,359 --> 00:46:15,599
decreasing so that's not

1327
00:46:13,280 --> 00:46:16,560
that's like too low basically so let's

1328
00:46:15,599 --> 00:46:18,720
try

1329
00:46:16,560 --> 00:46:20,160
this one

1330
00:46:18,720 --> 00:46:21,760
okay so we're decreasing the loss but

1331
00:46:20,160 --> 00:46:23,520
like not very quickly so that's a pretty

1332
00:46:21,760 --> 00:46:25,839
good low range

1333
00:46:23,520 --> 00:46:27,280
now let's reset it again

1334
00:46:25,839 --> 00:46:29,440
and now let's try to find the place at

1335
00:46:27,280 --> 00:46:32,880
which the loss kind of explodes

1336
00:46:29,440 --> 00:46:32,880
uh so maybe at negative one

1337
00:46:33,119 --> 00:46:36,880
okay we see that we're minimizing the

1338
00:46:34,960 --> 00:46:39,359
loss but you see how uh it's kind of

1339
00:46:36,880 --> 00:46:41,200
unstable it goes up and down quite a bit

1340
00:46:39,359 --> 00:46:44,319
um so negative one is probably like a

1341
00:46:41,200 --> 00:46:45,760
fast learning rate let's try negative

1342
00:46:44,319 --> 00:46:46,960
10.

1343
00:46:45,760 --> 00:46:48,880
okay so this

1344
00:46:46,960 --> 00:46:50,960
isn't optimizing this is not working

1345
00:46:48,880 --> 00:46:53,119
very well so negative 10 is way too big

1346
00:46:50,960 --> 00:46:54,800
negative one was already kind of big

1347
00:46:53,119 --> 00:46:56,400
um

1348
00:46:54,800 --> 00:46:57,599
so therefore

1349
00:46:56,400 --> 00:47:00,400
negative one was like somewhat

1350
00:46:57,599 --> 00:47:01,599
reasonable if i reset

1351
00:47:00,400 --> 00:47:03,359
so i'm thinking that the right learning

1352
00:47:01,599 --> 00:47:05,920
rate is somewhere between

1353
00:47:03,359 --> 00:47:06,960
uh negative zero point zero zero one and

1354
00:47:05,920 --> 00:47:08,240
um

1355
00:47:06,960 --> 00:47:10,000
negative one

1356
00:47:08,240 --> 00:47:13,119
so the way we can do this here is we can

1357
00:47:10,000 --> 00:47:14,319
use uh torch shot lens space

1358
00:47:13,119 --> 00:47:17,760
and we want to basically do something

1359
00:47:14,319 --> 00:47:19,119
like this between zero and one but

1360
00:47:17,760 --> 00:47:20,960
um

1361
00:47:19,119 --> 00:47:22,480
those number of steps is one more

1362
00:47:20,960 --> 00:47:26,319
parameter that's required let's do a

1363
00:47:22,480 --> 00:47:29,760
thousand steps this creates 1000

1364
00:47:26,319 --> 00:47:31,440
numbers between 0.01 and 1

1365
00:47:29,760 --> 00:47:33,599
but it doesn't really make sense to step

1366
00:47:31,440 --> 00:47:36,480
between these linearly so instead let me

1367
00:47:33,599 --> 00:47:39,200
create learning rate exponent

1368
00:47:36,480 --> 00:47:41,680
and instead of 0.001 this will be a

1369
00:47:39,200 --> 00:47:43,680
negative 3 and this will be a zero

1370
00:47:41,680 --> 00:47:45,440
and then the actual lrs that we want to

1371
00:47:43,680 --> 00:47:48,319
search over are going to be 10 to the

1372
00:47:45,440 --> 00:47:49,359
power of lre

1373
00:47:48,319 --> 00:47:51,359
so now what we're doing is we're

1374
00:47:49,359 --> 00:47:54,160
stepping linearly between the exponents

1375
00:47:51,359 --> 00:47:57,119
of these learning rates this is 0.001

1376
00:47:54,160 --> 00:47:58,560
and this is 1 because 10 to the power of

1377
00:47:57,119 --> 00:48:00,079
0 is 1.

1378
00:47:58,560 --> 00:48:02,079
and therefore we are spaced

1379
00:48:00,079 --> 00:48:03,440
exponentially in this interval

1380
00:48:02,079 --> 00:48:04,720
so these are the candidate learning

1381
00:48:03,440 --> 00:48:06,480
rates

1382
00:48:04,720 --> 00:48:07,760
that we want to sort of like search over

1383
00:48:06,480 --> 00:48:10,880
roughly

1384
00:48:07,760 --> 00:48:12,480
so now what we're going to do is

1385
00:48:10,880 --> 00:48:14,640
here we are going to run the

1386
00:48:12,480 --> 00:48:16,720
optimization for 1000 steps

1387
00:48:14,640 --> 00:48:19,520
and instead of using a fixed number

1388
00:48:16,720 --> 00:48:22,480
we are going to use learning rate

1389
00:48:19,520 --> 00:48:25,440
indexing into here lrs of i

1390
00:48:22,480 --> 00:48:25,440
and make this i

1391
00:48:25,680 --> 00:48:30,480
so basically let me reset this to be

1392
00:48:28,240 --> 00:48:32,079
again starting from random

1393
00:48:30,480 --> 00:48:33,280
creating these learning rates between

1394
00:48:32,079 --> 00:48:36,880
negative

1395
00:48:33,280 --> 00:48:39,520
zero points between 0.001 and um

1396
00:48:36,880 --> 00:48:41,440
one but exponentially stopped

1397
00:48:39,520 --> 00:48:43,280
and here what we're doing is we're

1398
00:48:41,440 --> 00:48:45,599
iterating a thousand times

1399
00:48:43,280 --> 00:48:48,079
we're going to use the learning rate

1400
00:48:45,599 --> 00:48:50,480
um that's in the beginning very very low

1401
00:48:48,079 --> 00:48:52,240
in the beginning is going to be 0.001

1402
00:48:50,480 --> 00:48:53,440
but by the end it's going to be

1403
00:48:52,240 --> 00:48:55,040
1.

1404
00:48:53,440 --> 00:48:57,119
and then we're going to step with that

1405
00:48:55,040 --> 00:48:58,720
learning rate

1406
00:48:57,119 --> 00:49:02,559
and now what we want to do is we want to

1407
00:48:58,720 --> 00:49:02,559
keep track of the uh

1408
00:49:04,240 --> 00:49:07,920
learning rates that we used and we want

1409
00:49:05,920 --> 00:49:09,839
to look at the losses

1410
00:49:07,920 --> 00:49:12,319
that resulted

1411
00:49:09,839 --> 00:49:14,160
and so here let me

1412
00:49:12,319 --> 00:49:16,960
track stats

1413
00:49:14,160 --> 00:49:20,240
so lri.append lr

1414
00:49:16,960 --> 00:49:22,400
and um lost side that append

1415
00:49:20,240 --> 00:49:23,680
loss that item

1416
00:49:22,400 --> 00:49:27,200
okay

1417
00:49:23,680 --> 00:49:30,400
so again reset everything

1418
00:49:27,200 --> 00:49:31,760
and then run

1419
00:49:30,400 --> 00:49:33,040
and so basically we started with a very

1420
00:49:31,760 --> 00:49:35,119
low learning rate and we went all the

1421
00:49:33,040 --> 00:49:35,920
way up to a learning rate of negative

1422
00:49:35,119 --> 00:49:37,680
one

1423
00:49:35,920 --> 00:49:38,960
and now what we can do is we can plt

1424
00:49:37,680 --> 00:49:41,280
that plot

1425
00:49:38,960 --> 00:49:43,520
and we can plot the two so we can plot

1426
00:49:41,280 --> 00:49:46,160
the learning rates on the x-axis and the

1427
00:49:43,520 --> 00:49:48,000
losses we saw on the y-axis

1428
00:49:46,160 --> 00:49:50,160
and often you're going to find that your

1429
00:49:48,000 --> 00:49:52,000
plot looks something like this

1430
00:49:50,160 --> 00:49:53,440
where in the beginning

1431
00:49:52,000 --> 00:49:54,960
you had very low learning rates so

1432
00:49:53,440 --> 00:49:57,040
basically anything

1433
00:49:54,960 --> 00:50:00,079
barely anything happened

1434
00:49:57,040 --> 00:50:01,440
then we got to like a nice spot here

1435
00:50:00,079 --> 00:50:02,960
and then as we increase the learning

1436
00:50:01,440 --> 00:50:04,319
rate enough

1437
00:50:02,960 --> 00:50:05,920
we basically started to be kind of

1438
00:50:04,319 --> 00:50:07,680
unstable here

1439
00:50:05,920 --> 00:50:10,079
so a good learning rate turns out to be

1440
00:50:07,680 --> 00:50:13,359
somewhere around here

1441
00:50:10,079 --> 00:50:14,720
um and because we have lri here

1442
00:50:13,359 --> 00:50:16,640
um

1443
00:50:14,720 --> 00:50:18,960
we actually may want to

1444
00:50:16,640 --> 00:50:18,960
um

1445
00:50:19,119 --> 00:50:22,640
do not lr

1446
00:50:20,960 --> 00:50:25,359
not the learning rate but the exponent

1447
00:50:22,640 --> 00:50:27,839
so that would be the lre at i is maybe

1448
00:50:25,359 --> 00:50:30,079
what we want to log so let me reset this

1449
00:50:27,839 --> 00:50:32,160
and redo that calculation

1450
00:50:30,079 --> 00:50:34,079
but now on the x axis we have the

1451
00:50:32,160 --> 00:50:36,079
[Music]

1452
00:50:34,079 --> 00:50:37,440
exponent of the learning rate and so we

1453
00:50:36,079 --> 00:50:38,800
can see the exponent of the learning

1454
00:50:37,440 --> 00:50:41,359
rate that is good to use it would be

1455
00:50:38,800 --> 00:50:42,640
sort of like roughly in the valley here

1456
00:50:41,359 --> 00:50:44,880
because here the learning rates are just

1457
00:50:42,640 --> 00:50:46,240
way too low and then here where we

1458
00:50:44,880 --> 00:50:47,920
expect relatively good learning rates

1459
00:50:46,240 --> 00:50:50,480
somewhere here and then here things are

1460
00:50:47,920 --> 00:50:51,920
starting to explode so somewhere around

1461
00:50:50,480 --> 00:50:54,160
negative one x the exponent of the

1462
00:50:51,920 --> 00:50:58,079
learning rate is a pretty good setting

1463
00:50:54,160 --> 00:50:59,839
and 10 to the negative one is 0.1 so 0.1

1464
00:50:58,079 --> 00:51:02,160
is actually 0.1 was actually a fairly

1465
00:50:59,839 --> 00:51:03,839
good learning rate around here

1466
00:51:02,160 --> 00:51:05,359
and that's what we had in the initial

1467
00:51:03,839 --> 00:51:06,559
setting

1468
00:51:05,359 --> 00:51:09,520
but that's roughly how you would

1469
00:51:06,559 --> 00:51:12,800
determine it and so here now we can take

1470
00:51:09,520 --> 00:51:15,599
out the tracking of these

1471
00:51:12,800 --> 00:51:18,000
and we can just simply set lr to be 10

1472
00:51:15,599 --> 00:51:20,800
to the negative one or

1473
00:51:18,000 --> 00:51:21,920
basically otherwise 0.1 as it was before

1474
00:51:20,800 --> 00:51:23,440
and now we have some confidence that

1475
00:51:21,920 --> 00:51:24,400
this is actually a fairly good learning

1476
00:51:23,440 --> 00:51:26,160
rate

1477
00:51:24,400 --> 00:51:27,760
and so now we can do is we can crank up

1478
00:51:26,160 --> 00:51:30,720
the iterations

1479
00:51:27,760 --> 00:51:32,079
we can reset our optimization

1480
00:51:30,720 --> 00:51:34,640
and

1481
00:51:32,079 --> 00:51:36,480
we can run for a pretty long time using

1482
00:51:34,640 --> 00:51:38,559
this learning rate

1483
00:51:36,480 --> 00:51:40,079
oops and we don't want to print that's

1484
00:51:38,559 --> 00:51:42,079
way too much printing

1485
00:51:40,079 --> 00:51:45,359
so let me again reset

1486
00:51:42,079 --> 00:51:45,359
and run ten thousand stops

1487
00:51:48,480 --> 00:51:55,280
okay so we're 0.2 2.48 roughly let's run

1488
00:51:52,079 --> 00:51:55,280
another 10 000 steps

1489
00:51:58,640 --> 00:52:02,400
2.46

1490
00:52:00,240 --> 00:52:03,680
and now let's do one learning rate decay

1491
00:52:02,400 --> 00:52:05,920
what this means is we're going to take

1492
00:52:03,680 --> 00:52:08,480
our learning rate and we're going to 10x

1493
00:52:05,920 --> 00:52:10,400
lower it and so we're at the late stages

1494
00:52:08,480 --> 00:52:12,880
of training potentially and we may want

1495
00:52:10,400 --> 00:52:16,800
to go a bit slower let's do one more

1496
00:52:12,880 --> 00:52:16,800
actually at 0.1 just to see if

1497
00:52:16,960 --> 00:52:20,640
we're making a dent here

1498
00:52:18,880 --> 00:52:21,680
okay we're still making dent and by the

1499
00:52:20,640 --> 00:52:24,240
way the

1500
00:52:21,680 --> 00:52:27,119
bi-gram loss that we achieved last video

1501
00:52:24,240 --> 00:52:29,200
was 2.45 so we've already surpassed the

1502
00:52:27,119 --> 00:52:30,400
bi-gram model

1503
00:52:29,200 --> 00:52:32,480
and once i get a sense that this is

1504
00:52:30,400 --> 00:52:34,240
actually kind of starting to plateau off

1505
00:52:32,480 --> 00:52:36,240
people like to do as i mentioned this

1506
00:52:34,240 --> 00:52:37,680
learning rate decay so let's try to

1507
00:52:36,240 --> 00:52:40,559
decay the loss

1508
00:52:37,680 --> 00:52:40,559
the learning rate i mean

1509
00:52:42,160 --> 00:52:48,079
and we achieve it about 2.3 now

1510
00:52:46,400 --> 00:52:50,000
obviously this is janky and not exactly

1511
00:52:48,079 --> 00:52:51,200
how you would train it in production but

1512
00:52:50,000 --> 00:52:53,359
this is roughly what you're going

1513
00:52:51,200 --> 00:52:54,800
through you first find a decent learning

1514
00:52:53,359 --> 00:52:55,680
rate using the approach that i showed

1515
00:52:54,800 --> 00:52:57,200
you

1516
00:52:55,680 --> 00:52:58,720
then you start with that learning rate

1517
00:52:57,200 --> 00:53:00,240
and you train for a while

1518
00:52:58,720 --> 00:53:02,240
and then at the end people like to do a

1519
00:53:00,240 --> 00:53:03,920
learning rate decay where you decay the

1520
00:53:02,240 --> 00:53:06,240
learning rate by say a factor of 10 and

1521
00:53:03,920 --> 00:53:08,640
you do a few more steps and then you get

1522
00:53:06,240 --> 00:53:11,599
a trained network roughly speaking

1523
00:53:08,640 --> 00:53:13,520
so we've achieved 2.3 and dramatically

1524
00:53:11,599 --> 00:53:15,359
improved on the bi-gram language model

1525
00:53:13,520 --> 00:53:17,119
using this simple neural net as

1526
00:53:15,359 --> 00:53:20,400
described here

1527
00:53:17,119 --> 00:53:22,480
using these 3 400 parameters now there's

1528
00:53:20,400 --> 00:53:24,079
something we have to be careful with

1529
00:53:22,480 --> 00:53:26,319
i said that we have a better model

1530
00:53:24,079 --> 00:53:28,880
because we are achieving a lower loss

1531
00:53:26,319 --> 00:53:30,640
2.3 much lower than 2.45 with the

1532
00:53:28,880 --> 00:53:32,559
bi-gram model previously

1533
00:53:30,640 --> 00:53:36,480
now that's not exactly true and the

1534
00:53:32,559 --> 00:53:36,480
reason that's not true is that

1535
00:53:37,200 --> 00:53:41,040
this is actually fairly small model but

1536
00:53:39,359 --> 00:53:42,319
these models can get larger and larger

1537
00:53:41,040 --> 00:53:44,480
if you keep adding neurons and

1538
00:53:42,319 --> 00:53:45,839
parameters so you can imagine that we

1539
00:53:44,480 --> 00:53:47,760
don't potentially have a thousand

1540
00:53:45,839 --> 00:53:49,839
parameters we could have 10 000 or 100

1541
00:53:47,760 --> 00:53:51,200
000 or millions of parameters

1542
00:53:49,839 --> 00:53:52,800
and as the capacity of the neural

1543
00:53:51,200 --> 00:53:54,960
network grows

1544
00:53:52,800 --> 00:53:56,880
it becomes more and more capable of

1545
00:53:54,960 --> 00:53:59,040
overfitting your training set

1546
00:53:56,880 --> 00:54:00,800
what that means is that the loss on the

1547
00:53:59,040 --> 00:54:03,440
training set on the data that you're

1548
00:54:00,800 --> 00:54:04,800
training on will become very very low as

1549
00:54:03,440 --> 00:54:06,240
low as zero

1550
00:54:04,800 --> 00:54:09,040
but all that the model is doing is

1551
00:54:06,240 --> 00:54:10,640
memorizing your training set verbatim so

1552
00:54:09,040 --> 00:54:12,400
if you take that model and it looks like

1553
00:54:10,640 --> 00:54:14,319
it's working really well but you try to

1554
00:54:12,400 --> 00:54:16,400
sample from it you will basically only

1555
00:54:14,319 --> 00:54:19,040
get examples exactly as they are in the

1556
00:54:16,400 --> 00:54:20,480
training set you won't get any new data

1557
00:54:19,040 --> 00:54:23,520
in addition to that if you try to

1558
00:54:20,480 --> 00:54:24,880
evaluate the loss on some withheld names

1559
00:54:23,520 --> 00:54:26,400
or other words

1560
00:54:24,880 --> 00:54:29,040
you will actually see that the loss on

1561
00:54:26,400 --> 00:54:30,800
those can be very high and so basically

1562
00:54:29,040 --> 00:54:32,880
it's not a good model

1563
00:54:30,800 --> 00:54:35,280
so the standard in the field is to split

1564
00:54:32,880 --> 00:54:37,680
up your data set into three splits as we

1565
00:54:35,280 --> 00:54:40,079
call them we have the training split the

1566
00:54:37,680 --> 00:54:42,319
dev split or the validation split

1567
00:54:40,079 --> 00:54:43,200
and the test split

1568
00:54:42,319 --> 00:54:45,520
so

1569
00:54:43,200 --> 00:54:49,920
training split

1570
00:54:45,520 --> 00:54:53,119
test or um sorry dev or validation split

1571
00:54:49,920 --> 00:54:54,880
and test split and typically this would

1572
00:54:53,119 --> 00:54:56,400
be say eighty percent of your data set

1573
00:54:54,880 --> 00:54:58,319
this could be ten percent and this ten

1574
00:54:56,400 --> 00:55:00,000
percent roughly

1575
00:54:58,319 --> 00:55:01,359
so you have these three splits of the

1576
00:55:00,000 --> 00:55:02,800
data

1577
00:55:01,359 --> 00:55:04,640
now these eighty percent of your

1578
00:55:02,800 --> 00:55:06,880
trainings of the data set the training

1579
00:55:04,640 --> 00:55:08,640
set is used to optimize the parameters

1580
00:55:06,880 --> 00:55:10,720
of the model just like we're doing here

1581
00:55:08,640 --> 00:55:12,480
using gradient descent

1582
00:55:10,720 --> 00:55:14,720
these 10 percent of the

1583
00:55:12,480 --> 00:55:16,800
examples the dev or validation split

1584
00:55:14,720 --> 00:55:19,119
they're used for development over all

1585
00:55:16,800 --> 00:55:21,040
the hyper parameters of your model so

1586
00:55:19,119 --> 00:55:22,640
hyper parameters are for example the

1587
00:55:21,040 --> 00:55:24,720
size of this hidden layer

1588
00:55:22,640 --> 00:55:26,480
the size of the embedding so this is a

1589
00:55:24,720 --> 00:55:28,000
hundred or a two for us but we could try

1590
00:55:26,480 --> 00:55:29,920
different things

1591
00:55:28,000 --> 00:55:31,920
the strength of the regularization which

1592
00:55:29,920 --> 00:55:33,119
we aren't using yet so far

1593
00:55:31,920 --> 00:55:34,720
so there's lots of different hybrid

1594
00:55:33,119 --> 00:55:36,800
parameters and settings that go into

1595
00:55:34,720 --> 00:55:38,800
defining your neural net and you can try

1596
00:55:36,800 --> 00:55:41,119
many different variations of them and

1597
00:55:38,800 --> 00:55:43,119
see whichever one works best on your

1598
00:55:41,119 --> 00:55:45,760
validation split

1599
00:55:43,119 --> 00:55:48,640
so this is used to train the parameters

1600
00:55:45,760 --> 00:55:51,680
this is used to train the hyperprimers

1601
00:55:48,640 --> 00:55:53,040
and test split is used to evaluate

1602
00:55:51,680 --> 00:55:54,079
basically the performance of the model

1603
00:55:53,040 --> 00:55:55,920
at the end

1604
00:55:54,079 --> 00:55:57,839
so we're only evaluating the loss on the

1605
00:55:55,920 --> 00:56:00,400
test plate very very sparingly and very

1606
00:55:57,839 --> 00:56:02,559
few times because every single time you

1607
00:56:00,400 --> 00:56:03,839
evaluate your test loss and you learn

1608
00:56:02,559 --> 00:56:06,480
something from it

1609
00:56:03,839 --> 00:56:08,000
you are basically starting to also train

1610
00:56:06,480 --> 00:56:10,480
on the test split

1611
00:56:08,000 --> 00:56:11,599
so you are only allowed to test the loss

1612
00:56:10,480 --> 00:56:13,119
on a test

1613
00:56:11,599 --> 00:56:15,680
set

1614
00:56:13,119 --> 00:56:17,599
very very few times otherwise you risk

1615
00:56:15,680 --> 00:56:19,520
overfitting to it as well as you

1616
00:56:17,599 --> 00:56:22,000
experiment on your model

1617
00:56:19,520 --> 00:56:25,040
so let's also split up our training data

1618
00:56:22,000 --> 00:56:26,559
into train dev and test and then we are

1619
00:56:25,040 --> 00:56:28,240
going to train on train

1620
00:56:26,559 --> 00:56:31,119
and only evaluate on tests very very

1621
00:56:28,240 --> 00:56:33,440
sparingly okay so here we go

1622
00:56:31,119 --> 00:56:36,160
here is where we took all the words and

1623
00:56:33,440 --> 00:56:38,240
put them into x and y tensors

1624
00:56:36,160 --> 00:56:39,920
so instead let me create a new cell here

1625
00:56:38,240 --> 00:56:40,799
and let me just copy paste some code

1626
00:56:39,920 --> 00:56:43,040
here

1627
00:56:40,799 --> 00:56:45,359
because i don't think it's that

1628
00:56:43,040 --> 00:56:46,480
complex but

1629
00:56:45,359 --> 00:56:47,599
we're going to try to save a little bit

1630
00:56:46,480 --> 00:56:49,599
of time

1631
00:56:47,599 --> 00:56:51,520
i'm converting this to be a function now

1632
00:56:49,599 --> 00:56:54,319
and this function takes some list of

1633
00:56:51,520 --> 00:56:56,559
words and builds the arrays x and y for

1634
00:56:54,319 --> 00:56:59,280
those words only

1635
00:56:56,559 --> 00:57:01,359
and then here i am shuffling up all the

1636
00:56:59,280 --> 00:57:02,240
words so these are the input words that

1637
00:57:01,359 --> 00:57:04,880
we get

1638
00:57:02,240 --> 00:57:06,640
we are randomly shuffling them all up

1639
00:57:04,880 --> 00:57:08,160
and then um

1640
00:57:06,640 --> 00:57:09,920
we're going to

1641
00:57:08,160 --> 00:57:11,680
set n1 to be

1642
00:57:09,920 --> 00:57:13,760
the number of examples that there's 80

1643
00:57:11,680 --> 00:57:14,559
of the words and n2 to be

1644
00:57:13,760 --> 00:57:16,960
90

1645
00:57:14,559 --> 00:57:21,599
of the way of the words so basically if

1646
00:57:16,960 --> 00:57:24,319
len of words is 32 000 n1 is

1647
00:57:21,599 --> 00:57:28,160
well sorry i should probably run this

1648
00:57:24,319 --> 00:57:29,839
n1 is 25 000 and n2 is 28 000.

1649
00:57:28,160 --> 00:57:32,000
and so here we see that

1650
00:57:29,839 --> 00:57:33,920
i'm calling build data set to build the

1651
00:57:32,000 --> 00:57:36,720
training set x and y

1652
00:57:33,920 --> 00:57:39,680
by indexing into up to and one so we're

1653
00:57:36,720 --> 00:57:42,480
going to have only 25 000 training words

1654
00:57:39,680 --> 00:57:44,240
and then we're going to have

1655
00:57:42,480 --> 00:57:46,079
roughly

1656
00:57:44,240 --> 00:57:49,520
n2 minus n1

1657
00:57:46,079 --> 00:57:53,359
3 3 000 validation examples or dev

1658
00:57:49,520 --> 00:57:53,359
examples and we're going to have

1659
00:57:53,520 --> 00:57:58,160
when of words basically minus and two

1660
00:57:57,280 --> 00:58:00,960
or

1661
00:57:58,160 --> 00:58:03,680
3 204 examples

1662
00:58:00,960 --> 00:58:04,799
here for the test set

1663
00:58:03,680 --> 00:58:07,119
so

1664
00:58:04,799 --> 00:58:10,480
now we have x's and y's

1665
00:58:07,119 --> 00:58:10,480
for all those three splits

1666
00:58:13,359 --> 00:58:17,520
oh yeah i'm printing their size here

1667
00:58:14,559 --> 00:58:17,520
inside the function as well

1668
00:58:18,880 --> 00:58:22,559
but here we don't have words but these

1669
00:58:20,559 --> 00:58:25,200
are already the individual examples made

1670
00:58:22,559 --> 00:58:27,599
from those words

1671
00:58:25,200 --> 00:58:31,200
so let's now scroll down here

1672
00:58:27,599 --> 00:58:33,200
and the data set now for training is

1673
00:58:31,200 --> 00:58:37,839
more like this

1674
00:58:33,200 --> 00:58:37,839
and then when we reset the network

1675
00:58:38,319 --> 00:58:43,920
when we're training we're only going to

1676
00:58:40,079 --> 00:58:47,760
be training using x train

1677
00:58:43,920 --> 00:58:49,280
x train and y train

1678
00:58:47,760 --> 00:58:51,839
so that's the only thing we're training

1679
00:58:49,280 --> 00:58:51,839
on

1680
00:58:57,680 --> 00:59:02,160
let's see where we are on the

1681
00:59:00,160 --> 00:59:06,240
single batch

1682
00:59:02,160 --> 00:59:06,240
let's now train maybe a few more steps

1683
00:59:08,000 --> 00:59:11,200
training neural networks can take a

1684
00:59:09,359 --> 00:59:12,720
while usually you don't do it inline you

1685
00:59:11,200 --> 00:59:15,280
launch a bunch of jobs and you wait for

1686
00:59:12,720 --> 00:59:16,640
them to finish um can take in multiple

1687
00:59:15,280 --> 00:59:19,839
days and so on

1688
00:59:16,640 --> 00:59:19,839
luckily this is a very small network

1689
00:59:20,960 --> 00:59:25,680
okay so the loss is pretty good

1690
00:59:23,280 --> 00:59:27,520
oh we accidentally used a learning rate

1691
00:59:25,680 --> 00:59:29,760
that is way too low

1692
00:59:27,520 --> 00:59:34,359
so let me actually come back

1693
00:59:29,760 --> 00:59:34,359
we use the decay learning rate of 0.01

1694
00:59:35,119 --> 00:59:39,280
so this will train much faster

1695
00:59:37,119 --> 00:59:42,319
and then here when we evaluate

1696
00:59:39,280 --> 00:59:43,599
let's use the dep set here

1697
00:59:42,319 --> 00:59:47,119
xdev

1698
00:59:43,599 --> 00:59:48,720
and ydev to evaluate the loss

1699
00:59:47,119 --> 00:59:50,640
okay

1700
00:59:48,720 --> 00:59:54,640
and let's now decay the learning rate

1701
00:59:50,640 --> 00:59:54,640
and only do say 10 000 examples

1702
00:59:55,440 --> 00:59:59,200
and let's evaluate the dev loss

1703
00:59:57,680 --> 01:00:01,440
ones here

1704
00:59:59,200 --> 01:00:02,640
okay so we're getting about 2.3 on dev

1705
01:00:01,440 --> 01:00:05,440
and so the neural network when it was

1706
01:00:02,640 --> 01:00:08,000
training did not see these dev examples

1707
01:00:05,440 --> 01:00:10,160
it hasn't optimized on them and yet

1708
01:00:08,000 --> 01:00:12,319
when we evaluate the loss on these dev

1709
01:00:10,160 --> 01:00:16,079
we actually get a pretty decent loss

1710
01:00:12,319 --> 01:00:18,960
and so we can also look at what the

1711
01:00:16,079 --> 01:00:20,559
loss is on all of training set

1712
01:00:18,960 --> 01:00:22,319
oops

1713
01:00:20,559 --> 01:00:24,640
and so we see that the training and the

1714
01:00:22,319 --> 01:00:25,760
dev loss are about equal so we're not

1715
01:00:24,640 --> 01:00:28,319
over fitting

1716
01:00:25,760 --> 01:00:31,119
um this model is not powerful enough to

1717
01:00:28,319 --> 01:00:33,839
just be purely memorizing the data and

1718
01:00:31,119 --> 01:00:35,920
so far we are what's called underfitting

1719
01:00:33,839 --> 01:00:38,160
because the training loss and the dev or

1720
01:00:35,920 --> 01:00:40,319
test losses are roughly equal so what

1721
01:00:38,160 --> 01:00:43,440
that typically means is that our network

1722
01:00:40,319 --> 01:00:46,079
is very tiny very small and we expect to

1723
01:00:43,440 --> 01:00:47,920
make performance improvements by scaling

1724
01:00:46,079 --> 01:00:50,160
up the size of this neural net so let's

1725
01:00:47,920 --> 01:00:51,599
do that now so let's come over here

1726
01:00:50,160 --> 01:00:53,839
and let's increase the size of the

1727
01:00:51,599 --> 01:00:55,200
neural net the easiest way to do this is

1728
01:00:53,839 --> 01:00:56,960
we can come here to the hidden layer

1729
01:00:55,200 --> 01:00:58,960
which currently has 100 neurons and

1730
01:00:56,960 --> 01:01:00,640
let's just bump this up so let's do 300

1731
01:00:58,960 --> 01:01:03,440
neurons

1732
01:01:00,640 --> 01:01:05,920
and then this is also 300 biases and

1733
01:01:03,440 --> 01:01:07,280
here we have 300 inputs into the final

1734
01:01:05,920 --> 01:01:08,640
layer

1735
01:01:07,280 --> 01:01:10,880
so

1736
01:01:08,640 --> 01:01:12,240
let's initialize our neural net we now

1737
01:01:10,880 --> 01:01:13,520
have ten thousand ex ten thousand

1738
01:01:12,240 --> 01:01:15,839
parameters instead of three thousand

1739
01:01:13,520 --> 01:01:18,240
parameters

1740
01:01:15,839 --> 01:01:19,599
and then we're not using this

1741
01:01:18,240 --> 01:01:23,280
and then here what i'd like to do is i'd

1742
01:01:19,599 --> 01:01:24,640
like to actually uh keep track of uh

1743
01:01:23,280 --> 01:01:26,880
tap

1744
01:01:24,640 --> 01:01:26,880
um

1745
01:01:27,599 --> 01:01:30,880
okay let's just do this let's keep stats

1746
01:01:29,839 --> 01:01:34,640
again

1747
01:01:30,880 --> 01:01:34,640
and here when we're keeping track of the

1748
01:01:34,880 --> 01:01:40,880
loss let's just also keep track of the

1749
01:01:37,839 --> 01:01:44,160
steps and let's just have i here

1750
01:01:40,880 --> 01:01:46,079
and let's train on thirty thousand

1751
01:01:44,160 --> 01:01:48,160
or rather say

1752
01:01:46,079 --> 01:01:51,200
okay let's try thirty thousand

1753
01:01:48,160 --> 01:01:52,480
and we are at point one

1754
01:01:51,200 --> 01:01:54,880
and

1755
01:01:52,480 --> 01:01:57,280
we should be able to run this

1756
01:01:54,880 --> 01:01:59,400
and optimize the neural net

1757
01:01:57,280 --> 01:02:00,960
and then here basically i want to

1758
01:01:59,400 --> 01:02:02,400
plt.plot

1759
01:02:00,960 --> 01:02:06,200
the steps

1760
01:02:02,400 --> 01:02:06,200
against the loss

1761
01:02:09,040 --> 01:02:13,520
so these are the x's and y's

1762
01:02:11,520 --> 01:02:15,359
and this is

1763
01:02:13,520 --> 01:02:16,559
the loss function and how it's being

1764
01:02:15,359 --> 01:02:18,160
optimized

1765
01:02:16,559 --> 01:02:19,839
now you see that there's quite a bit of

1766
01:02:18,160 --> 01:02:21,760
thickness to this and that's because we

1767
01:02:19,839 --> 01:02:23,440
are optimizing over these mini batches

1768
01:02:21,760 --> 01:02:24,480
and the mini batches create a little bit

1769
01:02:23,440 --> 01:02:25,839
of noise

1770
01:02:24,480 --> 01:02:28,079
in this

1771
01:02:25,839 --> 01:02:30,559
uh where are we in the def set we are at

1772
01:02:28,079 --> 01:02:32,079
2.5 so we still haven't optimized this

1773
01:02:30,559 --> 01:02:33,280
neural net very well

1774
01:02:32,079 --> 01:02:34,799
and that's probably because we made it

1775
01:02:33,280 --> 01:02:36,400
bigger it might take longer for this

1776
01:02:34,799 --> 01:02:37,520
neural net to converge

1777
01:02:36,400 --> 01:02:40,799
um

1778
01:02:37,520 --> 01:02:42,799
and so let's continue training

1779
01:02:40,799 --> 01:02:46,000
um

1780
01:02:42,799 --> 01:02:46,000
yeah let's just continue training

1781
01:02:46,559 --> 01:02:49,280
one possibility is that the batch size

1782
01:02:48,000 --> 01:02:51,680
is so low

1783
01:02:49,280 --> 01:02:53,359
that uh we just have way too much noise

1784
01:02:51,680 --> 01:02:54,720
in the training and we may want to

1785
01:02:53,359 --> 01:02:57,359
increase the batch size so that we have

1786
01:02:54,720 --> 01:02:59,680
a bit more um correct gradient and we're

1787
01:02:57,359 --> 01:03:03,839
not thrashing too much and we can

1788
01:02:59,680 --> 01:03:03,839
actually like optimize more properly

1789
01:03:07,920 --> 01:03:10,480
okay

1790
01:03:08,799 --> 01:03:13,200
this will now become meaningless because

1791
01:03:10,480 --> 01:03:14,880
we've reinitialized these so

1792
01:03:13,200 --> 01:03:16,960
yeah this looks not

1793
01:03:14,880 --> 01:03:18,559
pleasing right now but there probably is

1794
01:03:16,960 --> 01:03:20,319
like a tiny improvement but it's so hard

1795
01:03:18,559 --> 01:03:22,559
to tell

1796
01:03:20,319 --> 01:03:25,440
let's go again

1797
01:03:22,559 --> 01:03:25,440
2.52

1798
01:03:25,520 --> 01:03:30,599
let's try to decrease the learning rate

1799
01:03:27,039 --> 01:03:30,599
by factor two

1800
01:03:50,000 --> 01:03:55,880
okay we're at 2.32

1801
01:03:52,240 --> 01:03:55,880
let's continue training

1802
01:04:05,680 --> 01:04:08,960
we basically expect to see a lower loss

1803
01:04:07,520 --> 01:04:10,880
than what we had before because now we

1804
01:04:08,960 --> 01:04:12,799
have a much much bigger model and we

1805
01:04:10,880 --> 01:04:14,079
were under fitting so we'd expect that

1806
01:04:12,799 --> 01:04:16,079
increasing the size of the model should

1807
01:04:14,079 --> 01:04:18,400
help the neural net

1808
01:04:16,079 --> 01:04:19,440
2.32 okay so that's not happening too

1809
01:04:18,400 --> 01:04:21,200
well

1810
01:04:19,440 --> 01:04:23,599
now one other concern is that even

1811
01:04:21,200 --> 01:04:25,520
though we've made the 10h layer here or

1812
01:04:23,599 --> 01:04:26,720
the hidden layer much much bigger it

1813
01:04:25,520 --> 01:04:28,799
could be that the bottleneck of the

1814
01:04:26,720 --> 01:04:30,799
network right now are these embeddings

1815
01:04:28,799 --> 01:04:32,079
that are two dimensional it can be that

1816
01:04:30,799 --> 01:04:34,079
we're just cramming way too many

1817
01:04:32,079 --> 01:04:36,480
characters into just two dimensions and

1818
01:04:34,079 --> 01:04:38,720
the neural net is not able to really use

1819
01:04:36,480 --> 01:04:39,920
that space effectively and that that is

1820
01:04:38,720 --> 01:04:42,319
sort of like the bottleneck to our

1821
01:04:39,920 --> 01:04:45,039
network's performance

1822
01:04:42,319 --> 01:04:46,400
okay 2.23 so just by decreasing the

1823
01:04:45,039 --> 01:04:48,160
learning rate i was able to make quite a

1824
01:04:46,400 --> 01:04:50,400
bit of progress let's run this one more

1825
01:04:48,160 --> 01:04:50,400
time

1826
01:04:51,599 --> 01:04:55,839
and then evaluate the training and the

1827
01:04:53,280 --> 01:04:55,839
dev loss

1828
01:04:56,640 --> 01:05:00,880
now one more thing after training that

1829
01:04:58,160 --> 01:05:02,720
i'd like to do is i'd like to visualize

1830
01:05:00,880 --> 01:05:05,599
the um

1831
01:05:02,720 --> 01:05:07,920
embedding vectors for these

1832
01:05:05,599 --> 01:05:09,920
characters before we scale up the

1833
01:05:07,920 --> 01:05:11,280
embedding size from two

1834
01:05:09,920 --> 01:05:13,520
because we'd like to make uh this

1835
01:05:11,280 --> 01:05:15,280
bottleneck potentially go away

1836
01:05:13,520 --> 01:05:17,280
but once i make this greater than two we

1837
01:05:15,280 --> 01:05:18,240
won't be able to visualize them

1838
01:05:17,280 --> 01:05:21,520
so here

1839
01:05:18,240 --> 01:05:24,160
okay we're at 2.23 and 2.24

1840
01:05:21,520 --> 01:05:25,520
so um we're not improving much more and

1841
01:05:24,160 --> 01:05:28,480
maybe the bottleneck now is the

1842
01:05:25,520 --> 01:05:29,920
character embedding size which is two

1843
01:05:28,480 --> 01:05:31,039
so here i have a bunch of code that will

1844
01:05:29,920 --> 01:05:34,000
create a figure

1845
01:05:31,039 --> 01:05:35,520
and then we're going to visualize

1846
01:05:34,000 --> 01:05:36,720
the embeddings that were trained by the

1847
01:05:35,520 --> 01:05:38,480
neural net

1848
01:05:36,720 --> 01:05:40,400
on these characters because right now

1849
01:05:38,480 --> 01:05:42,240
the embedding has just two so we can

1850
01:05:40,400 --> 01:05:43,920
visualize all the characters with the x

1851
01:05:42,240 --> 01:05:46,079
and the y coordinates as the two

1852
01:05:43,920 --> 01:05:47,760
embedding locations for each of these

1853
01:05:46,079 --> 01:05:50,000
characters

1854
01:05:47,760 --> 01:05:51,760
and so here are the x coordinates and

1855
01:05:50,000 --> 01:05:52,799
the y coordinates which are the columns

1856
01:05:51,760 --> 01:05:55,440
of c

1857
01:05:52,799 --> 01:05:58,319
and then for each one i also include the

1858
01:05:55,440 --> 01:05:59,839
text of the little character

1859
01:05:58,319 --> 01:06:02,240
so here what we see is actually kind of

1860
01:05:59,839 --> 01:06:02,240
interesting

1861
01:06:02,400 --> 01:06:06,160
the network has basically learned to

1862
01:06:04,480 --> 01:06:08,319
separate out the characters and cluster

1863
01:06:06,160 --> 01:06:09,839
them a little bit uh so for example you

1864
01:06:08,319 --> 01:06:12,799
see how the vowels

1865
01:06:09,839 --> 01:06:14,240
a e i o u are clustered up here

1866
01:06:12,799 --> 01:06:16,160
so that's telling us that is that the

1867
01:06:14,240 --> 01:06:17,440
neural net treats these is very similar

1868
01:06:16,160 --> 01:06:18,559
right because when they feed into the

1869
01:06:17,440 --> 01:06:20,720
neural net

1870
01:06:18,559 --> 01:06:22,640
the embedding uh for all these

1871
01:06:20,720 --> 01:06:23,839
characters is very similar and so the

1872
01:06:22,640 --> 01:06:25,760
neural net thinks that they're very

1873
01:06:23,839 --> 01:06:27,359
similar and kind of like interchangeable

1874
01:06:25,760 --> 01:06:29,200
if that makes sense

1875
01:06:27,359 --> 01:06:31,119
um

1876
01:06:29,200 --> 01:06:33,440
then the the points that are like really

1877
01:06:31,119 --> 01:06:35,680
far away are for example q q is kind of

1878
01:06:33,440 --> 01:06:36,640
treated as an exception and q has a very

1879
01:06:35,680 --> 01:06:38,720
special

1880
01:06:36,640 --> 01:06:40,400
embedding vector so to speak

1881
01:06:38,720 --> 01:06:42,480
similarly dot which is a special

1882
01:06:40,400 --> 01:06:44,240
character is all the way out here

1883
01:06:42,480 --> 01:06:46,640
and a lot of the other letters are sort

1884
01:06:44,240 --> 01:06:48,000
of like clustered up here and so it's

1885
01:06:46,640 --> 01:06:50,079
kind of interesting that there's a

1886
01:06:48,000 --> 01:06:51,599
little bit of structure here

1887
01:06:50,079 --> 01:06:53,680
after the training

1888
01:06:51,599 --> 01:06:55,920
and it's not definitely not random and

1889
01:06:53,680 --> 01:06:57,760
these embeddings make sense

1890
01:06:55,920 --> 01:06:59,039
so we're now going to scale up the

1891
01:06:57,760 --> 01:07:01,680
embedding size and won't be able to

1892
01:06:59,039 --> 01:07:03,680
visualize it directly but we expect that

1893
01:07:01,680 --> 01:07:06,079
because we're under fitting

1894
01:07:03,680 --> 01:07:08,160
and we made this layer much bigger and

1895
01:07:06,079 --> 01:07:10,799
did not sufficiently improve the loss

1896
01:07:08,160 --> 01:07:12,559
we're thinking that the um

1897
01:07:10,799 --> 01:07:15,119
constraint to better performance right

1898
01:07:12,559 --> 01:07:16,640
now could be these embedding pictures so

1899
01:07:15,119 --> 01:07:17,839
let's make them bigger okay so let's

1900
01:07:16,640 --> 01:07:19,039
scroll up here

1901
01:07:17,839 --> 01:07:21,599
and now we don't have two dimensional

1902
01:07:19,039 --> 01:07:23,440
embeddings we are going to have

1903
01:07:21,599 --> 01:07:25,119
say 10 dimensional embeddings for each

1904
01:07:23,440 --> 01:07:26,160
word

1905
01:07:25,119 --> 01:07:30,480
then

1906
01:07:26,160 --> 01:07:31,520
this layer will receive 3 times 10 so 30

1907
01:07:30,480 --> 01:07:33,760
inputs

1908
01:07:31,520 --> 01:07:35,760
will go into

1909
01:07:33,760 --> 01:07:37,039
the hidden layer

1910
01:07:35,760 --> 01:07:38,880
let's also make the hidden layer a bit

1911
01:07:37,039 --> 01:07:41,599
smaller so instead of 300 let's just do

1912
01:07:38,880 --> 01:07:43,520
200 neurons in that hidden layer

1913
01:07:41,599 --> 01:07:47,200
so now the total number of elements will

1914
01:07:43,520 --> 01:07:48,400
be slightly bigger at 11 000

1915
01:07:47,200 --> 01:07:50,559
and then here we have to be a bit

1916
01:07:48,400 --> 01:07:53,200
careful because um

1917
01:07:50,559 --> 01:07:56,000
okay the learning rate we set to 0.1

1918
01:07:53,200 --> 01:07:56,880
here we are hardcoded in six and

1919
01:07:56,000 --> 01:07:57,760
obviously if you're working in

1920
01:07:56,880 --> 01:08:00,000
production you don't wanna be

1921
01:07:57,760 --> 01:08:02,319
hard-coding magic numbers but instead of

1922
01:08:00,000 --> 01:08:04,079
six this should now be thirty

1923
01:08:02,319 --> 01:08:05,680
um

1924
01:08:04,079 --> 01:08:07,920
and let's run for fifty thousand

1925
01:08:05,680 --> 01:08:10,799
iterations and let me split out the

1926
01:08:07,920 --> 01:08:12,640
initialization here outside

1927
01:08:10,799 --> 01:08:14,400
so that when we run this cell multiple

1928
01:08:12,640 --> 01:08:16,880
times it's not going to wipe out

1929
01:08:14,400 --> 01:08:16,880
our loss

1930
01:08:17,520 --> 01:08:20,640
in addition to that

1931
01:08:19,440 --> 01:08:22,640
here

1932
01:08:20,640 --> 01:08:23,440
let's instead of logging lost.item let's

1933
01:08:22,640 --> 01:08:25,520
actually

1934
01:08:23,440 --> 01:08:26,560
log the

1935
01:08:25,520 --> 01:08:28,560
let's

1936
01:08:26,560 --> 01:08:32,719
do log 10

1937
01:08:28,560 --> 01:08:34,319
i believe that's a function of the loss

1938
01:08:32,719 --> 01:08:37,040
and i'll show you why in a second let's

1939
01:08:34,319 --> 01:08:37,040
optimize this

1940
01:08:37,120 --> 01:08:40,880
basically i'd like to plot the log loss

1941
01:08:39,279 --> 01:08:42,239
instead of the loss because when you

1942
01:08:40,880 --> 01:08:44,960
plot the loss many times it can have

1943
01:08:42,239 --> 01:08:46,799
this hockey stick appearance and log

1944
01:08:44,960 --> 01:08:48,960
squashes it in

1945
01:08:46,799 --> 01:08:51,679
uh so it just kind of like looks nicer

1946
01:08:48,960 --> 01:08:55,440
so the x-axis is step i

1947
01:08:51,679 --> 01:08:55,440
and the y-axis will be the loss i

1948
01:09:00,799 --> 01:09:07,719
and then here this is 30.

1949
01:09:03,120 --> 01:09:07,719
ideally we wouldn't be hard-coding these

1950
01:09:08,719 --> 01:09:13,440
okay so let's look at the loss

1951
01:09:11,440 --> 01:09:15,520
okay it's again very thick because the

1952
01:09:13,440 --> 01:09:18,000
mini batch size is very small but the

1953
01:09:15,520 --> 01:09:20,880
total loss over the training set is 2.3

1954
01:09:18,000 --> 01:09:21,920
and the the tests or the def set is 2.38

1955
01:09:20,880 --> 01:09:24,080
as well

1956
01:09:21,920 --> 01:09:25,679
so so far so good uh let's try to now

1957
01:09:24,080 --> 01:09:28,799
decrease the learning rate

1958
01:09:25,679 --> 01:09:28,799
by a factor of 10

1959
01:09:29,120 --> 01:09:33,560
and train for another 50 000 iterations

1960
01:09:35,279 --> 01:09:40,600
we'd hope that we would be able to beat

1961
01:09:37,279 --> 01:09:40,600
uh 2.32

1962
01:09:43,279 --> 01:09:46,799
but again we're just kind of like doing

1963
01:09:44,719 --> 01:09:48,480
this very haphazardly so i don't

1964
01:09:46,799 --> 01:09:50,480
actually have confidence that our

1965
01:09:48,480 --> 01:09:52,799
learning rate is set very well that our

1966
01:09:50,480 --> 01:09:55,199
learning rate decay which we just do

1967
01:09:52,799 --> 01:09:57,520
at random is set very well

1968
01:09:55,199 --> 01:09:59,280
and um so the optimization here is kind

1969
01:09:57,520 --> 01:10:00,239
of suspect to be honest and this is not

1970
01:09:59,280 --> 01:10:02,000
how you would do it typically in

1971
01:10:00,239 --> 01:10:04,080
production in production you would

1972
01:10:02,000 --> 01:10:05,679
create parameters or hyper parameters

1973
01:10:04,080 --> 01:10:07,360
out of all these settings and then you

1974
01:10:05,679 --> 01:10:11,280
would run lots of experiments and see

1975
01:10:07,360 --> 01:10:12,640
whichever ones are working well for you

1976
01:10:11,280 --> 01:10:16,800
okay

1977
01:10:12,640 --> 01:10:19,440
so we have 2.17 now and 2.2 okay so you

1978
01:10:16,800 --> 01:10:21,600
see how the training and the validation

1979
01:10:19,440 --> 01:10:23,120
performance are starting to slightly

1980
01:10:21,600 --> 01:10:24,800
slowly depart

1981
01:10:23,120 --> 01:10:26,080
so maybe we're getting the sense that

1982
01:10:24,800 --> 01:10:28,400
the neural net

1983
01:10:26,080 --> 01:10:30,320
is getting good enough or

1984
01:10:28,400 --> 01:10:32,320
that number of parameters is large

1985
01:10:30,320 --> 01:10:34,159
enough that we are slowly starting to

1986
01:10:32,320 --> 01:10:36,400
overfit

1987
01:10:34,159 --> 01:10:37,920
let's maybe run one more iteration of

1988
01:10:36,400 --> 01:10:40,719
this

1989
01:10:37,920 --> 01:10:40,719
and see where we get

1990
01:10:41,520 --> 01:10:44,800
but yeah basically you would be running

1991
01:10:43,280 --> 01:10:46,400
lots of experiments and then you are

1992
01:10:44,800 --> 01:10:48,560
slowly scrutinizing whichever ones give

1993
01:10:46,400 --> 01:10:50,000
you the best depth performance and then

1994
01:10:48,560 --> 01:10:51,760
once you find all the

1995
01:10:50,000 --> 01:10:54,000
hyper parameters that make your dev

1996
01:10:51,760 --> 01:10:55,840
performance good you take that model and

1997
01:10:54,000 --> 01:10:57,679
you evaluate the test set performance a

1998
01:10:55,840 --> 01:10:59,679
single time and that's the number that

1999
01:10:57,679 --> 01:11:01,199
you report in your paper or wherever

2000
01:10:59,679 --> 01:11:04,400
else you want to talk about and brag

2001
01:11:01,199 --> 01:11:04,400
about your model

2002
01:11:05,520 --> 01:11:10,960
so let's then rerun the plot and rerun

2003
01:11:08,159 --> 01:11:10,960
the train and death

2004
01:11:11,120 --> 01:11:15,199
and because we're getting lower loss now

2005
01:11:12,800 --> 01:11:19,440
it is the case that the embedding size

2006
01:11:15,199 --> 01:11:19,440
of these was holding us back very likely

2007
01:11:20,080 --> 01:11:24,480
okay so 2.162.19 is what we're roughly

2008
01:11:22,880 --> 01:11:26,480
getting

2009
01:11:24,480 --> 01:11:28,239
so there's many ways to go from many

2010
01:11:26,480 --> 01:11:30,239
ways to go from here we can continue

2011
01:11:28,239 --> 01:11:32,080
tuning the optimization

2012
01:11:30,239 --> 01:11:34,080
we can continue for example playing with

2013
01:11:32,080 --> 01:11:36,159
the sizes of the neural net or we can

2014
01:11:34,080 --> 01:11:38,320
increase the number of uh

2015
01:11:36,159 --> 01:11:39,840
words or characters in our case that we

2016
01:11:38,320 --> 01:11:41,440
are taking as an input so instead of

2017
01:11:39,840 --> 01:11:43,600
just three characters we could be taking

2018
01:11:41,440 --> 01:11:46,480
more characters as an input and that

2019
01:11:43,600 --> 01:11:48,159
could further improve the loss

2020
01:11:46,480 --> 01:11:50,480
okay so i changed the code slightly so

2021
01:11:48,159 --> 01:11:52,800
we have here 200 000 steps of the

2022
01:11:50,480 --> 01:11:54,560
optimization and in the first 100 000

2023
01:11:52,800 --> 01:11:56,320
we're using a learning rate of 0.1 and

2024
01:11:54,560 --> 01:11:58,480
then in the next 100 000 we're using a

2025
01:11:56,320 --> 01:12:00,400
learning rate of 0.01

2026
01:11:58,480 --> 01:12:01,920
this is the loss that i achieve

2027
01:12:00,400 --> 01:12:03,920
and these are the performance on the

2028
01:12:01,920 --> 01:12:05,760
training and validation loss

2029
01:12:03,920 --> 01:12:07,040
and in particular the best validation

2030
01:12:05,760 --> 01:12:10,560
loss i've been able to obtain in the

2031
01:12:07,040 --> 01:12:12,800
last 30 minutes or so is 2.17

2032
01:12:10,560 --> 01:12:14,400
so now i invite you to beat this number

2033
01:12:12,800 --> 01:12:17,040
and you have quite a few knobs available

2034
01:12:14,400 --> 01:12:18,719
to you to i think surpass this number

2035
01:12:17,040 --> 01:12:20,320
so number one you can of course change

2036
01:12:18,719 --> 01:12:22,239
the number of neurons in the hidden

2037
01:12:20,320 --> 01:12:24,239
layer of this model you can change the

2038
01:12:22,239 --> 01:12:25,440
dimensionality of the embedding

2039
01:12:24,239 --> 01:12:26,800
lookup table

2040
01:12:25,440 --> 01:12:29,760
you can change the number of characters

2041
01:12:26,800 --> 01:12:32,400
that are feeding in as an input

2042
01:12:29,760 --> 01:12:33,520
as the context into this model

2043
01:12:32,400 --> 01:12:35,520
and then of course you can change the

2044
01:12:33,520 --> 01:12:37,520
details of the optimization how long are

2045
01:12:35,520 --> 01:12:39,360
we running what is the learning rate how

2046
01:12:37,520 --> 01:12:41,120
does it change over time

2047
01:12:39,360 --> 01:12:42,640
how does it decay

2048
01:12:41,120 --> 01:12:44,320
you can change the batch size and you

2049
01:12:42,640 --> 01:12:46,400
may be able to actually achieve a much

2050
01:12:44,320 --> 01:12:47,520
better convergence speed

2051
01:12:46,400 --> 01:12:49,040
in terms of

2052
01:12:47,520 --> 01:12:51,520
how many seconds or minutes it takes to

2053
01:12:49,040 --> 01:12:54,000
train the model and get

2054
01:12:51,520 --> 01:12:55,679
your result in terms of really good

2055
01:12:54,000 --> 01:12:57,120
loss

2056
01:12:55,679 --> 01:12:59,600
and then of course i actually invite you

2057
01:12:57,120 --> 01:13:00,960
to read this paper it is 19 pages but at

2058
01:12:59,600 --> 01:13:03,120
this point you should actually be able

2059
01:13:00,960 --> 01:13:04,880
to read a good chunk of this paper and

2060
01:13:03,120 --> 01:13:06,480
understand

2061
01:13:04,880 --> 01:13:08,080
pretty good chunks of it

2062
01:13:06,480 --> 01:13:09,840
and this paper also has quite a few

2063
01:13:08,080 --> 01:13:11,040
ideas for improvements that you can play

2064
01:13:09,840 --> 01:13:13,120
with

2065
01:13:11,040 --> 01:13:14,560
so all of those are not available to you

2066
01:13:13,120 --> 01:13:16,400
and you should be able to beat this

2067
01:13:14,560 --> 01:13:18,880
number i'm leaving that as an exercise

2068
01:13:16,400 --> 01:13:21,600
to the reader and that's it for now and

2069
01:13:18,880 --> 01:13:21,600
i'll see you next time

2070
01:13:24,239 --> 01:13:28,159
before we wrap up i also wanted to show

2071
01:13:25,760 --> 01:13:31,040
how you would sample from the model

2072
01:13:28,159 --> 01:13:33,760
so we're going to generate 20 samples

2073
01:13:31,040 --> 01:13:35,360
at first we begin with all dots so

2074
01:13:33,760 --> 01:13:37,440
that's the context

2075
01:13:35,360 --> 01:13:40,000
and then until we generate

2076
01:13:37,440 --> 01:13:43,679
the zeroth character again

2077
01:13:40,000 --> 01:13:47,199
we're going to embed the current context

2078
01:13:43,679 --> 01:13:49,440
using the embedding table c now usually

2079
01:13:47,199 --> 01:13:51,120
uh here the first dimension was the size

2080
01:13:49,440 --> 01:13:52,400
of the training set but here we're only

2081
01:13:51,120 --> 01:13:55,040
working with a single example that we're

2082
01:13:52,400 --> 01:13:58,480
generating so this is just the mission

2083
01:13:55,040 --> 01:13:58,480
one just for simplicity

2084
01:13:58,560 --> 01:14:02,480
and so this embedding then gets

2085
01:14:00,640 --> 01:14:03,600
projected into the end state you get the

2086
01:14:02,480 --> 01:14:05,679
logits

2087
01:14:03,600 --> 01:14:09,120
now we calculate the probabilities for

2088
01:14:05,679 --> 01:14:10,880
that you can use f.softmax

2089
01:14:09,120 --> 01:14:12,320
of logits and that just basically

2090
01:14:10,880 --> 01:14:15,120
exponentiates the logits and makes them

2091
01:14:12,320 --> 01:14:18,640
sum to one and similar to cross entropy

2092
01:14:15,120 --> 01:14:20,159
it is careful that there's no overflows

2093
01:14:18,640 --> 01:14:22,000
once we have the probabilities we sample

2094
01:14:20,159 --> 01:14:24,480
from them using torture multinomial to

2095
01:14:22,000 --> 01:14:26,800
get our next index and then we shift the

2096
01:14:24,480 --> 01:14:29,679
context window to append the index and

2097
01:14:26,800 --> 01:14:31,920
record it and then we can just

2098
01:14:29,679 --> 01:14:33,360
decode all the integers to strings

2099
01:14:31,920 --> 01:14:34,960
and print them out

2100
01:14:33,360 --> 01:14:36,880
and so these are some example samples

2101
01:14:34,960 --> 01:14:38,960
and you can see that the model now works

2102
01:14:36,880 --> 01:14:41,520
much better so the words here are much

2103
01:14:38,960 --> 01:14:44,400
more word like or name like so we have

2104
01:14:41,520 --> 01:14:46,800
things like ham

2105
01:14:44,400 --> 01:14:46,800
joes

2106
01:14:48,080 --> 01:14:51,280
you know it's starting to sound a little

2107
01:14:49,520 --> 01:14:53,440
bit more name-like so we're definitely

2108
01:14:51,280 --> 01:14:55,440
making progress but we can still improve

2109
01:14:53,440 --> 01:14:57,520
on this model quite a lot

2110
01:14:55,440 --> 01:14:59,440
okay sorry there's some bonus content i

2111
01:14:57,520 --> 01:15:01,840
wanted to mention that i want to make

2112
01:14:59,440 --> 01:15:03,360
these notebooks more accessible and so i

2113
01:15:01,840 --> 01:15:04,719
don't want you to have to like install

2114
01:15:03,360 --> 01:15:06,719
jupyter notebooks and torch and

2115
01:15:04,719 --> 01:15:08,960
everything else so i will be sharing a

2116
01:15:06,719 --> 01:15:10,719
link to a google colab

2117
01:15:08,960 --> 01:15:13,120
and google collab will look like a

2118
01:15:10,719 --> 01:15:15,360
notebook in your browser and you can

2119
01:15:13,120 --> 01:15:17,760
just go to the url and you'll be able to

2120
01:15:15,360 --> 01:15:20,000
execute all of the code that you saw in

2121
01:15:17,760 --> 01:15:22,320
the google collab and so this is me

2122
01:15:20,000 --> 01:15:24,719
executing the code in this lecture and i

2123
01:15:22,320 --> 01:15:25,920
shortened it a little bit but basically

2124
01:15:24,719 --> 01:15:28,480
you're able to train the exact same

2125
01:15:25,920 --> 01:15:30,000
network and then plot and sample from

2126
01:15:28,480 --> 01:15:31,679
the model and everything is ready for

2127
01:15:30,000 --> 01:15:33,120
you to like tinker with the numbers

2128
01:15:31,679 --> 01:15:35,520
right there in your browser no

2129
01:15:33,120 --> 01:15:36,719
installation necessary

2130
01:15:35,520 --> 01:15:38,159
so i just wanted to point that out and

2131
01:15:36,719 --> 01:15:40,800
the link to this will be in the video

2132
01:15:38,159 --> 01:15:40,800
description

