1
00:00:00,240 --> 00:00:04,080
hi everyone hope you're well

2
00:00:02,240 --> 00:00:06,160
and next up what i'd like to do is i'd

3
00:00:04,080 --> 00:00:08,559
like to build out make more

4
00:00:06,160 --> 00:00:10,240
like micrograd before it make more is a

5
00:00:08,559 --> 00:00:11,360
repository that i have on my github

6
00:00:10,240 --> 00:00:12,639
webpage

7
00:00:11,360 --> 00:00:14,400
you can look at it

8
00:00:12,639 --> 00:00:16,320
but just like with micrograd i'm going

9
00:00:14,400 --> 00:00:17,920
to build it out step by step and i'm

10
00:00:16,320 --> 00:00:19,039
going to spell everything out so we're

11
00:00:17,920 --> 00:00:20,240
going to build it out slowly and

12
00:00:19,039 --> 00:00:22,160
together

13
00:00:20,240 --> 00:00:24,560
now what is make more

14
00:00:22,160 --> 00:00:27,519
make more as the name suggests

15
00:00:24,560 --> 00:00:29,039
makes more of things that you give it

16
00:00:27,519 --> 00:00:31,519
so here's an example

17
00:00:29,039 --> 00:00:32,480
names.txt is an example dataset to make

18
00:00:31,519 --> 00:00:34,399
more

19
00:00:32,480 --> 00:00:36,800
and when you look at names.txt you'll

20
00:00:34,399 --> 00:00:38,160
find that it's a very large data set of

21
00:00:36,800 --> 00:00:40,160
names

22
00:00:38,160 --> 00:00:41,680
so

23
00:00:40,160 --> 00:00:44,160
here's lots of different types of names

24
00:00:41,680 --> 00:00:46,079
in fact i believe there are 32 000 names

25
00:00:44,160 --> 00:00:47,840
that i've sort of found randomly on the

26
00:00:46,079 --> 00:00:50,079
government website

27
00:00:47,840 --> 00:00:53,199
and if you train make more on this data

28
00:00:50,079 --> 00:00:55,280
set it will learn to make more of things

29
00:00:53,199 --> 00:00:57,600
like this

30
00:00:55,280 --> 00:01:00,320
and in particular in this case that will

31
00:00:57,600 --> 00:01:02,320
mean more things that sound name-like

32
00:01:00,320 --> 00:01:03,840
but are actually unique names

33
00:01:02,320 --> 00:01:05,280
and maybe if you have a baby and you're

34
00:01:03,840 --> 00:01:07,040
trying to assign name maybe you're

35
00:01:05,280 --> 00:01:09,520
looking for a cool new sounding unique

36
00:01:07,040 --> 00:01:11,280
name make more might help you

37
00:01:09,520 --> 00:01:13,360
so here are some example generations

38
00:01:11,280 --> 00:01:16,159
from the neural network

39
00:01:13,360 --> 00:01:17,759
once we train it on our data set

40
00:01:16,159 --> 00:01:19,680
so here's some example

41
00:01:17,759 --> 00:01:21,680
unique names that it will generate

42
00:01:19,680 --> 00:01:23,439
dontel

43
00:01:21,680 --> 00:01:24,479
irot

44
00:01:23,439 --> 00:01:26,960
zhendi

45
00:01:24,479 --> 00:01:28,880
and so on and so all these are sound

46
00:01:26,960 --> 00:01:30,640
name like but they're not of course

47
00:01:28,880 --> 00:01:32,880
names

48
00:01:30,640 --> 00:01:35,200
so under the hood make more is a

49
00:01:32,880 --> 00:01:37,280
character level language model so what

50
00:01:35,200 --> 00:01:39,680
that means is that it is treating every

51
00:01:37,280 --> 00:01:42,079
single line here as an example and

52
00:01:39,680 --> 00:01:44,000
within each example it's treating them

53
00:01:42,079 --> 00:01:48,880
all as sequences of individual

54
00:01:44,000 --> 00:01:50,560
characters so r e e s e is this example

55
00:01:48,880 --> 00:01:51,840
and that's the sequence of characters

56
00:01:50,560 --> 00:01:54,799
and that's the level on which we are

57
00:01:51,840 --> 00:01:56,479
building out make more and what it means

58
00:01:54,799 --> 00:01:58,399
to be a character level language model

59
00:01:56,479 --> 00:01:59,920
then is that it's just uh sort of

60
00:01:58,399 --> 00:02:01,360
modeling those sequences of characters

61
00:01:59,920 --> 00:02:03,600
and it knows how to predict the next

62
00:02:01,360 --> 00:02:05,280
character in the sequence

63
00:02:03,600 --> 00:02:07,439
now we're actually going to implement a

64
00:02:05,280 --> 00:02:09,200
large number of character level language

65
00:02:07,439 --> 00:02:10,879
models in terms of the neural networks

66
00:02:09,200 --> 00:02:13,440
that are involved in predicting the next

67
00:02:10,879 --> 00:02:15,280
character in a sequence so very simple

68
00:02:13,440 --> 00:02:17,120
bi-gram and back of work models

69
00:02:15,280 --> 00:02:19,599
multilingual perceptrons recurrent

70
00:02:17,120 --> 00:02:21,680
neural networks all the way to modern

71
00:02:19,599 --> 00:02:23,680
transformers in fact the transformer

72
00:02:21,680 --> 00:02:26,239
that we will build will be basically the

73
00:02:23,680 --> 00:02:28,640
equivalent transformer to gpt2 if you

74
00:02:26,239 --> 00:02:30,879
have heard of gpt uh so that's kind of a

75
00:02:28,640 --> 00:02:32,560
big deal it's a modern network and by

76
00:02:30,879 --> 00:02:34,640
the end of the series you will actually

77
00:02:32,560 --> 00:02:36,800
understand how that works um on the

78
00:02:34,640 --> 00:02:39,840
level of characters now to give you a

79
00:02:36,800 --> 00:02:41,440
sense of the extensions here uh after

80
00:02:39,840 --> 00:02:43,280
characters we will probably spend some

81
00:02:41,440 --> 00:02:45,040
time on the word level so that we can

82
00:02:43,280 --> 00:02:47,519
generate documents of words not just

83
00:02:45,040 --> 00:02:49,440
little you know segments of characters

84
00:02:47,519 --> 00:02:50,959
but we can generate entire large much

85
00:02:49,440 --> 00:02:52,319
larger documents

86
00:02:50,959 --> 00:02:54,640
and then we're probably going to go into

87
00:02:52,319 --> 00:02:57,040
images and image text

88
00:02:54,640 --> 00:03:00,000
networks such as dolly stable diffusion

89
00:02:57,040 --> 00:03:02,159
and so on but for now we have to start

90
00:03:00,000 --> 00:03:03,360
here character level language modeling

91
00:03:02,159 --> 00:03:04,800
let's go

92
00:03:03,360 --> 00:03:06,959
so like before we are starting with a

93
00:03:04,800 --> 00:03:08,560
completely blank jupiter notebook page

94
00:03:06,959 --> 00:03:11,760
the first thing is i would like to

95
00:03:08,560 --> 00:03:13,840
basically load up the dataset names.txt

96
00:03:11,760 --> 00:03:15,360
so we're going to open up names.txt for

97
00:03:13,840 --> 00:03:17,360
reading

98
00:03:15,360 --> 00:03:19,760
and we're going to read in everything

99
00:03:17,360 --> 00:03:21,360
into a massive string

100
00:03:19,760 --> 00:03:23,120
and then because it's a massive string

101
00:03:21,360 --> 00:03:24,480
we'd only like the individual words and

102
00:03:23,120 --> 00:03:26,480
put them in the list

103
00:03:24,480 --> 00:03:27,760
so let's call split lines

104
00:03:26,480 --> 00:03:30,239
on that string

105
00:03:27,760 --> 00:03:32,000
to get all of our words as a python list

106
00:03:30,239 --> 00:03:33,360
of strings

107
00:03:32,000 --> 00:03:35,760
so basically we can look at for example

108
00:03:33,360 --> 00:03:39,040
the first 10 words

109
00:03:35,760 --> 00:03:41,440
and we have that it's a list of emma

110
00:03:39,040 --> 00:03:43,680
olivia eva and so on

111
00:03:41,440 --> 00:03:45,519
and if we look at

112
00:03:43,680 --> 00:03:47,040
the top of the page here that is indeed

113
00:03:45,519 --> 00:03:48,159
what we see

114
00:03:47,040 --> 00:03:49,599
um

115
00:03:48,159 --> 00:03:52,239
so that's good

116
00:03:49,599 --> 00:03:55,599
this list actually makes me feel that

117
00:03:52,239 --> 00:03:57,280
this is probably sorted by frequency

118
00:03:55,599 --> 00:03:58,959
but okay so

119
00:03:57,280 --> 00:04:00,560
these are the words now we'd like to

120
00:03:58,959 --> 00:04:02,319
actually like learn a little bit more

121
00:04:00,560 --> 00:04:03,920
about this data set let's look at the

122
00:04:02,319 --> 00:04:06,400
total number of words we expect this to

123
00:04:03,920 --> 00:04:07,680
be roughly 32 000

124
00:04:06,400 --> 00:04:09,120
and then what is the for example

125
00:04:07,680 --> 00:04:10,959
shortest word

126
00:04:09,120 --> 00:04:13,599
so min of

127
00:04:10,959 --> 00:04:17,040
length of each word for w inwards

128
00:04:13,599 --> 00:04:18,239
so the shortest word will be length

129
00:04:17,040 --> 00:04:21,199
two

130
00:04:18,239 --> 00:04:23,120
and max of one w for w in words so the

131
00:04:21,199 --> 00:04:24,560
longest word will be

132
00:04:23,120 --> 00:04:25,840
15 characters

133
00:04:24,560 --> 00:04:27,360
so let's now think through our very

134
00:04:25,840 --> 00:04:28,720
first language model

135
00:04:27,360 --> 00:04:30,720
as i mentioned a character level

136
00:04:28,720 --> 00:04:33,440
language model is predicting the next

137
00:04:30,720 --> 00:04:35,199
character in a sequence given already

138
00:04:33,440 --> 00:04:36,560
some concrete sequence of characters

139
00:04:35,199 --> 00:04:38,000
before it

140
00:04:36,560 --> 00:04:40,960
now we have to realize here is that

141
00:04:38,000 --> 00:04:43,919
every single word here like isabella is

142
00:04:40,960 --> 00:04:45,520
actually quite a few examples packed in

143
00:04:43,919 --> 00:04:47,280
to that single word

144
00:04:45,520 --> 00:04:48,960
because what is an existence of a word

145
00:04:47,280 --> 00:04:51,120
like isabella in the data set telling us

146
00:04:48,960 --> 00:04:53,600
really it's saying that

147
00:04:51,120 --> 00:04:56,479
the character i is a very likely

148
00:04:53,600 --> 00:04:58,560
character to come first in the sequence

149
00:04:56,479 --> 00:05:01,680
of a name

150
00:04:58,560 --> 00:05:04,320
the character s is likely to come

151
00:05:01,680 --> 00:05:06,400
after i

152
00:05:04,320 --> 00:05:07,600
the character a is likely to come after

153
00:05:06,400 --> 00:05:09,360
is

154
00:05:07,600 --> 00:05:12,479
the character b is very likely to come

155
00:05:09,360 --> 00:05:14,400
after isa and so on all the way to a

156
00:05:12,479 --> 00:05:15,680
following isabel

157
00:05:14,400 --> 00:05:17,280
and then there's one more example

158
00:05:15,680 --> 00:05:19,039
actually packed in here

159
00:05:17,280 --> 00:05:21,360
and that is that

160
00:05:19,039 --> 00:05:23,759
after there's isabella

161
00:05:21,360 --> 00:05:25,440
the word is very likely to end

162
00:05:23,759 --> 00:05:27,360
so that's one more sort of explicit

163
00:05:25,440 --> 00:05:29,600
piece of information that we have here

164
00:05:27,360 --> 00:05:31,680
that we have to be careful with

165
00:05:29,600 --> 00:05:33,680
and so there's a lot backed into a

166
00:05:31,680 --> 00:05:35,360
single individual word in terms of the

167
00:05:33,680 --> 00:05:38,000
statistical structure of what's likely

168
00:05:35,360 --> 00:05:39,199
to follow in these character sequences

169
00:05:38,000 --> 00:05:41,520
and then of course we don't have just an

170
00:05:39,199 --> 00:05:42,800
individual word we actually have 32 000

171
00:05:41,520 --> 00:05:44,800
of these and so there's a lot of

172
00:05:42,800 --> 00:05:46,160
structure here to model

173
00:05:44,800 --> 00:05:48,080
now in the beginning what i'd like to

174
00:05:46,160 --> 00:05:51,199
start with is i'd like to start with

175
00:05:48,080 --> 00:05:53,039
building a bi-gram language model

176
00:05:51,199 --> 00:05:54,639
now in the bigram language model we're

177
00:05:53,039 --> 00:05:56,720
always working with just

178
00:05:54,639 --> 00:05:59,280
two characters at a time

179
00:05:56,720 --> 00:06:00,880
so we're only looking at one character

180
00:05:59,280 --> 00:06:02,479
that we are given and we're trying to

181
00:06:00,880 --> 00:06:03,840
predict the next character in the

182
00:06:02,479 --> 00:06:06,319
sequence

183
00:06:03,840 --> 00:06:08,319
so um what characters are likely to

184
00:06:06,319 --> 00:06:10,160
follow are what characters are likely to

185
00:06:08,319 --> 00:06:11,840
follow a and so on and we're just

186
00:06:10,160 --> 00:06:12,880
modeling that kind of a little local

187
00:06:11,840 --> 00:06:14,960
structure

188
00:06:12,880 --> 00:06:16,960
and we're forgetting the fact that we

189
00:06:14,960 --> 00:06:18,319
may have a lot more information we're

190
00:06:16,960 --> 00:06:20,319
always just looking at the previous

191
00:06:18,319 --> 00:06:21,680
character to predict the next one so

192
00:06:20,319 --> 00:06:23,199
it's a very simple and weak language

193
00:06:21,680 --> 00:06:24,080
model but i think it's a great place to

194
00:06:23,199 --> 00:06:25,680
start

195
00:06:24,080 --> 00:06:27,440
so now let's begin by looking at these

196
00:06:25,680 --> 00:06:29,039
bi-grams in our data set and what they

197
00:06:27,440 --> 00:06:30,880
look like and these bi-grams again are

198
00:06:29,039 --> 00:06:33,120
just two characters in a row

199
00:06:30,880 --> 00:06:35,199
so for w in words

200
00:06:33,120 --> 00:06:36,319
each w here is an individual word a

201
00:06:35,199 --> 00:06:39,600
string

202
00:06:36,319 --> 00:06:41,680
we want to iterate uh for

203
00:06:39,600 --> 00:06:44,000
we're going to iterate this word

204
00:06:41,680 --> 00:06:45,919
with consecutive characters so two

205
00:06:44,000 --> 00:06:49,360
characters at a time sliding it through

206
00:06:45,919 --> 00:06:51,440
the word now a interesting nice way cute

207
00:06:49,360 --> 00:06:53,199
way to do this in python by the way is

208
00:06:51,440 --> 00:06:56,319
doing something like this for character

209
00:06:53,199 --> 00:07:00,000
one character two in zip off

210
00:06:56,319 --> 00:07:01,680
w and w at one

211
00:07:00,000 --> 00:07:02,800
one column

212
00:07:01,680 --> 00:07:04,560
print

213
00:07:02,800 --> 00:07:05,919
character one character two

214
00:07:04,560 --> 00:07:07,440
and let's not do all the words let's

215
00:07:05,919 --> 00:07:09,039
just do the first three words and i'm

216
00:07:07,440 --> 00:07:09,919
going to show you in a second how this

217
00:07:09,039 --> 00:07:11,919
works

218
00:07:09,919 --> 00:07:13,680
but for now basically as an example

219
00:07:11,919 --> 00:07:15,280
let's just do the very first word alone

220
00:07:13,680 --> 00:07:18,319
emma

221
00:07:15,280 --> 00:07:20,960
you see how we have a emma and this will

222
00:07:18,319 --> 00:07:23,759
just print e m m m a

223
00:07:20,960 --> 00:07:26,800
and the reason this works is because w

224
00:07:23,759 --> 00:07:28,639
is the string emma w at one column is

225
00:07:26,800 --> 00:07:29,680
the string mma

226
00:07:28,639 --> 00:07:33,360
and zip

227
00:07:29,680 --> 00:07:35,039
takes two iterators and it pairs them up

228
00:07:33,360 --> 00:07:37,360
and then creates an iterator over the

229
00:07:35,039 --> 00:07:39,440
tuples of their consecutive entries

230
00:07:37,360 --> 00:07:41,840
and if any one of these lists is shorter

231
00:07:39,440 --> 00:07:43,680
than the other then it will just

232
00:07:41,840 --> 00:07:48,960
halt and return

233
00:07:43,680 --> 00:07:49,919
so basically that's why we return em mmm

234
00:07:48,960 --> 00:07:52,080
ma

235
00:07:49,919 --> 00:07:55,120
but then because this iterator second

236
00:07:52,080 --> 00:07:56,879
one here runs out of elements zip just

237
00:07:55,120 --> 00:07:59,680
ends and that's why we only get these

238
00:07:56,879 --> 00:08:01,840
tuples so pretty cute

239
00:07:59,680 --> 00:08:03,919
so these are the consecutive elements in

240
00:08:01,840 --> 00:08:04,879
the first word now we have to be careful

241
00:08:03,919 --> 00:08:07,280
because we actually have more

242
00:08:04,879 --> 00:08:09,919
information here than just these three

243
00:08:07,280 --> 00:08:12,319
examples as i mentioned we know that e

244
00:08:09,919 --> 00:08:14,400
is the is very likely to come first and

245
00:08:12,319 --> 00:08:15,599
we know that a in this case is coming

246
00:08:14,400 --> 00:08:17,759
last

247
00:08:15,599 --> 00:08:19,280
so one way to do this is basically we're

248
00:08:17,759 --> 00:08:21,520
going to create

249
00:08:19,280 --> 00:08:23,039
a special array here all

250
00:08:21,520 --> 00:08:25,280
characters

251
00:08:23,039 --> 00:08:28,400
and um we're going to hallucinate a

252
00:08:25,280 --> 00:08:29,840
special start token here

253
00:08:28,400 --> 00:08:32,399
i'm going to

254
00:08:29,840 --> 00:08:34,560
call it like special start

255
00:08:32,399 --> 00:08:36,479
so this is a list of one element

256
00:08:34,560 --> 00:08:37,760
plus

257
00:08:36,479 --> 00:08:41,279
w

258
00:08:37,760 --> 00:08:42,719
and then plus a special end character

259
00:08:41,279 --> 00:08:45,920
and the reason i'm wrapping the list of

260
00:08:42,719 --> 00:08:48,640
w here is because w is a string emma

261
00:08:45,920 --> 00:08:50,800
list of w will just have the individual

262
00:08:48,640 --> 00:08:51,839
characters in the list

263
00:08:50,800 --> 00:08:54,720
and then

264
00:08:51,839 --> 00:08:58,080
doing this again now but not iterating

265
00:08:54,720 --> 00:09:00,160
over w's but over the characters

266
00:08:58,080 --> 00:09:02,720
will give us something like this

267
00:09:00,160 --> 00:09:05,120
so e is likely so this is a bigram of

268
00:09:02,720 --> 00:09:06,399
the start character and e and this is a

269
00:09:05,120 --> 00:09:09,040
bigram of the

270
00:09:06,399 --> 00:09:10,480
a and the special end character

271
00:09:09,040 --> 00:09:12,000
and now we can look at for example what

272
00:09:10,480 --> 00:09:14,480
this looks like for

273
00:09:12,000 --> 00:09:16,160
olivia or eva

274
00:09:14,480 --> 00:09:17,600
and indeed we can actually

275
00:09:16,160 --> 00:09:19,360
potentially do this for the entire data

276
00:09:17,600 --> 00:09:20,640
set but we won't print that that's going

277
00:09:19,360 --> 00:09:22,480
to be too much

278
00:09:20,640 --> 00:09:24,959
but these are the individual character

279
00:09:22,480 --> 00:09:26,480
diagrams and we can print them

280
00:09:24,959 --> 00:09:28,320
now in order to learn the statistics

281
00:09:26,480 --> 00:09:30,560
about which characters are likely to

282
00:09:28,320 --> 00:09:32,720
follow other characters the simplest way

283
00:09:30,560 --> 00:09:34,399
in the bigram language models is to

284
00:09:32,720 --> 00:09:36,399
simply do it by counting

285
00:09:34,399 --> 00:09:38,399
so we're basically just going to count

286
00:09:36,399 --> 00:09:40,240
how often any one of these combinations

287
00:09:38,399 --> 00:09:41,760
occurs in the training set

288
00:09:40,240 --> 00:09:43,120
in these words

289
00:09:41,760 --> 00:09:44,880
so we're going to need some kind of a

290
00:09:43,120 --> 00:09:47,279
dictionary that's going to maintain some

291
00:09:44,880 --> 00:09:49,760
counts for every one of these diagrams

292
00:09:47,279 --> 00:09:52,880
so let's use a dictionary b

293
00:09:49,760 --> 00:09:54,480
and this will map these bi-grams so

294
00:09:52,880 --> 00:09:56,160
bi-gram is a tuple of character one

295
00:09:54,480 --> 00:09:58,720
character two

296
00:09:56,160 --> 00:10:01,040
and then b at bi-gram

297
00:09:58,720 --> 00:10:03,200
will be b dot get of bi-gram

298
00:10:01,040 --> 00:10:04,640
which is basically the same as b at

299
00:10:03,200 --> 00:10:07,040
bigram

300
00:10:04,640 --> 00:10:09,360
but in the case that bigram is not in

301
00:10:07,040 --> 00:10:11,839
the dictionary b we would like to by

302
00:10:09,360 --> 00:10:13,040
default return to zero

303
00:10:11,839 --> 00:10:15,519
plus one

304
00:10:13,040 --> 00:10:18,240
so this will basically add up all the

305
00:10:15,519 --> 00:10:20,240
bigrams and count how often they occur

306
00:10:18,240 --> 00:10:22,399
let's get rid of printing

307
00:10:20,240 --> 00:10:24,000
or rather

308
00:10:22,399 --> 00:10:27,120
let's keep the printing and let's just

309
00:10:24,000 --> 00:10:29,279
inspect what b is in this case

310
00:10:27,120 --> 00:10:31,120
and we see that many bi-grams occur just

311
00:10:29,279 --> 00:10:32,959
a single time this one allegedly

312
00:10:31,120 --> 00:10:35,360
occurred three times

313
00:10:32,959 --> 00:10:37,920
so a was an ending character three times

314
00:10:35,360 --> 00:10:41,920
and that's true for all of these words

315
00:10:37,920 --> 00:10:46,600
all of emma olivia and eva and with a

316
00:10:41,920 --> 00:10:46,600
so that's why this occurred three times

317
00:10:46,640 --> 00:10:50,480
now let's do it for all the words

318
00:10:51,279 --> 00:10:56,720
oops i should not have printed

319
00:10:55,040 --> 00:10:58,720
i'm going to erase that

320
00:10:56,720 --> 00:11:00,560
let's kill this

321
00:10:58,720 --> 00:11:02,320
let's just run

322
00:11:00,560 --> 00:11:04,079
and now b will have the statistics of

323
00:11:02,320 --> 00:11:05,839
the entire data set

324
00:11:04,079 --> 00:11:08,320
so these are the counts across all the

325
00:11:05,839 --> 00:11:09,920
words of the individual pie grams

326
00:11:08,320 --> 00:11:11,519
and we could for example look at some of

327
00:11:09,920 --> 00:11:13,360
the most common ones and least common

328
00:11:11,519 --> 00:11:15,040
ones

329
00:11:13,360 --> 00:11:17,440
this kind of grows in python but the way

330
00:11:15,040 --> 00:11:19,519
to do this the simplest way i like is we

331
00:11:17,440 --> 00:11:21,760
just use b dot items

332
00:11:19,519 --> 00:11:24,320
b dot items returns

333
00:11:21,760 --> 00:11:27,040
the tuples of

334
00:11:24,320 --> 00:11:29,200
key value in this case the keys are

335
00:11:27,040 --> 00:11:30,880
the character diagrams and the values

336
00:11:29,200 --> 00:11:32,399
are the counts

337
00:11:30,880 --> 00:11:34,800
and so then what we want to do is we

338
00:11:32,399 --> 00:11:34,800
want to do

339
00:11:35,680 --> 00:11:42,399
sorted of this

340
00:11:38,399 --> 00:11:42,399
but by default sort is on the first

341
00:11:43,519 --> 00:11:47,519
on the first item of a tuple but we want

342
00:11:45,920 --> 00:11:49,279
to sort by the values which are the

343
00:11:47,519 --> 00:11:50,639
second element of a tuple that is the

344
00:11:49,279 --> 00:11:52,959
key value

345
00:11:50,639 --> 00:11:55,279
so we want to use the key

346
00:11:52,959 --> 00:11:57,600
equals lambda

347
00:11:55,279 --> 00:11:58,880
that takes the key value

348
00:11:57,600 --> 00:12:02,240
and returns

349
00:11:58,880 --> 00:12:04,560
the key value at the one not at zero but

350
00:12:02,240 --> 00:12:07,120
at one which is the count so we want to

351
00:12:04,560 --> 00:12:10,399
sort by the count

352
00:12:07,120 --> 00:12:12,639
of these elements

353
00:12:10,399 --> 00:12:14,639
and actually we wanted to go backwards

354
00:12:12,639 --> 00:12:17,200
so here we have is

355
00:12:14,639 --> 00:12:18,399
the bi-gram q and r occurs only a single

356
00:12:17,200 --> 00:12:20,639
time

357
00:12:18,399 --> 00:12:21,839
dz occurred only a single time

358
00:12:20,639 --> 00:12:23,360
and when we sort this the other way

359
00:12:21,839 --> 00:12:25,279
around

360
00:12:23,360 --> 00:12:28,320
we're going to see the most likely

361
00:12:25,279 --> 00:12:30,639
bigrams so we see that n was

362
00:12:28,320 --> 00:12:32,959
very often an ending character

363
00:12:30,639 --> 00:12:34,399
many many times and apparently n almost

364
00:12:32,959 --> 00:12:36,160
always follows an a

365
00:12:34,399 --> 00:12:38,639
and that's a very likely combination as

366
00:12:36,160 --> 00:12:39,839
well

367
00:12:38,639 --> 00:12:41,920
so

368
00:12:39,839 --> 00:12:44,880
this is kind of the individual counts

369
00:12:41,920 --> 00:12:46,079
that we achieve over the entire data set

370
00:12:44,880 --> 00:12:48,320
now it's actually going to be

371
00:12:46,079 --> 00:12:49,760
significantly more convenient for us to

372
00:12:48,320 --> 00:12:51,680
keep this information in a

373
00:12:49,760 --> 00:12:53,600
two-dimensional array instead of a

374
00:12:51,680 --> 00:12:54,480
python dictionary

375
00:12:53,600 --> 00:12:56,720
so

376
00:12:54,480 --> 00:12:58,480
we're going to store this information

377
00:12:56,720 --> 00:13:00,000
in a 2d array

378
00:12:58,480 --> 00:13:01,440
and

379
00:13:00,000 --> 00:13:03,440
the rows are going to be the first

380
00:13:01,440 --> 00:13:05,279
character of the bigram and the columns

381
00:13:03,440 --> 00:13:06,880
are going to be the second character and

382
00:13:05,279 --> 00:13:08,880
each entry in this two-dimensional array

383
00:13:06,880 --> 00:13:11,279
will tell us how often that first

384
00:13:08,880 --> 00:13:12,639
character files the second character in

385
00:13:11,279 --> 00:13:14,639
the data set

386
00:13:12,639 --> 00:13:16,079
so in particular the array

387
00:13:14,639 --> 00:13:18,800
representation that we're going to use

388
00:13:16,079 --> 00:13:20,240
or the library is that of pytorch

389
00:13:18,800 --> 00:13:22,399
and pytorch is a deep

390
00:13:20,240 --> 00:13:25,200
learning neural network framework but

391
00:13:22,399 --> 00:13:26,160
part of it is also this torch.tensor

392
00:13:25,200 --> 00:13:28,000
which allows us to create

393
00:13:26,160 --> 00:13:29,760
multi-dimensional arrays and manipulate

394
00:13:28,000 --> 00:13:30,720
them very efficiently

395
00:13:29,760 --> 00:13:32,720
so

396
00:13:30,720 --> 00:13:34,720
let's import pytorch which you can do by

397
00:13:32,720 --> 00:13:36,320
import torch

398
00:13:34,720 --> 00:13:37,440
and then we can create

399
00:13:36,320 --> 00:13:40,639
arrays

400
00:13:37,440 --> 00:13:42,880
so let's create a array of zeros

401
00:13:40,639 --> 00:13:44,560
and we give it a

402
00:13:42,880 --> 00:13:47,040
size of this array let's create a three

403
00:13:44,560 --> 00:13:48,560
by five array as an example

404
00:13:47,040 --> 00:13:51,519
and

405
00:13:48,560 --> 00:13:53,839
this is a three by five array of zeros

406
00:13:51,519 --> 00:13:56,560
and by default you'll notice a.d type

407
00:13:53,839 --> 00:13:58,160
which is short for data type is float32

408
00:13:56,560 --> 00:13:59,440
so these are single precision floating

409
00:13:58,160 --> 00:14:01,760
point numbers

410
00:13:59,440 --> 00:14:04,320
because we are going to represent counts

411
00:14:01,760 --> 00:14:06,000
let's actually use d type as torch dot

412
00:14:04,320 --> 00:14:07,760
and 32

413
00:14:06,000 --> 00:14:10,079
so these are

414
00:14:07,760 --> 00:14:12,800
32-bit integers

415
00:14:10,079 --> 00:14:14,560
so now you see that we have integer data

416
00:14:12,800 --> 00:14:17,360
inside this tensor

417
00:14:14,560 --> 00:14:18,959
now tensors allow us to really

418
00:14:17,360 --> 00:14:20,720
manipulate all the individual entries

419
00:14:18,959 --> 00:14:22,880
and do it very efficiently

420
00:14:20,720 --> 00:14:23,680
so for example if we want to change this

421
00:14:22,880 --> 00:14:25,920
bit

422
00:14:23,680 --> 00:14:29,680
we have to index into the tensor and in

423
00:14:25,920 --> 00:14:31,440
particular here this is the first row

424
00:14:29,680 --> 00:14:34,160
and the

425
00:14:31,440 --> 00:14:37,839
because it's zero indexed so this is row

426
00:14:34,160 --> 00:14:38,720
index one and column index zero one two

427
00:14:37,839 --> 00:14:41,839
three

428
00:14:38,720 --> 00:14:43,600
so a at one comma three we can set that

429
00:14:41,839 --> 00:14:47,040
to one

430
00:14:43,600 --> 00:14:48,720
and then a we'll have a 1 over there

431
00:14:47,040 --> 00:14:52,480
we can of course also do things like

432
00:14:48,720 --> 00:14:53,839
this so now a will be 2 over there

433
00:14:52,480 --> 00:14:56,079
or 3.

434
00:14:53,839 --> 00:14:57,199
and also we can for example say a 0 0 is

435
00:14:56,079 --> 00:15:00,079
5

436
00:14:57,199 --> 00:15:02,240
and then a will have a 5 over here

437
00:15:00,079 --> 00:15:04,320
so that's how we can index into the

438
00:15:02,240 --> 00:15:06,480
arrays now of course the array that we

439
00:15:04,320 --> 00:15:08,800
are interested in is much much bigger so

440
00:15:06,480 --> 00:15:09,920
for our purposes we have 26 letters of

441
00:15:08,800 --> 00:15:12,399
the alphabet

442
00:15:09,920 --> 00:15:14,000
and then we have two special characters

443
00:15:12,399 --> 00:15:18,160
s and e

444
00:15:14,000 --> 00:15:19,199
so uh we want 26 plus 2 or 28 by 28

445
00:15:18,160 --> 00:15:21,120
array

446
00:15:19,199 --> 00:15:22,560
and let's call it the capital n because

447
00:15:21,120 --> 00:15:24,480
it's going to represent sort of the

448
00:15:22,560 --> 00:15:26,720
counts

449
00:15:24,480 --> 00:15:28,720
let me erase this stuff

450
00:15:26,720 --> 00:15:30,399
so that's the array that starts at zeros

451
00:15:28,720 --> 00:15:33,680
28 by 28

452
00:15:30,399 --> 00:15:34,560
and now let's copy paste this

453
00:15:33,680 --> 00:15:36,959
here

454
00:15:34,560 --> 00:15:39,120
but instead of having a dictionary b

455
00:15:36,959 --> 00:15:40,959
which we're going to erase we now have

456
00:15:39,120 --> 00:15:42,399
an n

457
00:15:40,959 --> 00:15:44,160
now the problem here is that we have

458
00:15:42,399 --> 00:15:45,519
these characters which are strings but

459
00:15:44,160 --> 00:15:48,079
we have to now

460
00:15:45,519 --> 00:15:50,480
um basically index into a

461
00:15:48,079 --> 00:15:52,079
um array and we have to index using

462
00:15:50,480 --> 00:15:55,199
integers so we need some kind of a

463
00:15:52,079 --> 00:15:56,800
lookup table from characters to integers

464
00:15:55,199 --> 00:15:58,000
so let's construct such a character

465
00:15:56,800 --> 00:15:59,279
array

466
00:15:58,000 --> 00:16:01,120
and the way we're going to do this is

467
00:15:59,279 --> 00:16:02,720
we're going to take all the words which

468
00:16:01,120 --> 00:16:04,320
is a list of strings

469
00:16:02,720 --> 00:16:06,000
we're going to concatenate all of it

470
00:16:04,320 --> 00:16:07,600
into a massive string so this is just

471
00:16:06,000 --> 00:16:09,199
simply the entire data set as a single

472
00:16:07,600 --> 00:16:10,800
string

473
00:16:09,199 --> 00:16:13,519
we're going to pass this to the set

474
00:16:10,800 --> 00:16:14,399
constructor which takes this massive

475
00:16:13,519 --> 00:16:16,959
string

476
00:16:14,399 --> 00:16:18,880
and throws out duplicates because sets

477
00:16:16,959 --> 00:16:21,120
do not allow duplicates

478
00:16:18,880 --> 00:16:24,160
so set of this will just be the set of

479
00:16:21,120 --> 00:16:26,000
all the lowercase characters

480
00:16:24,160 --> 00:16:28,240
and there should be a total of 26 of

481
00:16:26,000 --> 00:16:28,240
them

482
00:16:28,560 --> 00:16:32,639
and now we actually don't want a set we

483
00:16:29,920 --> 00:16:34,560
want a list

484
00:16:32,639 --> 00:16:36,160
but we don't want a list sorted in some

485
00:16:34,560 --> 00:16:37,600
weird arbitrary way we want it to be

486
00:16:36,160 --> 00:16:39,759
sorted

487
00:16:37,600 --> 00:16:41,839
from a to z

488
00:16:39,759 --> 00:16:45,040
so sorted list

489
00:16:41,839 --> 00:16:45,040
so those are our characters

490
00:16:45,600 --> 00:16:49,920
now what we want is this lookup table as

491
00:16:47,279 --> 00:16:52,959
i mentioned so let's create a special

492
00:16:49,920 --> 00:16:55,759
s2i i will call it

493
00:16:52,959 --> 00:16:58,800
um s is string or character and this

494
00:16:55,759 --> 00:16:59,920
will be an s2i mapping

495
00:16:58,800 --> 00:17:04,240
for

496
00:16:59,920 --> 00:17:06,880
is in enumerate of these characters

497
00:17:04,240 --> 00:17:10,000
so enumerate basically gives us this

498
00:17:06,880 --> 00:17:12,319
iterator over the integer index and the

499
00:17:10,000 --> 00:17:15,199
actual element of the list and then we

500
00:17:12,319 --> 00:17:16,720
are mapping the character to the integer

501
00:17:15,199 --> 00:17:19,919
so s2i

502
00:17:16,720 --> 00:17:23,520
is a mapping from a to 0 b to 1 etc all

503
00:17:19,919 --> 00:17:23,520
the way from z to 25

504
00:17:24,079 --> 00:17:27,199
and that's going to be useful here but

505
00:17:25,439 --> 00:17:29,760
we actually also have to specifically

506
00:17:27,199 --> 00:17:33,600
set that s will be 26

507
00:17:29,760 --> 00:17:35,919
and s to i at e will be 27 right because

508
00:17:33,600 --> 00:17:38,080
z was 25.

509
00:17:35,919 --> 00:17:39,840
so those are the lookups and now we can

510
00:17:38,080 --> 00:17:41,520
come here and we can map

511
00:17:39,840 --> 00:17:42,720
both character 1 and character 2 to

512
00:17:41,520 --> 00:17:45,280
their integers

513
00:17:42,720 --> 00:17:49,360
so this will be s2i at character 1

514
00:17:45,280 --> 00:17:52,080
and ix2 will be s2i of character 2.

515
00:17:49,360 --> 00:17:55,679
and now we should be able to

516
00:17:52,080 --> 00:17:58,880
do this line but using our array so n at

517
00:17:55,679 --> 00:18:00,559
x1 ix2 this is the two-dimensional array

518
00:17:58,880 --> 00:18:02,960
indexing i've shown you before

519
00:18:00,559 --> 00:18:04,799
and honestly just plus equals one

520
00:18:02,960 --> 00:18:06,160
because everything starts at

521
00:18:04,799 --> 00:18:07,919
zero

522
00:18:06,160 --> 00:18:08,880
so this should

523
00:18:07,919 --> 00:18:12,960
work

524
00:18:08,880 --> 00:18:15,039
and give us a large 28 by 28 array

525
00:18:12,960 --> 00:18:16,880
of all these counts so

526
00:18:15,039 --> 00:18:19,120
if we print n

527
00:18:16,880 --> 00:18:21,840
this is the array but of course it looks

528
00:18:19,120 --> 00:18:23,679
ugly so let's erase this ugly mess and

529
00:18:21,840 --> 00:18:24,799
let's try to visualize it a bit more

530
00:18:23,679 --> 00:18:26,880
nicer

531
00:18:24,799 --> 00:18:28,799
so for that we're going to use a library

532
00:18:26,880 --> 00:18:30,320
called matplotlib

533
00:18:28,799 --> 00:18:32,480
so matplotlib allows us to create

534
00:18:30,320 --> 00:18:36,080
figures so we can do things like plt

535
00:18:32,480 --> 00:18:39,039
item show of the counter array

536
00:18:36,080 --> 00:18:41,919
so this is the 28x28 array

537
00:18:39,039 --> 00:18:43,840
and this is structure but even this i

538
00:18:41,919 --> 00:18:45,679
would say is still pretty ugly

539
00:18:43,840 --> 00:18:47,520
so we're going to try to create a much

540
00:18:45,679 --> 00:18:49,679
nicer visualization of it and i wrote a

541
00:18:47,520 --> 00:18:51,840
bunch of code for that

542
00:18:49,679 --> 00:18:53,840
the first thing we're going to need is

543
00:18:51,840 --> 00:18:57,360
we're going to need to invert

544
00:18:53,840 --> 00:18:59,760
this array here this dictionary so s2i

545
00:18:57,360 --> 00:19:02,240
is mapping from s to i

546
00:18:59,760 --> 00:19:04,480
and in i2s we're going to reverse this

547
00:19:02,240 --> 00:19:06,559
dictionary so iterator of all the items

548
00:19:04,480 --> 00:19:08,320
and just reverse that array

549
00:19:06,559 --> 00:19:12,559
so i2s

550
00:19:08,320 --> 00:19:14,160
maps inversely from 0 to a 1 to b etc

551
00:19:12,559 --> 00:19:16,160
so we'll need that

552
00:19:14,160 --> 00:19:17,600
and then here's the code that i came up

553
00:19:16,160 --> 00:19:20,000
with to try to make this a little bit

554
00:19:17,600 --> 00:19:20,000
nicer

555
00:19:20,480 --> 00:19:23,360
we create a figure

556
00:19:22,000 --> 00:19:24,400
we plot

557
00:19:23,360 --> 00:19:26,080
n

558
00:19:24,400 --> 00:19:28,000
and then we do and then we visualize a

559
00:19:26,080 --> 00:19:31,280
bunch of things later let me just run it

560
00:19:28,000 --> 00:19:31,280
so you get a sense of what this is

561
00:19:31,919 --> 00:19:35,280
okay

562
00:19:32,960 --> 00:19:37,120
so you see here that we have

563
00:19:35,280 --> 00:19:39,840
the array spaced out

564
00:19:37,120 --> 00:19:42,320
and every one of these is basically like

565
00:19:39,840 --> 00:19:44,640
b follows g zero times

566
00:19:42,320 --> 00:19:47,840
b follows h 41 times

567
00:19:44,640 --> 00:19:49,280
um so a follows j 175 times

568
00:19:47,840 --> 00:19:52,880
and so what you can see that i'm doing

569
00:19:49,280 --> 00:19:54,240
here is first i show that entire array

570
00:19:52,880 --> 00:19:56,720
and then i iterate over all the

571
00:19:54,240 --> 00:19:59,280
individual little cells here

572
00:19:56,720 --> 00:20:02,559
and i create a character string here

573
00:19:59,280 --> 00:20:05,200
which is the inverse mapping i2s of the

574
00:20:02,559 --> 00:20:06,720
integer i and the integer j so those are

575
00:20:05,200 --> 00:20:08,559
the bi-grams in a character

576
00:20:06,720 --> 00:20:12,000
representation

577
00:20:08,559 --> 00:20:14,159
and then i plot just the diagram text

578
00:20:12,000 --> 00:20:16,000
and then i plot the number of times that

579
00:20:14,159 --> 00:20:17,520
this bigram occurs

580
00:20:16,000 --> 00:20:20,000
now the reason that there's a dot item

581
00:20:17,520 --> 00:20:22,960
here is because when you index into

582
00:20:20,000 --> 00:20:25,919
these arrays these are torch tensors

583
00:20:22,960 --> 00:20:28,080
you see that we still get a tensor back

584
00:20:25,919 --> 00:20:29,919
so the type of this thing you'd think it

585
00:20:28,080 --> 00:20:31,919
would be just an integer 149 but it's

586
00:20:29,919 --> 00:20:32,880
actually a torch.tensor

587
00:20:31,919 --> 00:20:35,360
and so

588
00:20:32,880 --> 00:20:38,400
if you do dot item then it will pop out

589
00:20:35,360 --> 00:20:40,640
that in individual integer

590
00:20:38,400 --> 00:20:42,480
so it will just be 149.

591
00:20:40,640 --> 00:20:43,760
so that's what's happening there and

592
00:20:42,480 --> 00:20:45,200
these are just some options to make it

593
00:20:43,760 --> 00:20:48,640
look nice

594
00:20:45,200 --> 00:20:48,640
so what is the structure of this array

595
00:20:49,120 --> 00:20:52,159
we have all these counts and we see that

596
00:20:50,480 --> 00:20:53,919
some of them occur often and some of

597
00:20:52,159 --> 00:20:56,000
them do not occur often

598
00:20:53,919 --> 00:20:57,120
now if you scrutinize this carefully you

599
00:20:56,000 --> 00:20:58,559
will notice that we're not actually

600
00:20:57,120 --> 00:21:00,559
being very clever

601
00:20:58,559 --> 00:21:02,240
that's because when you come over here

602
00:21:00,559 --> 00:21:04,720
you'll notice that for example we have

603
00:21:02,240 --> 00:21:06,960
an entire row of completely zeros and

604
00:21:04,720 --> 00:21:08,640
that's because the end character

605
00:21:06,960 --> 00:21:10,159
is never possibly going to be the first

606
00:21:08,640 --> 00:21:12,400
character of a bi-gram because we're

607
00:21:10,159 --> 00:21:14,240
always placing these end tokens all at

608
00:21:12,400 --> 00:21:16,799
the end of the diagram

609
00:21:14,240 --> 00:21:19,600
similarly we have entire columns zeros

610
00:21:16,799 --> 00:21:21,360
here because the s

611
00:21:19,600 --> 00:21:23,679
character will never possibly be the

612
00:21:21,360 --> 00:21:25,600
second element of a bigram because we

613
00:21:23,679 --> 00:21:27,600
always start with s and we end with e

614
00:21:25,600 --> 00:21:30,080
and we only have the words in between

615
00:21:27,600 --> 00:21:32,240
so we have an entire column of zeros an

616
00:21:30,080 --> 00:21:34,320
entire row of zeros and in this little

617
00:21:32,240 --> 00:21:36,799
two by two matrix here as well the only

618
00:21:34,320 --> 00:21:38,480
one that can possibly happen is if s

619
00:21:36,799 --> 00:21:41,360
directly follows e

620
00:21:38,480 --> 00:21:43,600
that can be non-zero if we have a word

621
00:21:41,360 --> 00:21:44,880
that has no letters so in that case

622
00:21:43,600 --> 00:21:47,440
there's no letters in the word it's an

623
00:21:44,880 --> 00:21:50,080
empty word and we just have s follows e

624
00:21:47,440 --> 00:21:51,679
but the other ones are just not possible

625
00:21:50,080 --> 00:21:53,679
and so we're basically wasting space and

626
00:21:51,679 --> 00:21:55,520
not only that but the s and the e are

627
00:21:53,679 --> 00:21:57,440
getting very crowded here

628
00:21:55,520 --> 00:21:58,960
i was using these brackets because

629
00:21:57,440 --> 00:22:00,320
there's convention and natural language

630
00:21:58,960 --> 00:22:03,120
processing to use these kinds of

631
00:22:00,320 --> 00:22:05,120
brackets to denote special tokens

632
00:22:03,120 --> 00:22:06,880
but we're going to use something else

633
00:22:05,120 --> 00:22:08,159
so let's fix all this and make it

634
00:22:06,880 --> 00:22:09,520
prettier

635
00:22:08,159 --> 00:22:11,120
we're not actually going to have two

636
00:22:09,520 --> 00:22:12,960
special tokens we're only going to have

637
00:22:11,120 --> 00:22:13,679
one special token

638
00:22:12,960 --> 00:22:15,440
so

639
00:22:13,679 --> 00:22:18,799
we're going to have n by n

640
00:22:15,440 --> 00:22:20,400
array of 27 by 27 instead

641
00:22:18,799 --> 00:22:22,400
instead of having two

642
00:22:20,400 --> 00:22:24,640
we will just have one and i will call it

643
00:22:22,400 --> 00:22:27,280
a dot

644
00:22:24,640 --> 00:22:30,400
okay

645
00:22:27,280 --> 00:22:31,679
let me swing this over here

646
00:22:30,400 --> 00:22:33,520
now one more thing that i would like to

647
00:22:31,679 --> 00:22:36,159
do is i would actually like to make this

648
00:22:33,520 --> 00:22:37,760
special character half position zero

649
00:22:36,159 --> 00:22:39,760
and i would like to offset all the other

650
00:22:37,760 --> 00:22:40,480
letters off i find that a little bit

651
00:22:39,760 --> 00:22:42,559
more

652
00:22:40,480 --> 00:22:44,640
pleasing

653
00:22:42,559 --> 00:22:46,400
so

654
00:22:44,640 --> 00:22:48,480
we need a plus one here so that the

655
00:22:46,400 --> 00:22:49,760
first character which is a will start at

656
00:22:48,480 --> 00:22:51,200
one

657
00:22:49,760 --> 00:22:55,840
so s2i

658
00:22:51,200 --> 00:22:56,720
will now be a starts at one and dot is 0

659
00:22:55,840 --> 00:22:58,880
and

660
00:22:56,720 --> 00:23:00,480
i2s of course we're not changing this

661
00:22:58,880 --> 00:23:02,880
because i2s just creates a reverse

662
00:23:00,480 --> 00:23:04,320
mapping and this will work fine so 1 is

663
00:23:02,880 --> 00:23:06,480
a 2 is b

664
00:23:04,320 --> 00:23:09,039
0 is dot

665
00:23:06,480 --> 00:23:10,240
so we've reversed that here

666
00:23:09,039 --> 00:23:12,880
we have

667
00:23:10,240 --> 00:23:14,720
a dot and a dot

668
00:23:12,880 --> 00:23:17,840
this should work fine

669
00:23:14,720 --> 00:23:18,720
make sure i start at zeros

670
00:23:17,840 --> 00:23:20,559
count

671
00:23:18,720 --> 00:23:22,559
and then here we don't go up to 28 we go

672
00:23:20,559 --> 00:23:26,240
up to 27

673
00:23:22,559 --> 00:23:26,240
and this should just work

674
00:23:30,799 --> 00:23:33,600
okay

675
00:23:31,679 --> 00:23:35,360
so we see that dot never happened it's

676
00:23:33,600 --> 00:23:36,400
at zero because we don't have empty

677
00:23:35,360 --> 00:23:38,960
words

678
00:23:36,400 --> 00:23:40,720
then this row here now is just uh very

679
00:23:38,960 --> 00:23:44,880
simply the um

680
00:23:40,720 --> 00:23:47,760
counts for all the first letters so

681
00:23:44,880 --> 00:23:50,159
uh j starts a word h starts a word i

682
00:23:47,760 --> 00:23:51,760
starts a word etc and then these are all

683
00:23:50,159 --> 00:23:52,960
the ending

684
00:23:51,760 --> 00:23:54,640
characters

685
00:23:52,960 --> 00:23:56,960
and in between we have the structure of

686
00:23:54,640 --> 00:23:59,520
what characters follow each other

687
00:23:56,960 --> 00:24:00,720
so this is the counts array of our

688
00:23:59,520 --> 00:24:03,279
entire

689
00:24:00,720 --> 00:24:04,960
data set so this array actually has all

690
00:24:03,279 --> 00:24:07,440
the information necessary for us to

691
00:24:04,960 --> 00:24:09,600
actually sample from this bigram

692
00:24:07,440 --> 00:24:11,760
uh character level language model

693
00:24:09,600 --> 00:24:13,200
and um roughly speaking what we're going

694
00:24:11,760 --> 00:24:14,880
to do is we're just going to start

695
00:24:13,200 --> 00:24:16,720
following these probabilities and these

696
00:24:14,880 --> 00:24:18,720
counts and we're going to start sampling

697
00:24:16,720 --> 00:24:20,480
from the from the model

698
00:24:18,720 --> 00:24:23,679
so in the beginning of course

699
00:24:20,480 --> 00:24:24,559
we start with the dot the start token

700
00:24:23,679 --> 00:24:27,440
dot

701
00:24:24,559 --> 00:24:30,480
so to sample the first character of a

702
00:24:27,440 --> 00:24:32,640
name we're looking at this row here

703
00:24:30,480 --> 00:24:34,559
so we see that we have the counts and

704
00:24:32,640 --> 00:24:37,200
those concepts terminally are telling us

705
00:24:34,559 --> 00:24:39,520
how often any one of these characters is

706
00:24:37,200 --> 00:24:41,840
to start a word

707
00:24:39,520 --> 00:24:44,720
so if we take this n

708
00:24:41,840 --> 00:24:47,440
and we grab the first row

709
00:24:44,720 --> 00:24:48,320
we can do that by using just indexing as

710
00:24:47,440 --> 00:24:51,600
zero

711
00:24:48,320 --> 00:24:53,600
and then using this notation column for

712
00:24:51,600 --> 00:24:56,480
the rest of that row

713
00:24:53,600 --> 00:24:58,799
so n zero colon

714
00:24:56,480 --> 00:25:00,559
is indexing into the zeroth

715
00:24:58,799 --> 00:25:01,919
row and then it's grabbing all the

716
00:25:00,559 --> 00:25:03,520
columns

717
00:25:01,919 --> 00:25:05,200
and so this will give us a

718
00:25:03,520 --> 00:25:08,320
one-dimensional array

719
00:25:05,200 --> 00:25:10,960
of the first row so zero four four ten

720
00:25:08,320 --> 00:25:13,200
you know zero four four ten one three oh

721
00:25:10,960 --> 00:25:15,679
six one five four two etc it's just the

722
00:25:13,200 --> 00:25:19,840
first row the shape of this

723
00:25:15,679 --> 00:25:21,120
is 27 it's just the row of 27

724
00:25:19,840 --> 00:25:22,400
and the other way that you can do this

725
00:25:21,120 --> 00:25:23,679
also is you just you don't need to

726
00:25:22,400 --> 00:25:26,080
actually give this

727
00:25:23,679 --> 00:25:28,080
you just grab the zeroth row like this

728
00:25:26,080 --> 00:25:29,919
this is equivalent

729
00:25:28,080 --> 00:25:31,760
now these are the counts

730
00:25:29,919 --> 00:25:34,960
and now what we'd like to do is we'd

731
00:25:31,760 --> 00:25:36,080
like to basically um sample from this

732
00:25:34,960 --> 00:25:37,279
since these are the raw counts we

733
00:25:36,080 --> 00:25:39,120
actually have to convert this to

734
00:25:37,279 --> 00:25:42,799
probabilities

735
00:25:39,120 --> 00:25:44,960
so we create a probability vector

736
00:25:42,799 --> 00:25:48,159
so we'll take n of zero

737
00:25:44,960 --> 00:25:49,919
and we'll actually convert this to float

738
00:25:48,159 --> 00:25:51,760
first

739
00:25:49,919 --> 00:25:52,720
okay so these integers are converted to

740
00:25:51,760 --> 00:25:54,559
float

741
00:25:52,720 --> 00:25:56,240
floating point numbers and the reason

742
00:25:54,559 --> 00:25:58,799
we're creating floats is because we're

743
00:25:56,240 --> 00:26:00,799
about to normalize these counts

744
00:25:58,799 --> 00:26:03,760
so to create a probability distribution

745
00:26:00,799 --> 00:26:05,919
here we want to divide

746
00:26:03,760 --> 00:26:08,320
we basically want to do p p p divide p

747
00:26:05,919 --> 00:26:08,320
that sum

748
00:26:09,600 --> 00:26:13,679
and now we get a vector of smaller

749
00:26:11,440 --> 00:26:15,200
numbers and these are now probabilities

750
00:26:13,679 --> 00:26:18,720
so of course because we divided by the

751
00:26:15,200 --> 00:26:20,400
sum the sum of p now is 1.

752
00:26:18,720 --> 00:26:22,559
so this is a nice proper probability

753
00:26:20,400 --> 00:26:24,320
distribution it sums to 1 and this is

754
00:26:22,559 --> 00:26:26,000
giving us the probability for any single

755
00:26:24,320 --> 00:26:27,919
character to be the first

756
00:26:26,000 --> 00:26:29,600
character of a word

757
00:26:27,919 --> 00:26:31,520
so now we can try to sample from this

758
00:26:29,600 --> 00:26:33,000
distribution to sample from these

759
00:26:31,520 --> 00:26:34,880
distributions we're going to use

760
00:26:33,000 --> 00:26:36,080
storch.multinomial which i've pulled up

761
00:26:34,880 --> 00:26:40,320
here

762
00:26:36,080 --> 00:26:40,320
so torch.multinomial returns uh

763
00:26:40,480 --> 00:26:44,320
samples from the multinomial probability

764
00:26:42,240 --> 00:26:46,400
distribution which is a complicated way

765
00:26:44,320 --> 00:26:48,400
of saying you give me probabilities and

766
00:26:46,400 --> 00:26:49,440
i will give you integers which are

767
00:26:48,400 --> 00:26:51,520
sampled

768
00:26:49,440 --> 00:26:53,120
according to the property distribution

769
00:26:51,520 --> 00:26:54,720
so this is the signature of the method

770
00:26:53,120 --> 00:26:57,360
and to make everything deterministic

771
00:26:54,720 --> 00:26:59,200
we're going to use a generator object in

772
00:26:57,360 --> 00:27:00,880
pytorch

773
00:26:59,200 --> 00:27:02,480
so this makes everything deterministic

774
00:27:00,880 --> 00:27:04,080
so when you run this on your computer

775
00:27:02,480 --> 00:27:05,840
you're going to the exact get the exact

776
00:27:04,080 --> 00:27:07,200
same results that i'm getting here on my

777
00:27:05,840 --> 00:27:10,640
computer

778
00:27:07,200 --> 00:27:10,640
so let me show you how this works

779
00:27:12,640 --> 00:27:18,159
here's the deterministic way of creating

780
00:27:15,360 --> 00:27:20,000
a torch generator object

781
00:27:18,159 --> 00:27:21,200
seeding it with some number that we can

782
00:27:20,000 --> 00:27:23,600
agree on

783
00:27:21,200 --> 00:27:24,799
so that seeds a generator gets gives us

784
00:27:23,600 --> 00:27:26,480
an object g

785
00:27:24,799 --> 00:27:28,320
and then we can pass that g

786
00:27:26,480 --> 00:27:30,159
to a function

787
00:27:28,320 --> 00:27:32,640
that creates um

788
00:27:30,159 --> 00:27:35,200
here random numbers twerk.rand creates

789
00:27:32,640 --> 00:27:37,520
random numbers three of them

790
00:27:35,200 --> 00:27:40,399
and it's using this generator object to

791
00:27:37,520 --> 00:27:41,840
as a source of randomness

792
00:27:40,399 --> 00:27:44,240
so

793
00:27:41,840 --> 00:27:46,480
without normalizing it

794
00:27:44,240 --> 00:27:48,399
i can just print

795
00:27:46,480 --> 00:27:50,720
this is sort of like numbers between 0

796
00:27:48,399 --> 00:27:53,039
and 1 that are random according to this

797
00:27:50,720 --> 00:27:54,559
thing and whenever i run it again

798
00:27:53,039 --> 00:27:56,240
i'm always going to get the same result

799
00:27:54,559 --> 00:27:58,720
because i keep using the same generator

800
00:27:56,240 --> 00:28:01,600
object which i'm seeing here

801
00:27:58,720 --> 00:28:04,000
and then if i divide

802
00:28:01,600 --> 00:28:05,919
to normalize i'm going to get a nice

803
00:28:04,000 --> 00:28:07,520
probability distribution of just three

804
00:28:05,919 --> 00:28:09,279
elements

805
00:28:07,520 --> 00:28:11,600
and then we can use torsion multinomial

806
00:28:09,279 --> 00:28:13,600
to draw samples from it so this is what

807
00:28:11,600 --> 00:28:16,720
that looks like

808
00:28:13,600 --> 00:28:18,320
tertiary multinomial we'll take the

809
00:28:16,720 --> 00:28:20,960
torch tensor

810
00:28:18,320 --> 00:28:22,480
of probability distributions

811
00:28:20,960 --> 00:28:24,480
then we can ask for a number of samples

812
00:28:22,480 --> 00:28:26,960
let's say 20.

813
00:28:24,480 --> 00:28:28,799
replacement equals true means that when

814
00:28:26,960 --> 00:28:30,880
we draw an element

815
00:28:28,799 --> 00:28:32,960
we will uh we can draw it and then we

816
00:28:30,880 --> 00:28:35,760
can put it back into the list of

817
00:28:32,960 --> 00:28:37,440
eligible indices to draw again

818
00:28:35,760 --> 00:28:39,760
and we have to specify replacement as

819
00:28:37,440 --> 00:28:41,520
true because by default uh for some

820
00:28:39,760 --> 00:28:42,960
reason it's false

821
00:28:41,520 --> 00:28:44,240
and i think

822
00:28:42,960 --> 00:28:45,679
you know it's just something to be

823
00:28:44,240 --> 00:28:47,600
careful with

824
00:28:45,679 --> 00:28:49,279
and the generator is passed in here so

825
00:28:47,600 --> 00:28:51,840
we're going to always get deterministic

826
00:28:49,279 --> 00:28:53,679
results the same results so if i run

827
00:28:51,840 --> 00:28:55,279
these two

828
00:28:53,679 --> 00:28:57,120
we're going to get a bunch of samples

829
00:28:55,279 --> 00:28:58,880
from this distribution

830
00:28:57,120 --> 00:29:00,399
now you'll notice here that the

831
00:28:58,880 --> 00:29:04,480
probability for the

832
00:29:00,399 --> 00:29:08,240
first element in this tensor is 60

833
00:29:04,480 --> 00:29:10,559
so in these 20 samples we'd expect 60 of

834
00:29:08,240 --> 00:29:12,799
them to be zero

835
00:29:10,559 --> 00:29:14,159
we'd expect thirty percent of them to be

836
00:29:12,799 --> 00:29:17,440
one

837
00:29:14,159 --> 00:29:19,679
and because the the element index two

838
00:29:17,440 --> 00:29:22,240
has only ten percent probability very

839
00:29:19,679 --> 00:29:24,080
few of these samples should be two and

840
00:29:22,240 --> 00:29:25,279
indeed we only have a small number of

841
00:29:24,080 --> 00:29:28,880
twos

842
00:29:25,279 --> 00:29:31,039
and we can sample as many as we'd like

843
00:29:28,880 --> 00:29:33,279
and the more we sample the more

844
00:29:31,039 --> 00:29:35,840
these numbers should um roughly have the

845
00:29:33,279 --> 00:29:38,320
distribution here

846
00:29:35,840 --> 00:29:41,600
so we should have lots of zeros

847
00:29:38,320 --> 00:29:44,880
half as many um

848
00:29:41,600 --> 00:29:46,080
ones and we should have um three times

849
00:29:44,880 --> 00:29:49,039
as few

850
00:29:46,080 --> 00:29:50,480
oh sorry s few ones and three times as

851
00:29:49,039 --> 00:29:51,679
few uh

852
00:29:50,480 --> 00:29:53,440
twos

853
00:29:51,679 --> 00:29:55,679
so you see that we have very few twos we

854
00:29:53,440 --> 00:29:57,600
have some ones and most of them are zero

855
00:29:55,679 --> 00:29:58,799
so that's what torsion multinomial is

856
00:29:57,600 --> 00:30:01,039
doing

857
00:29:58,799 --> 00:30:02,559
for us here

858
00:30:01,039 --> 00:30:05,120
we are interested in this row we've

859
00:30:02,559 --> 00:30:05,120
created this

860
00:30:05,600 --> 00:30:09,600
p here

861
00:30:06,799 --> 00:30:11,360
and now we can sample from it

862
00:30:09,600 --> 00:30:12,799
so if we use the same

863
00:30:11,360 --> 00:30:14,399
seed

864
00:30:12,799 --> 00:30:18,080
and then we sample from this

865
00:30:14,399 --> 00:30:22,640
distribution let's just get one sample

866
00:30:18,080 --> 00:30:25,200
then we see that the sample is say 13.

867
00:30:22,640 --> 00:30:27,360
so this will be the index

868
00:30:25,200 --> 00:30:30,399
and let's you see how it's a tensor that

869
00:30:27,360 --> 00:30:32,880
wraps 13 we again have to use that item

870
00:30:30,399 --> 00:30:35,279
to pop out that integer

871
00:30:32,880 --> 00:30:37,360
and now index would be just the number

872
00:30:35,279 --> 00:30:40,159
13.

873
00:30:37,360 --> 00:30:43,520
and of course the um we can do

874
00:30:40,159 --> 00:30:45,039
we can map the i2s of ix to figure out

875
00:30:43,520 --> 00:30:48,000
exactly which character

876
00:30:45,039 --> 00:30:50,080
we're sampling here we're sampling m

877
00:30:48,000 --> 00:30:51,120
so we're saying that the first character

878
00:30:50,080 --> 00:30:53,039
is

879
00:30:51,120 --> 00:30:55,120
in our generation

880
00:30:53,039 --> 00:30:57,200
and just looking at the road here

881
00:30:55,120 --> 00:30:59,679
m was drawn and you we can see that m

882
00:30:57,200 --> 00:31:01,039
actually starts a large number of words

883
00:30:59,679 --> 00:31:04,640
uh m

884
00:31:01,039 --> 00:31:06,240
started 2 500 words out of 32 000 words

885
00:31:04,640 --> 00:31:07,919
so almost

886
00:31:06,240 --> 00:31:09,760
a bit less than 10 percent of the words

887
00:31:07,919 --> 00:31:13,200
start with them so this was actually a

888
00:31:09,760 --> 00:31:15,120
fairly likely character to draw

889
00:31:13,200 --> 00:31:16,480
um

890
00:31:15,120 --> 00:31:18,080
so that would be the first character of

891
00:31:16,480 --> 00:31:20,480
our work and now we can continue to

892
00:31:18,080 --> 00:31:22,880
sample more characters because now we

893
00:31:20,480 --> 00:31:24,640
know that m started

894
00:31:22,880 --> 00:31:26,880
m is already sampled

895
00:31:24,640 --> 00:31:29,519
so now to draw the next character we

896
00:31:26,880 --> 00:31:30,880
will come back here and we will look for

897
00:31:29,519 --> 00:31:32,640
the row

898
00:31:30,880 --> 00:31:34,559
that starts with m

899
00:31:32,640 --> 00:31:36,559
so you see m

900
00:31:34,559 --> 00:31:39,440
and we have a row here

901
00:31:36,559 --> 00:31:43,200
so we see that m dot is

902
00:31:39,440 --> 00:31:44,960
516 m a is this many and b is this many

903
00:31:43,200 --> 00:31:46,559
etc so these are the counts for the next

904
00:31:44,960 --> 00:31:48,559
row and that's the next character that

905
00:31:46,559 --> 00:31:50,000
we are going to now generate

906
00:31:48,559 --> 00:31:51,600
so i think we are ready to actually just

907
00:31:50,000 --> 00:31:52,720
write out the loop because i think

908
00:31:51,600 --> 00:31:54,480
you're starting to get a sense of how

909
00:31:52,720 --> 00:31:56,159
this is going to go

910
00:31:54,480 --> 00:31:57,840
the um

911
00:31:56,159 --> 00:32:02,320
we always begin at

912
00:31:57,840 --> 00:32:04,880
index 0 because that's the start token

913
00:32:02,320 --> 00:32:06,080
and then while true

914
00:32:04,880 --> 00:32:08,399
we're going to grab the row

915
00:32:06,080 --> 00:32:11,120
corresponding to index

916
00:32:08,399 --> 00:32:14,399
that we're currently on so that's p

917
00:32:11,120 --> 00:32:18,480
so that's n array at ix

918
00:32:14,399 --> 00:32:18,480
converted to float is rp

919
00:32:19,039 --> 00:32:24,640
then we normalize

920
00:32:21,200 --> 00:32:24,640
this p to sum to one

921
00:32:25,279 --> 00:32:30,720
i accidentally ran the infinite loop we

922
00:32:28,240 --> 00:32:33,760
normalize p to something one

923
00:32:30,720 --> 00:32:35,600
then we need this generator object

924
00:32:33,760 --> 00:32:36,960
now we're going to initialize up here

925
00:32:35,600 --> 00:32:39,919
and we're going to draw a single sample

926
00:32:36,960 --> 00:32:39,919
from this distribution

927
00:32:40,799 --> 00:32:46,559
and then this is going to tell us what

928
00:32:42,799 --> 00:32:48,720
index is going to be next

929
00:32:46,559 --> 00:32:49,679
if the index sampled is

930
00:32:48,720 --> 00:32:52,640
0

931
00:32:49,679 --> 00:32:55,360
then that's now the end token

932
00:32:52,640 --> 00:32:57,760
so we will break

933
00:32:55,360 --> 00:33:01,279
otherwise we are going to print

934
00:32:57,760 --> 00:33:01,279
s2i of ix

935
00:33:02,240 --> 00:33:04,799
i2s

936
00:33:05,279 --> 00:33:12,080
and uh that's pretty much it we're just

937
00:33:07,840 --> 00:33:13,840
uh this should work okay more

938
00:33:12,080 --> 00:33:16,960
so that's that's the name that we've

939
00:33:13,840 --> 00:33:21,120
sampled we started with m the next step

940
00:33:16,960 --> 00:33:21,120
was o then r and then dot

941
00:33:21,360 --> 00:33:26,399
and this dot we it here as well

942
00:33:24,799 --> 00:33:29,760
so

943
00:33:26,399 --> 00:33:29,760
let's now do this a few times

944
00:33:29,919 --> 00:33:36,559
so let's actually create an

945
00:33:33,440 --> 00:33:36,559
out list here

946
00:33:37,120 --> 00:33:39,679
and instead of printing we're going to

947
00:33:38,559 --> 00:33:43,760
append

948
00:33:39,679 --> 00:33:43,760
so out that append this character

949
00:33:44,399 --> 00:33:49,360
and then here let's just print it at the

950
00:33:46,480 --> 00:33:51,919
end so let's just join up all the outs

951
00:33:49,360 --> 00:33:53,360
and we're just going to print more okay

952
00:33:51,919 --> 00:33:55,039
now we're always getting the same result

953
00:33:53,360 --> 00:33:57,039
because of the generator

954
00:33:55,039 --> 00:34:00,080
so if we want to do this a few times we

955
00:33:57,039 --> 00:34:02,480
can go for i in range

956
00:34:00,080 --> 00:34:05,679
10 we can sample 10 names

957
00:34:02,480 --> 00:34:06,799
and we can just do that 10 times

958
00:34:05,679 --> 00:34:08,399
and these are the names that we're

959
00:34:06,799 --> 00:34:11,119
getting out

960
00:34:08,399 --> 00:34:11,119
let's do 20.

961
00:34:14,240 --> 00:34:16,480
i'll be honest with you this doesn't

962
00:34:15,440 --> 00:34:18,240
look right

963
00:34:16,480 --> 00:34:20,480
so i started a few minutes to convince

964
00:34:18,240 --> 00:34:22,399
myself that it actually is right

965
00:34:20,480 --> 00:34:24,800
the reason these samples are so terrible

966
00:34:22,399 --> 00:34:26,480
is that bigram language model

967
00:34:24,800 --> 00:34:27,679
is actually look just like really

968
00:34:26,480 --> 00:34:30,000
terrible

969
00:34:27,679 --> 00:34:30,960
we can generate a few more here

970
00:34:30,000 --> 00:34:33,200
and you can see that they're kind of

971
00:34:30,960 --> 00:34:36,159
like their name like a little bit like

972
00:34:33,200 --> 00:34:38,560
yanu o'reilly etc but they're just like

973
00:34:36,159 --> 00:34:40,480
totally messed up um

974
00:34:38,560 --> 00:34:42,879
and i mean the reason that this is so

975
00:34:40,480 --> 00:34:44,480
bad like we're generating h as a name

976
00:34:42,879 --> 00:34:47,200
but you have to think through

977
00:34:44,480 --> 00:34:49,760
it from the model's eyes it doesn't know

978
00:34:47,200 --> 00:34:52,480
that this h is the very first h all it

979
00:34:49,760 --> 00:34:55,839
knows is that h was previously and now

980
00:34:52,480 --> 00:34:57,040
how likely is h the last character well

981
00:34:55,839 --> 00:34:58,800
it's somewhat

982
00:34:57,040 --> 00:35:00,000
likely and so it just makes it last

983
00:34:58,800 --> 00:35:02,000
character it doesn't know that there

984
00:35:00,000 --> 00:35:04,400
were other things before it or there

985
00:35:02,000 --> 00:35:05,599
were not other things before it and so

986
00:35:04,400 --> 00:35:06,480
that's why it's generating all these

987
00:35:05,599 --> 00:35:08,079
like

988
00:35:06,480 --> 00:35:11,359
nonsense names

989
00:35:08,079 --> 00:35:11,359
another way to do this is

990
00:35:11,920 --> 00:35:14,480
to convince yourself that this is

991
00:35:12,960 --> 00:35:17,680
actually doing something reasonable even

992
00:35:14,480 --> 00:35:20,960
though it's so terrible is

993
00:35:17,680 --> 00:35:23,040
these little piece here are 27 right

994
00:35:20,960 --> 00:35:24,320
like 27.

995
00:35:23,040 --> 00:35:26,240
so how about if we did something like

996
00:35:24,320 --> 00:35:27,599
this

997
00:35:26,240 --> 00:35:28,880
instead of p having any structure

998
00:35:27,599 --> 00:35:30,720
whatsoever

999
00:35:28,880 --> 00:35:34,240
how about if p was just

1000
00:35:30,720 --> 00:35:34,240
torch dot once

1001
00:35:34,880 --> 00:35:39,359
of 27

1002
00:35:37,200 --> 00:35:42,800
by default this is a float 32 so this is

1003
00:35:39,359 --> 00:35:44,960
fine divide 27

1004
00:35:42,800 --> 00:35:47,119
so what i'm doing here is this is the

1005
00:35:44,960 --> 00:35:49,839
uniform distribution which will make

1006
00:35:47,119 --> 00:35:52,320
everything equally likely

1007
00:35:49,839 --> 00:35:54,160
and we can sample from that so let's see

1008
00:35:52,320 --> 00:35:55,920
if that does any better

1009
00:35:54,160 --> 00:35:57,440
okay so it's

1010
00:35:55,920 --> 00:35:59,119
this is what you have from a model that

1011
00:35:57,440 --> 00:36:01,440
is completely untrained where everything

1012
00:35:59,119 --> 00:36:03,520
is equally likely so it's obviously

1013
00:36:01,440 --> 00:36:07,040
garbage and then if we have a trained

1014
00:36:03,520 --> 00:36:09,119
model which is trained on just bi-grams

1015
00:36:07,040 --> 00:36:11,040
this is what we get so you can see that

1016
00:36:09,119 --> 00:36:14,079
it is more name-like it is actually

1017
00:36:11,040 --> 00:36:15,920
working it's just um

1018
00:36:14,079 --> 00:36:17,839
my gram is so terrible and we have to do

1019
00:36:15,920 --> 00:36:20,160
better now next i would like to fix an

1020
00:36:17,839 --> 00:36:21,920
inefficiency that we have going on here

1021
00:36:20,160 --> 00:36:24,320
because what we're doing here is we're

1022
00:36:21,920 --> 00:36:26,400
always fetching a row of n from the

1023
00:36:24,320 --> 00:36:27,680
counts matrix up ahead

1024
00:36:26,400 --> 00:36:28,960
and then we're always doing the same

1025
00:36:27,680 --> 00:36:30,800
things we're converting to float and

1026
00:36:28,960 --> 00:36:32,800
we're dividing and we're doing this

1027
00:36:30,800 --> 00:36:34,320
every single iteration of this loop and

1028
00:36:32,800 --> 00:36:35,680
we just keep renormalizing these rows

1029
00:36:34,320 --> 00:36:37,680
over and over again and it's extremely

1030
00:36:35,680 --> 00:36:38,960
inefficient and wasteful so what i'd

1031
00:36:37,680 --> 00:36:41,920
like to do is i'd like to actually

1032
00:36:38,960 --> 00:36:44,079
prepare a matrix capital p that will

1033
00:36:41,920 --> 00:36:45,599
just have the probabilities in it so in

1034
00:36:44,079 --> 00:36:48,079
other words it's going to be the same as

1035
00:36:45,599 --> 00:36:50,079
the capital n matrix here of counts but

1036
00:36:48,079 --> 00:36:52,880
every single row will have the row of

1037
00:36:50,079 --> 00:36:54,400
probabilities uh that is normalized to 1

1038
00:36:52,880 --> 00:36:56,560
indicating the probability distribution

1039
00:36:54,400 --> 00:36:57,920
for the next character given the

1040
00:36:56,560 --> 00:37:01,520
character before it

1041
00:36:57,920 --> 00:37:03,119
um as defined by which row we're in

1042
00:37:01,520 --> 00:37:04,960
so basically what we'd like to do is

1043
00:37:03,119 --> 00:37:06,480
we'd like to just do it up front here

1044
00:37:04,960 --> 00:37:09,520
and then we would like to just use that

1045
00:37:06,480 --> 00:37:12,880
row here so here we would like to just

1046
00:37:09,520 --> 00:37:14,880
do p equals p of ix instead

1047
00:37:12,880 --> 00:37:16,000
okay

1048
00:37:14,880 --> 00:37:17,760
the other reason i want to do this is

1049
00:37:16,000 --> 00:37:19,359
not just for efficiency but also i would

1050
00:37:17,760 --> 00:37:21,520
like us to practice

1051
00:37:19,359 --> 00:37:23,599
these n-dimensional tensors and i'd like

1052
00:37:21,520 --> 00:37:24,560
us to practice their manipulation and

1053
00:37:23,599 --> 00:37:26,000
especially something that's called

1054
00:37:24,560 --> 00:37:26,880
broadcasting that we'll go into in a

1055
00:37:26,000 --> 00:37:28,160
second

1056
00:37:26,880 --> 00:37:30,480
we're actually going to have to become

1057
00:37:28,160 --> 00:37:32,000
very good at these tensor manipulations

1058
00:37:30,480 --> 00:37:33,520
because if we're going to build out all

1059
00:37:32,000 --> 00:37:35,359
the way to transformers we're going to

1060
00:37:33,520 --> 00:37:37,920
be doing some pretty complicated um

1061
00:37:35,359 --> 00:37:39,440
array operations for efficiency and we

1062
00:37:37,920 --> 00:37:42,079
need to really understand that and be

1063
00:37:39,440 --> 00:37:43,760
very good at it

1064
00:37:42,079 --> 00:37:45,839
so intuitively what we want to do is we

1065
00:37:43,760 --> 00:37:48,160
first want to grab the floating point

1066
00:37:45,839 --> 00:37:49,359
copy of n

1067
00:37:48,160 --> 00:37:50,880
and i'm mimicking the line here

1068
00:37:49,359 --> 00:37:53,200
basically

1069
00:37:50,880 --> 00:37:55,599
and then we want to divide all the rows

1070
00:37:53,200 --> 00:37:57,359
so that they sum to 1.

1071
00:37:55,599 --> 00:38:00,560
so we'd like to do something like this p

1072
00:37:57,359 --> 00:38:01,280
divide p dot sum

1073
00:38:00,560 --> 00:38:02,800
but

1074
00:38:01,280 --> 00:38:05,280
now we have to be careful

1075
00:38:02,800 --> 00:38:08,000
because p dot sum actually

1076
00:38:05,280 --> 00:38:10,640
produces a sum

1077
00:38:08,000 --> 00:38:14,960
sorry equals and that float copy

1078
00:38:10,640 --> 00:38:16,960
p dot sum produces a um

1079
00:38:14,960 --> 00:38:19,520
sums up all of the counts of this entire

1080
00:38:16,960 --> 00:38:21,359
matrix n and gives us a single number of

1081
00:38:19,520 --> 00:38:22,800
just the summation of everything so

1082
00:38:21,359 --> 00:38:25,200
that's not the way we want to define

1083
00:38:22,800 --> 00:38:27,599
divide we want to simultaneously and in

1084
00:38:25,200 --> 00:38:30,560
parallel divide all the rows

1085
00:38:27,599 --> 00:38:32,640
by their respective sums

1086
00:38:30,560 --> 00:38:35,760
so what we have to do now is we have to

1087
00:38:32,640 --> 00:38:37,280
go into documentation for torch.sum

1088
00:38:35,760 --> 00:38:38,960
and we can scroll down here to a

1089
00:38:37,280 --> 00:38:41,680
definition that is relevant to us which

1090
00:38:38,960 --> 00:38:43,839
is where we don't only provide an input

1091
00:38:41,680 --> 00:38:45,760
array that we want to sum but we also

1092
00:38:43,839 --> 00:38:47,200
provide the dimension along which we

1093
00:38:45,760 --> 00:38:49,839
want to sum

1094
00:38:47,200 --> 00:38:51,040
and in particular we want to sum up

1095
00:38:49,839 --> 00:38:52,320
over rows

1096
00:38:51,040 --> 00:38:53,760
right

1097
00:38:52,320 --> 00:38:56,240
now one more argument that i want you to

1098
00:38:53,760 --> 00:38:57,839
pay attention to here is the keep them

1099
00:38:56,240 --> 00:39:00,240
is false

1100
00:38:57,839 --> 00:39:02,400
if keep them is true then the output

1101
00:39:00,240 --> 00:39:03,760
tensor is of the same size as input

1102
00:39:02,400 --> 00:39:05,839
except of course the dimension along

1103
00:39:03,760 --> 00:39:07,280
which is summed which will become just

1104
00:39:05,839 --> 00:39:12,000
one

1105
00:39:07,280 --> 00:39:14,480
but if you pass in keep them as false

1106
00:39:12,000 --> 00:39:16,800
then this dimension is squeezed out and

1107
00:39:14,480 --> 00:39:18,800
so torch.sum not only does the sum and

1108
00:39:16,800 --> 00:39:20,400
collapses dimension to be of size one

1109
00:39:18,800 --> 00:39:22,160
but in addition it does what's called a

1110
00:39:20,400 --> 00:39:24,560
squeeze where it squeezes out it

1111
00:39:22,160 --> 00:39:25,760
squeezes out that dimension

1112
00:39:24,560 --> 00:39:27,040
so

1113
00:39:25,760 --> 00:39:29,040
basically what we want here is we

1114
00:39:27,040 --> 00:39:30,720
instead want to do p dot sum of some

1115
00:39:29,040 --> 00:39:32,480
axis

1116
00:39:30,720 --> 00:39:35,520
and in particular notice that p dot

1117
00:39:32,480 --> 00:39:37,839
shape is 27 by 27

1118
00:39:35,520 --> 00:39:40,000
so when we sum up across axis zero then

1119
00:39:37,839 --> 00:39:42,560
we would be taking the zeroth dimension

1120
00:39:40,000 --> 00:39:44,960
and we would be summing across it

1121
00:39:42,560 --> 00:39:46,960
so when keep them as true

1122
00:39:44,960 --> 00:39:50,000
then this thing will not only give us

1123
00:39:46,960 --> 00:39:51,839
the counts across um

1124
00:39:50,000 --> 00:39:53,599
along the columns

1125
00:39:51,839 --> 00:39:57,359
but notice that basically the shape of

1126
00:39:53,599 --> 00:39:58,960
this is 1 by 27 we just get a row vector

1127
00:39:57,359 --> 00:40:00,240
and the reason we get a row vector here

1128
00:39:58,960 --> 00:40:02,480
again is because we passed in zero

1129
00:40:00,240 --> 00:40:04,720
dimension so this zero dimension becomes

1130
00:40:02,480 --> 00:40:06,640
one and we've done a sum

1131
00:40:04,720 --> 00:40:08,079
and we get a row and so basically we've

1132
00:40:06,640 --> 00:40:09,280
done the sum

1133
00:40:08,079 --> 00:40:11,119
this way

1134
00:40:09,280 --> 00:40:12,880
vertically and arrived at just a single

1135
00:40:11,119 --> 00:40:15,200
1 by 27

1136
00:40:12,880 --> 00:40:17,680
vector of counts

1137
00:40:15,200 --> 00:40:20,240
what happens when you take out keep them

1138
00:40:17,680 --> 00:40:23,520
is that we just get 27. so it squeezes

1139
00:40:20,240 --> 00:40:27,839
out that dimension and we just get

1140
00:40:23,520 --> 00:40:27,839
a one-dimensional vector of size 27.

1141
00:40:28,560 --> 00:40:33,680
now we don't actually want

1142
00:40:31,280 --> 00:40:37,440
one by 27 row vector because that gives

1143
00:40:33,680 --> 00:40:39,359
us the counts or the sums across

1144
00:40:37,440 --> 00:40:41,119
the columns

1145
00:40:39,359 --> 00:40:43,280
we actually want to sum the other way

1146
00:40:41,119 --> 00:40:46,240
along dimension one and you'll see that

1147
00:40:43,280 --> 00:40:49,839
the shape of this is 27 by one so it's a

1148
00:40:46,240 --> 00:40:52,800
column vector it's a 27 by one

1149
00:40:49,839 --> 00:40:53,839
vector of counts

1150
00:40:52,800 --> 00:40:55,119
okay

1151
00:40:53,839 --> 00:40:56,960
and that's because what's happened here

1152
00:40:55,119 --> 00:41:01,440
is that we're going horizontally and

1153
00:40:56,960 --> 00:41:03,440
this 27 by 27 matrix becomes a 27 by 1

1154
00:41:01,440 --> 00:41:06,319
array

1155
00:41:03,440 --> 00:41:08,079
now you'll notice by the way that um the

1156
00:41:06,319 --> 00:41:10,480
actual numbers

1157
00:41:08,079 --> 00:41:12,560
of these counts are identical

1158
00:41:10,480 --> 00:41:13,920
and that's because this special array of

1159
00:41:12,560 --> 00:41:15,760
counts here comes from bi-gram

1160
00:41:13,920 --> 00:41:17,680
statistics and actually it just so

1161
00:41:15,760 --> 00:41:19,200
happens by chance

1162
00:41:17,680 --> 00:41:21,040
or because of the way this array is

1163
00:41:19,200 --> 00:41:23,440
constructed that the sums along the

1164
00:41:21,040 --> 00:41:26,079
columns or along the rows horizontally

1165
00:41:23,440 --> 00:41:27,359
or vertically is identical

1166
00:41:26,079 --> 00:41:30,079
but actually what we want to do in this

1167
00:41:27,359 --> 00:41:31,119
case is we want to sum across the

1168
00:41:30,079 --> 00:41:33,839
rows

1169
00:41:31,119 --> 00:41:37,200
horizontally so what we want here is p

1170
00:41:33,839 --> 00:41:39,440
that sum of one with keep in true

1171
00:41:37,200 --> 00:41:40,960
27 by one column vector

1172
00:41:39,440 --> 00:41:43,680
and now what we want to do is we want to

1173
00:41:40,960 --> 00:41:43,680
divide by that

1174
00:41:44,560 --> 00:41:48,800
now we have to be careful here again is

1175
00:41:46,319 --> 00:41:51,680
it possible to take

1176
00:41:48,800 --> 00:41:55,200
what's a um p dot shape you see here 27

1177
00:41:51,680 --> 00:41:59,200
by 27 is it possible to take a 27 by 27

1178
00:41:55,200 --> 00:42:01,280
array and divide it by what is a 27 by 1

1179
00:41:59,200 --> 00:42:03,760
array

1180
00:42:01,280 --> 00:42:05,359
is that an operation that you can do

1181
00:42:03,760 --> 00:42:06,800
and whether or not you can perform this

1182
00:42:05,359 --> 00:42:09,520
operation is determined by what's called

1183
00:42:06,800 --> 00:42:11,920
broadcasting rules so if you just search

1184
00:42:09,520 --> 00:42:12,960
broadcasting semantics in torch

1185
00:42:11,920 --> 00:42:14,319
you'll notice that there's a special

1186
00:42:12,960 --> 00:42:16,880
definition for

1187
00:42:14,319 --> 00:42:20,079
what's called broadcasting that uh for

1188
00:42:16,880 --> 00:42:22,079
whether or not um these two uh arrays

1189
00:42:20,079 --> 00:42:23,839
can be combined in a binary operation

1190
00:42:22,079 --> 00:42:25,520
like division

1191
00:42:23,839 --> 00:42:27,200
so the first condition is each tensor

1192
00:42:25,520 --> 00:42:28,480
has at least one dimension which is the

1193
00:42:27,200 --> 00:42:29,599
case for us

1194
00:42:28,480 --> 00:42:31,280
and then when iterating over the

1195
00:42:29,599 --> 00:42:32,319
dimension sizes starting at the trailing

1196
00:42:31,280 --> 00:42:34,560
dimension

1197
00:42:32,319 --> 00:42:36,240
the dimension sizes must either be equal

1198
00:42:34,560 --> 00:42:38,000
one of them is one or one of them does

1199
00:42:36,240 --> 00:42:38,880
not exist

1200
00:42:38,000 --> 00:42:41,280
okay

1201
00:42:38,880 --> 00:42:44,240
so let's do that we need to align the

1202
00:42:41,280 --> 00:42:45,680
two arrays and their shapes which is

1203
00:42:44,240 --> 00:42:48,000
very easy because both of these shapes

1204
00:42:45,680 --> 00:42:50,160
have two elements so they're aligned

1205
00:42:48,000 --> 00:42:52,240
then we iterate over from the from the

1206
00:42:50,160 --> 00:42:55,440
right and going to the left

1207
00:42:52,240 --> 00:42:57,040
each dimension must be either equal one

1208
00:42:55,440 --> 00:42:59,200
of them is a one or one of them does not

1209
00:42:57,040 --> 00:43:01,839
exist so in this case they're not equal

1210
00:42:59,200 --> 00:43:03,200
but one of them is a one so this is fine

1211
00:43:01,839 --> 00:43:04,000
and then this dimension they're both

1212
00:43:03,200 --> 00:43:05,839
equal

1213
00:43:04,000 --> 00:43:08,480
so uh this is fine

1214
00:43:05,839 --> 00:43:10,839
so all the dimensions are fine and

1215
00:43:08,480 --> 00:43:12,800
therefore the this operation is

1216
00:43:10,839 --> 00:43:14,480
broadcastable so that means that this

1217
00:43:12,800 --> 00:43:16,800
operation is allowed

1218
00:43:14,480 --> 00:43:19,760
and what is it that these arrays do when

1219
00:43:16,800 --> 00:43:21,680
you divide 27 by 27 by 27 by one

1220
00:43:19,760 --> 00:43:24,160
what it does is that it takes this

1221
00:43:21,680 --> 00:43:27,119
dimension one and it stretches it out it

1222
00:43:24,160 --> 00:43:28,960
copies it to match

1223
00:43:27,119 --> 00:43:30,720
27 here in this case

1224
00:43:28,960 --> 00:43:32,800
so in our case it takes this column

1225
00:43:30,720 --> 00:43:36,640
vector which is 27 by 1

1226
00:43:32,800 --> 00:43:37,760
and it copies it 27 times

1227
00:43:36,640 --> 00:43:40,560
to make

1228
00:43:37,760 --> 00:43:42,160
these both be 27 by 27 internally you

1229
00:43:40,560 --> 00:43:44,160
can think of it that way and so it

1230
00:43:42,160 --> 00:43:45,520
copies those counts

1231
00:43:44,160 --> 00:43:47,280
and then it does an element-wise

1232
00:43:45,520 --> 00:43:48,720
division

1233
00:43:47,280 --> 00:43:50,720
which is what we want because these

1234
00:43:48,720 --> 00:43:52,960
counts we want to divide by them on

1235
00:43:50,720 --> 00:43:54,720
every single one of these columns in

1236
00:43:52,960 --> 00:43:56,480
this matrix

1237
00:43:54,720 --> 00:43:57,680
so this actually we expect will

1238
00:43:56,480 --> 00:43:59,599
normalize

1239
00:43:57,680 --> 00:44:01,599
every single row

1240
00:43:59,599 --> 00:44:04,079
and we can check that this is true by

1241
00:44:01,599 --> 00:44:06,839
taking the first row for example and

1242
00:44:04,079 --> 00:44:10,240
taking its sum we expect this to be

1243
00:44:06,839 --> 00:44:13,040
1. because it's not normalized

1244
00:44:10,240 --> 00:44:14,720
and then we expect this now because if

1245
00:44:13,040 --> 00:44:16,880
we actually correctly normalize all the

1246
00:44:14,720 --> 00:44:19,119
rows we expect to get the exact same

1247
00:44:16,880 --> 00:44:21,760
result here so let's run this

1248
00:44:19,119 --> 00:44:23,680
it's the exact same result

1249
00:44:21,760 --> 00:44:25,200
this is correct so now i would like to

1250
00:44:23,680 --> 00:44:27,200
scare you a little bit

1251
00:44:25,200 --> 00:44:28,560
uh you actually have to like i basically

1252
00:44:27,200 --> 00:44:30,400
encourage you very strongly to read

1253
00:44:28,560 --> 00:44:31,760
through broadcasting semantics

1254
00:44:30,400 --> 00:44:34,240
and i encourage you to treat this with

1255
00:44:31,760 --> 00:44:35,760
respect and it's not something to play

1256
00:44:34,240 --> 00:44:37,599
fast and loose with it's something to

1257
00:44:35,760 --> 00:44:38,960
really respect really understand and

1258
00:44:37,599 --> 00:44:40,480
look up maybe some tutorials for

1259
00:44:38,960 --> 00:44:42,319
broadcasting and practice it and be

1260
00:44:40,480 --> 00:44:44,160
careful with it because you can very

1261
00:44:42,319 --> 00:44:46,640
quickly run into books let me show you

1262
00:44:44,160 --> 00:44:46,640
what i mean

1263
00:44:47,119 --> 00:44:50,400
you see how here we have p dot sum of

1264
00:44:48,640 --> 00:44:53,200
one keep them as true

1265
00:44:50,400 --> 00:44:55,599
the shape of this is 27 by one let me

1266
00:44:53,200 --> 00:44:58,400
take out this line just so we have the n

1267
00:44:55,599 --> 00:45:00,560
and then we can see the counts

1268
00:44:58,400 --> 00:45:02,480
we can see that this is a all the counts

1269
00:45:00,560 --> 00:45:03,440
across all the

1270
00:45:02,480 --> 00:45:07,040
rows

1271
00:45:03,440 --> 00:45:09,440
and it's a 27 by one column vector right

1272
00:45:07,040 --> 00:45:10,400
now suppose that i tried to do the

1273
00:45:09,440 --> 00:45:13,839
following

1274
00:45:10,400 --> 00:45:15,920
but i erase keep them just true here

1275
00:45:13,839 --> 00:45:18,079
what does that do if keep them is not

1276
00:45:15,920 --> 00:45:20,240
true it's false then remember according

1277
00:45:18,079 --> 00:45:23,040
to documentation it gets rid of this

1278
00:45:20,240 --> 00:45:24,800
dimension one it squeezes it out so

1279
00:45:23,040 --> 00:45:27,200
basically we just get all the same

1280
00:45:24,800 --> 00:45:30,000
counts the same result except the shape

1281
00:45:27,200 --> 00:45:31,599
of it is not 27 by 1 it is just 27 the

1282
00:45:30,000 --> 00:45:34,160
one disappears

1283
00:45:31,599 --> 00:45:37,920
but all the counts are the same

1284
00:45:34,160 --> 00:45:40,000
so you'd think that this divide that

1285
00:45:37,920 --> 00:45:42,079
would uh would work

1286
00:45:40,000 --> 00:45:44,240
first of all can we even uh write this

1287
00:45:42,079 --> 00:45:46,240
and will it is it even is it even

1288
00:45:44,240 --> 00:45:47,599
expected to run is it broadcastable

1289
00:45:46,240 --> 00:45:49,119
let's determine if this result is

1290
00:45:47,599 --> 00:45:51,520
broadcastable

1291
00:45:49,119 --> 00:45:52,800
p.summit one is shape

1292
00:45:51,520 --> 00:45:57,520
is 27.

1293
00:45:52,800 --> 00:45:57,520
this is 27 by 27. so 27 by 27

1294
00:45:57,599 --> 00:46:03,680
broadcasting into 27. so now

1295
00:46:01,599 --> 00:46:06,400
rules of broadcasting number one align

1296
00:46:03,680 --> 00:46:07,760
all the dimensions on the right done now

1297
00:46:06,400 --> 00:46:09,119
iteration over all the dimensions

1298
00:46:07,760 --> 00:46:10,079
starting from the right going to the

1299
00:46:09,119 --> 00:46:12,960
left

1300
00:46:10,079 --> 00:46:14,960
all the dimensions must either be equal

1301
00:46:12,960 --> 00:46:17,680
one of them must be one or one that does

1302
00:46:14,960 --> 00:46:19,920
not exist so here they are all equal

1303
00:46:17,680 --> 00:46:21,440
here the dimension does not exist

1304
00:46:19,920 --> 00:46:24,240
so internally what broadcasting will do

1305
00:46:21,440 --> 00:46:25,839
is it will create a one here

1306
00:46:24,240 --> 00:46:27,760
and then

1307
00:46:25,839 --> 00:46:30,240
we see that one of them is a one and

1308
00:46:27,760 --> 00:46:32,400
this will get copied and this will run

1309
00:46:30,240 --> 00:46:34,800
this will broadcast

1310
00:46:32,400 --> 00:46:37,200
okay so you'd expect this

1311
00:46:34,800 --> 00:46:37,200
to work

1312
00:46:37,280 --> 00:46:40,240
because we we are

1313
00:46:41,200 --> 00:46:43,520
this broadcast and this we can divide

1314
00:46:42,800 --> 00:46:45,119
this

1315
00:46:43,520 --> 00:46:46,640
now if i run this you'd expect it to

1316
00:46:45,119 --> 00:46:47,839
work but

1317
00:46:46,640 --> 00:46:49,680
it doesn't

1318
00:46:47,839 --> 00:46:51,359
uh you actually get garbage you get a

1319
00:46:49,680 --> 00:46:52,400
wrong dissolve because this is actually

1320
00:46:51,359 --> 00:46:56,680
a bug

1321
00:46:52,400 --> 00:46:56,680
this keep them equals true

1322
00:46:57,119 --> 00:46:59,839
makes it work

1323
00:47:00,560 --> 00:47:04,720
this is a bug

1324
00:47:02,800 --> 00:47:07,280
in both cases we are doing

1325
00:47:04,720 --> 00:47:09,200
the correct counts we are summing up

1326
00:47:07,280 --> 00:47:10,880
across the rows

1327
00:47:09,200 --> 00:47:12,640
but keep them is saving us and making it

1328
00:47:10,880 --> 00:47:14,240
work so in this case

1329
00:47:12,640 --> 00:47:15,760
i'd like to encourage you to potentially

1330
00:47:14,240 --> 00:47:18,400
like pause this video at this point and

1331
00:47:15,760 --> 00:47:21,920
try to think about why this is buggy and

1332
00:47:18,400 --> 00:47:21,920
why the keep dim was necessary here

1333
00:47:22,160 --> 00:47:24,400
okay

1334
00:47:22,960 --> 00:47:26,559
so the reason to do

1335
00:47:24,400 --> 00:47:27,839
for this is i'm trying to hint it here

1336
00:47:26,559 --> 00:47:29,440
when i was sort of giving you a bit of a

1337
00:47:27,839 --> 00:47:30,319
hint on how this works

1338
00:47:29,440 --> 00:47:32,160
this

1339
00:47:30,319 --> 00:47:34,400
27 vector

1340
00:47:32,160 --> 00:47:36,400
internally inside the broadcasting this

1341
00:47:34,400 --> 00:47:39,520
becomes a 1 by 27

1342
00:47:36,400 --> 00:47:42,000
and 1 by 27 is a row vector right

1343
00:47:39,520 --> 00:47:43,119
and now we are dividing 27 by 27 by 1 by

1344
00:47:42,000 --> 00:47:45,839
27

1345
00:47:43,119 --> 00:47:47,599
and torch will replicate this dimension

1346
00:47:45,839 --> 00:47:49,680
so basically

1347
00:47:47,599 --> 00:47:51,040
uh it will take

1348
00:47:49,680 --> 00:47:53,599
it will take this

1349
00:47:51,040 --> 00:47:55,280
row vector and it will copy it

1350
00:47:53,599 --> 00:47:57,920
vertically now

1351
00:47:55,280 --> 00:48:00,240
27 times so the 27 by 27 lies exactly

1352
00:47:57,920 --> 00:48:02,160
and element wise divides

1353
00:48:00,240 --> 00:48:04,319
and so basically what's happening here

1354
00:48:02,160 --> 00:48:06,079
is

1355
00:48:04,319 --> 00:48:09,359
we're actually normalizing the columns

1356
00:48:06,079 --> 00:48:11,359
instead of normalizing the rows

1357
00:48:09,359 --> 00:48:13,599
so you can check that what's happening

1358
00:48:11,359 --> 00:48:16,240
here is that p at zero which is the

1359
00:48:13,599 --> 00:48:18,559
first row of p dot sum

1360
00:48:16,240 --> 00:48:20,720
is not one it's seven

1361
00:48:18,559 --> 00:48:23,599
it is the first column as an example

1362
00:48:20,720 --> 00:48:24,480
that sums to one

1363
00:48:23,599 --> 00:48:26,240
so

1364
00:48:24,480 --> 00:48:28,079
to summarize where does the issue come

1365
00:48:26,240 --> 00:48:30,079
from the issue comes from the silent

1366
00:48:28,079 --> 00:48:31,920
adding of a dimension here because in

1367
00:48:30,079 --> 00:48:34,000
broadcasting rules you align on the

1368
00:48:31,920 --> 00:48:36,079
right and go from right to left and if

1369
00:48:34,000 --> 00:48:38,000
dimension doesn't exist you create it

1370
00:48:36,079 --> 00:48:39,599
so that's where the problem happens we

1371
00:48:38,000 --> 00:48:41,680
still did the counts correctly we did

1372
00:48:39,599 --> 00:48:44,559
the counts across the rows and we got

1373
00:48:41,680 --> 00:48:46,240
the the counts on the right here as a

1374
00:48:44,559 --> 00:48:48,319
column vector but because the keep

1375
00:48:46,240 --> 00:48:49,839
things was true this this uh this

1376
00:48:48,319 --> 00:48:52,400
dimension was discarded and now we just

1377
00:48:49,839 --> 00:48:54,079
have a vector of 27. and because of

1378
00:48:52,400 --> 00:48:56,000
broadcasting the way it works this

1379
00:48:54,079 --> 00:48:56,960
vector of 27 suddenly becomes a row

1380
00:48:56,000 --> 00:48:58,880
vector

1381
00:48:56,960 --> 00:49:00,960
and then this row vector gets replicated

1382
00:48:58,880 --> 00:49:05,280
vertically and that every single point

1383
00:49:00,960 --> 00:49:05,280
we are dividing by the by the count

1384
00:49:05,440 --> 00:49:08,800
in the opposite direction

1385
00:49:07,280 --> 00:49:11,599
so uh

1386
00:49:08,800 --> 00:49:12,960
so this thing just uh doesn't work this

1387
00:49:11,599 --> 00:49:14,160
needs to be keep things equal true in

1388
00:49:12,960 --> 00:49:16,319
this case

1389
00:49:14,160 --> 00:49:17,839
so then

1390
00:49:16,319 --> 00:49:19,839
then we have that p at zero is

1391
00:49:17,839 --> 00:49:21,680
normalized

1392
00:49:19,839 --> 00:49:24,559
and conversely the first column you'd

1393
00:49:21,680 --> 00:49:27,599
expect to potentially not be normalized

1394
00:49:24,559 --> 00:49:31,040
and this is what makes it work

1395
00:49:27,599 --> 00:49:33,040
so pretty subtle and uh hopefully this

1396
00:49:31,040 --> 00:49:34,960
helps to scare you that you should have

1397
00:49:33,040 --> 00:49:37,839
a respect for broadcasting be careful

1398
00:49:34,960 --> 00:49:39,040
check your work uh and uh understand how

1399
00:49:37,839 --> 00:49:40,400
it works under the hood and make sure

1400
00:49:39,040 --> 00:49:41,839
that it's broadcasting in the direction

1401
00:49:40,400 --> 00:49:44,079
that you like otherwise you're going to

1402
00:49:41,839 --> 00:49:46,720
introduce very subtle bugs very hard to

1403
00:49:44,079 --> 00:49:48,720
find bugs and uh just be careful one

1404
00:49:46,720 --> 00:49:51,440
more note on efficiency we don't want to

1405
00:49:48,720 --> 00:49:53,280
be doing this here because this creates

1406
00:49:51,440 --> 00:49:54,319
a completely new tensor that we store

1407
00:49:53,280 --> 00:49:56,400
into p

1408
00:49:54,319 --> 00:49:57,760
we prefer to use in place operations if

1409
00:49:56,400 --> 00:50:00,000
possible

1410
00:49:57,760 --> 00:50:01,839
so this would be an in-place operation

1411
00:50:00,000 --> 00:50:03,440
it has the potential to be faster it

1412
00:50:01,839 --> 00:50:06,160
doesn't create new memory

1413
00:50:03,440 --> 00:50:07,920
under the hood and then let's erase this

1414
00:50:06,160 --> 00:50:09,119
we don't need it

1415
00:50:07,920 --> 00:50:10,880
and let's

1416
00:50:09,119 --> 00:50:13,599
also

1417
00:50:10,880 --> 00:50:14,559
um just do fewer just so i'm not wasting

1418
00:50:13,599 --> 00:50:15,920
space

1419
00:50:14,559 --> 00:50:16,960
okay so we're actually in a pretty good

1420
00:50:15,920 --> 00:50:19,280
spot now

1421
00:50:16,960 --> 00:50:22,079
we trained a bigram language model and

1422
00:50:19,280 --> 00:50:24,400
we trained it really just by counting uh

1423
00:50:22,079 --> 00:50:26,319
how frequently any pairing occurs and

1424
00:50:24,400 --> 00:50:27,839
then normalizing so that we get a nice

1425
00:50:26,319 --> 00:50:31,040
property distribution

1426
00:50:27,839 --> 00:50:32,720
so really these elements of this array p

1427
00:50:31,040 --> 00:50:34,559
are really the parameters of our biogram

1428
00:50:32,720 --> 00:50:36,880
language model giving us and summarizing

1429
00:50:34,559 --> 00:50:38,640
the statistics of these bigrams

1430
00:50:36,880 --> 00:50:40,480
so we train the model and then we know

1431
00:50:38,640 --> 00:50:43,359
how to sample from a model we just

1432
00:50:40,480 --> 00:50:45,520
iteratively uh sample the next character

1433
00:50:43,359 --> 00:50:46,960
and feed it in each time and get a next

1434
00:50:45,520 --> 00:50:48,480
character

1435
00:50:46,960 --> 00:50:50,319
now what i'd like to do is i'd like to

1436
00:50:48,480 --> 00:50:52,960
somehow evaluate the quality of this

1437
00:50:50,319 --> 00:50:54,720
model we'd like to somehow summarize the

1438
00:50:52,960 --> 00:50:57,440
quality of this model into a single

1439
00:50:54,720 --> 00:50:58,960
number how good is it at predicting

1440
00:50:57,440 --> 00:51:00,960
the training set

1441
00:50:58,960 --> 00:51:04,160
and as an example so in the training set

1442
00:51:00,960 --> 00:51:05,920
we can evaluate now the training loss

1443
00:51:04,160 --> 00:51:06,880
and this training loss is telling us

1444
00:51:05,920 --> 00:51:08,160
about

1445
00:51:06,880 --> 00:51:10,000
sort of the quality of this model in a

1446
00:51:08,160 --> 00:51:11,839
single number just like we saw in

1447
00:51:10,000 --> 00:51:13,359
micrograd

1448
00:51:11,839 --> 00:51:14,480
so let's try to think through the

1449
00:51:13,359 --> 00:51:16,960
quality of the model and how we would

1450
00:51:14,480 --> 00:51:18,079
evaluate it

1451
00:51:16,960 --> 00:51:20,559
basically what we're going to do is

1452
00:51:18,079 --> 00:51:22,880
we're going to copy paste this code

1453
00:51:20,559 --> 00:51:24,079
that we previously used for counting

1454
00:51:22,880 --> 00:51:25,599
okay

1455
00:51:24,079 --> 00:51:27,760
and let me just print these diagrams

1456
00:51:25,599 --> 00:51:29,680
first we're gonna use f strings

1457
00:51:27,760 --> 00:51:30,960
and i'm gonna print character one

1458
00:51:29,680 --> 00:51:32,880
followed by character two these are the

1459
00:51:30,960 --> 00:51:34,160
diagrams and then i don't wanna do it

1460
00:51:32,880 --> 00:51:37,599
for all the words just do the first

1461
00:51:34,160 --> 00:51:40,079
three words so here we have emma olivia

1462
00:51:37,599 --> 00:51:41,760
and ava bigrams

1463
00:51:40,079 --> 00:51:44,640
now what we'd like to do is we'd like to

1464
00:51:41,760 --> 00:51:46,720
basically look at the probability that

1465
00:51:44,640 --> 00:51:48,079
the model assigns to every one of these

1466
00:51:46,720 --> 00:51:49,440
diagrams

1467
00:51:48,079 --> 00:51:51,119
so in other words we can look at the

1468
00:51:49,440 --> 00:51:52,960
probability which is

1469
00:51:51,119 --> 00:51:56,000
summarized in the matrix b

1470
00:51:52,960 --> 00:51:57,920
of i x 1 x 2

1471
00:51:56,000 --> 00:52:00,640
and then we can print it here

1472
00:51:57,920 --> 00:52:02,000
as probability

1473
00:52:00,640 --> 00:52:04,720
and because these properties are way too

1474
00:52:02,000 --> 00:52:06,720
large let me present

1475
00:52:04,720 --> 00:52:09,040
or call in 0.4 f

1476
00:52:06,720 --> 00:52:10,319
to like truncate it a bit

1477
00:52:09,040 --> 00:52:11,760
so what do we have here right we're

1478
00:52:10,319 --> 00:52:13,119
looking at the probabilities that the

1479
00:52:11,760 --> 00:52:15,119
model assigns to every one of these

1480
00:52:13,119 --> 00:52:16,640
bigrams in the dataset

1481
00:52:15,119 --> 00:52:18,559
and so we can see some of them are four

1482
00:52:16,640 --> 00:52:19,920
percent three percent etc

1483
00:52:18,559 --> 00:52:23,280
just to have a measuring stick in our

1484
00:52:19,920 --> 00:52:25,359
mind by the way um we have 27 possible

1485
00:52:23,280 --> 00:52:27,440
characters or tokens and if everything

1486
00:52:25,359 --> 00:52:28,880
was equally likely then you'd expect all

1487
00:52:27,440 --> 00:52:30,559
these probabilities

1488
00:52:28,880 --> 00:52:32,400
to be

1489
00:52:30,559 --> 00:52:34,319
four percent roughly

1490
00:52:32,400 --> 00:52:35,920
so anything above four percent means

1491
00:52:34,319 --> 00:52:38,079
that we've learned something useful from

1492
00:52:35,920 --> 00:52:39,440
these bigram statistics and you see that

1493
00:52:38,079 --> 00:52:40,880
roughly some of these are four percent

1494
00:52:39,440 --> 00:52:41,839
but some of them are as high as 40

1495
00:52:40,880 --> 00:52:44,400
percent

1496
00:52:41,839 --> 00:52:45,599
35 percent and so on so you see that the

1497
00:52:44,400 --> 00:52:47,440
model actually assigned a pretty high

1498
00:52:45,599 --> 00:52:50,000
probability to whatever's in the

1499
00:52:47,440 --> 00:52:51,680
training set and so that's a good thing

1500
00:52:50,000 --> 00:52:53,200
um basically if you have a very good

1501
00:52:51,680 --> 00:52:54,960
model you'd expect that these

1502
00:52:53,200 --> 00:52:57,119
probabilities should be near one because

1503
00:52:54,960 --> 00:52:58,640
that means that your model is correctly

1504
00:52:57,119 --> 00:53:00,240
predicting what's going to come next

1505
00:52:58,640 --> 00:53:02,800
especially on the training set where you

1506
00:53:00,240 --> 00:53:03,680
where you trained your model

1507
00:53:02,800 --> 00:53:05,280
so

1508
00:53:03,680 --> 00:53:07,520
now we'd like to think about how can we

1509
00:53:05,280 --> 00:53:09,440
summarize these probabilities into a

1510
00:53:07,520 --> 00:53:11,680
single number that measures the quality

1511
00:53:09,440 --> 00:53:13,200
of this model

1512
00:53:11,680 --> 00:53:15,119
now when you look at the literature into

1513
00:53:13,200 --> 00:53:17,200
maximum likelihood estimation and

1514
00:53:15,119 --> 00:53:18,720
statistical modeling and so on

1515
00:53:17,200 --> 00:53:21,359
you'll see that what's typically used

1516
00:53:18,720 --> 00:53:23,599
here is something called the likelihood

1517
00:53:21,359 --> 00:53:25,760
and the likelihood is the product of all

1518
00:53:23,599 --> 00:53:27,280
of these probabilities

1519
00:53:25,760 --> 00:53:29,440
and so the product of all these

1520
00:53:27,280 --> 00:53:31,839
probabilities is the likelihood and it's

1521
00:53:29,440 --> 00:53:34,960
really telling us about the probability

1522
00:53:31,839 --> 00:53:37,599
of the entire data set assigned uh

1523
00:53:34,960 --> 00:53:39,359
assigned by the model that we've trained

1524
00:53:37,599 --> 00:53:41,200
and that is a measure of quality

1525
00:53:39,359 --> 00:53:43,200
so the product of these

1526
00:53:41,200 --> 00:53:44,640
should be as high as possible

1527
00:53:43,200 --> 00:53:46,559
when you are training the model and when

1528
00:53:44,640 --> 00:53:47,760
you have a good model your pro your

1529
00:53:46,559 --> 00:53:49,359
product of these probabilities should be

1530
00:53:47,760 --> 00:53:50,319
very high

1531
00:53:49,359 --> 00:53:51,520
um

1532
00:53:50,319 --> 00:53:53,440
now because the product of these

1533
00:53:51,520 --> 00:53:55,040
probabilities is an unwieldy thing to

1534
00:53:53,440 --> 00:53:56,960
work with you can see that all of them

1535
00:53:55,040 --> 00:53:58,400
are between zero and one so your product

1536
00:53:56,960 --> 00:54:00,079
of these probabilities will be a very

1537
00:53:58,400 --> 00:54:00,880
tiny number

1538
00:54:00,079 --> 00:54:01,760
um

1539
00:54:00,880 --> 00:54:03,280
so

1540
00:54:01,760 --> 00:54:04,960
for convenience what people work with

1541
00:54:03,280 --> 00:54:06,079
usually is not the likelihood but they

1542
00:54:04,960 --> 00:54:07,839
work with what's called the log

1543
00:54:06,079 --> 00:54:08,880
likelihood

1544
00:54:07,839 --> 00:54:10,800
so

1545
00:54:08,880 --> 00:54:12,720
the product of these is the likelihood

1546
00:54:10,800 --> 00:54:14,960
to get the log likelihood we just have

1547
00:54:12,720 --> 00:54:17,119
to take the log of the probability

1548
00:54:14,960 --> 00:54:19,680
and so the log of the probability here i

1549
00:54:17,119 --> 00:54:21,920
have the log of x from zero to one

1550
00:54:19,680 --> 00:54:24,559
the log is a you see here monotonic

1551
00:54:21,920 --> 00:54:27,200
transformation of the probability

1552
00:54:24,559 --> 00:54:28,720
where if you pass in one

1553
00:54:27,200 --> 00:54:30,640
you get zero

1554
00:54:28,720 --> 00:54:32,160
so probability one gets your log

1555
00:54:30,640 --> 00:54:33,599
probability of zero

1556
00:54:32,160 --> 00:54:35,680
and then as you go lower and lower

1557
00:54:33,599 --> 00:54:37,359
probability the log will grow more and

1558
00:54:35,680 --> 00:54:41,200
more negative until all the way to

1559
00:54:37,359 --> 00:54:41,200
negative infinity at zero

1560
00:54:41,839 --> 00:54:46,720
so here we have a log prob which is

1561
00:54:44,000 --> 00:54:48,480
really just a torch.log of probability

1562
00:54:46,720 --> 00:54:49,920
let's print it out to get a sense of

1563
00:54:48,480 --> 00:54:51,599
what that looks like

1564
00:54:49,920 --> 00:54:54,640
log prob

1565
00:54:51,599 --> 00:54:54,640
also 0.4 f

1566
00:54:54,720 --> 00:54:58,559
okay

1567
00:54:56,559 --> 00:55:00,640
so as you can see when we plug in

1568
00:54:58,559 --> 00:55:02,319
numbers that are very close some of our

1569
00:55:00,640 --> 00:55:03,359
higher numbers we get closer and closer

1570
00:55:02,319 --> 00:55:04,960
to zero

1571
00:55:03,359 --> 00:55:06,559
and then if we plug in very bad

1572
00:55:04,960 --> 00:55:09,520
probabilities we get more and more

1573
00:55:06,559 --> 00:55:10,799
negative number that's bad

1574
00:55:09,520 --> 00:55:12,400
so

1575
00:55:10,799 --> 00:55:15,200
and the reason we work with this is for

1576
00:55:12,400 --> 00:55:16,880
a large extent convenience right

1577
00:55:15,200 --> 00:55:18,880
because we have mathematically that if

1578
00:55:16,880 --> 00:55:21,119
you have some product a times b times c

1579
00:55:18,880 --> 00:55:23,680
of all these probabilities right

1580
00:55:21,119 --> 00:55:25,359
the likelihood is the product of all

1581
00:55:23,680 --> 00:55:27,359
these probabilities

1582
00:55:25,359 --> 00:55:28,400
then the log

1583
00:55:27,359 --> 00:55:30,799
of these

1584
00:55:28,400 --> 00:55:33,280
is just log of a plus

1585
00:55:30,799 --> 00:55:33,280
log of b

1586
00:55:33,680 --> 00:55:37,440
plus log of c if you remember your logs

1587
00:55:36,160 --> 00:55:39,680
from your

1588
00:55:37,440 --> 00:55:41,440
high school or undergrad and so on

1589
00:55:39,680 --> 00:55:42,480
so we have that basically

1590
00:55:41,440 --> 00:55:44,640
the likelihood of the product

1591
00:55:42,480 --> 00:55:46,880
probabilities the log likelihood is just

1592
00:55:44,640 --> 00:55:48,799
the sum of the logs of the individual

1593
00:55:46,880 --> 00:55:50,000
probabilities

1594
00:55:48,799 --> 00:55:52,720
so

1595
00:55:50,000 --> 00:55:54,559
log likelihood

1596
00:55:52,720 --> 00:55:57,119
starts at zero

1597
00:55:54,559 --> 00:56:00,240
and then log likelihood here we can just

1598
00:55:57,119 --> 00:56:00,240
accumulate simply

1599
00:56:00,319 --> 00:56:03,839
and in the end we can print this

1600
00:56:05,359 --> 00:56:08,640
print the log likelihood

1601
00:56:09,520 --> 00:56:13,839
f strings

1602
00:56:11,680 --> 00:56:18,319
maybe you're familiar with this

1603
00:56:13,839 --> 00:56:18,319
so log likelihood is negative 38.

1604
00:56:19,920 --> 00:56:22,480
okay

1605
00:56:21,280 --> 00:56:25,200
now

1606
00:56:22,480 --> 00:56:27,839
we actually want um

1607
00:56:25,200 --> 00:56:30,319
so how high can log likelihood get it

1608
00:56:27,839 --> 00:56:31,920
can go to zero so when all the

1609
00:56:30,319 --> 00:56:33,599
probabilities are one log likelihood

1610
00:56:31,920 --> 00:56:35,440
will be zero and then when all the

1611
00:56:33,599 --> 00:56:37,440
probabilities are lower this will grow

1612
00:56:35,440 --> 00:56:39,760
more and more negative

1613
00:56:37,440 --> 00:56:41,839
now we don't actually like this because

1614
00:56:39,760 --> 00:56:44,640
what we'd like is a loss function and a

1615
00:56:41,839 --> 00:56:46,160
loss function has the semantics that low

1616
00:56:44,640 --> 00:56:47,440
is good

1617
00:56:46,160 --> 00:56:50,319
because we're trying to minimize the

1618
00:56:47,440 --> 00:56:51,760
loss so we actually need to invert this

1619
00:56:50,319 --> 00:56:55,520
and that's what gives us something

1620
00:56:51,760 --> 00:56:55,520
called the negative log likelihood

1621
00:56:55,920 --> 00:57:01,599
negative log likelihood is just negative

1622
00:56:58,559 --> 00:57:01,599
of the log likelihood

1623
00:57:03,760 --> 00:57:06,400
these are f strings by the way if you'd

1624
00:57:05,040 --> 00:57:09,280
like to look this up

1625
00:57:06,400 --> 00:57:10,880
negative log likelihood equals

1626
00:57:09,280 --> 00:57:12,880
so negative log likelihood now is just

1627
00:57:10,880 --> 00:57:15,599
negative of it and so the negative log

1628
00:57:12,880 --> 00:57:17,599
block load is a very nice loss function

1629
00:57:15,599 --> 00:57:19,680
because um

1630
00:57:17,599 --> 00:57:22,160
the lowest it can get is zero

1631
00:57:19,680 --> 00:57:24,640
and the higher it is the worse off the

1632
00:57:22,160 --> 00:57:26,000
predictions are that you're making

1633
00:57:24,640 --> 00:57:27,920
and then one more modification to this

1634
00:57:26,000 --> 00:57:29,839
that sometimes people do is that for

1635
00:57:27,920 --> 00:57:32,160
convenience uh they actually like to

1636
00:57:29,839 --> 00:57:34,319
normalize by they like to make it an

1637
00:57:32,160 --> 00:57:37,040
average instead of a sum

1638
00:57:34,319 --> 00:57:39,280
and so uh here

1639
00:57:37,040 --> 00:57:41,440
let's just keep some counts as well

1640
00:57:39,280 --> 00:57:42,799
so n plus equals one

1641
00:57:41,440 --> 00:57:44,000
starts at zero

1642
00:57:42,799 --> 00:57:46,160
and then here

1643
00:57:44,000 --> 00:57:47,760
um we can have sort of like a normalized

1644
00:57:46,160 --> 00:57:50,000
log likelihood

1645
00:57:47,760 --> 00:57:50,000
um

1646
00:57:50,400 --> 00:57:54,640
if we just normalize it by the count

1647
00:57:52,400 --> 00:57:56,880
then we will sort of get the average

1648
00:57:54,640 --> 00:57:59,200
log likelihood so this would be

1649
00:57:56,880 --> 00:58:02,079
usually our loss function here is what

1650
00:57:59,200 --> 00:58:03,920
this we would this is what we would use

1651
00:58:02,079 --> 00:58:06,559
uh so our loss function for the training

1652
00:58:03,920 --> 00:58:08,480
set assigned by the model is 2.4 that's

1653
00:58:06,559 --> 00:58:10,400
the quality of this model

1654
00:58:08,480 --> 00:58:12,000
and the lower it is the better off we

1655
00:58:10,400 --> 00:58:13,359
are and the higher it is the worse off

1656
00:58:12,000 --> 00:58:14,240
we are

1657
00:58:13,359 --> 00:58:17,040
and

1658
00:58:14,240 --> 00:58:19,520
the job of our you know training is to

1659
00:58:17,040 --> 00:58:22,799
find the parameters that minimize the

1660
00:58:19,520 --> 00:58:24,559
negative log likelihood loss

1661
00:58:22,799 --> 00:58:26,480
and that would be like a high quality

1662
00:58:24,559 --> 00:58:28,000
model okay so to summarize i actually

1663
00:58:26,480 --> 00:58:30,799
wrote it out here

1664
00:58:28,000 --> 00:58:31,920
so our goal is to maximize likelihood

1665
00:58:30,799 --> 00:58:34,000
which is the

1666
00:58:31,920 --> 00:58:35,520
product of all the probabilities

1667
00:58:34,000 --> 00:58:37,520
assigned by the model

1668
00:58:35,520 --> 00:58:39,760
and we want to maximize this likelihood

1669
00:58:37,520 --> 00:58:41,920
with respect to the model parameters and

1670
00:58:39,760 --> 00:58:43,920
in our case the model parameters here

1671
00:58:41,920 --> 00:58:45,440
are defined in the table these numbers

1672
00:58:43,920 --> 00:58:46,400
the probabilities

1673
00:58:45,440 --> 00:58:47,839
are

1674
00:58:46,400 --> 00:58:50,319
the model parameters sort of in our

1675
00:58:47,839 --> 00:58:52,000
program language models so far but you

1676
00:58:50,319 --> 00:58:53,680
have to keep in mind that here we are

1677
00:58:52,000 --> 00:58:55,839
storing everything in a table format the

1678
00:58:53,680 --> 00:58:58,480
probabilities but what's coming up as a

1679
00:58:55,839 --> 00:59:00,720
brief preview is that these numbers will

1680
00:58:58,480 --> 00:59:03,040
not be kept explicitly but these numbers

1681
00:59:00,720 --> 00:59:04,480
will be calculated by a neural network

1682
00:59:03,040 --> 00:59:06,240
so that's coming up

1683
00:59:04,480 --> 00:59:08,160
and we want to change and tune the

1684
00:59:06,240 --> 00:59:09,599
parameters of these neural networks we

1685
00:59:08,160 --> 00:59:11,440
want to change these parameters to

1686
00:59:09,599 --> 00:59:13,280
maximize the likelihood the product of

1687
00:59:11,440 --> 00:59:15,040
the probabilities

1688
00:59:13,280 --> 00:59:16,480
now maximizing the likelihood is

1689
00:59:15,040 --> 00:59:18,640
equivalent to maximizing the log

1690
00:59:16,480 --> 00:59:19,839
likelihood because log is a monotonic

1691
00:59:18,640 --> 00:59:22,000
function

1692
00:59:19,839 --> 00:59:24,319
here's the graph of log

1693
00:59:22,000 --> 00:59:27,119
and basically all it is doing is it's

1694
00:59:24,319 --> 00:59:29,359
just scaling your um you can look at it

1695
00:59:27,119 --> 00:59:32,480
as just a scaling of the loss function

1696
00:59:29,359 --> 00:59:34,400
and so the optimization problem here and

1697
00:59:32,480 --> 00:59:36,000
here are actually equivalent because

1698
00:59:34,400 --> 00:59:36,960
this is just scaling you can look at it

1699
00:59:36,000 --> 00:59:38,640
that way

1700
00:59:36,960 --> 00:59:41,040
and so these are two identical

1701
00:59:38,640 --> 00:59:41,839
optimization problems

1702
00:59:41,040 --> 00:59:43,119
um

1703
00:59:41,839 --> 00:59:44,559
maximizing the log-likelihood is

1704
00:59:43,119 --> 00:59:47,040
equivalent to minimizing the negative

1705
00:59:44,559 --> 00:59:48,799
log likelihood and then in practice

1706
00:59:47,040 --> 00:59:50,799
people actually minimize the average

1707
00:59:48,799 --> 00:59:52,880
negative log likelihood to get numbers

1708
00:59:50,799 --> 00:59:55,200
like 2.4

1709
00:59:52,880 --> 00:59:57,359
and then this summarizes the quality of

1710
00:59:55,200 --> 00:59:59,599
your model and we'd like to minimize it

1711
00:59:57,359 --> 01:00:02,319
and make it as small as possible

1712
00:59:59,599 --> 01:00:04,160
and the lowest it can get is zero

1713
01:00:02,319 --> 01:00:05,920
and the lower it is

1714
01:00:04,160 --> 01:00:07,440
the better off your model is because

1715
01:00:05,920 --> 01:00:09,839
it's signing it's assigning high

1716
01:00:07,440 --> 01:00:11,280
probabilities to your data now let's

1717
01:00:09,839 --> 01:00:12,720
estimate the probability over the entire

1718
01:00:11,280 --> 01:00:15,359
training set just to make sure that we

1719
01:00:12,720 --> 01:00:17,200
get something around 2.4 let's run this

1720
01:00:15,359 --> 01:00:20,640
over the entire oops

1721
01:00:17,200 --> 01:00:24,400
let's take out the print segment as well

1722
01:00:20,640 --> 01:00:25,680
okay 2.45 or the entire training set

1723
01:00:24,400 --> 01:00:26,559
now what i'd like to show you is that

1724
01:00:25,680 --> 01:00:28,240
you can actually evaluate the

1725
01:00:26,559 --> 01:00:30,400
probability for any word that you want

1726
01:00:28,240 --> 01:00:32,880
like for example

1727
01:00:30,400 --> 01:00:35,760
if we just test a single word andre and

1728
01:00:32,880 --> 01:00:37,119
bring back the print statement

1729
01:00:35,760 --> 01:00:40,640
then you see that andre is actually kind

1730
01:00:37,119 --> 01:00:41,680
of like an unlikely word like on average

1731
01:00:40,640 --> 01:00:42,559
we take

1732
01:00:41,680 --> 01:00:44,799
three

1733
01:00:42,559 --> 01:00:46,480
log probability to represent it and

1734
01:00:44,799 --> 01:00:50,000
roughly that's because ej apparently is

1735
01:00:46,480 --> 01:00:51,200
very uncommon as an example

1736
01:00:50,000 --> 01:00:53,760
now

1737
01:00:51,200 --> 01:00:55,920
think through this um

1738
01:00:53,760 --> 01:00:59,760
when i take andre and i append q and i

1739
01:00:55,920 --> 01:00:59,760
test the probability of it under q

1740
01:01:00,160 --> 01:01:02,960
we actually get

1741
01:01:01,680 --> 01:01:05,599
infinity

1742
01:01:02,960 --> 01:01:07,680
and that's because jq has a zero percent

1743
01:01:05,599 --> 01:01:09,280
probability according to our model so

1744
01:01:07,680 --> 01:01:11,200
the log likelihood

1745
01:01:09,280 --> 01:01:14,400
so the log of zero will be negative

1746
01:01:11,200 --> 01:01:15,680
infinity we get infinite loss

1747
01:01:14,400 --> 01:01:16,880
so this is kind of undesirable right

1748
01:01:15,680 --> 01:01:19,119
because we plugged in a string that

1749
01:01:16,880 --> 01:01:20,480
could be like a somewhat reasonable name

1750
01:01:19,119 --> 01:01:22,400
but basically what this is saying is

1751
01:01:20,480 --> 01:01:25,440
that this model is exactly zero percent

1752
01:01:22,400 --> 01:01:26,400
likely to uh to predict this

1753
01:01:25,440 --> 01:01:29,680
name

1754
01:01:26,400 --> 01:01:31,440
and our loss is infinity on this example

1755
01:01:29,680 --> 01:01:32,799
and really what the reason for that is

1756
01:01:31,440 --> 01:01:34,960
that j

1757
01:01:32,799 --> 01:01:36,880
is followed by q

1758
01:01:34,960 --> 01:01:40,640
uh zero times

1759
01:01:36,880 --> 01:01:42,160
uh where's q jq is zero and so jq is uh

1760
01:01:40,640 --> 01:01:43,599
zero percent likely

1761
01:01:42,160 --> 01:01:45,599
so it's actually kind of gross and

1762
01:01:43,599 --> 01:01:47,040
people don't like this too much to fix

1763
01:01:45,599 --> 01:01:49,280
this there's a very simple fix that

1764
01:01:47,040 --> 01:01:50,400
people like to do to sort of like smooth

1765
01:01:49,280 --> 01:01:52,559
out your model a little bit and it's

1766
01:01:50,400 --> 01:01:53,920
called model smoothing and roughly

1767
01:01:52,559 --> 01:01:56,240
what's happening is that we will eight

1768
01:01:53,920 --> 01:01:57,119
we will add some fake counts

1769
01:01:56,240 --> 01:01:59,280
so

1770
01:01:57,119 --> 01:02:00,880
imagine adding a count of one to

1771
01:01:59,280 --> 01:02:03,359
everything

1772
01:02:00,880 --> 01:02:04,559
so we add a count of one

1773
01:02:03,359 --> 01:02:05,520
like this

1774
01:02:04,559 --> 01:02:07,680
and then we recalculate the

1775
01:02:05,520 --> 01:02:09,200
probabilities

1776
01:02:07,680 --> 01:02:10,880
and that's model smoothing and you can

1777
01:02:09,200 --> 01:02:12,799
add as much as you like you can add five

1778
01:02:10,880 --> 01:02:14,720
and it will give you a smoother model

1779
01:02:12,799 --> 01:02:15,839
and the more you add here

1780
01:02:14,720 --> 01:02:17,760
the more

1781
01:02:15,839 --> 01:02:19,599
uniform model you're going to have and

1782
01:02:17,760 --> 01:02:21,119
the less you add

1783
01:02:19,599 --> 01:02:22,240
the more peaked model you are going to

1784
01:02:21,119 --> 01:02:24,880
have of course

1785
01:02:22,240 --> 01:02:25,680
so one is like a pretty decent count to

1786
01:02:24,880 --> 01:02:27,520
add

1787
01:02:25,680 --> 01:02:30,799
and that will ensure that there will be

1788
01:02:27,520 --> 01:02:32,000
no zeros in our probability matrix p

1789
01:02:30,799 --> 01:02:34,160
and so this will of course change the

1790
01:02:32,000 --> 01:02:36,480
generations a little bit in this case it

1791
01:02:34,160 --> 01:02:38,319
didn't but in principle it could

1792
01:02:36,480 --> 01:02:41,119
but what that's going to do now is that

1793
01:02:38,319 --> 01:02:42,319
nothing will be infinity unlikely

1794
01:02:41,119 --> 01:02:43,920
so now

1795
01:02:42,319 --> 01:02:46,079
our model will predict some other

1796
01:02:43,920 --> 01:02:48,079
probability and we see that jq now has a

1797
01:02:46,079 --> 01:02:49,599
very small probability so the model

1798
01:02:48,079 --> 01:02:52,000
still finds it very surprising that this

1799
01:02:49,599 --> 01:02:54,000
was a word or a bigram but we don't get

1800
01:02:52,000 --> 01:02:55,280
negative infinity so it's kind of like a

1801
01:02:54,000 --> 01:02:56,240
nice fix that people like to apply

1802
01:02:55,280 --> 01:02:58,319
sometimes and it's called model

1803
01:02:56,240 --> 01:03:00,480
smoothing okay so we've now trained a

1804
01:02:58,319 --> 01:03:04,160
respectable bi-gram character level

1805
01:03:00,480 --> 01:03:05,920
language model and we saw that we both

1806
01:03:04,160 --> 01:03:08,079
sort of trained the model by looking at

1807
01:03:05,920 --> 01:03:10,000
the counts of all the bigrams and

1808
01:03:08,079 --> 01:03:11,440
normalizing the rows to get probability

1809
01:03:10,000 --> 01:03:14,160
distributions

1810
01:03:11,440 --> 01:03:16,559
we saw that we can also then use those

1811
01:03:14,160 --> 01:03:19,440
parameters of this model to perform

1812
01:03:16,559 --> 01:03:20,960
sampling of new words

1813
01:03:19,440 --> 01:03:22,960
so we sample new names according to

1814
01:03:20,960 --> 01:03:24,559
those distributions and we also saw that

1815
01:03:22,960 --> 01:03:26,559
we can evaluate the quality of this

1816
01:03:24,559 --> 01:03:28,319
model and the quality of this model is

1817
01:03:26,559 --> 01:03:30,160
summarized in a single number which is

1818
01:03:28,319 --> 01:03:32,079
the negative log likelihood and the

1819
01:03:30,160 --> 01:03:33,280
lower this number is the better the

1820
01:03:32,079 --> 01:03:35,440
model is

1821
01:03:33,280 --> 01:03:37,680
because it is giving high probabilities

1822
01:03:35,440 --> 01:03:40,160
to the actual next characters in all the

1823
01:03:37,680 --> 01:03:42,400
bi-grams in our training set

1824
01:03:40,160 --> 01:03:44,400
so that's all well and good but we've

1825
01:03:42,400 --> 01:03:46,160
arrived at this model explicitly by

1826
01:03:44,400 --> 01:03:48,880
doing something that felt sensible we

1827
01:03:46,160 --> 01:03:50,880
were just performing counts and then we

1828
01:03:48,880 --> 01:03:52,079
were normalizing those counts

1829
01:03:50,880 --> 01:03:54,160
now what i would like to do is i would

1830
01:03:52,079 --> 01:03:55,760
like to take an alternative approach we

1831
01:03:54,160 --> 01:03:57,440
will end up in a very very similar

1832
01:03:55,760 --> 01:03:59,280
position but the approach will look very

1833
01:03:57,440 --> 01:04:00,880
different because i would like to cast

1834
01:03:59,280 --> 01:04:02,640
the problem of bi-gram character level

1835
01:04:00,880 --> 01:04:04,160
language modeling into the neural

1836
01:04:02,640 --> 01:04:05,599
network framework

1837
01:04:04,160 --> 01:04:07,599
in the neural network framework we're

1838
01:04:05,599 --> 01:04:09,280
going to approach things slightly

1839
01:04:07,599 --> 01:04:12,559
differently but again end up in a very

1840
01:04:09,280 --> 01:04:14,880
similar spot i'll go into that later now

1841
01:04:12,559 --> 01:04:16,240
our neural network is going to be a

1842
01:04:14,880 --> 01:04:18,640
still a background character level

1843
01:04:16,240 --> 01:04:20,319
language model so it receives a single

1844
01:04:18,640 --> 01:04:21,760
character as an input

1845
01:04:20,319 --> 01:04:24,079
then there's neural network with some

1846
01:04:21,760 --> 01:04:26,480
weights or some parameters w

1847
01:04:24,079 --> 01:04:28,400
and it's going to output the probability

1848
01:04:26,480 --> 01:04:30,559
distribution over the next character in

1849
01:04:28,400 --> 01:04:32,640
a sequence it's going to make guesses as

1850
01:04:30,559 --> 01:04:35,839
to what is likely to follow this

1851
01:04:32,640 --> 01:04:37,680
character that was input to the model

1852
01:04:35,839 --> 01:04:39,680
and then in addition to that we're going

1853
01:04:37,680 --> 01:04:41,520
to be able to evaluate any setting of

1854
01:04:39,680 --> 01:04:43,599
the parameters of the neural net because

1855
01:04:41,520 --> 01:04:45,200
we have the loss function

1856
01:04:43,599 --> 01:04:46,480
the negative log likelihood so we're

1857
01:04:45,200 --> 01:04:48,240
going to take a look at its probability

1858
01:04:46,480 --> 01:04:50,000
distributions and we're going to use the

1859
01:04:48,240 --> 01:04:51,680
labels

1860
01:04:50,000 --> 01:04:53,359
which are basically just the identity of

1861
01:04:51,680 --> 01:04:54,640
the next character in that diagram the

1862
01:04:53,359 --> 01:04:56,400
second character

1863
01:04:54,640 --> 01:04:58,400
so knowing what second character

1864
01:04:56,400 --> 01:05:00,799
actually comes next in the bigram allows

1865
01:04:58,400 --> 01:05:02,799
us to then look at what how high of

1866
01:05:00,799 --> 01:05:03,839
probability the model assigns to that

1867
01:05:02,799 --> 01:05:05,039
character

1868
01:05:03,839 --> 01:05:06,960
and then we of course want the

1869
01:05:05,039 --> 01:05:08,799
probability to be very high

1870
01:05:06,960 --> 01:05:10,720
and that is another way of saying that

1871
01:05:08,799 --> 01:05:12,480
the loss is low

1872
01:05:10,720 --> 01:05:14,319
so we're going to use gradient-based

1873
01:05:12,480 --> 01:05:16,160
optimization then to tune the parameters

1874
01:05:14,319 --> 01:05:18,319
of this network because we have the loss

1875
01:05:16,160 --> 01:05:20,240
function and we're going to minimize it

1876
01:05:18,319 --> 01:05:21,680
so we're going to tune the weights so

1877
01:05:20,240 --> 01:05:22,960
that the neural net is correctly

1878
01:05:21,680 --> 01:05:24,319
predicting the probabilities for the

1879
01:05:22,960 --> 01:05:26,079
next character

1880
01:05:24,319 --> 01:05:27,520
so let's get started the first thing i

1881
01:05:26,079 --> 01:05:29,200
want to do is i want to compile the

1882
01:05:27,520 --> 01:05:30,240
training set of this neural network

1883
01:05:29,200 --> 01:05:31,280
right so

1884
01:05:30,240 --> 01:05:33,039
create

1885
01:05:31,280 --> 01:05:36,319
the training set

1886
01:05:33,039 --> 01:05:37,599
of all the bigrams

1887
01:05:36,319 --> 01:05:39,359
okay

1888
01:05:37,599 --> 01:05:40,640
and

1889
01:05:39,359 --> 01:05:43,680
here

1890
01:05:40,640 --> 01:05:45,119
i'm going to copy paste this code

1891
01:05:43,680 --> 01:05:47,359
because this code iterates over all the

1892
01:05:45,119 --> 01:05:48,960
programs

1893
01:05:47,359 --> 01:05:50,559
so here we start with the words we

1894
01:05:48,960 --> 01:05:52,240
iterate over all the bygrams and

1895
01:05:50,559 --> 01:05:54,000
previously as you recall we did the

1896
01:05:52,240 --> 01:05:55,760
counts but now we're not going to do

1897
01:05:54,000 --> 01:05:56,799
counts we're just creating a training

1898
01:05:55,760 --> 01:05:58,640
set

1899
01:05:56,799 --> 01:06:01,680
now this training set will be made up of

1900
01:05:58,640 --> 01:06:01,680
two lists

1901
01:06:02,000 --> 01:06:06,240
we have the

1902
01:06:04,640 --> 01:06:07,680
inputs

1903
01:06:06,240 --> 01:06:09,440
and the targets

1904
01:06:07,680 --> 01:06:11,440
the the labels

1905
01:06:09,440 --> 01:06:13,119
and these bi-grams will denote x y those

1906
01:06:11,440 --> 01:06:14,799
are the characters right

1907
01:06:13,119 --> 01:06:16,160
and so we're given the first character

1908
01:06:14,799 --> 01:06:17,680
of the bi-gram and then we're trying to

1909
01:06:16,160 --> 01:06:19,359
predict the next one

1910
01:06:17,680 --> 01:06:22,240
both of these are going to be integers

1911
01:06:19,359 --> 01:06:23,520
so here we'll take x's that append is

1912
01:06:22,240 --> 01:06:27,599
just

1913
01:06:23,520 --> 01:06:29,280
x1 ystat append ix2

1914
01:06:27,599 --> 01:06:31,440
and then here

1915
01:06:29,280 --> 01:06:34,079
we actually don't want lists of integers

1916
01:06:31,440 --> 01:06:38,119
we will create tensors out of these so

1917
01:06:34,079 --> 01:06:41,440
axis is torch.tensor of axis and wise a

1918
01:06:38,119 --> 01:06:42,400
storage.tensor of ys

1919
01:06:41,440 --> 01:06:43,680
and then

1920
01:06:42,400 --> 01:06:45,280
we don't actually want to take all the

1921
01:06:43,680 --> 01:06:46,960
words just yet because i want everything

1922
01:06:45,280 --> 01:06:48,880
to be manageable

1923
01:06:46,960 --> 01:06:51,200
so let's just do the first word which is

1924
01:06:48,880 --> 01:06:52,880
emma

1925
01:06:51,200 --> 01:06:55,359
and then it's clear what these x's and

1926
01:06:52,880 --> 01:06:57,760
y's would be

1927
01:06:55,359 --> 01:06:59,599
here let me print

1928
01:06:57,760 --> 01:07:01,520
character 1 character 2 just so you see

1929
01:06:59,599 --> 01:07:04,960
what's going on here

1930
01:07:01,520 --> 01:07:09,520
so the bigrams of these characters is

1931
01:07:04,960 --> 01:07:11,680
dot e e m m m a a dot so this single

1932
01:07:09,520 --> 01:07:13,440
word as i mentioned has one two three

1933
01:07:11,680 --> 01:07:14,640
four five examples for our neural

1934
01:07:13,440 --> 01:07:17,520
network

1935
01:07:14,640 --> 01:07:19,200
there are five separate examples in emma

1936
01:07:17,520 --> 01:07:21,200
and those examples are summarized here

1937
01:07:19,200 --> 01:07:23,039
when the input to the neural network is

1938
01:07:21,200 --> 01:07:25,920
integer 0

1939
01:07:23,039 --> 01:07:28,640
the desired label is integer 5 which

1940
01:07:25,920 --> 01:07:31,119
corresponds to e when the input to the

1941
01:07:28,640 --> 01:07:33,599
neural network is 5 we want its weights

1942
01:07:31,119 --> 01:07:35,039
to be arranged so that 13 gets a very

1943
01:07:33,599 --> 01:07:37,760
high probability

1944
01:07:35,039 --> 01:07:39,119
when 13 is put in we want 13 to have a

1945
01:07:37,760 --> 01:07:41,520
high probability

1946
01:07:39,119 --> 01:07:43,599
when 13 is put in we also want 1 to have

1947
01:07:41,520 --> 01:07:45,839
a high probability

1948
01:07:43,599 --> 01:07:48,160
when one is input we want zero to have a

1949
01:07:45,839 --> 01:07:51,440
very high probability so there are five

1950
01:07:48,160 --> 01:07:54,480
separate input examples to a neural nut

1951
01:07:51,440 --> 01:07:54,480
in this data set

1952
01:07:54,960 --> 01:07:58,960
i wanted to add a tangent of a node of

1953
01:07:57,520 --> 01:08:01,440
caution to be careful with a lot of the

1954
01:07:58,960 --> 01:08:03,920
apis of some of these frameworks

1955
01:08:01,440 --> 01:08:05,839
you saw me silently use torch.tensor

1956
01:08:03,920 --> 01:08:07,839
with a lowercase t

1957
01:08:05,839 --> 01:08:09,520
and the output looked right

1958
01:08:07,839 --> 01:08:10,880
but you should be aware that there's

1959
01:08:09,520 --> 01:08:13,839
actually two ways of constructing a

1960
01:08:10,880 --> 01:08:16,319
tensor there's a torch.lowercase tensor

1961
01:08:13,839 --> 01:08:18,560
and there's also a torch.capital tensor

1962
01:08:16,319 --> 01:08:20,319
class which you can also construct

1963
01:08:18,560 --> 01:08:22,799
so you can actually call both you can

1964
01:08:20,319 --> 01:08:25,359
also do torch.capital tensor

1965
01:08:22,799 --> 01:08:27,440
and you get a nexus and wise as well

1966
01:08:25,359 --> 01:08:28,799
so that's not confusing at all

1967
01:08:27,440 --> 01:08:29,839
um

1968
01:08:28,799 --> 01:08:31,520
there are threads on what is the

1969
01:08:29,839 --> 01:08:33,359
difference between these two

1970
01:08:31,520 --> 01:08:35,120
and um

1971
01:08:33,359 --> 01:08:36,319
unfortunately the docs are just like not

1972
01:08:35,120 --> 01:08:38,719
clear on the difference and when you

1973
01:08:36,319 --> 01:08:40,880
look at the the docs of lower case

1974
01:08:38,719 --> 01:08:43,520
tensor construct tensor with no autograd

1975
01:08:40,880 --> 01:08:45,600
history by copying data

1976
01:08:43,520 --> 01:08:47,199
it's just like it doesn't

1977
01:08:45,600 --> 01:08:48,560
it doesn't make sense so the actual

1978
01:08:47,199 --> 01:08:50,000
difference as far as i can tell is

1979
01:08:48,560 --> 01:08:51,520
explained eventually in this random

1980
01:08:50,000 --> 01:08:53,199
thread that you can google

1981
01:08:51,520 --> 01:08:55,040
and really it comes down to

1982
01:08:53,199 --> 01:08:56,560
i believe

1983
01:08:55,040 --> 01:08:58,480
that um

1984
01:08:56,560 --> 01:09:00,640
what is this

1985
01:08:58,480 --> 01:09:02,480
torch.tensor in first d-type the data

1986
01:09:00,640 --> 01:09:04,319
type automatically while torch.tensor

1987
01:09:02,480 --> 01:09:05,440
just returns a float tensor

1988
01:09:04,319 --> 01:09:07,759
i would recommend stick to

1989
01:09:05,440 --> 01:09:09,520
torch.lowercase tensor

1990
01:09:07,759 --> 01:09:11,520
so um

1991
01:09:09,520 --> 01:09:13,839
indeed we see that when i

1992
01:09:11,520 --> 01:09:18,080
construct this with a capital t the data

1993
01:09:13,839 --> 01:09:21,040
type here of xs is float32

1994
01:09:18,080 --> 01:09:24,560
but towards that lowercase tensor

1995
01:09:21,040 --> 01:09:26,719
you see how it's now x dot d type is now

1996
01:09:24,560 --> 01:09:28,159
integer

1997
01:09:26,719 --> 01:09:30,719
so um

1998
01:09:28,159 --> 01:09:32,159
it's advised that you use lowercase t

1999
01:09:30,719 --> 01:09:34,400
and you can read more about it if you

2000
01:09:32,159 --> 01:09:35,679
like in some of these threads but

2001
01:09:34,400 --> 01:09:36,480
basically

2002
01:09:35,679 --> 01:09:38,080
um

2003
01:09:36,480 --> 01:09:39,679
i'm pointing out some of these things

2004
01:09:38,080 --> 01:09:41,600
because i want to caution you and i want

2005
01:09:39,679 --> 01:09:43,600
you to re get used to reading a lot of

2006
01:09:41,600 --> 01:09:44,319
documentation and reading through a lot

2007
01:09:43,600 --> 01:09:46,960
of

2008
01:09:44,319 --> 01:09:48,159
q and a's and threads like this

2009
01:09:46,960 --> 01:09:49,120
and

2010
01:09:48,159 --> 01:09:50,560
you know some of the stuff is

2011
01:09:49,120 --> 01:09:51,920
unfortunately not easy and not very well

2012
01:09:50,560 --> 01:09:54,640
documented and you have to be careful

2013
01:09:51,920 --> 01:09:56,960
out there what we want here is integers

2014
01:09:54,640 --> 01:09:58,000
because that's what makes uh sense

2015
01:09:56,960 --> 01:09:59,360
um

2016
01:09:58,000 --> 01:10:01,120
and so

2017
01:09:59,360 --> 01:10:02,560
lowercase tensor is what we are using

2018
01:10:01,120 --> 01:10:04,480
okay now we want to think through how

2019
01:10:02,560 --> 01:10:06,239
we're going to feed in these examples

2020
01:10:04,480 --> 01:10:09,040
into a neural network

2021
01:10:06,239 --> 01:10:10,960
now it's not quite as straightforward as

2022
01:10:09,040 --> 01:10:12,960
plugging it in because these examples

2023
01:10:10,960 --> 01:10:15,840
right now are integers so there's like a

2024
01:10:12,960 --> 01:10:17,520
0 5 or 13 it gives us the index of the

2025
01:10:15,840 --> 01:10:19,920
character and you can't just plug an

2026
01:10:17,520 --> 01:10:22,400
integer index into a neural net

2027
01:10:19,920 --> 01:10:24,480
these neural nets right are sort of made

2028
01:10:22,400 --> 01:10:25,360
up of these neurons

2029
01:10:24,480 --> 01:10:27,280
and

2030
01:10:25,360 --> 01:10:29,280
these neurons have weights and as you

2031
01:10:27,280 --> 01:10:32,000
saw in micrograd these weights act

2032
01:10:29,280 --> 01:10:34,560
multiplicatively on the inputs w x plus

2033
01:10:32,000 --> 01:10:35,679
b there's 10 h's and so on and so it

2034
01:10:34,560 --> 01:10:37,840
doesn't really make sense to make an

2035
01:10:35,679 --> 01:10:40,400
input neuron take on integer values that

2036
01:10:37,840 --> 01:10:41,600
you feed in and then multiply on with

2037
01:10:40,400 --> 01:10:42,800
weights

2038
01:10:41,600 --> 01:10:44,640
so instead

2039
01:10:42,800 --> 01:10:46,960
a common way of encoding integers is

2040
01:10:44,640 --> 01:10:48,719
what's called one hot encoding

2041
01:10:46,960 --> 01:10:51,280
in one hot encoding

2042
01:10:48,719 --> 01:10:54,000
we take an integer like 13 and we create

2043
01:10:51,280 --> 01:10:56,560
a vector that is all zeros except for

2044
01:10:54,000 --> 01:10:59,440
the 13th dimension which we turn to a

2045
01:10:56,560 --> 01:11:01,120
one and then that vector can feed into a

2046
01:10:59,440 --> 01:11:03,120
neural net

2047
01:11:01,120 --> 01:11:04,640
now conveniently

2048
01:11:03,120 --> 01:11:07,520
uh pi torch actually has something

2049
01:11:04,640 --> 01:11:07,520
called the one hot

2050
01:11:07,840 --> 01:11:13,840
function inside torching and functional

2051
01:11:10,239 --> 01:11:14,960
it takes a tensor made up of integers

2052
01:11:13,840 --> 01:11:18,159
um

2053
01:11:14,960 --> 01:11:19,120
long is a is a as an integer

2054
01:11:18,159 --> 01:11:22,480
um

2055
01:11:19,120 --> 01:11:24,320
and it also takes a number of classes um

2056
01:11:22,480 --> 01:11:27,679
which is how large you want your uh

2057
01:11:24,320 --> 01:11:30,040
tensor uh your vector to be

2058
01:11:27,679 --> 01:11:32,320
so here let's import

2059
01:11:30,040 --> 01:11:34,080
torch.n.functional sf this is a common

2060
01:11:32,320 --> 01:11:36,640
way of importing it

2061
01:11:34,080 --> 01:11:38,640
and then let's do f.1 hot

2062
01:11:36,640 --> 01:11:41,280
and we feed in the integers that we want

2063
01:11:38,640 --> 01:11:44,000
to encode so we can actually feed in the

2064
01:11:41,280 --> 01:11:46,159
entire array of x's

2065
01:11:44,000 --> 01:11:47,679
and we can tell it that num classes is

2066
01:11:46,159 --> 01:11:49,520
27.

2067
01:11:47,679 --> 01:11:51,679
so it doesn't have to try to guess it it

2068
01:11:49,520 --> 01:11:54,560
may have guessed that it's only 13 and

2069
01:11:51,679 --> 01:11:57,440
would give us an incorrect result

2070
01:11:54,560 --> 01:12:01,440
so this is the one hot let's call this x

2071
01:11:57,440 --> 01:12:01,440
inc for x encoded

2072
01:12:02,000 --> 01:12:07,040
and then we see that x encoded that

2073
01:12:03,679 --> 01:12:10,320
shape is 5 by 27

2074
01:12:07,040 --> 01:12:12,239
and uh we can also visualize it plt.i am

2075
01:12:10,320 --> 01:12:13,520
show of x inc

2076
01:12:12,239 --> 01:12:15,360
to make it a little bit more clear

2077
01:12:13,520 --> 01:12:17,120
because this is a little messy

2078
01:12:15,360 --> 01:12:20,640
so we see that we've encoded all the

2079
01:12:17,120 --> 01:12:22,800
five examples uh into vectors we have

2080
01:12:20,640 --> 01:12:24,960
five examples so we have five rows and

2081
01:12:22,800 --> 01:12:26,239
each row here is now an example into a

2082
01:12:24,960 --> 01:12:28,239
neural nut

2083
01:12:26,239 --> 01:12:30,480
and we see that the appropriate bit is

2084
01:12:28,239 --> 01:12:31,840
turned on as a one and everything else

2085
01:12:30,480 --> 01:12:33,440
is zero

2086
01:12:31,840 --> 01:12:35,760
so um

2087
01:12:33,440 --> 01:12:38,239
here for example the zeroth bit is

2088
01:12:35,760 --> 01:12:40,159
turned on the fifth bit is turned on

2089
01:12:38,239 --> 01:12:42,640
13th bits are turned on for both of

2090
01:12:40,159 --> 01:12:44,560
these examples and then the first bit

2091
01:12:42,640 --> 01:12:47,040
here is turned on

2092
01:12:44,560 --> 01:12:49,760
so that's how we can encode

2093
01:12:47,040 --> 01:12:52,080
integers into vectors and then these

2094
01:12:49,760 --> 01:12:53,520
vectors can feed in to neural nets one

2095
01:12:52,080 --> 01:12:55,120
more issue to be careful with here by

2096
01:12:53,520 --> 01:12:56,880
the way is

2097
01:12:55,120 --> 01:12:58,320
let's look at the data type of encoding

2098
01:12:56,880 --> 01:12:59,360
we always want to be careful with data

2099
01:12:58,320 --> 01:13:01,679
types

2100
01:12:59,360 --> 01:13:02,719
what would you expect x encoding's data

2101
01:13:01,679 --> 01:13:04,640
type to be

2102
01:13:02,719 --> 01:13:06,159
when we're plugging numbers into neural

2103
01:13:04,640 --> 01:13:07,440
nuts we don't want them to be integers

2104
01:13:06,159 --> 01:13:10,400
we want them to be floating point

2105
01:13:07,440 --> 01:13:13,120
numbers that can take on various values

2106
01:13:10,400 --> 01:13:14,239
but the d type here is actually 64-bit

2107
01:13:13,120 --> 01:13:15,679
integer

2108
01:13:14,239 --> 01:13:18,880
and the reason for that i suspect is

2109
01:13:15,679 --> 01:13:21,840
that one hot received a 64-bit integer

2110
01:13:18,880 --> 01:13:23,199
here and it returned the same data type

2111
01:13:21,840 --> 01:13:25,440
and when you look at the signature of

2112
01:13:23,199 --> 01:13:28,400
one hot it doesn't even take a d type a

2113
01:13:25,440 --> 01:13:30,560
desired data type of the output tensor

2114
01:13:28,400 --> 01:13:32,000
and so we can't in a lot of functions in

2115
01:13:30,560 --> 01:13:34,320
torch we'd be able to do something like

2116
01:13:32,000 --> 01:13:36,480
d type equal storage.float32

2117
01:13:34,320 --> 01:13:37,920
which is what we want but one heart does

2118
01:13:36,480 --> 01:13:39,679
not support that

2119
01:13:37,920 --> 01:13:43,199
so instead we're going to want to cast

2120
01:13:39,679 --> 01:13:44,800
this to float like this

2121
01:13:43,199 --> 01:13:46,400
so that these

2122
01:13:44,800 --> 01:13:48,640
everything is the same

2123
01:13:46,400 --> 01:13:52,239
everything looks the same but the d-type

2124
01:13:48,640 --> 01:13:54,320
is float32 and floats can feed into

2125
01:13:52,239 --> 01:13:56,000
neural nets so now let's construct our

2126
01:13:54,320 --> 01:13:58,560
first neuron

2127
01:13:56,000 --> 01:14:00,080
this neuron will look at these input

2128
01:13:58,560 --> 01:14:02,080
vectors

2129
01:14:00,080 --> 01:14:03,760
and as you remember from micrograd these

2130
01:14:02,080 --> 01:14:07,280
neurons basically perform a very simple

2131
01:14:03,760 --> 01:14:08,239
function w x plus b where w x is a dot

2132
01:14:07,280 --> 01:14:09,520
product

2133
01:14:08,239 --> 01:14:12,000
right

2134
01:14:09,520 --> 01:14:14,159
so we can achieve the same thing here

2135
01:14:12,000 --> 01:14:15,679
let's first define the weights of this

2136
01:14:14,159 --> 01:14:17,679
neuron basically what are the initial

2137
01:14:15,679 --> 01:14:18,719
weights at initialization for this

2138
01:14:17,679 --> 01:14:21,600
neuron

2139
01:14:18,719 --> 01:14:23,280
let's initialize them with torch.rendin

2140
01:14:21,600 --> 01:14:24,560
torch.rendin

2141
01:14:23,280 --> 01:14:27,040
is um

2142
01:14:24,560 --> 01:14:29,199
fills a tensor with random numbers

2143
01:14:27,040 --> 01:14:31,040
drawn from a normal distribution

2144
01:14:29,199 --> 01:14:31,840
and a normal distribution

2145
01:14:31,040 --> 01:14:34,320
has

2146
01:14:31,840 --> 01:14:35,840
a probability density function like this

2147
01:14:34,320 --> 01:14:39,040
and so most of the numbers drawn from

2148
01:14:35,840 --> 01:14:40,640
this distribution will be around 0

2149
01:14:39,040 --> 01:14:42,400
but some of them will be as high as

2150
01:14:40,640 --> 01:14:46,320
almost three and so on and very few

2151
01:14:42,400 --> 01:14:49,040
numbers will be above three in magnitude

2152
01:14:46,320 --> 01:14:50,400
so we need to take a size as an input

2153
01:14:49,040 --> 01:14:52,880
here

2154
01:14:50,400 --> 01:14:54,560
and i'm going to use size as to be 27 by

2155
01:14:52,880 --> 01:14:55,520
one

2156
01:14:54,560 --> 01:14:58,719
so

2157
01:14:55,520 --> 01:15:03,040
27 by one and then let's visualize w so

2158
01:14:58,719 --> 01:15:03,920
w is a column vector of 27 numbers

2159
01:15:03,040 --> 01:15:06,880
and

2160
01:15:03,920 --> 01:15:08,640
these weights are then multiplied by the

2161
01:15:06,880 --> 01:15:10,719
inputs

2162
01:15:08,640 --> 01:15:13,120
so now to perform this multiplication we

2163
01:15:10,719 --> 01:15:14,960
can take x encoding and we can multiply

2164
01:15:13,120 --> 01:15:17,520
it with w

2165
01:15:14,960 --> 01:15:19,840
this is a matrix multiplication operator

2166
01:15:17,520 --> 01:15:22,400
in pi torch

2167
01:15:19,840 --> 01:15:23,600
and the output of this operation is five

2168
01:15:22,400 --> 01:15:24,800
by one

2169
01:15:23,600 --> 01:15:25,840
the reason is five by five is the

2170
01:15:24,800 --> 01:15:28,000
following

2171
01:15:25,840 --> 01:15:30,560
we took x encoding which is five by

2172
01:15:28,000 --> 01:15:33,520
twenty seven and we multiplied it by

2173
01:15:30,560 --> 01:15:34,400
twenty seven by one

2174
01:15:33,520 --> 01:15:36,480
and

2175
01:15:34,400 --> 01:15:38,800
in matrix multiplication

2176
01:15:36,480 --> 01:15:41,600
you see that the output will become five

2177
01:15:38,800 --> 01:15:44,800
by one because these 27

2178
01:15:41,600 --> 01:15:46,880
will multiply and add

2179
01:15:44,800 --> 01:15:48,719
so basically what we're seeing here outs

2180
01:15:46,880 --> 01:15:51,840
out of this operation

2181
01:15:48,719 --> 01:15:53,679
is we are seeing the five

2182
01:15:51,840 --> 01:15:56,320
activations

2183
01:15:53,679 --> 01:15:58,159
of this neuron

2184
01:15:56,320 --> 01:15:59,760
on these five inputs

2185
01:15:58,159 --> 01:16:01,840
and we've evaluated all of them in

2186
01:15:59,760 --> 01:16:03,920
parallel we didn't feed in just a single

2187
01:16:01,840 --> 01:16:06,640
input to the single neuron we fed in

2188
01:16:03,920 --> 01:16:08,080
simultaneously all the five inputs into

2189
01:16:06,640 --> 01:16:11,120
the same neuron

2190
01:16:08,080 --> 01:16:14,400
and in parallel patrol has evaluated

2191
01:16:11,120 --> 01:16:15,840
the wx plus b but here is just the wx

2192
01:16:14,400 --> 01:16:19,679
there's no bias

2193
01:16:15,840 --> 01:16:21,440
it has value w times x for all of them

2194
01:16:19,679 --> 01:16:23,360
independently now instead of a single

2195
01:16:21,440 --> 01:16:25,199
neuron though i would like to have 27

2196
01:16:23,360 --> 01:16:27,520
neurons and i'll show you in a second

2197
01:16:25,199 --> 01:16:29,760
why i want 27 neurons

2198
01:16:27,520 --> 01:16:31,199
so instead of having just a 1 here which

2199
01:16:29,760 --> 01:16:32,400
is indicating this presence of one

2200
01:16:31,199 --> 01:16:34,640
single neuron

2201
01:16:32,400 --> 01:16:38,320
we can use 27

2202
01:16:34,640 --> 01:16:41,760
and then when w is 27 by 27

2203
01:16:38,320 --> 01:16:46,480
this will in parallel evaluate all the

2204
01:16:41,760 --> 01:16:48,560
27 neurons on all the 5 inputs

2205
01:16:46,480 --> 01:16:51,360
giving us a much better much much bigger

2206
01:16:48,560 --> 01:16:54,000
result so now what we've done is 5 by 27

2207
01:16:51,360 --> 01:16:57,760
multiplied 27 by 27

2208
01:16:54,000 --> 01:17:01,440
and the output of this is now 5 by 27

2209
01:16:57,760 --> 01:17:01,440
so we can see that the shape of this

2210
01:17:01,760 --> 01:17:06,239
is 5 by 27.

2211
01:17:03,840 --> 01:17:07,040
so what is every element here telling us

2212
01:17:06,239 --> 01:17:09,199
right

2213
01:17:07,040 --> 01:17:12,719
it's telling us for every one of 27

2214
01:17:09,199 --> 01:17:12,719
neurons that we created

2215
01:17:13,199 --> 01:17:19,600
what is the firing rate of those neurons

2216
01:17:16,640 --> 01:17:20,719
on every one of those five examples

2217
01:17:19,600 --> 01:17:25,280
so

2218
01:17:20,719 --> 01:17:28,239
the element for example 3 comma 13

2219
01:17:25,280 --> 01:17:29,280
is giving us the firing rate of the 13th

2220
01:17:28,239 --> 01:17:31,840
neuron

2221
01:17:29,280 --> 01:17:34,480
looking at the third input

2222
01:17:31,840 --> 01:17:36,239
and the way this was achieved is by a

2223
01:17:34,480 --> 01:17:37,760
dot product

2224
01:17:36,239 --> 01:17:38,880
between the third

2225
01:17:37,760 --> 01:17:41,440
input

2226
01:17:38,880 --> 01:17:44,800
and the 13th column

2227
01:17:41,440 --> 01:17:45,600
of this w matrix here

2228
01:17:44,800 --> 01:17:46,480
okay

2229
01:17:45,600 --> 01:17:48,239
so

2230
01:17:46,480 --> 01:17:50,800
using matrix multiplication we can very

2231
01:17:48,239 --> 01:17:52,640
efficiently evaluate

2232
01:17:50,800 --> 01:17:54,960
the dot product between lots of input

2233
01:17:52,640 --> 01:17:57,520
examples in a batch

2234
01:17:54,960 --> 01:17:59,679
and lots of neurons where all those

2235
01:17:57,520 --> 01:18:01,040
neurons have weights in the columns of

2236
01:17:59,679 --> 01:18:02,400
those w's

2237
01:18:01,040 --> 01:18:04,480
and in matrix multiplication we're just

2238
01:18:02,400 --> 01:18:06,239
doing those dot products and

2239
01:18:04,480 --> 01:18:08,480
in parallel just to show you that this

2240
01:18:06,239 --> 01:18:10,400
is the case we can take x and we can

2241
01:18:08,480 --> 01:18:12,080
take the third

2242
01:18:10,400 --> 01:18:14,640
row

2243
01:18:12,080 --> 01:18:16,960
and we can take the w and take its 13th

2244
01:18:14,640 --> 01:18:16,960
column

2245
01:18:17,360 --> 01:18:21,520
and then we can do

2246
01:18:18,800 --> 01:18:26,640
x and get three

2247
01:18:21,520 --> 01:18:29,600
elementwise multiply with w at 13.

2248
01:18:26,640 --> 01:18:31,920
and sum that up that's wx plus b

2249
01:18:29,600 --> 01:18:32,800
well there's no plus b it's just wx dot

2250
01:18:31,920 --> 01:18:34,080
product

2251
01:18:32,800 --> 01:18:35,199
and that's

2252
01:18:34,080 --> 01:18:36,880
this number

2253
01:18:35,199 --> 01:18:39,520
so you see that this is just being done

2254
01:18:36,880 --> 01:18:40,400
efficiently by the matrix multiplication

2255
01:18:39,520 --> 01:18:42,719
operation

2256
01:18:40,400 --> 01:18:46,000
for all the input examples and for all

2257
01:18:42,719 --> 01:18:49,280
the output neurons of this first layer

2258
01:18:46,000 --> 01:18:51,120
okay so we fed our 27-dimensional inputs

2259
01:18:49,280 --> 01:18:54,400
into a first layer of a neural net that

2260
01:18:51,120 --> 01:18:57,199
has 27 neurons right so we have 27

2261
01:18:54,400 --> 01:19:00,000
inputs and now we have 27 neurons these

2262
01:18:57,199 --> 01:19:01,520
neurons perform w times x they don't

2263
01:19:00,000 --> 01:19:03,600
have a bias and they don't have a

2264
01:19:01,520 --> 01:19:06,480
non-linearity like 10 h we're going to

2265
01:19:03,600 --> 01:19:07,920
leave them to be a linear layer

2266
01:19:06,480 --> 01:19:09,520
in addition to that we're not going to

2267
01:19:07,920 --> 01:19:11,440
have any other layers this is going to

2268
01:19:09,520 --> 01:19:13,520
be it it's just going to be

2269
01:19:11,440 --> 01:19:16,400
the dumbest smallest simplest neural net

2270
01:19:13,520 --> 01:19:18,080
which is just a single linear layer

2271
01:19:16,400 --> 01:19:21,199
and now i'd like to explain what i want

2272
01:19:18,080 --> 01:19:22,560
those 27 outputs to be

2273
01:19:21,199 --> 01:19:24,640
intuitively what we're trying to produce

2274
01:19:22,560 --> 01:19:26,000
here for every single input example is

2275
01:19:24,640 --> 01:19:27,679
we're trying to produce some kind of a

2276
01:19:26,000 --> 01:19:29,280
probability distribution for the next

2277
01:19:27,679 --> 01:19:31,440
character in a sequence

2278
01:19:29,280 --> 01:19:33,040
and there's 27 of them

2279
01:19:31,440 --> 01:19:34,800
but we have to come up with like precise

2280
01:19:33,040 --> 01:19:37,679
semantics for exactly how we're going to

2281
01:19:34,800 --> 01:19:39,679
interpret these 27 numbers that these

2282
01:19:37,679 --> 01:19:41,120
neurons take on

2283
01:19:39,679 --> 01:19:42,560
now intuitively

2284
01:19:41,120 --> 01:19:44,000
you see here that these numbers are

2285
01:19:42,560 --> 01:19:45,120
negative and some of them are positive

2286
01:19:44,000 --> 01:19:46,480
etc

2287
01:19:45,120 --> 01:19:48,880
and that's because these are coming out

2288
01:19:46,480 --> 01:19:51,199
of a neural net layer initialized with

2289
01:19:48,880 --> 01:19:51,199
these

2290
01:19:51,360 --> 01:19:54,239
normal distribution

2291
01:19:52,960 --> 01:19:55,840
parameters

2292
01:19:54,239 --> 01:19:57,199
but what we want is we want something

2293
01:19:55,840 --> 01:19:58,960
like we had here

2294
01:19:57,199 --> 01:20:01,120
like each row here

2295
01:19:58,960 --> 01:20:02,159
told us the counts and then we

2296
01:20:01,120 --> 01:20:04,239
normalized the counts to get

2297
01:20:02,159 --> 01:20:06,400
probabilities and we want something

2298
01:20:04,239 --> 01:20:07,840
similar to come out of the neural net

2299
01:20:06,400 --> 01:20:10,560
but what we just have right now is just

2300
01:20:07,840 --> 01:20:12,239
some negative and positive numbers

2301
01:20:10,560 --> 01:20:14,000
now we want those numbers to somehow

2302
01:20:12,239 --> 01:20:15,280
represent the probabilities for the next

2303
01:20:14,000 --> 01:20:17,600
character

2304
01:20:15,280 --> 01:20:19,920
but you see that probabilities they they

2305
01:20:17,600 --> 01:20:21,920
have a special structure they um

2306
01:20:19,920 --> 01:20:22,800
they're positive numbers and they sum to

2307
01:20:21,920 --> 01:20:24,560
one

2308
01:20:22,800 --> 01:20:25,760
and so that doesn't just come out of a

2309
01:20:24,560 --> 01:20:27,760
neural net

2310
01:20:25,760 --> 01:20:31,040
and then they can't be counts

2311
01:20:27,760 --> 01:20:32,719
because these counts are positive and

2312
01:20:31,040 --> 01:20:34,480
counts are integers

2313
01:20:32,719 --> 01:20:36,639
so counts are also not really a good

2314
01:20:34,480 --> 01:20:38,159
thing to output from a neural net

2315
01:20:36,639 --> 01:20:39,760
so instead what the neural net is going

2316
01:20:38,159 --> 01:20:42,080
to output and how we are going to

2317
01:20:39,760 --> 01:20:45,600
interpret the um

2318
01:20:42,080 --> 01:20:48,480
the 27 numbers is that these 27 numbers

2319
01:20:45,600 --> 01:20:49,679
are giving us log counts

2320
01:20:48,480 --> 01:20:50,400
basically

2321
01:20:49,679 --> 01:20:52,880
um

2322
01:20:50,400 --> 01:20:54,800
so instead of giving us counts directly

2323
01:20:52,880 --> 01:20:56,000
like in this table they're giving us log

2324
01:20:54,800 --> 01:20:57,679
counts

2325
01:20:56,000 --> 01:20:59,360
and to get the counts we're going to

2326
01:20:57,679 --> 01:21:01,360
take the log counts and we're going to

2327
01:20:59,360 --> 01:21:02,320
exponentiate them

2328
01:21:01,360 --> 01:21:04,159
now

2329
01:21:02,320 --> 01:21:07,199
exponentiation

2330
01:21:04,159 --> 01:21:08,800
takes the following form

2331
01:21:07,199 --> 01:21:10,880
it takes numbers

2332
01:21:08,800 --> 01:21:12,800
that are negative or they are positive

2333
01:21:10,880 --> 01:21:14,719
it takes the entire real line

2334
01:21:12,800 --> 01:21:17,199
and then if you plug in negative numbers

2335
01:21:14,719 --> 01:21:20,560
you're going to get e to the x

2336
01:21:17,199 --> 01:21:23,360
which is uh always below one

2337
01:21:20,560 --> 01:21:25,280
so you're getting numbers lower than one

2338
01:21:23,360 --> 01:21:27,440
and if you plug in numbers greater than

2339
01:21:25,280 --> 01:21:30,800
zero you're getting numbers greater than

2340
01:21:27,440 --> 01:21:33,199
one all the way growing to the infinity

2341
01:21:30,800 --> 01:21:35,120
and this here grows to zero

2342
01:21:33,199 --> 01:21:37,760
so basically we're going to

2343
01:21:35,120 --> 01:21:40,000
take these numbers

2344
01:21:37,760 --> 01:21:40,000
here

2345
01:21:40,320 --> 01:21:44,560
and

2346
01:21:43,280 --> 01:21:46,159
instead of them being positive and

2347
01:21:44,560 --> 01:21:48,719
negative and all over the place we're

2348
01:21:46,159 --> 01:21:50,159
going to interpret them as log counts

2349
01:21:48,719 --> 01:21:52,719
and then we're going to element wise

2350
01:21:50,159 --> 01:21:54,480
exponentiate these numbers

2351
01:21:52,719 --> 01:21:56,400
exponentiating them now gives us

2352
01:21:54,480 --> 01:21:57,600
something like this

2353
01:21:56,400 --> 01:21:59,280
and you see that these numbers now

2354
01:21:57,600 --> 01:22:00,800
because they went through an exponent

2355
01:21:59,280 --> 01:22:04,560
all the negative numbers turned into

2356
01:22:00,800 --> 01:22:07,120
numbers below 1 like 0.338 and all the

2357
01:22:04,560 --> 01:22:08,639
positive numbers originally turned into

2358
01:22:07,120 --> 01:22:10,719
even more positive numbers sort of

2359
01:22:08,639 --> 01:22:12,480
greater than one

2360
01:22:10,719 --> 01:22:14,480
so like for example

2361
01:22:12,480 --> 01:22:18,400
seven

2362
01:22:14,480 --> 01:22:21,040
is some positive number over here

2363
01:22:18,400 --> 01:22:24,800
that is greater than zero

2364
01:22:21,040 --> 01:22:26,320
but exponentiated outputs here

2365
01:22:24,800 --> 01:22:29,040
basically give us something that we can

2366
01:22:26,320 --> 01:22:31,520
use and interpret as the equivalent of

2367
01:22:29,040 --> 01:22:36,320
counts originally so you see these

2368
01:22:31,520 --> 01:22:38,560
counts here 112 7 51 1 etc

2369
01:22:36,320 --> 01:22:40,239
the neural net is kind of now predicting

2370
01:22:38,560 --> 01:22:41,520
uh

2371
01:22:40,239 --> 01:22:43,840
counts

2372
01:22:41,520 --> 01:22:45,679
and these counts are positive numbers

2373
01:22:43,840 --> 01:22:46,719
they can never be below zero so that

2374
01:22:45,679 --> 01:22:48,560
makes sense

2375
01:22:46,719 --> 01:22:49,679
and uh they can now take on various

2376
01:22:48,560 --> 01:22:54,159
values

2377
01:22:49,679 --> 01:22:56,159
depending on the settings of w

2378
01:22:54,159 --> 01:22:58,239
so let me break this down

2379
01:22:56,159 --> 01:23:01,040
we're going to interpret these to be the

2380
01:22:58,239 --> 01:23:01,040
log counts

2381
01:23:01,199 --> 01:23:05,120
in other words for this that is often

2382
01:23:02,960 --> 01:23:08,560
used is so-called logits

2383
01:23:05,120 --> 01:23:11,199
these are logits log counts

2384
01:23:08,560 --> 01:23:13,360
then these will be sort of the counts

2385
01:23:11,199 --> 01:23:16,239
largest exponentiated

2386
01:23:13,360 --> 01:23:18,000
and this is equivalent to the n matrix

2387
01:23:16,239 --> 01:23:20,080
sort of the n

2388
01:23:18,000 --> 01:23:21,600
array that we used previously remember

2389
01:23:20,080 --> 01:23:24,080
this was the n

2390
01:23:21,600 --> 01:23:27,760
this is the the array of counts

2391
01:23:24,080 --> 01:23:28,960
and each row here are the counts for the

2392
01:23:27,760 --> 01:23:32,239
for the um

2393
01:23:28,960 --> 01:23:32,239
next character sort of

2394
01:23:32,639 --> 01:23:37,920
so those are the counts and now the

2395
01:23:34,800 --> 01:23:39,600
probabilities are just the counts um

2396
01:23:37,920 --> 01:23:41,600
normalized

2397
01:23:39,600 --> 01:23:43,040
and so um

2398
01:23:41,600 --> 01:23:44,800
i'm not going to find the same but

2399
01:23:43,040 --> 01:23:46,000
basically i'm not going to scroll all

2400
01:23:44,800 --> 01:23:48,159
over the place

2401
01:23:46,000 --> 01:23:50,080
we've already done this we want to

2402
01:23:48,159 --> 01:23:52,239
counts that sum

2403
01:23:50,080 --> 01:23:54,719
along the first dimension and we want to

2404
01:23:52,239 --> 01:23:56,800
keep them as true

2405
01:23:54,719 --> 01:23:59,920
we've went over this and this is how we

2406
01:23:56,800 --> 01:24:03,199
normalize the rows of our counts matrix

2407
01:23:59,920 --> 01:24:04,880
to get our probabilities

2408
01:24:03,199 --> 01:24:07,920
props

2409
01:24:04,880 --> 01:24:08,639
so now these are the probabilities

2410
01:24:07,920 --> 01:24:10,159
and

2411
01:24:08,639 --> 01:24:11,360
these are the counts that we ask

2412
01:24:10,159 --> 01:24:13,679
currently and now when i show the

2413
01:24:11,360 --> 01:24:15,600
probabilities

2414
01:24:13,679 --> 01:24:17,360
you see that um

2415
01:24:15,600 --> 01:24:19,440
every row here

2416
01:24:17,360 --> 01:24:21,360
of course

2417
01:24:19,440 --> 01:24:23,120
will sum to 1

2418
01:24:21,360 --> 01:24:25,280
because they're normalized

2419
01:24:23,120 --> 01:24:27,360
and the shape of this

2420
01:24:25,280 --> 01:24:29,760
is 5 by 27

2421
01:24:27,360 --> 01:24:31,679
and so really what we've achieved is for

2422
01:24:29,760 --> 01:24:33,760
every one of our five examples

2423
01:24:31,679 --> 01:24:35,040
we now have a row that came out of a

2424
01:24:33,760 --> 01:24:37,520
neural net

2425
01:24:35,040 --> 01:24:39,199
and because of the transformations here

2426
01:24:37,520 --> 01:24:41,440
we made sure that this output of this

2427
01:24:39,199 --> 01:24:44,000
neural net now are probabilities or we

2428
01:24:41,440 --> 01:24:45,280
can interpret to be probabilities

2429
01:24:44,000 --> 01:24:48,000
so

2430
01:24:45,280 --> 01:24:49,679
our wx here gave us logits

2431
01:24:48,000 --> 01:24:50,639
and then we interpret those to be log

2432
01:24:49,679 --> 01:24:52,400
counts

2433
01:24:50,639 --> 01:24:53,920
we exponentiate to get something that

2434
01:24:52,400 --> 01:24:55,360
looks like counts

2435
01:24:53,920 --> 01:24:57,440
and then we normalize those counts to

2436
01:24:55,360 --> 01:24:59,040
get a probability distribution

2437
01:24:57,440 --> 01:25:00,320
and all of these are differentiable

2438
01:24:59,040 --> 01:25:02,320
operations

2439
01:25:00,320 --> 01:25:04,639
so what we've done now is we're taking

2440
01:25:02,320 --> 01:25:06,800
inputs we have differentiable operations

2441
01:25:04,639 --> 01:25:08,239
that we can back propagate through

2442
01:25:06,800 --> 01:25:09,840
and we're getting out probability

2443
01:25:08,239 --> 01:25:11,040
distributions

2444
01:25:09,840 --> 01:25:13,360
so

2445
01:25:11,040 --> 01:25:15,120
for example for the zeroth example that

2446
01:25:13,360 --> 01:25:17,040
fed in

2447
01:25:15,120 --> 01:25:18,800
right which was um

2448
01:25:17,040 --> 01:25:20,560
the zeroth example here was a one-half

2449
01:25:18,800 --> 01:25:22,480
vector of zero

2450
01:25:20,560 --> 01:25:26,560
and um

2451
01:25:22,480 --> 01:25:28,639
it basically corresponded to feeding in

2452
01:25:26,560 --> 01:25:31,040
this example here so we're feeding in a

2453
01:25:28,639 --> 01:25:32,639
dot into a neural net and the way we fed

2454
01:25:31,040 --> 01:25:34,320
the dot into a neural net is that we

2455
01:25:32,639 --> 01:25:36,560
first got its index

2456
01:25:34,320 --> 01:25:39,040
then we one hot encoded it

2457
01:25:36,560 --> 01:25:40,560
then it went into the neural net and out

2458
01:25:39,040 --> 01:25:43,360
came

2459
01:25:40,560 --> 01:25:46,400
this distribution of probabilities

2460
01:25:43,360 --> 01:25:49,120
and its shape

2461
01:25:46,400 --> 01:25:51,360
is 27 there's 27 numbers and we're going

2462
01:25:49,120 --> 01:25:54,400
to interpret this as the neural nets

2463
01:25:51,360 --> 01:25:56,560
assignment for how likely every one of

2464
01:25:54,400 --> 01:25:59,679
these characters um

2465
01:25:56,560 --> 01:26:02,320
the 27 characters are to come next

2466
01:25:59,679 --> 01:26:03,520
and as we tune the weights w

2467
01:26:02,320 --> 01:26:05,360
we're going to be of course getting

2468
01:26:03,520 --> 01:26:07,120
different probabilities out for any

2469
01:26:05,360 --> 01:26:08,960
character that you input

2470
01:26:07,120 --> 01:26:10,960
and so now the question is just can we

2471
01:26:08,960 --> 01:26:12,960
optimize and find a good w

2472
01:26:10,960 --> 01:26:15,280
such that the probabilities coming out

2473
01:26:12,960 --> 01:26:17,360
are pretty good and the way we measure

2474
01:26:15,280 --> 01:26:18,960
pretty good is by the loss function okay

2475
01:26:17,360 --> 01:26:20,400
so i organized everything into a single

2476
01:26:18,960 --> 01:26:22,560
summary so that hopefully it's a bit

2477
01:26:20,400 --> 01:26:24,400
more clear so it starts here

2478
01:26:22,560 --> 01:26:26,320
with an input data set

2479
01:26:24,400 --> 01:26:28,560
we have some inputs to the neural net

2480
01:26:26,320 --> 01:26:30,800
and we have some labels for the correct

2481
01:26:28,560 --> 01:26:32,719
next character in a sequence these are

2482
01:26:30,800 --> 01:26:35,360
integers

2483
01:26:32,719 --> 01:26:37,440
here i'm using uh torch generators now

2484
01:26:35,360 --> 01:26:38,560
so that you see the same numbers that i

2485
01:26:37,440 --> 01:26:40,800
see

2486
01:26:38,560 --> 01:26:42,800
and i'm generating um

2487
01:26:40,800 --> 01:26:47,520
27 neurons weights

2488
01:26:42,800 --> 01:26:47,520
and each neuron here receives 27 inputs

2489
01:26:48,560 --> 01:26:52,880
then here we're going to plug in all the

2490
01:26:50,400 --> 01:26:55,679
input examples x's into a neural net so

2491
01:26:52,880 --> 01:26:57,600
here this is a forward pass

2492
01:26:55,679 --> 01:27:00,400
first we have to encode all of the

2493
01:26:57,600 --> 01:27:02,719
inputs into one hot representations

2494
01:27:00,400 --> 01:27:04,400
so we have 27 classes we pass in these

2495
01:27:02,719 --> 01:27:09,679
integers and

2496
01:27:04,400 --> 01:27:12,239
x inc becomes a array that is 5 by 27

2497
01:27:09,679 --> 01:27:14,239
zeros except for a few ones

2498
01:27:12,239 --> 01:27:16,719
we then multiply this in the first layer

2499
01:27:14,239 --> 01:27:18,880
of a neural net to get logits

2500
01:27:16,719 --> 01:27:20,639
exponentiate the logits to get fake

2501
01:27:18,880 --> 01:27:22,159
counts sort of

2502
01:27:20,639 --> 01:27:24,320
and normalize these counts to get

2503
01:27:22,159 --> 01:27:26,480
probabilities

2504
01:27:24,320 --> 01:27:30,000
so we lock these last two lines by the

2505
01:27:26,480 --> 01:27:33,199
way here are called the softmax

2506
01:27:30,000 --> 01:27:35,920
which i pulled up here soft max is a

2507
01:27:33,199 --> 01:27:38,880
very often used layer in a neural net

2508
01:27:35,920 --> 01:27:40,960
that takes these z's which are logics

2509
01:27:38,880 --> 01:27:43,679
exponentiates them

2510
01:27:40,960 --> 01:27:44,639
and divides and normalizes it's a way of

2511
01:27:43,679 --> 01:27:47,199
taking

2512
01:27:44,639 --> 01:27:48,560
outputs of a neural net layer and these

2513
01:27:47,199 --> 01:27:49,760
these outputs can be positive or

2514
01:27:48,560 --> 01:27:52,400
negative

2515
01:27:49,760 --> 01:27:55,199
and it outputs probability distributions

2516
01:27:52,400 --> 01:27:56,800
it outputs something that is always

2517
01:27:55,199 --> 01:27:58,480
sums to one and are positive numbers

2518
01:27:56,800 --> 01:28:00,159
just like probabilities

2519
01:27:58,480 --> 01:28:01,440
um so it's kind of like a normalization

2520
01:28:00,159 --> 01:28:03,360
function if you want to think of it that

2521
01:28:01,440 --> 01:28:05,600
way and you can put it on top of any

2522
01:28:03,360 --> 01:28:07,120
other linear layer inside a neural net

2523
01:28:05,600 --> 01:28:09,760
and it basically makes a neural net

2524
01:28:07,120 --> 01:28:13,280
output probabilities that's very often

2525
01:28:09,760 --> 01:28:14,639
used and we used it as well here

2526
01:28:13,280 --> 01:28:16,159
so this is the forward pass and that's

2527
01:28:14,639 --> 01:28:17,840
how we made a neural net output

2528
01:28:16,159 --> 01:28:19,360
probability

2529
01:28:17,840 --> 01:28:20,639
now

2530
01:28:19,360 --> 01:28:22,800
you'll notice that

2531
01:28:20,639 --> 01:28:22,800
um

2532
01:28:22,960 --> 01:28:25,920
all of these

2533
01:28:24,239 --> 01:28:27,280
this entire forward pass is made up of

2534
01:28:25,920 --> 01:28:29,120
differentiable

2535
01:28:27,280 --> 01:28:30,960
layers everything here we can back

2536
01:28:29,120 --> 01:28:33,199
propagate through and we saw some of the

2537
01:28:30,960 --> 01:28:34,719
back propagation in micrograd

2538
01:28:33,199 --> 01:28:36,800
this is just

2539
01:28:34,719 --> 01:28:38,239
multiplication and addition all that's

2540
01:28:36,800 --> 01:28:39,760
happening here is just multiply and then

2541
01:28:38,239 --> 01:28:40,719
add and we know how to backpropagate

2542
01:28:39,760 --> 01:28:42,000
through them

2543
01:28:40,719 --> 01:28:43,840
exponentiation we know how to

2544
01:28:42,000 --> 01:28:46,480
backpropagate through

2545
01:28:43,840 --> 01:28:49,199
and then here we are summing

2546
01:28:46,480 --> 01:28:50,080
and sum is is easily backpropagable as

2547
01:28:49,199 --> 01:28:52,560
well

2548
01:28:50,080 --> 01:28:54,560
and division as well so everything here

2549
01:28:52,560 --> 01:28:57,520
is differentiable operation

2550
01:28:54,560 --> 01:28:59,760
and we can back propagate through

2551
01:28:57,520 --> 01:29:01,600
now we achieve these probabilities which

2552
01:28:59,760 --> 01:29:03,920
are 5 by 27

2553
01:29:01,600 --> 01:29:06,320
for every single example we have a

2554
01:29:03,920 --> 01:29:08,480
vector of probabilities that's into one

2555
01:29:06,320 --> 01:29:10,400
and then here i wrote a bunch of stuff

2556
01:29:08,480 --> 01:29:11,440
to sort of like break down uh the

2557
01:29:10,400 --> 01:29:14,320
examples

2558
01:29:11,440 --> 01:29:16,320
so we have five examples making up emma

2559
01:29:14,320 --> 01:29:20,000
right

2560
01:29:16,320 --> 01:29:23,600
and there are five bigrams inside emma

2561
01:29:20,000 --> 01:29:26,320
so bigram example a bigram example1 is

2562
01:29:23,600 --> 01:29:28,239
that e is the beginning character right

2563
01:29:26,320 --> 01:29:30,400
after dot

2564
01:29:28,239 --> 01:29:31,280
and the indexes for these are zero and

2565
01:29:30,400 --> 01:29:34,080
five

2566
01:29:31,280 --> 01:29:35,920
so then we feed in a zero

2567
01:29:34,080 --> 01:29:38,080
that's the input of the neural net

2568
01:29:35,920 --> 01:29:41,280
we get probabilities from the neural net

2569
01:29:38,080 --> 01:29:44,080
that are 27 numbers

2570
01:29:41,280 --> 01:29:45,840
and then the label is 5 because e

2571
01:29:44,080 --> 01:29:47,920
actually comes after dot

2572
01:29:45,840 --> 01:29:49,360
so that's the label

2573
01:29:47,920 --> 01:29:52,320
and then

2574
01:29:49,360 --> 01:29:54,320
we use this label 5 to index into the

2575
01:29:52,320 --> 01:29:55,120
probability distribution here

2576
01:29:54,320 --> 01:29:56,000
so

2577
01:29:55,120 --> 01:30:00,000
this

2578
01:29:56,000 --> 01:30:01,280
index 5 here is 0 1 2 3 4 5. it's this

2579
01:30:00,000 --> 01:30:04,000
number here

2580
01:30:01,280 --> 01:30:05,360
which is here

2581
01:30:04,000 --> 01:30:07,280
so that's basically the probability

2582
01:30:05,360 --> 01:30:08,719
assigned by the neural net to the actual

2583
01:30:07,280 --> 01:30:10,159
correct character

2584
01:30:08,719 --> 01:30:12,480
you see that the network currently

2585
01:30:10,159 --> 01:30:15,280
thinks that this next character that e

2586
01:30:12,480 --> 01:30:16,960
following dot is only one percent likely

2587
01:30:15,280 --> 01:30:18,800
which is of course not very good right

2588
01:30:16,960 --> 01:30:20,800
because this actually is a training

2589
01:30:18,800 --> 01:30:22,800
example and the network thinks this is

2590
01:30:20,800 --> 01:30:24,800
currently very very unlikely but that's

2591
01:30:22,800 --> 01:30:27,280
just because we didn't get very lucky in

2592
01:30:24,800 --> 01:30:29,280
generating a good setting of w so right

2593
01:30:27,280 --> 01:30:31,920
now this network things it says unlikely

2594
01:30:29,280 --> 01:30:34,719
and 0.01 is not a good outcome

2595
01:30:31,920 --> 01:30:35,760
so the log likelihood then is very

2596
01:30:34,719 --> 01:30:38,239
negative

2597
01:30:35,760 --> 01:30:39,360
and the negative log likelihood is very

2598
01:30:38,239 --> 01:30:42,400
positive

2599
01:30:39,360 --> 01:30:44,000
and so four is a very high negative log

2600
01:30:42,400 --> 01:30:45,280
likelihood and that means we're going to

2601
01:30:44,000 --> 01:30:47,440
have a high loss

2602
01:30:45,280 --> 01:30:51,600
because what is the loss the loss is

2603
01:30:47,440 --> 01:30:53,600
just the average negative log likelihood

2604
01:30:51,600 --> 01:30:55,280
so the second character is em

2605
01:30:53,600 --> 01:30:57,760
and you see here that also the network

2606
01:30:55,280 --> 01:31:00,800
thought that m following e is very

2607
01:30:57,760 --> 01:31:00,800
unlikely one percent

2608
01:31:00,960 --> 01:31:04,239
the for m following m i thought it was

2609
01:31:03,040 --> 01:31:06,080
two percent

2610
01:31:04,239 --> 01:31:08,639
and for a following m it actually

2611
01:31:06,080 --> 01:31:10,719
thought it was seven percent likely so

2612
01:31:08,639 --> 01:31:12,320
just by chance this one actually has a

2613
01:31:10,719 --> 01:31:15,280
pretty good probability and therefore

2614
01:31:12,320 --> 01:31:17,040
pretty low negative log likelihood

2615
01:31:15,280 --> 01:31:18,320
and finally here it thought this was one

2616
01:31:17,040 --> 01:31:20,480
percent likely

2617
01:31:18,320 --> 01:31:22,880
so overall our average negative log

2618
01:31:20,480 --> 01:31:24,719
likelihood which is the loss the total

2619
01:31:22,880 --> 01:31:26,480
loss that summarizes

2620
01:31:24,719 --> 01:31:28,159
basically the how well this network

2621
01:31:26,480 --> 01:31:30,080
currently works at least on this one

2622
01:31:28,159 --> 01:31:33,199
word not on the full data suggested one

2623
01:31:30,080 --> 01:31:35,040
word is 3.76 which is actually very

2624
01:31:33,199 --> 01:31:36,800
fairly high loss this is not a very good

2625
01:31:35,040 --> 01:31:38,560
setting of w's

2626
01:31:36,800 --> 01:31:41,199
now here's what we can do

2627
01:31:38,560 --> 01:31:42,960
we're currently getting 3.76

2628
01:31:41,199 --> 01:31:45,920
we can actually come here and we can

2629
01:31:42,960 --> 01:31:48,639
change our w we can resample it so let

2630
01:31:45,920 --> 01:31:50,480
me just add one to have a different seed

2631
01:31:48,639 --> 01:31:52,800
and then we get a different w

2632
01:31:50,480 --> 01:31:54,639
and then we can rerun this

2633
01:31:52,800 --> 01:31:58,560
and with this different c with this

2634
01:31:54,639 --> 01:32:00,800
different setting of w's we now get 3.37

2635
01:31:58,560 --> 01:32:02,159
so this is a much better w right and

2636
01:32:00,800 --> 01:32:04,719
that and it's better because the

2637
01:32:02,159 --> 01:32:07,040
probabilities just happen to come out

2638
01:32:04,719 --> 01:32:08,800
higher for the for the characters that

2639
01:32:07,040 --> 01:32:10,000
actually are next

2640
01:32:08,800 --> 01:32:14,320
and so you can imagine actually just

2641
01:32:10,000 --> 01:32:15,440
resampling this you know we can try two

2642
01:32:14,320 --> 01:32:17,120
so

2643
01:32:15,440 --> 01:32:18,480
okay this was not very good

2644
01:32:17,120 --> 01:32:20,880
let's try one more

2645
01:32:18,480 --> 01:32:22,800
we can try three

2646
01:32:20,880 --> 01:32:24,840
okay this was terrible setting because

2647
01:32:22,800 --> 01:32:29,280
we have a very high loss

2648
01:32:24,840 --> 01:32:29,280
so anyway i'm going to erase this

2649
01:32:29,920 --> 01:32:32,960
what i'm doing here which is just guess

2650
01:32:31,440 --> 01:32:34,639
and check of randomly assigning

2651
01:32:32,960 --> 01:32:37,440
parameters and seeing if the network is

2652
01:32:34,639 --> 01:32:39,280
good that is uh amateur hour that's not

2653
01:32:37,440 --> 01:32:40,719
how you optimize a neural net the way

2654
01:32:39,280 --> 01:32:42,480
you optimize your neural net is you

2655
01:32:40,719 --> 01:32:43,760
start with some random guess and we're

2656
01:32:42,480 --> 01:32:45,120
going to commit to this one even though

2657
01:32:43,760 --> 01:32:46,880
it's not very good

2658
01:32:45,120 --> 01:32:48,239
but now the big deal is we have a loss

2659
01:32:46,880 --> 01:32:49,920
function

2660
01:32:48,239 --> 01:32:52,840
so this loss

2661
01:32:49,920 --> 01:32:56,320
is made up only of differentiable

2662
01:32:52,840 --> 01:32:57,520
operations and we can minimize the loss

2663
01:32:56,320 --> 01:32:58,639
by tuning

2664
01:32:57,520 --> 01:33:01,199
ws

2665
01:32:58,639 --> 01:33:02,560
by computing the gradients of the loss

2666
01:33:01,199 --> 01:33:04,960
with respect to

2667
01:33:02,560 --> 01:33:07,120
these w matrices

2668
01:33:04,960 --> 01:33:09,520
and so then we can tune w to minimize

2669
01:33:07,120 --> 01:33:11,760
the loss and find a good setting of w

2670
01:33:09,520 --> 01:33:13,360
using gradient based optimization so

2671
01:33:11,760 --> 01:33:14,560
let's see how that will work now things

2672
01:33:13,360 --> 01:33:17,040
are actually going to look almost

2673
01:33:14,560 --> 01:33:18,159
identical to what we had with micrograd

2674
01:33:17,040 --> 01:33:20,880
so here

2675
01:33:18,159 --> 01:33:23,760
i pulled up the lecture from micrograd

2676
01:33:20,880 --> 01:33:24,960
the notebook it's from this repository

2677
01:33:23,760 --> 01:33:26,880
and when i scroll all the way to the end

2678
01:33:24,960 --> 01:33:28,560
where we left off with micrograd we had

2679
01:33:26,880 --> 01:33:29,600
something very very similar

2680
01:33:28,560 --> 01:33:31,199
we had

2681
01:33:29,600 --> 01:33:34,159
a number of input examples in this case

2682
01:33:31,199 --> 01:33:36,560
we had four input examples inside axis

2683
01:33:34,159 --> 01:33:37,600
and we had their targets these are

2684
01:33:36,560 --> 01:33:39,679
targets

2685
01:33:37,600 --> 01:33:41,199
just like here we have our axes now but

2686
01:33:39,679 --> 01:33:44,080
we have five of them and they're now

2687
01:33:41,199 --> 01:33:45,840
integers instead of vectors

2688
01:33:44,080 --> 01:33:48,880
but we're going to convert our integers

2689
01:33:45,840 --> 01:33:51,920
to vectors except our vectors will be 27

2690
01:33:48,880 --> 01:33:53,600
large instead of three large

2691
01:33:51,920 --> 01:33:55,840
and then here what we did is first we

2692
01:33:53,600 --> 01:33:58,239
did a forward pass where we ran a neural

2693
01:33:55,840 --> 01:34:00,239
net on all of the inputs

2694
01:33:58,239 --> 01:34:02,880
to get predictions

2695
01:34:00,239 --> 01:34:05,040
our neural net at the time this nfx was

2696
01:34:02,880 --> 01:34:06,560
a multi-layer perceptron

2697
01:34:05,040 --> 01:34:08,239
our neural net is going to look

2698
01:34:06,560 --> 01:34:10,400
different because our neural net is just

2699
01:34:08,239 --> 01:34:12,400
a single layer

2700
01:34:10,400 --> 01:34:13,760
single linear layer followed by a soft

2701
01:34:12,400 --> 01:34:15,840
max

2702
01:34:13,760 --> 01:34:17,679
so that's our neural net

2703
01:34:15,840 --> 01:34:19,520
and the loss here was the mean squared

2704
01:34:17,679 --> 01:34:21,040
error so we simply subtracted the

2705
01:34:19,520 --> 01:34:23,120
prediction from the ground truth and

2706
01:34:21,040 --> 01:34:25,280
squared it and summed it all up and that

2707
01:34:23,120 --> 01:34:27,520
was the loss and loss was the single

2708
01:34:25,280 --> 01:34:30,719
number that summarized the quality of

2709
01:34:27,520 --> 01:34:33,760
the neural net and when loss is low like

2710
01:34:30,719 --> 01:34:36,159
almost zero that means the neural net is

2711
01:34:33,760 --> 01:34:38,480
predicting correctly

2712
01:34:36,159 --> 01:34:40,960
so we had a single number that uh that

2713
01:34:38,480 --> 01:34:42,800
summarized the uh the performance of the

2714
01:34:40,960 --> 01:34:44,639
neural net and everything here was

2715
01:34:42,800 --> 01:34:46,800
differentiable and was stored in massive

2716
01:34:44,639 --> 01:34:48,719
compute graph

2717
01:34:46,800 --> 01:34:50,080
and then we iterated over all the

2718
01:34:48,719 --> 01:34:52,320
parameters we made sure that the

2719
01:34:50,080 --> 01:34:54,080
gradients are set to zero and we called

2720
01:34:52,320 --> 01:34:56,080
lost up backward

2721
01:34:54,080 --> 01:34:58,400
and lasted backward initiated back

2722
01:34:56,080 --> 01:34:59,280
propagation at the final output node of

2723
01:34:58,400 --> 01:35:00,800
loss

2724
01:34:59,280 --> 01:35:02,400
right so

2725
01:35:00,800 --> 01:35:03,920
yeah remember these expressions we had

2726
01:35:02,400 --> 01:35:05,520
loss all the way at the end we start

2727
01:35:03,920 --> 01:35:06,320
back propagation and we went all the way

2728
01:35:05,520 --> 01:35:08,400
back

2729
01:35:06,320 --> 01:35:10,719
and we made sure that we populated all

2730
01:35:08,400 --> 01:35:12,639
the parameters dot grad

2731
01:35:10,719 --> 01:35:14,480
so that graph started at zero but back

2732
01:35:12,639 --> 01:35:16,400
propagation filled it in

2733
01:35:14,480 --> 01:35:18,639
and then in the update we iterated over

2734
01:35:16,400 --> 01:35:21,440
all the parameters and we simply did a

2735
01:35:18,639 --> 01:35:24,560
parameter update where every single

2736
01:35:21,440 --> 01:35:27,520
element of our parameters was nudged in

2737
01:35:24,560 --> 01:35:30,239
the opposite direction of the gradient

2738
01:35:27,520 --> 01:35:31,679
and so we're going to do the exact same

2739
01:35:30,239 --> 01:35:34,400
thing here

2740
01:35:31,679 --> 01:35:37,119
so i'm going to pull this up

2741
01:35:34,400 --> 01:35:37,119
on the side here

2742
01:35:38,480 --> 01:35:41,280
so that we have it available and we're

2743
01:35:40,080 --> 01:35:44,320
actually going to do the exact same

2744
01:35:41,280 --> 01:35:46,880
thing so this was the forward pass so

2745
01:35:44,320 --> 01:35:49,440
where we did this

2746
01:35:46,880 --> 01:35:51,199
and probs is our wipe red so now we have

2747
01:35:49,440 --> 01:35:52,880
to evaluate the loss but we're not using

2748
01:35:51,199 --> 01:35:54,480
the mean squared error we're using the

2749
01:35:52,880 --> 01:35:56,320
negative log likelihood because we are

2750
01:35:54,480 --> 01:35:58,960
doing classification we're not doing

2751
01:35:56,320 --> 01:36:02,400
regression as it's called

2752
01:35:58,960 --> 01:36:04,480
so here we want to calculate loss

2753
01:36:02,400 --> 01:36:07,119
now the way we calculate it is it's just

2754
01:36:04,480 --> 01:36:10,639
this average negative log likelihood

2755
01:36:07,119 --> 01:36:13,280
now this probs here

2756
01:36:10,639 --> 01:36:15,679
has a shape of 5 by 27

2757
01:36:13,280 --> 01:36:18,239
and so to get all the we basically want

2758
01:36:15,679 --> 01:36:20,000
to pluck out the probabilities at the

2759
01:36:18,239 --> 01:36:22,080
correct indices here

2760
01:36:20,000 --> 01:36:24,480
so in particular because the labels are

2761
01:36:22,080 --> 01:36:26,000
stored here in array wise

2762
01:36:24,480 --> 01:36:27,520
basically what we're after is for the

2763
01:36:26,000 --> 01:36:30,880
first example we're looking at

2764
01:36:27,520 --> 01:36:32,719
probability of five right at index five

2765
01:36:30,880 --> 01:36:36,159
for the second example

2766
01:36:32,719 --> 01:36:37,440
at the the second row or row index one

2767
01:36:36,159 --> 01:36:40,239
we are interested in the probability

2768
01:36:37,440 --> 01:36:43,440
assigned to index 13.

2769
01:36:40,239 --> 01:36:47,360
at the second example we also have 13.

2770
01:36:43,440 --> 01:36:49,760
at the third row we want one

2771
01:36:47,360 --> 01:36:52,320
and then the last row which is four we

2772
01:36:49,760 --> 01:36:54,000
want zero so these are the probabilities

2773
01:36:52,320 --> 01:36:55,920
we're interested in right

2774
01:36:54,000 --> 01:36:58,560
and you can see that they're not amazing

2775
01:36:55,920 --> 01:37:00,080
as we saw above

2776
01:36:58,560 --> 01:37:02,239
so these are the probabilities we want

2777
01:37:00,080 --> 01:37:04,480
but we want like a more efficient way to

2778
01:37:02,239 --> 01:37:06,239
access these probabilities

2779
01:37:04,480 --> 01:37:07,920
not just listing them out in a tuple

2780
01:37:06,239 --> 01:37:09,840
like this so it turns out that the way

2781
01:37:07,920 --> 01:37:12,880
to do this in pytorch uh one of the ways

2782
01:37:09,840 --> 01:37:15,440
at least is we can basically pass in

2783
01:37:12,880 --> 01:37:15,440
all of these

2784
01:37:16,639 --> 01:37:22,080
sorry about that all of these um

2785
01:37:19,679 --> 01:37:22,880
integers in the vectors

2786
01:37:22,080 --> 01:37:23,840
so

2787
01:37:22,880 --> 01:37:25,920
the

2788
01:37:23,840 --> 01:37:27,040
these ones you see how they're just 0 1

2789
01:37:25,920 --> 01:37:29,199
2 3 4

2790
01:37:27,040 --> 01:37:32,800
we can actually create that using mp

2791
01:37:29,199 --> 01:37:34,400
not mp sorry torch dot range of 5

2792
01:37:32,800 --> 01:37:37,040
0 1 2 3 4.

2793
01:37:34,400 --> 01:37:38,239
so we can index here with torch.range of

2794
01:37:37,040 --> 01:37:41,119
5

2795
01:37:38,239 --> 01:37:43,119
and here we index with ys

2796
01:37:41,119 --> 01:37:46,920
and you see that that gives us

2797
01:37:43,119 --> 01:37:46,920
exactly these numbers

2798
01:37:48,960 --> 01:37:53,920
so that plucks out the probabilities of

2799
01:37:51,440 --> 01:37:56,239
that the neural network assigns to the

2800
01:37:53,920 --> 01:37:58,320
correct next character

2801
01:37:56,239 --> 01:37:59,600
now we take those probabilities and we

2802
01:37:58,320 --> 01:38:03,520
don't we actually look at the log

2803
01:37:59,600 --> 01:38:05,679
probability so we want to dot log

2804
01:38:03,520 --> 01:38:07,679
and then we want to just

2805
01:38:05,679 --> 01:38:08,560
average that up so take the mean of all

2806
01:38:07,679 --> 01:38:10,320
of that

2807
01:38:08,560 --> 01:38:14,159
and then it's the negative

2808
01:38:10,320 --> 01:38:18,080
average log likelihood that is the loss

2809
01:38:14,159 --> 01:38:21,679
so the loss here is 3.7 something and

2810
01:38:18,080 --> 01:38:23,440
you see that this loss 3.76 3.76 is

2811
01:38:21,679 --> 01:38:25,280
exactly as we've obtained before but

2812
01:38:23,440 --> 01:38:26,480
this is a vectorized form of that

2813
01:38:25,280 --> 01:38:27,199
expression

2814
01:38:26,480 --> 01:38:29,520
so

2815
01:38:27,199 --> 01:38:31,360
we get the same loss

2816
01:38:29,520 --> 01:38:34,000
and the same loss we can consider

2817
01:38:31,360 --> 01:38:36,320
service part of this forward pass

2818
01:38:34,000 --> 01:38:37,920
and we've achieved here now loss

2819
01:38:36,320 --> 01:38:40,080
okay so we made our way all the way to

2820
01:38:37,920 --> 01:38:42,000
loss we've defined the forward pass

2821
01:38:40,080 --> 01:38:44,239
we forwarded the network and the loss

2822
01:38:42,000 --> 01:38:47,199
now we're ready to do the backward pass

2823
01:38:44,239 --> 01:38:47,199
so backward pass

2824
01:38:48,000 --> 01:38:52,400
we want to first make sure that all the

2825
01:38:49,520 --> 01:38:55,360
gradients are reset so they're at zero

2826
01:38:52,400 --> 01:38:57,199
now in pytorch you can set the gradients

2827
01:38:55,360 --> 01:38:59,360
to be zero but you can also just set it

2828
01:38:57,199 --> 01:39:01,600
to none and setting it to none is more

2829
01:38:59,360 --> 01:39:04,320
efficient and pi torch will interpret

2830
01:39:01,600 --> 01:39:05,760
none as like a lack of a gradient and is

2831
01:39:04,320 --> 01:39:07,920
the same as zeros

2832
01:39:05,760 --> 01:39:10,320
so this is a way to set to zero the

2833
01:39:07,920 --> 01:39:10,320
gradient

2834
01:39:10,400 --> 01:39:13,840
and now we do lost it backward

2835
01:39:14,639 --> 01:39:17,600
before we do lost that backward we need

2836
01:39:16,159 --> 01:39:18,800
one more thing if you remember from

2837
01:39:17,600 --> 01:39:21,280
micrograd

2838
01:39:18,800 --> 01:39:25,119
pytorch actually requires

2839
01:39:21,280 --> 01:39:26,960
that we pass in requires grad is true

2840
01:39:25,119 --> 01:39:28,480
so that when we tell

2841
01:39:26,960 --> 01:39:30,719
pythorge that we are interested in

2842
01:39:28,480 --> 01:39:33,440
calculating gradients for this leaf

2843
01:39:30,719 --> 01:39:35,840
tensor by default this is false

2844
01:39:33,440 --> 01:39:37,920
so let me recalculate with that

2845
01:39:35,840 --> 01:39:40,639
and then set to none and lost that

2846
01:39:37,920 --> 01:39:42,320
backward

2847
01:39:40,639 --> 01:39:44,400
now something magical happened when

2848
01:39:42,320 --> 01:39:47,199
lasted backward was run

2849
01:39:44,400 --> 01:39:49,040
because pytorch just like micrograd when

2850
01:39:47,199 --> 01:39:51,440
we did the forward pass here

2851
01:39:49,040 --> 01:39:53,199
it keeps track of all the operations

2852
01:39:51,440 --> 01:39:55,600
under the hood it builds a full

2853
01:39:53,199 --> 01:39:56,480
computational graph just like the graphs

2854
01:39:55,600 --> 01:39:58,800
we've

2855
01:39:56,480 --> 01:40:00,800
produced in micrograd those graphs exist

2856
01:39:58,800 --> 01:40:02,719
inside pi torch

2857
01:40:00,800 --> 01:40:04,000
and so it knows all the dependencies and

2858
01:40:02,719 --> 01:40:04,960
all the mathematical operations of

2859
01:40:04,000 --> 01:40:07,040
everything

2860
01:40:04,960 --> 01:40:09,520
and when you then calculate the loss

2861
01:40:07,040 --> 01:40:11,679
we can call a dot backward on it

2862
01:40:09,520 --> 01:40:13,600
and that backward then fills in the

2863
01:40:11,679 --> 01:40:15,199
gradients of

2864
01:40:13,600 --> 01:40:18,320
all the intermediates

2865
01:40:15,199 --> 01:40:20,400
all the way back to w's which are the

2866
01:40:18,320 --> 01:40:23,119
parameters of our neural net so now we

2867
01:40:20,400 --> 01:40:26,800
can do w grad and we see that it has

2868
01:40:23,119 --> 01:40:26,800
structure there's stuff inside it

2869
01:40:29,040 --> 01:40:33,360
and these gradients

2870
01:40:30,800 --> 01:40:36,800
every single element here

2871
01:40:33,360 --> 01:40:40,639
so w dot shape is 27 by 27

2872
01:40:36,800 --> 01:40:43,040
w grad shape is the same 27 by 27

2873
01:40:40,639 --> 01:40:44,400
and every element of w that grad

2874
01:40:43,040 --> 01:40:47,600
is telling us

2875
01:40:44,400 --> 01:40:48,719
the influence of that weight on the loss

2876
01:40:47,600 --> 01:40:50,800
function

2877
01:40:48,719 --> 01:40:51,920
so for example this number all the way

2878
01:40:50,800 --> 01:40:54,320
here

2879
01:40:51,920 --> 01:40:55,520
if this element the zero zero element of

2880
01:40:54,320 --> 01:40:57,600
w

2881
01:40:55,520 --> 01:41:00,000
because the gradient is positive is

2882
01:40:57,600 --> 01:41:03,040
telling us that this has a positive

2883
01:41:00,000 --> 01:41:04,400
influence in the loss slightly nudging

2884
01:41:03,040 --> 01:41:06,960
w

2885
01:41:04,400 --> 01:41:07,760
slightly taking w 0 0

2886
01:41:06,960 --> 01:41:10,400
and

2887
01:41:07,760 --> 01:41:12,400
adding a small h to it

2888
01:41:10,400 --> 01:41:15,679
would increase the loss

2889
01:41:12,400 --> 01:41:16,719
mildly because this gradient is positive

2890
01:41:15,679 --> 01:41:18,560
some of these gradients are also

2891
01:41:16,719 --> 01:41:20,239
negative

2892
01:41:18,560 --> 01:41:22,400
so that's telling us about the gradient

2893
01:41:20,239 --> 01:41:25,360
information and we can use this gradient

2894
01:41:22,400 --> 01:41:27,440
information to update the weights of

2895
01:41:25,360 --> 01:41:29,360
this neural network so let's now do the

2896
01:41:27,440 --> 01:41:32,239
update it's going to be very similar to

2897
01:41:29,360 --> 01:41:33,760
what we had in micrograd we need no loop

2898
01:41:32,239 --> 01:41:36,080
over all the parameters because we only

2899
01:41:33,760 --> 01:41:37,040
have one parameter uh tensor and that is

2900
01:41:36,080 --> 01:41:40,960
w

2901
01:41:37,040 --> 01:41:42,159
so we simply do w dot data plus equals

2902
01:41:40,960 --> 01:41:43,840
uh the

2903
01:41:42,159 --> 01:41:48,800
we can actually copy this almost exactly

2904
01:41:43,840 --> 01:41:48,800
negative 0.1 times w dot grad

2905
01:41:49,360 --> 01:41:54,480
and that would be the update to the

2906
01:41:52,239 --> 01:41:55,920
tensor

2907
01:41:54,480 --> 01:41:58,560
so that updates

2908
01:41:55,920 --> 01:41:58,560
the tensor

2909
01:41:58,639 --> 01:42:01,119
and

2910
01:41:59,520 --> 01:42:04,320
because the tensor is updated we would

2911
01:42:01,119 --> 01:42:05,360
expect that now the loss should decrease

2912
01:42:04,320 --> 01:42:09,360
so

2913
01:42:05,360 --> 01:42:09,360
here if i print loss

2914
01:42:09,440 --> 01:42:13,119
that item

2915
01:42:11,119 --> 01:42:16,480
it was 3.76 right

2916
01:42:13,119 --> 01:42:18,960
so we've updated the w here so if i

2917
01:42:16,480 --> 01:42:21,600
recalculate forward pass

2918
01:42:18,960 --> 01:42:23,520
loss now should be slightly lower so

2919
01:42:21,600 --> 01:42:25,679
3.76 goes to

2920
01:42:23,520 --> 01:42:26,719
3.74

2921
01:42:25,679 --> 01:42:29,600
and then

2922
01:42:26,719 --> 01:42:30,800
we can again set to set grad to none and

2923
01:42:29,600 --> 01:42:32,480
backward

2924
01:42:30,800 --> 01:42:34,800
update

2925
01:42:32,480 --> 01:42:37,199
and now the parameters changed again

2926
01:42:34,800 --> 01:42:41,560
so if we recalculate the forward pass we

2927
01:42:37,199 --> 01:42:41,560
expect a lower loss again 3.72

2928
01:42:42,159 --> 01:42:47,600
okay and this is again doing the we're

2929
01:42:44,480 --> 01:42:47,600
now doing gradient descent

2930
01:42:48,480 --> 01:42:52,639
and when we achieve a low loss that will

2931
01:42:50,639 --> 01:42:54,400
mean that the network is assigning high

2932
01:42:52,639 --> 01:42:56,080
probabilities to the correctness

2933
01:42:54,400 --> 01:42:57,920
characters okay so i rearranged

2934
01:42:56,080 --> 01:42:59,440
everything and i put it all together

2935
01:42:57,920 --> 01:43:01,360
from scratch

2936
01:42:59,440 --> 01:43:03,199
so here is where we construct our data

2937
01:43:01,360 --> 01:43:04,719
set of bigrams

2938
01:43:03,199 --> 01:43:06,880
you see that we are still iterating only

2939
01:43:04,719 --> 01:43:09,280
on the first word emma

2940
01:43:06,880 --> 01:43:11,360
i'm going to change that in a second i

2941
01:43:09,280 --> 01:43:14,560
added a number that counts the number of

2942
01:43:11,360 --> 01:43:16,880
elements in x's so that we explicitly

2943
01:43:14,560 --> 01:43:18,000
see that number of examples is five

2944
01:43:16,880 --> 01:43:19,440
because currently we're just working

2945
01:43:18,000 --> 01:43:20,560
with emma and there's five backgrounds

2946
01:43:19,440 --> 01:43:22,560
there

2947
01:43:20,560 --> 01:43:25,280
and here i added a loop of exactly what

2948
01:43:22,560 --> 01:43:27,440
we had before so we had 10 iterations of

2949
01:43:25,280 --> 01:43:28,880
grainy descent of forward pass backward

2950
01:43:27,440 --> 01:43:30,320
pass and an update

2951
01:43:28,880 --> 01:43:32,800
and so running these two cells

2952
01:43:30,320 --> 01:43:35,360
initialization and gradient descent

2953
01:43:32,800 --> 01:43:36,159
gives us some improvement

2954
01:43:35,360 --> 01:43:38,159
on

2955
01:43:36,159 --> 01:43:41,679
the loss function

2956
01:43:38,159 --> 01:43:45,360
but now i want to use all the words

2957
01:43:41,679 --> 01:43:46,639
and there's not 5 but 228 000 bigrams

2958
01:43:45,360 --> 01:43:48,239
now

2959
01:43:46,639 --> 01:43:49,760
however this should require no

2960
01:43:48,239 --> 01:43:51,679
modification whatsoever everything

2961
01:43:49,760 --> 01:43:53,040
should just run because all the code we

2962
01:43:51,679 --> 01:43:56,159
wrote doesn't care if there's five

2963
01:43:53,040 --> 01:43:58,400
migrants or 228 000 bigrams and with

2964
01:43:56,159 --> 01:44:00,320
everything we should just work so

2965
01:43:58,400 --> 01:44:01,679
you see that this will just run

2966
01:44:00,320 --> 01:44:04,639
but now we are optimizing over the

2967
01:44:01,679 --> 01:44:06,480
entire training set of all the bigrams

2968
01:44:04,639 --> 01:44:08,159
and you see now that we are decreasing

2969
01:44:06,480 --> 01:44:11,920
very slightly so actually we can

2970
01:44:08,159 --> 01:44:11,920
probably afford a larger learning rate

2971
01:44:12,320 --> 01:44:17,000
and probably for even larger learning

2972
01:44:14,000 --> 01:44:17,000
rate

2973
01:44:20,639 --> 01:44:24,320
even 50 seems to work on this very very

2974
01:44:22,560 --> 01:44:26,880
simple example right so let me

2975
01:44:24,320 --> 01:44:29,280
re-initialize and let's run 100

2976
01:44:26,880 --> 01:44:31,920
iterations

2977
01:44:29,280 --> 01:44:31,920
see what happens

2978
01:44:32,880 --> 01:44:35,840
okay

2979
01:44:36,239 --> 01:44:40,480
we seem to be

2980
01:44:39,040 --> 01:44:42,719
coming up to some pretty good losses

2981
01:44:40,480 --> 01:44:44,480
here 2.47

2982
01:44:42,719 --> 01:44:46,080
let me run 100 more

2983
01:44:44,480 --> 01:44:48,000
what is the number that we expect by the

2984
01:44:46,080 --> 01:44:50,239
way in the loss we expect to get

2985
01:44:48,000 --> 01:44:52,000
something around what we had originally

2986
01:44:50,239 --> 01:44:53,520
actually

2987
01:44:52,000 --> 01:44:55,840
so all the way back if you remember in

2988
01:44:53,520 --> 01:44:58,880
the beginning of this video when we

2989
01:44:55,840 --> 01:45:01,520
optimized uh just by counting

2990
01:44:58,880 --> 01:45:03,440
our loss was roughly 2.47

2991
01:45:01,520 --> 01:45:06,560
after we had it smoothing

2992
01:45:03,440 --> 01:45:08,320
but before smoothing we had roughly 2.45

2993
01:45:06,560 --> 01:45:09,679
likelihood

2994
01:45:08,320 --> 01:45:10,960
sorry loss

2995
01:45:09,679 --> 01:45:13,760
and so that's actually roughly the

2996
01:45:10,960 --> 01:45:15,840
vicinity of what we expect to achieve

2997
01:45:13,760 --> 01:45:17,840
but before we achieved it by counting

2998
01:45:15,840 --> 01:45:19,679
and here we are achieving the roughly

2999
01:45:17,840 --> 01:45:20,960
the same result but with gradient based

3000
01:45:19,679 --> 01:45:23,920
optimization

3001
01:45:20,960 --> 01:45:26,239
so we come to about 2.4

3002
01:45:23,920 --> 01:45:27,119
6 2.45 etc

3003
01:45:26,239 --> 01:45:28,719
and that makes sense because

3004
01:45:27,119 --> 01:45:30,480
fundamentally we're not taking any

3005
01:45:28,719 --> 01:45:31,920
additional information we're still just

3006
01:45:30,480 --> 01:45:33,840
taking in the previous character and

3007
01:45:31,920 --> 01:45:35,360
trying to predict the next one but

3008
01:45:33,840 --> 01:45:38,159
instead of doing it explicitly by

3009
01:45:35,360 --> 01:45:39,360
counting and normalizing

3010
01:45:38,159 --> 01:45:41,440
we are doing it with gradient-based

3011
01:45:39,360 --> 01:45:44,000
learning and it just so happens that the

3012
01:45:41,440 --> 01:45:46,320
explicit approach happens to very well

3013
01:45:44,000 --> 01:45:48,480
optimize the loss function without any

3014
01:45:46,320 --> 01:45:50,080
need for a gradient based optimization

3015
01:45:48,480 --> 01:45:52,000
because the setup for bigram language

3016
01:45:50,080 --> 01:45:54,320
models are is so straightforward that's

3017
01:45:52,000 --> 01:45:56,000
so simple we can just afford to estimate

3018
01:45:54,320 --> 01:45:57,119
those probabilities directly and

3019
01:45:56,000 --> 01:45:58,800
maintain them

3020
01:45:57,119 --> 01:46:00,800
in a table

3021
01:45:58,800 --> 01:46:02,880
but the gradient-based approach is

3022
01:46:00,800 --> 01:46:04,800
significantly more flexible

3023
01:46:02,880 --> 01:46:06,560
so we've actually gained a lot

3024
01:46:04,800 --> 01:46:09,119
because

3025
01:46:06,560 --> 01:46:11,040
what we can do now is

3026
01:46:09,119 --> 01:46:13,280
we can expand this approach and

3027
01:46:11,040 --> 01:46:14,719
complexify the neural net so currently

3028
01:46:13,280 --> 01:46:16,320
we're just taking a single character and

3029
01:46:14,719 --> 01:46:18,320
feeding into a neural net and the neural

3030
01:46:16,320 --> 01:46:20,560
that's extremely simple but we're about

3031
01:46:18,320 --> 01:46:22,480
to iterate on this substantially we're

3032
01:46:20,560 --> 01:46:24,560
going to be taking multiple previous

3033
01:46:22,480 --> 01:46:26,080
characters and we're going to be feeding

3034
01:46:24,560 --> 01:46:28,560
feeding them into increasingly more

3035
01:46:26,080 --> 01:46:30,159
complex neural nets but fundamentally

3036
01:46:28,560 --> 01:46:32,560
out the output of the neural net will

3037
01:46:30,159 --> 01:46:34,000
always just be logics

3038
01:46:32,560 --> 01:46:36,000
and those logits will go through the

3039
01:46:34,000 --> 01:46:37,840
exact same transformation we are going

3040
01:46:36,000 --> 01:46:39,520
to take them through a soft max

3041
01:46:37,840 --> 01:46:42,080
calculate the loss function and the

3042
01:46:39,520 --> 01:46:44,880
negative log likelihood and do gradient

3043
01:46:42,080 --> 01:46:47,440
based optimization and so actually

3044
01:46:44,880 --> 01:46:49,760
as we complexify the neural nets and

3045
01:46:47,440 --> 01:46:51,440
work all the way up to transformers

3046
01:46:49,760 --> 01:46:52,960
none of this will really fundamentally

3047
01:46:51,440 --> 01:46:54,639
change none of this will fundamentally

3048
01:46:52,960 --> 01:46:55,600
change the only thing that will change

3049
01:46:54,639 --> 01:46:57,679
is

3050
01:46:55,600 --> 01:46:59,119
the way we do the forward pass where we

3051
01:46:57,679 --> 01:47:01,679
take in some previous characters and

3052
01:46:59,119 --> 01:47:03,920
calculate logits for the next character

3053
01:47:01,679 --> 01:47:05,119
in the sequence that will become more

3054
01:47:03,920 --> 01:47:07,360
complex

3055
01:47:05,119 --> 01:47:08,800
and uh but we'll use the same machinery

3056
01:47:07,360 --> 01:47:10,639
to optimize it

3057
01:47:08,800 --> 01:47:12,080
and um

3058
01:47:10,639 --> 01:47:13,119
it's not obvious how we would have

3059
01:47:12,080 --> 01:47:14,880
extended

3060
01:47:13,119 --> 01:47:17,360
this bigram approach

3061
01:47:14,880 --> 01:47:19,360
into the case where there are many more

3062
01:47:17,360 --> 01:47:21,360
characters at the input because

3063
01:47:19,360 --> 01:47:23,119
eventually these tables would get way

3064
01:47:21,360 --> 01:47:26,159
too large because there's way too many

3065
01:47:23,119 --> 01:47:27,760
combinations of what previous characters

3066
01:47:26,159 --> 01:47:29,600
could be

3067
01:47:27,760 --> 01:47:31,119
if you only have one previous character

3068
01:47:29,600 --> 01:47:33,600
we can just keep everything in a table

3069
01:47:31,119 --> 01:47:35,440
that counts but if you have the last 10

3070
01:47:33,600 --> 01:47:36,719
characters that are input we can't

3071
01:47:35,440 --> 01:47:38,480
actually keep everything in the table

3072
01:47:36,719 --> 01:47:40,239
anymore so this is fundamentally an

3073
01:47:38,480 --> 01:47:42,080
unscalable approach and the neural

3074
01:47:40,239 --> 01:47:44,000
network approach is significantly more

3075
01:47:42,080 --> 01:47:46,800
scalable and it's something that

3076
01:47:44,000 --> 01:47:48,560
actually we can improve on over time so

3077
01:47:46,800 --> 01:47:51,040
that's where we will be digging next i

3078
01:47:48,560 --> 01:47:52,239
wanted to point out two more things

3079
01:47:51,040 --> 01:47:54,000
number one

3080
01:47:52,239 --> 01:47:55,040
i want you to notice that

3081
01:47:54,000 --> 01:47:56,639
this

3082
01:47:55,040 --> 01:47:59,040
x ink here

3083
01:47:56,639 --> 01:48:00,400
this is made up of one hot vectors and

3084
01:47:59,040 --> 01:48:03,119
then those one hot vectors are

3085
01:48:00,400 --> 01:48:05,760
multiplied by this w matrix

3086
01:48:03,119 --> 01:48:07,440
and we think of this as multiple neurons

3087
01:48:05,760 --> 01:48:08,639
being forwarded in a fully connected

3088
01:48:07,440 --> 01:48:10,000
manner

3089
01:48:08,639 --> 01:48:11,920
but actually what's happening here is

3090
01:48:10,000 --> 01:48:14,320
that for example

3091
01:48:11,920 --> 01:48:17,679
if you have a one hot vector here that

3092
01:48:14,320 --> 01:48:19,119
has a one at say the fifth dimension

3093
01:48:17,679 --> 01:48:21,119
then because of the way the matrix

3094
01:48:19,119 --> 01:48:23,440
multiplication works

3095
01:48:21,119 --> 01:48:25,600
multiplying that one-half vector with w

3096
01:48:23,440 --> 01:48:27,440
actually ends up plucking out the fifth

3097
01:48:25,600 --> 01:48:29,520
row of w

3098
01:48:27,440 --> 01:48:31,199
log logits would become just the fifth

3099
01:48:29,520 --> 01:48:32,960
row of w

3100
01:48:31,199 --> 01:48:35,199
and that's because of the way the matrix

3101
01:48:32,960 --> 01:48:36,880
multiplication works

3102
01:48:35,199 --> 01:48:37,760
um

3103
01:48:36,880 --> 01:48:40,000
so

3104
01:48:37,760 --> 01:48:42,000
that's actually what ends up happening

3105
01:48:40,000 --> 01:48:43,199
so but that's actually exactly what

3106
01:48:42,000 --> 01:48:46,639
happened before

3107
01:48:43,199 --> 01:48:48,239
because remember all the way up here

3108
01:48:46,639 --> 01:48:50,320
we have a bigram we took the first

3109
01:48:48,239 --> 01:48:54,960
character and then that first character

3110
01:48:50,320 --> 01:48:56,560
indexed into a row of this array here

3111
01:48:54,960 --> 01:48:58,719
and that row gave us the probability

3112
01:48:56,560 --> 01:49:01,119
distribution for the next character so

3113
01:48:58,719 --> 01:49:03,600
the first character was used as a lookup

3114
01:49:01,119 --> 01:49:05,119
into a

3115
01:49:03,600 --> 01:49:06,239
matrix here to get the probability

3116
01:49:05,119 --> 01:49:07,440
distribution

3117
01:49:06,239 --> 01:49:09,600
well that's actually exactly what's

3118
01:49:07,440 --> 01:49:11,840
happening here because we're taking the

3119
01:49:09,600 --> 01:49:13,440
index we're encoding it as one hot and

3120
01:49:11,840 --> 01:49:15,679
multiplying it by w

3121
01:49:13,440 --> 01:49:17,840
so logics literally becomes

3122
01:49:15,679 --> 01:49:17,840
the

3123
01:49:18,000 --> 01:49:22,560
the appropriate row of w

3124
01:49:20,800 --> 01:49:24,880
and that gets just as before

3125
01:49:22,560 --> 01:49:26,239
exponentiated to create the counts

3126
01:49:24,880 --> 01:49:27,440
and then normalized and becomes

3127
01:49:26,239 --> 01:49:29,440
probability

3128
01:49:27,440 --> 01:49:31,360
so this w here

3129
01:49:29,440 --> 01:49:35,040
is literally

3130
01:49:31,360 --> 01:49:38,239
the same as this array here

3131
01:49:35,040 --> 01:49:40,639
but w remember is the log counts not the

3132
01:49:38,239 --> 01:49:42,800
counts so it's more precise to say that

3133
01:49:40,639 --> 01:49:46,159
w exponentiated

3134
01:49:42,800 --> 01:49:49,280
w dot x is this array

3135
01:49:46,159 --> 01:49:50,400
but this array was filled in by counting

3136
01:49:49,280 --> 01:49:51,920
and by

3137
01:49:50,400 --> 01:49:53,840
basically

3138
01:49:51,920 --> 01:49:55,840
populating the counts of bi-grams

3139
01:49:53,840 --> 01:49:57,920
whereas in the gradient-based framework

3140
01:49:55,840 --> 01:49:59,360
we initialize it randomly and then we

3141
01:49:57,920 --> 01:50:00,480
let the loss

3142
01:49:59,360 --> 01:50:03,199
guide us

3143
01:50:00,480 --> 01:50:05,760
to arrive at the exact same array

3144
01:50:03,199 --> 01:50:06,719
so this array exactly here

3145
01:50:05,760 --> 01:50:09,119
is

3146
01:50:06,719 --> 01:50:12,239
basically the array w at the end of

3147
01:50:09,119 --> 01:50:14,960
optimization except we arrived at it

3148
01:50:12,239 --> 01:50:16,639
piece by piece by following the loss

3149
01:50:14,960 --> 01:50:18,560
and that's why we also obtain the same

3150
01:50:16,639 --> 01:50:20,480
loss function at the end and the second

3151
01:50:18,560 --> 01:50:22,560
note is if i come here

3152
01:50:20,480 --> 01:50:24,880
remember the smoothing where we added

3153
01:50:22,560 --> 01:50:26,000
fake counts to our counts

3154
01:50:24,880 --> 01:50:28,480
in order to

3155
01:50:26,000 --> 01:50:30,960
smooth out and make more uniform the

3156
01:50:28,480 --> 01:50:32,480
distributions of these probabilities

3157
01:50:30,960 --> 01:50:34,960
and that prevented us from assigning

3158
01:50:32,480 --> 01:50:37,119
zero probability to

3159
01:50:34,960 --> 01:50:40,239
to any one bigram

3160
01:50:37,119 --> 01:50:42,800
now if i increase the count here

3161
01:50:40,239 --> 01:50:45,280
what's happening to the probability

3162
01:50:42,800 --> 01:50:47,920
as i increase the count probability

3163
01:50:45,280 --> 01:50:50,320
becomes more and more uniform

3164
01:50:47,920 --> 01:50:52,560
right because these counts go only up to

3165
01:50:50,320 --> 01:50:54,320
like 900 or whatever so if i'm adding

3166
01:50:52,560 --> 01:50:56,400
plus a million to every single number

3167
01:50:54,320 --> 01:50:58,400
here you can see how

3168
01:50:56,400 --> 01:51:00,159
the row and its probability then when we

3169
01:50:58,400 --> 01:51:02,960
divide is just going to become more and

3170
01:51:00,159 --> 01:51:05,119
more close to exactly even probability

3171
01:51:02,960 --> 01:51:06,560
uniform distribution

3172
01:51:05,119 --> 01:51:10,800
it turns out that the gradient based

3173
01:51:06,560 --> 01:51:13,119
framework has an equivalent to smoothing

3174
01:51:10,800 --> 01:51:15,840
in particular

3175
01:51:13,119 --> 01:51:18,480
think through these w's here

3176
01:51:15,840 --> 01:51:20,080
which we initialized randomly

3177
01:51:18,480 --> 01:51:22,080
we could also think about initializing

3178
01:51:20,080 --> 01:51:25,920
w's to be zero

3179
01:51:22,080 --> 01:51:27,520
if all the entries of w are zero

3180
01:51:25,920 --> 01:51:28,719
then you'll see that logits will become

3181
01:51:27,520 --> 01:51:30,159
all zero

3182
01:51:28,719 --> 01:51:32,080
and then exponentiating those logics

3183
01:51:30,159 --> 01:51:33,840
becomes all one

3184
01:51:32,080 --> 01:51:35,679
and then the probabilities turned out to

3185
01:51:33,840 --> 01:51:38,000
be exactly uniform

3186
01:51:35,679 --> 01:51:41,199
so basically when w's are all equal to

3187
01:51:38,000 --> 01:51:42,639
each other or say especially zero

3188
01:51:41,199 --> 01:51:44,400
then the probabilities come out

3189
01:51:42,639 --> 01:51:45,440
completely uniform

3190
01:51:44,400 --> 01:51:49,199
so

3191
01:51:45,440 --> 01:51:51,840
trying to incentivize w to be near zero

3192
01:51:49,199 --> 01:51:53,440
is basically equivalent to

3193
01:51:51,840 --> 01:51:55,440
label smoothing and the more you

3194
01:51:53,440 --> 01:51:57,360
incentivize that in the loss function

3195
01:51:55,440 --> 01:51:58,880
the more smooth distribution you're

3196
01:51:57,360 --> 01:52:00,080
going to achieve

3197
01:51:58,880 --> 01:52:00,840
so this brings us to something that's

3198
01:52:00,080 --> 01:52:02,880
called

3199
01:52:00,840 --> 01:52:04,960
regularization where we can actually

3200
01:52:02,880 --> 01:52:06,480
augment the loss function to have a

3201
01:52:04,960 --> 01:52:08,960
small component that we call a

3202
01:52:06,480 --> 01:52:10,159
regularization loss

3203
01:52:08,960 --> 01:52:12,400
in particular what we're going to do is

3204
01:52:10,159 --> 01:52:14,560
we can take w and we can for example

3205
01:52:12,400 --> 01:52:17,760
square all of its entries

3206
01:52:14,560 --> 01:52:18,960
and then we can um whoops

3207
01:52:17,760 --> 01:52:20,800
sorry about that

3208
01:52:18,960 --> 01:52:23,360
we can take all the entries of w and we

3209
01:52:20,800 --> 01:52:23,360
can sum them

3210
01:52:23,520 --> 01:52:28,239
and because we're squaring uh there will

3211
01:52:25,760 --> 01:52:29,840
be no signs anymore um

3212
01:52:28,239 --> 01:52:31,440
negatives and positives all get squashed

3213
01:52:29,840 --> 01:52:33,760
to be positive numbers

3214
01:52:31,440 --> 01:52:36,239
and then the way this works is you

3215
01:52:33,760 --> 01:52:39,440
achieve zero loss if w is exactly or

3216
01:52:36,239 --> 01:52:41,119
zero but if w has non-zero numbers you

3217
01:52:39,440 --> 01:52:42,800
accumulate loss

3218
01:52:41,119 --> 01:52:44,880
and so we can actually take this and we

3219
01:52:42,800 --> 01:52:48,800
can add it on here

3220
01:52:44,880 --> 01:52:50,400
so we can do something like loss plus

3221
01:52:48,800 --> 01:52:51,920
w square

3222
01:52:50,400 --> 01:52:53,599
dot sum

3223
01:52:51,920 --> 01:52:55,440
or let's actually instead of sum let's

3224
01:52:53,599 --> 01:52:57,440
take a mean because otherwise the sum

3225
01:52:55,440 --> 01:52:58,880
gets too large

3226
01:52:57,440 --> 01:53:01,280
so mean is like a little bit more

3227
01:52:58,880 --> 01:53:02,960
manageable

3228
01:53:01,280 --> 01:53:05,520
and then we have a regularization loss

3229
01:53:02,960 --> 01:53:06,719
here say 0.01 times

3230
01:53:05,520 --> 01:53:09,280
or something like that you can choose

3231
01:53:06,719 --> 01:53:12,239
the regularization strength

3232
01:53:09,280 --> 01:53:14,400
and then we can just optimize this and

3233
01:53:12,239 --> 01:53:16,400
now this optimization actually has two

3234
01:53:14,400 --> 01:53:18,560
components not only is it trying to make

3235
01:53:16,400 --> 01:53:19,840
all the probabilities work out but in

3236
01:53:18,560 --> 01:53:22,080
addition to that there's an additional

3237
01:53:19,840 --> 01:53:24,719
component that simultaneously tries to

3238
01:53:22,080 --> 01:53:26,480
make all w's be zero because if w's are

3239
01:53:24,719 --> 01:53:28,480
non-zero you feel a loss and so

3240
01:53:26,480 --> 01:53:30,639
minimizing this the only way to achieve

3241
01:53:28,480 --> 01:53:32,480
that is for w to be zero

3242
01:53:30,639 --> 01:53:34,639
and so you can think of this as adding

3243
01:53:32,480 --> 01:53:38,320
like a spring force or like a gravity

3244
01:53:34,639 --> 01:53:40,000
force that that pushes w to be zero so w

3245
01:53:38,320 --> 01:53:42,000
wants to be zero and the probabilities

3246
01:53:40,000 --> 01:53:44,239
want to be uniform but they also

3247
01:53:42,000 --> 01:53:46,320
simultaneously want to match up your

3248
01:53:44,239 --> 01:53:47,360
your probabilities as indicated by the

3249
01:53:46,320 --> 01:53:49,199
data

3250
01:53:47,360 --> 01:53:52,639
and so the strength of this

3251
01:53:49,199 --> 01:53:54,400
regularization is exactly controlling

3252
01:53:52,639 --> 01:53:57,199
the amount of counts

3253
01:53:54,400 --> 01:53:59,360
that you add here

3254
01:53:57,199 --> 01:54:00,639
adding a lot more counts

3255
01:53:59,360 --> 01:54:02,800
here

3256
01:54:00,639 --> 01:54:04,639
corresponds to

3257
01:54:02,800 --> 01:54:06,480
increasing this number

3258
01:54:04,639 --> 01:54:08,000
because the more you increase it the

3259
01:54:06,480 --> 01:54:10,880
more this part of the loss function

3260
01:54:08,000 --> 01:54:13,599
dominates this part and the more these

3261
01:54:10,880 --> 01:54:15,440
these weights will un be unable to grow

3262
01:54:13,599 --> 01:54:18,320
because as they grow

3263
01:54:15,440 --> 01:54:21,199
they uh accumulate way too much loss

3264
01:54:18,320 --> 01:54:23,440
and so if this is strong enough

3265
01:54:21,199 --> 01:54:26,719
then we are not able to overcome the

3266
01:54:23,440 --> 01:54:28,080
force of this loss and we will never

3267
01:54:26,719 --> 01:54:29,280
and basically everything will be uniform

3268
01:54:28,080 --> 01:54:30,960
predictions

3269
01:54:29,280 --> 01:54:33,040
so i thought that's kind of cool okay

3270
01:54:30,960 --> 01:54:34,239
and lastly before we wrap up

3271
01:54:33,040 --> 01:54:36,800
i wanted to show you how you would

3272
01:54:34,239 --> 01:54:39,280
sample from this neural net model

3273
01:54:36,800 --> 01:54:40,719
and i copy-pasted the sampling code from

3274
01:54:39,280 --> 01:54:43,280
before

3275
01:54:40,719 --> 01:54:44,239
where remember that we sampled five

3276
01:54:43,280 --> 01:54:46,400
times

3277
01:54:44,239 --> 01:54:50,480
and all we did we start at zero we

3278
01:54:46,400 --> 01:54:52,320
grabbed the current ix row of p and that

3279
01:54:50,480 --> 01:54:54,880
was our probability row

3280
01:54:52,320 --> 01:54:56,719
from which we sampled the next index and

3281
01:54:54,880 --> 01:54:58,880
just accumulated that and

3282
01:54:56,719 --> 01:55:00,320
break when zero

3283
01:54:58,880 --> 01:55:02,840
and running this

3284
01:55:00,320 --> 01:55:05,520
gave us these

3285
01:55:02,840 --> 01:55:07,599
results still have the

3286
01:55:05,520 --> 01:55:09,119
p in memory so this is fine

3287
01:55:07,599 --> 01:55:11,840
now

3288
01:55:09,119 --> 01:55:14,880
the speed doesn't come from the row of b

3289
01:55:11,840 --> 01:55:17,040
instead it comes from this neural net

3290
01:55:14,880 --> 01:55:21,040
first we take ix

3291
01:55:17,040 --> 01:55:22,400
and we encode it into a one hot row of x

3292
01:55:21,040 --> 01:55:25,119
inc

3293
01:55:22,400 --> 01:55:26,880
this x inc multiplies rw

3294
01:55:25,119 --> 01:55:29,360
which really just plucks out the row of

3295
01:55:26,880 --> 01:55:30,400
w corresponding to ix really that's

3296
01:55:29,360 --> 01:55:33,119
what's happening

3297
01:55:30,400 --> 01:55:34,719
and that gets our logits and then we

3298
01:55:33,119 --> 01:55:36,560
normalize those low jets

3299
01:55:34,719 --> 01:55:39,040
exponentiate to get counts and then

3300
01:55:36,560 --> 01:55:41,280
normalize to get uh the distribution and

3301
01:55:39,040 --> 01:55:44,080
then we can sample from the distribution

3302
01:55:41,280 --> 01:55:44,080
so if i run this

3303
01:55:45,119 --> 01:55:48,880
kind of anticlimactic or climatic

3304
01:55:47,199 --> 01:55:50,880
depending how you look at it but we get

3305
01:55:48,880 --> 01:55:52,239
the exact same result

3306
01:55:50,880 --> 01:55:53,599
um

3307
01:55:52,239 --> 01:55:55,679
and that's because this is in the

3308
01:55:53,599 --> 01:55:56,960
identical model not only does it achieve

3309
01:55:55,679 --> 01:55:58,080
the same loss

3310
01:55:56,960 --> 01:55:59,280
but

3311
01:55:58,080 --> 01:56:02,480
as i mentioned these are identical

3312
01:55:59,280 --> 01:56:05,119
models and this w is the log counts of

3313
01:56:02,480 --> 01:56:07,360
what we've estimated before but we came

3314
01:56:05,119 --> 01:56:08,320
to this answer in a very different way

3315
01:56:07,360 --> 01:56:10,320
and it's got a very different

3316
01:56:08,320 --> 01:56:11,679
interpretation but fundamentally this is

3317
01:56:10,320 --> 01:56:14,800
basically the same model and gives the

3318
01:56:11,679 --> 01:56:16,159
same samples here and so

3319
01:56:14,800 --> 01:56:18,159
that's kind of cool okay so we've

3320
01:56:16,159 --> 01:56:20,480
actually covered a lot of ground we

3321
01:56:18,159 --> 01:56:21,920
introduced the bigram character level

3322
01:56:20,480 --> 01:56:24,080
language model

3323
01:56:21,920 --> 01:56:25,920
we saw how we can train the model how we

3324
01:56:24,080 --> 01:56:28,080
can sample from the model and how we can

3325
01:56:25,920 --> 01:56:30,159
evaluate the quality of the model using

3326
01:56:28,080 --> 01:56:31,599
the negative log likelihood loss

3327
01:56:30,159 --> 01:56:33,440
and then we actually trained the model

3328
01:56:31,599 --> 01:56:34,960
in two completely different ways that

3329
01:56:33,440 --> 01:56:36,239
actually get the same result and the

3330
01:56:34,960 --> 01:56:38,639
same model

3331
01:56:36,239 --> 01:56:40,159
in the first way we just counted up the

3332
01:56:38,639 --> 01:56:41,440
frequency of all the bigrams and

3333
01:56:40,159 --> 01:56:44,560
normalized

3334
01:56:41,440 --> 01:56:47,679
in a second way we used the

3335
01:56:44,560 --> 01:56:50,800
negative log likelihood loss as a guide

3336
01:56:47,679 --> 01:56:52,719
to optimizing the counts matrix

3337
01:56:50,800 --> 01:56:54,960
or the counts array so that the loss is

3338
01:56:52,719 --> 01:56:56,800
minimized in the in a gradient-based

3339
01:56:54,960 --> 01:56:58,400
framework and we saw that both of them

3340
01:56:56,800 --> 01:57:00,320
give the same result

3341
01:56:58,400 --> 01:57:01,360
and

3342
01:57:00,320 --> 01:57:02,480
that's it

3343
01:57:01,360 --> 01:57:03,920
now the second one of these the

3344
01:57:02,480 --> 01:57:06,080
gradient-based framework is much more

3345
01:57:03,920 --> 01:57:08,159
flexible and right now our neural

3346
01:57:06,080 --> 01:57:10,560
network is super simple we're taking a

3347
01:57:08,159 --> 01:57:12,320
single previous character and we're

3348
01:57:10,560 --> 01:57:14,000
taking it through a single linear layer

3349
01:57:12,320 --> 01:57:16,239
to calculate the logits

3350
01:57:14,000 --> 01:57:17,760
this is about to complexify so in the

3351
01:57:16,239 --> 01:57:20,480
follow-up videos we're going to be

3352
01:57:17,760 --> 01:57:21,840
taking more and more of these characters

3353
01:57:20,480 --> 01:57:23,760
and we're going to be feeding them into

3354
01:57:21,840 --> 01:57:25,360
a neural net but this neural net will

3355
01:57:23,760 --> 01:57:27,920
still output the exact same thing the

3356
01:57:25,360 --> 01:57:29,199
neural net will output logits

3357
01:57:27,920 --> 01:57:30,880
and these logits will still be

3358
01:57:29,199 --> 01:57:32,239
normalized in the exact same way and all

3359
01:57:30,880 --> 01:57:33,679
the loss and everything else and the

3360
01:57:32,239 --> 01:57:35,679
gradient gradient-based framework

3361
01:57:33,679 --> 01:57:38,560
everything stays identical it's just

3362
01:57:35,679 --> 01:57:40,480
that this neural net will now complexify

3363
01:57:38,560 --> 01:57:42,239
all the way to transformers

3364
01:57:40,480 --> 01:57:46,719
so that's gonna be pretty awesome and

3365
01:57:42,239 --> 01:57:46,719
i'm looking forward to it for now bye

