1
00:00:00,060 --> 00:00:03,120
hi everyone

2
00:00:01,380 --> 00:00:05,400
so by now you have probably heard of

3
00:00:03,120 --> 00:00:07,980
Chachi PT it has taken the world and the

4
00:00:05,400 --> 00:00:10,559
AI Community by storm and it is a system

5
00:00:07,980 --> 00:00:13,139
that allows you to interact with an AI

6
00:00:10,559 --> 00:00:15,540
and give it text-based tasks so for

7
00:00:13,139 --> 00:00:17,039
example we can ask chatgpt to write us a

8
00:00:15,540 --> 00:00:18,900
small haiku about how important it is

9
00:00:17,039 --> 00:00:20,340
that people understand Ai and then they

10
00:00:18,900 --> 00:00:23,640
can use it to improve the world and make

11
00:00:20,340 --> 00:00:25,740
it more prosperous so when we run this

12
00:00:23,640 --> 00:00:29,039
AI knowledge brings prosperity for all

13
00:00:25,740 --> 00:00:30,900
to see Embrace its power okay not bad

14
00:00:29,039 --> 00:00:33,300
and so you could see that Chachi PT went

15
00:00:30,900 --> 00:00:35,820
from left to right and generated all

16
00:00:33,300 --> 00:00:38,100
these words seek sort of sequentially

17
00:00:35,820 --> 00:00:40,140
now I asked it already the exact same

18
00:00:38,100 --> 00:00:41,879
prompt a little bit earlier and it

19
00:00:40,140 --> 00:00:44,579
generated a slightly different outcome

20
00:00:41,879 --> 00:00:47,280
AI is power to grow ignorance holds us

21
00:00:44,579 --> 00:00:49,320
back learn Prosperity weights

22
00:00:47,280 --> 00:00:50,820
so uh pretty good in both cases and

23
00:00:49,320 --> 00:00:53,039
slightly different so you can see that

24
00:00:50,820 --> 00:00:54,719
chatgpt is a probabilistic system and

25
00:00:53,039 --> 00:00:58,140
for any one prompt it can give us

26
00:00:54,719 --> 00:01:00,300
multiple answers sort of replying to it

27
00:00:58,140 --> 00:01:01,680
now this is just one example of a prompt

28
00:01:00,300 --> 00:01:03,719
people have come up with many many

29
00:01:01,680 --> 00:01:06,479
examples and there are entire websites

30
00:01:03,719 --> 00:01:09,240
that index interactions with charge EBT

31
00:01:06,479 --> 00:01:12,420
and so many of them are quite humorous

32
00:01:09,240 --> 00:01:15,240
explain HTML to me like I'm a dog write

33
00:01:12,420 --> 00:01:17,159
release notes for chess 2. write a note

34
00:01:15,240 --> 00:01:19,080
about Elon Musk buying on Twitter

35
00:01:17,159 --> 00:01:21,540
and so on

36
00:01:19,080 --> 00:01:23,220
so as an example please write a breaking

37
00:01:21,540 --> 00:01:24,060
news article about a leaf falling from a

38
00:01:23,220 --> 00:01:26,939
tree

39
00:01:24,060 --> 00:01:28,020
uh and a shocking turn of events a leaf

40
00:01:26,939 --> 00:01:30,000
has fallen from a treat in the local

41
00:01:28,020 --> 00:01:31,500
park Witnesses report that the leaf

42
00:01:30,000 --> 00:01:33,479
which was previously attached to a

43
00:01:31,500 --> 00:01:36,540
branch of a tree detached itself and

44
00:01:33,479 --> 00:01:37,979
fell to the ground very dramatic so you

45
00:01:36,540 --> 00:01:40,320
can see that this is a pretty remarkable

46
00:01:37,979 --> 00:01:44,280
system and it is what we call a language

47
00:01:40,320 --> 00:01:47,400
model because it it models the sequence

48
00:01:44,280 --> 00:01:49,799
of words or characters or tokens more

49
00:01:47,400 --> 00:01:51,960
generally and it knows how sort of words

50
00:01:49,799 --> 00:01:54,240
follow each other in English language

51
00:01:51,960 --> 00:01:56,700
and so from its perspective what it is

52
00:01:54,240 --> 00:01:59,520
doing is it is completing the sequence

53
00:01:56,700 --> 00:02:01,560
so I give it the start of a sequence and

54
00:01:59,520 --> 00:02:03,659
it completes the sequence with the

55
00:02:01,560 --> 00:02:04,500
outcome and so it's a language model in

56
00:02:03,659 --> 00:02:06,540
that sense

57
00:02:04,500 --> 00:02:08,280
now I would like to focus on the under

58
00:02:06,540 --> 00:02:10,020
the hood of

59
00:02:08,280 --> 00:02:12,120
um under the hood components of what

60
00:02:10,020 --> 00:02:13,800
makes chat GPT work so what is the

61
00:02:12,120 --> 00:02:16,319
neural network under the hood that

62
00:02:13,800 --> 00:02:18,840
models the sequence of these words

63
00:02:16,319 --> 00:02:22,020
and that comes from this paper called

64
00:02:18,840 --> 00:02:24,300
attention is all you need in 2017 a

65
00:02:22,020 --> 00:02:26,700
landmark paper a landmark paper and AI

66
00:02:24,300 --> 00:02:28,739
that produced and proposed the

67
00:02:26,700 --> 00:02:31,879
Transformer architecture

68
00:02:28,739 --> 00:02:34,560
so GPT is short for generally

69
00:02:31,879 --> 00:02:36,120
generatively pre-trained Transformer so

70
00:02:34,560 --> 00:02:37,500
Transformer is the neural nut that

71
00:02:36,120 --> 00:02:39,660
actually does all the heavy lifting

72
00:02:37,500 --> 00:02:42,959
under the hood it comes from this paper

73
00:02:39,660 --> 00:02:44,819
in 2017. now if you read this paper this

74
00:02:42,959 --> 00:02:46,620
reads like a pretty random machine

75
00:02:44,819 --> 00:02:47,640
translation paper and that's because I

76
00:02:46,620 --> 00:02:49,200
think the authors didn't fully

77
00:02:47,640 --> 00:02:51,360
anticipate the impact that the

78
00:02:49,200 --> 00:02:53,099
Transformer would have on the field and

79
00:02:51,360 --> 00:02:55,019
this architecture that they produced in

80
00:02:53,099 --> 00:02:57,000
the context of machine translation in

81
00:02:55,019 --> 00:02:59,819
their case actually ended up taking over

82
00:02:57,000 --> 00:03:02,760
the rest of AI in the next five years

83
00:02:59,819 --> 00:03:05,640
after and so this architecture with

84
00:03:02,760 --> 00:03:07,860
minor changes was copy pasted into a

85
00:03:05,640 --> 00:03:10,620
huge amount of applications in AI in

86
00:03:07,860 --> 00:03:12,659
more recent years and that includes at

87
00:03:10,620 --> 00:03:15,120
the core of chat GPT

88
00:03:12,659 --> 00:03:16,379
now we are not going to what I'd like to

89
00:03:15,120 --> 00:03:19,379
do now is I'd like to build out

90
00:03:16,379 --> 00:03:20,700
something like chatgpt but we're not

91
00:03:19,379 --> 00:03:22,560
going to be able to of course reproduce

92
00:03:20,700 --> 00:03:25,220
chatgpt this is a very serious

93
00:03:22,560 --> 00:03:28,500
production grade system it is trained on

94
00:03:25,220 --> 00:03:30,180
a good chunk of internet and then

95
00:03:28,500 --> 00:03:32,220
there's a lot of pre-training and

96
00:03:30,180 --> 00:03:34,019
fine-tuning stages to it and so it's

97
00:03:32,220 --> 00:03:37,560
very complicated what I'd like to focus

98
00:03:34,019 --> 00:03:39,599
on is just to train a Transformer based

99
00:03:37,560 --> 00:03:41,220
language model and in our case it's

100
00:03:39,599 --> 00:03:43,860
going to be a character level

101
00:03:41,220 --> 00:03:45,360
a language model I still think that is a

102
00:03:43,860 --> 00:03:47,580
very educational with respect to how

103
00:03:45,360 --> 00:03:49,500
these systems work so I don't want to

104
00:03:47,580 --> 00:03:51,840
train on the chunk of Internet we need a

105
00:03:49,500 --> 00:03:54,060
smaller data set in this case I propose

106
00:03:51,840 --> 00:03:56,400
that we work with my favorite toy data

107
00:03:54,060 --> 00:03:58,200
set it's called tiny Shakespeare and

108
00:03:56,400 --> 00:03:59,760
what it is is basically it's a

109
00:03:58,200 --> 00:04:02,099
concatenation of all of the works of

110
00:03:59,760 --> 00:04:04,319
Shakespeare in my understanding and so

111
00:04:02,099 --> 00:04:07,379
this is all of Shakespeare in a single

112
00:04:04,319 --> 00:04:09,480
file this file is about one megabyte

113
00:04:07,379 --> 00:04:11,519
and it's just all of Shakespeare

114
00:04:09,480 --> 00:04:13,319
and what we are going to do now is we're

115
00:04:11,519 --> 00:04:15,720
going to basically model how these

116
00:04:13,319 --> 00:04:17,040
characters follow each other so for

117
00:04:15,720 --> 00:04:18,660
example given a chunk of these

118
00:04:17,040 --> 00:04:21,840
characters like this

119
00:04:18,660 --> 00:04:23,880
are given some context of characters in

120
00:04:21,840 --> 00:04:25,440
the past the Transformer neural network

121
00:04:23,880 --> 00:04:27,300
will look at the characters that I've

122
00:04:25,440 --> 00:04:29,580
highlighted and is going to predict that

123
00:04:27,300 --> 00:04:31,199
g is likely to come next in the sequence

124
00:04:29,580 --> 00:04:33,120
and it's going to do that because we're

125
00:04:31,199 --> 00:04:35,040
going to train that Transformer on

126
00:04:33,120 --> 00:04:37,860
Shakespeare and it's just going to try

127
00:04:35,040 --> 00:04:39,300
to produce uh character sequences that

128
00:04:37,860 --> 00:04:40,919
look like this

129
00:04:39,300 --> 00:04:43,500
and in that process is going to model

130
00:04:40,919 --> 00:04:45,419
all the patterns inside this data so

131
00:04:43,500 --> 00:04:47,639
once we've trained the system I'd just

132
00:04:45,419 --> 00:04:49,919
like to give you a preview we can

133
00:04:47,639 --> 00:04:52,139
generate infinite Shakespeare and of

134
00:04:49,919 --> 00:04:54,900
course it's a fake thing that looks kind

135
00:04:52,139 --> 00:04:56,820
of like Shakespeare

136
00:04:54,900 --> 00:04:59,400
um

137
00:04:56,820 --> 00:05:02,940
apologies for there's some junk that I'm

138
00:04:59,400 --> 00:05:03,840
not able to resolve in in here but

139
00:05:02,940 --> 00:05:05,580
um

140
00:05:03,840 --> 00:05:07,259
you can see how this is going character

141
00:05:05,580 --> 00:05:10,560
by character and it's kind of like

142
00:05:07,259 --> 00:05:13,380
predicting Shakespeare like language so

143
00:05:10,560 --> 00:05:16,320
verily my Lord the sights have left the

144
00:05:13,380 --> 00:05:19,860
again the king coming with my curses

145
00:05:16,320 --> 00:05:21,660
with precious pale and then tronio says

146
00:05:19,860 --> 00:05:23,520
something else Etc and this is just

147
00:05:21,660 --> 00:05:25,560
coming out of the Transformer in a very

148
00:05:23,520 --> 00:05:27,720
similar manner as it would come out in

149
00:05:25,560 --> 00:05:31,320
Chachi PT in our case character by

150
00:05:27,720 --> 00:05:33,360
character in Chachi PT it's coming out

151
00:05:31,320 --> 00:05:35,280
on the token by token level and tokens

152
00:05:33,360 --> 00:05:37,139
are these a sort of like little sub word

153
00:05:35,280 --> 00:05:40,259
pieces so they're not Word level they're

154
00:05:37,139 --> 00:05:43,500
kind of like work chunk level

155
00:05:40,259 --> 00:05:46,560
um and now the I've already written this

156
00:05:43,500 --> 00:05:49,620
entire code to train these Transformers

157
00:05:46,560 --> 00:05:52,740
um and it is in a GitHub repository that

158
00:05:49,620 --> 00:05:54,660
you can find and it's called a nano GPT

159
00:05:52,740 --> 00:05:57,300
so Nano GPT is a repository that you can

160
00:05:54,660 --> 00:05:59,160
find on my GitHub and it's a repository

161
00:05:57,300 --> 00:06:01,080
for training Transformers

162
00:05:59,160 --> 00:06:02,880
um On Any Given text

163
00:06:01,080 --> 00:06:03,960
and what I think is interesting about it

164
00:06:02,880 --> 00:06:05,759
because there's many ways to train

165
00:06:03,960 --> 00:06:08,340
Transformers but this is a very simple

166
00:06:05,759 --> 00:06:11,160
implementation so it's just two files of

167
00:06:08,340 --> 00:06:13,860
300 lines of code each one file defines

168
00:06:11,160 --> 00:06:15,780
the GPT model the Transformer and one

169
00:06:13,860 --> 00:06:17,880
file trains it on some given Text data

170
00:06:15,780 --> 00:06:19,860
set and here I'm showing that if you

171
00:06:17,880 --> 00:06:21,300
train it on a open webtext data set

172
00:06:19,860 --> 00:06:24,000
which is a fairly large data set of web

173
00:06:21,300 --> 00:06:26,340
pages then I reproduce the the

174
00:06:24,000 --> 00:06:29,759
performance of gpt2

175
00:06:26,340 --> 00:06:33,900
so gpt2 is an early version of openai's

176
00:06:29,759 --> 00:06:35,639
GPT from 2017 if I occur correctly and

177
00:06:33,900 --> 00:06:38,460
I've only so far reproduced the the

178
00:06:35,639 --> 00:06:39,660
smallest 124 million parameter model but

179
00:06:38,460 --> 00:06:41,639
basically this is just proving that the

180
00:06:39,660 --> 00:06:44,699
code base is correctly arranged and I'm

181
00:06:41,639 --> 00:06:46,979
able to load the neural network weights

182
00:06:44,699 --> 00:06:49,199
that open AI has released later

183
00:06:46,979 --> 00:06:51,539
so you can take a look at the finished

184
00:06:49,199 --> 00:06:53,160
code here in Nano GPT but what I would

185
00:06:51,539 --> 00:06:56,340
like to do in this lecture is I would

186
00:06:53,160 --> 00:06:57,900
like to basically write this repository

187
00:06:56,340 --> 00:07:00,060
from scratch so we're going to begin

188
00:06:57,900 --> 00:07:03,419
with an empty file and we're going to

189
00:07:00,060 --> 00:07:05,280
define a Transformer piece by piece

190
00:07:03,419 --> 00:07:07,380
we're going to train it on the tiny

191
00:07:05,280 --> 00:07:09,300
Shakespeare data set and we'll see how

192
00:07:07,380 --> 00:07:11,699
we can then generate infinite

193
00:07:09,300 --> 00:07:13,380
Shakespeare and of course this can copy

194
00:07:11,699 --> 00:07:16,259
paste to any arbitrary Text data set

195
00:07:13,380 --> 00:07:17,520
that you like but my goal really here is

196
00:07:16,259 --> 00:07:20,340
to just make you understand and

197
00:07:17,520 --> 00:07:23,880
appreciate how under the hood chat GPT

198
00:07:20,340 --> 00:07:27,300
works and really all that's required is

199
00:07:23,880 --> 00:07:29,819
a Proficiency in Python and some basic

200
00:07:27,300 --> 00:07:32,280
understanding of calculus and statistics

201
00:07:29,819 --> 00:07:34,020
and it would help if you also see my

202
00:07:32,280 --> 00:07:36,000
previous videos on the same YouTube

203
00:07:34,020 --> 00:07:38,639
channel in particular my make more

204
00:07:36,000 --> 00:07:41,580
series where I

205
00:07:38,639 --> 00:07:43,800
Define smaller and simpler neural

206
00:07:41,580 --> 00:07:45,660
network language models so multilevel

207
00:07:43,800 --> 00:07:47,099
perceptrons and so on it really

208
00:07:45,660 --> 00:07:49,500
introduces the language modeling

209
00:07:47,099 --> 00:07:50,940
framework and then here in this video

210
00:07:49,500 --> 00:07:52,620
we're going to focus on the Transformer

211
00:07:50,940 --> 00:07:55,500
neural network itself

212
00:07:52,620 --> 00:07:57,780
okay so I created a new Google collab uh

213
00:07:55,500 --> 00:07:59,639
jupyter notebook here and this will

214
00:07:57,780 --> 00:08:00,720
allow me to later easily share this code

215
00:07:59,639 --> 00:08:03,120
that we're going to develop together

216
00:08:00,720 --> 00:08:05,060
with you so you can follow along so this

217
00:08:03,120 --> 00:08:07,620
will be in the video description later

218
00:08:05,060 --> 00:08:09,720
now here I've just done some

219
00:08:07,620 --> 00:08:11,220
preliminaries I downloaded the data set

220
00:08:09,720 --> 00:08:12,960
the tiny Shakespeare data set at this

221
00:08:11,220 --> 00:08:14,639
URL and you can see that it's about a

222
00:08:12,960 --> 00:08:17,460
one megabyte file

223
00:08:14,639 --> 00:08:19,379
then here I open the input.txt file and

224
00:08:17,460 --> 00:08:21,360
just read in all the text as a string

225
00:08:19,379 --> 00:08:23,400
and we see that we are working with 1

226
00:08:21,360 --> 00:08:25,319
million characters roughly

227
00:08:23,400 --> 00:08:27,060
and the first 1000 characters if we just

228
00:08:25,319 --> 00:08:28,919
print them out are basically what you

229
00:08:27,060 --> 00:08:31,259
would expect this is the first 1000

230
00:08:28,919 --> 00:08:33,659
characters of the tiny Shakespeare data

231
00:08:31,259 --> 00:08:36,120
set roughly up to here

232
00:08:33,659 --> 00:08:38,580
so so far so good next we're going to

233
00:08:36,120 --> 00:08:40,620
take this text and the text is a

234
00:08:38,580 --> 00:08:43,080
sequence of characters in Python so when

235
00:08:40,620 --> 00:08:45,660
I call the set Constructor on it I'm

236
00:08:43,080 --> 00:08:48,180
just going to get the set of all the

237
00:08:45,660 --> 00:08:51,480
characters that occur in this text

238
00:08:48,180 --> 00:08:52,800
and then I call list on that to create a

239
00:08:51,480 --> 00:08:54,600
list of those characters instead of just

240
00:08:52,800 --> 00:08:56,160
a set so that I have an ordering an

241
00:08:54,600 --> 00:08:58,200
arbitrary ordering

242
00:08:56,160 --> 00:08:59,820
and then I sort that

243
00:08:58,200 --> 00:09:01,440
so basically we get just all the

244
00:08:59,820 --> 00:09:03,660
characters that occur in the entire data

245
00:09:01,440 --> 00:09:05,399
set and they're sorted now the number of

246
00:09:03,660 --> 00:09:07,800
them is going to be our vocabulary size

247
00:09:05,399 --> 00:09:10,080
these are the possible elements of our

248
00:09:07,800 --> 00:09:12,120
sequences and we see that when I print

249
00:09:10,080 --> 00:09:14,820
here the characters

250
00:09:12,120 --> 00:09:16,560
there's 65 of them in total there's a

251
00:09:14,820 --> 00:09:18,120
space character and then all kinds of

252
00:09:16,560 --> 00:09:21,180
special characters

253
00:09:18,120 --> 00:09:23,100
and then capitals and lowercase letters

254
00:09:21,180 --> 00:09:25,740
so that's our vocabulary and that's the

255
00:09:23,100 --> 00:09:28,500
sort of like possible characters that

256
00:09:25,740 --> 00:09:30,480
the model can see or emit

257
00:09:28,500 --> 00:09:33,899
okay so next we would like to develop

258
00:09:30,480 --> 00:09:36,420
some strategy to tokenize the input text

259
00:09:33,899 --> 00:09:39,360
now when people say tokenize they mean

260
00:09:36,420 --> 00:09:41,459
convert the raw text as a string to some

261
00:09:39,360 --> 00:09:43,980
sequence of integers According to some

262
00:09:41,459 --> 00:09:45,480
notebook According to some vocabulary of

263
00:09:43,980 --> 00:09:47,820
possible elements

264
00:09:45,480 --> 00:09:49,019
so as an example here we are going to be

265
00:09:47,820 --> 00:09:50,519
building a character level language

266
00:09:49,019 --> 00:09:52,260
model so we're simply going to be

267
00:09:50,519 --> 00:09:53,459
translating individual characters into

268
00:09:52,260 --> 00:09:55,680
integers

269
00:09:53,459 --> 00:09:57,180
so let me show you a chunk of code that

270
00:09:55,680 --> 00:09:59,040
sort of does that for us

271
00:09:57,180 --> 00:10:01,140
so we're building both the encoder and

272
00:09:59,040 --> 00:10:03,120
the decoder and let me just talk through

273
00:10:01,140 --> 00:10:05,880
What's Happening Here

274
00:10:03,120 --> 00:10:08,700
when we encode an arbitrary text like hi

275
00:10:05,880 --> 00:10:11,820
there we're going to receive a list of

276
00:10:08,700 --> 00:10:14,399
integers that represents that string so

277
00:10:11,820 --> 00:10:16,080
for example 46 47 Etc

278
00:10:14,399 --> 00:10:19,320
and then we also have the reverse

279
00:10:16,080 --> 00:10:21,420
mapping so we can take this list and

280
00:10:19,320 --> 00:10:22,380
decode it to get back the exact same

281
00:10:21,420 --> 00:10:23,880
string

282
00:10:22,380 --> 00:10:26,160
so it's really just like a translation

283
00:10:23,880 --> 00:10:28,500
two integers and back for arbitrary

284
00:10:26,160 --> 00:10:30,000
string and for us it is done on a

285
00:10:28,500 --> 00:10:32,160
character level

286
00:10:30,000 --> 00:10:34,380
now the way this was achieved is we just

287
00:10:32,160 --> 00:10:35,940
iterate over all the characters here and

288
00:10:34,380 --> 00:10:38,519
create a lookup table from the character

289
00:10:35,940 --> 00:10:40,740
to the integer and vice versa and then

290
00:10:38,519 --> 00:10:41,519
to encode some string we simply

291
00:10:40,740 --> 00:10:44,160
translate all the characters

292
00:10:41,519 --> 00:10:46,560
individually and to decode it back we

293
00:10:44,160 --> 00:10:47,519
use the reverse mapping and concatenate

294
00:10:46,560 --> 00:10:49,860
all of it

295
00:10:47,519 --> 00:10:51,720
now this is only one of many possible

296
00:10:49,860 --> 00:10:53,820
encodings or many possible sort of

297
00:10:51,720 --> 00:10:55,860
tokenizers and it's a very simple one

298
00:10:53,820 --> 00:10:57,720
but there's many other schemas that

299
00:10:55,860 --> 00:11:00,600
people have come up with in practice so

300
00:10:57,720 --> 00:11:02,700
for example Google uses a sentence piece

301
00:11:00,600 --> 00:11:05,940
uh so sentence piece will also encode

302
00:11:02,700 --> 00:11:08,880
text into integers but in a different

303
00:11:05,940 --> 00:11:12,300
schema and using a different vocabulary

304
00:11:08,880 --> 00:11:14,760
and sentence piece is a sub word sort of

305
00:11:12,300 --> 00:11:17,100
tokenizer and what that means is that

306
00:11:14,760 --> 00:11:18,540
you're not encoding entire words but

307
00:11:17,100 --> 00:11:21,420
you're not also encoding individual

308
00:11:18,540 --> 00:11:23,579
characters it's it's a sub word unit

309
00:11:21,420 --> 00:11:25,980
level and that's usually what's adopted

310
00:11:23,579 --> 00:11:28,140
in practice for example also openai has

311
00:11:25,980 --> 00:11:31,260
this Library called tick token that uses

312
00:11:28,140 --> 00:11:33,240
a pipe pair encoding tokenizer

313
00:11:31,260 --> 00:11:35,519
um and that's what GPT uses

314
00:11:33,240 --> 00:11:38,760
and you can also just encode words into

315
00:11:35,519 --> 00:11:40,560
like hello world into a list of integers

316
00:11:38,760 --> 00:11:42,120
so as an example I'm using the tick

317
00:11:40,560 --> 00:11:44,339
token Library here

318
00:11:42,120 --> 00:11:46,200
I'm getting the encoding for gpt2 or

319
00:11:44,339 --> 00:11:48,180
that was used for gpt2

320
00:11:46,200 --> 00:11:51,540
instead of just having 65 possible

321
00:11:48,180 --> 00:11:52,800
characters or tokens they have 50 000

322
00:11:51,540 --> 00:11:54,720
tokens

323
00:11:52,800 --> 00:11:57,660
and so when they encode the exact same

324
00:11:54,720 --> 00:11:59,940
string High there we only get a list of

325
00:11:57,660 --> 00:12:02,640
three integers but those integers are

326
00:11:59,940 --> 00:12:06,720
not between 0 and 64. they are between 0

327
00:12:02,640 --> 00:12:09,600
and 5000 50 256.

328
00:12:06,720 --> 00:12:12,300
so basically you can trade off the code

329
00:12:09,600 --> 00:12:14,040
book size and the sequence lengths so

330
00:12:12,300 --> 00:12:16,500
you can have a very long sequences of

331
00:12:14,040 --> 00:12:18,240
integers with very small vocabularies or

332
00:12:16,500 --> 00:12:19,200
you can have a short

333
00:12:18,240 --> 00:12:21,899
um

334
00:12:19,200 --> 00:12:25,320
sequences of integers with very large

335
00:12:21,899 --> 00:12:28,260
vocabularies and so typically people use

336
00:12:25,320 --> 00:12:30,060
in practice the sub word encodings but

337
00:12:28,260 --> 00:12:31,440
I'd like to keep our tokenizer very

338
00:12:30,060 --> 00:12:32,700
simple so we're using character level

339
00:12:31,440 --> 00:12:34,260
tokenizer

340
00:12:32,700 --> 00:12:36,600
and that means that we have very small

341
00:12:34,260 --> 00:12:40,019
code books we have very simple encode

342
00:12:36,600 --> 00:12:42,480
and decode functions but we do get very

343
00:12:40,019 --> 00:12:43,740
long sequences as a result but that's

344
00:12:42,480 --> 00:12:44,940
the level at which we're going to stick

345
00:12:43,740 --> 00:12:46,680
with this lecture because it's the

346
00:12:44,940 --> 00:12:49,200
simplest thing okay so now that we have

347
00:12:46,680 --> 00:12:51,480
an encoder and a decoder effectively a

348
00:12:49,200 --> 00:12:54,000
tokenizer we can tokenize the entire

349
00:12:51,480 --> 00:12:55,500
training set of Shakespeare so here's a

350
00:12:54,000 --> 00:12:56,760
chunk of code that does that

351
00:12:55,500 --> 00:12:58,500
and I'm going to start to use the

352
00:12:56,760 --> 00:13:01,019
pytorch library and specifically the

353
00:12:58,500 --> 00:13:02,579
torch.tensor from the pytorch library

354
00:13:01,019 --> 00:13:05,519
so we're going to take all of the text

355
00:13:02,579 --> 00:13:08,040
in tiny Shakespeare encode it and then

356
00:13:05,519 --> 00:13:10,139
wrap it into a torch.tensor to get the

357
00:13:08,040 --> 00:13:11,639
data tensor so here's what the data

358
00:13:10,139 --> 00:13:14,279
tensor looks like when I look at just

359
00:13:11,639 --> 00:13:15,600
the first 1000 characters or the 1000

360
00:13:14,279 --> 00:13:16,920
elements of it

361
00:13:15,600 --> 00:13:19,079
so we see that we have a massive

362
00:13:16,920 --> 00:13:21,060
sequence of integers and this sequence

363
00:13:19,079 --> 00:13:23,100
of integers here is basically an

364
00:13:21,060 --> 00:13:25,320
identical translation of the first 1000

365
00:13:23,100 --> 00:13:27,480
characters here

366
00:13:25,320 --> 00:13:29,880
so I believe for example that zero is a

367
00:13:27,480 --> 00:13:33,060
new line character and maybe one is a

368
00:13:29,880 --> 00:13:34,200
space not 100 sure but from now on the

369
00:13:33,060 --> 00:13:36,420
entire data set of text is

370
00:13:34,200 --> 00:13:38,700
re-represented as just it just stretched

371
00:13:36,420 --> 00:13:40,260
out as a single very large uh sequence

372
00:13:38,700 --> 00:13:41,940
of integers

373
00:13:40,260 --> 00:13:43,860
let me do one more thing before we move

374
00:13:41,940 --> 00:13:46,200
on here I'd like to separate out our

375
00:13:43,860 --> 00:13:48,600
data set into a train and a validation

376
00:13:46,200 --> 00:13:51,899
split so in particular we're going to

377
00:13:48,600 --> 00:13:53,160
take the first 90 of the data set and

378
00:13:51,899 --> 00:13:54,779
consider that to be the training data

379
00:13:53,160 --> 00:13:56,940
for the Transformer and we're going to

380
00:13:54,779 --> 00:13:59,639
withhold the last 10 percent at the end

381
00:13:56,940 --> 00:14:01,260
of it to be the validation data and this

382
00:13:59,639 --> 00:14:03,180
will help us understand to what extent

383
00:14:01,260 --> 00:14:04,800
our model is overfitting so we're going

384
00:14:03,180 --> 00:14:06,959
to basically hide and keep the

385
00:14:04,800 --> 00:14:08,339
validation data on the side because we

386
00:14:06,959 --> 00:14:11,279
don't want just a perfect memorization

387
00:14:08,339 --> 00:14:12,720
of this exact Shakespeare we want a

388
00:14:11,279 --> 00:14:15,660
neural network that sort of creates

389
00:14:12,720 --> 00:14:17,880
Shakespeare like text and so it should

390
00:14:15,660 --> 00:14:22,079
be fairly likely for it to produce

391
00:14:17,880 --> 00:14:24,120
the actual like stowed away uh true

392
00:14:22,079 --> 00:14:26,220
Shakespeare text

393
00:14:24,120 --> 00:14:28,019
um and so we're going to use this to get

394
00:14:26,220 --> 00:14:29,940
a sense of the overfitting okay so now

395
00:14:28,019 --> 00:14:32,100
we would like to start plugging these

396
00:14:29,940 --> 00:14:34,260
text sequences or integer sequences into

397
00:14:32,100 --> 00:14:35,820
the Transformer so that it can train and

398
00:14:34,260 --> 00:14:38,040
learn those patterns

399
00:14:35,820 --> 00:14:39,720
now the important thing to realize is

400
00:14:38,040 --> 00:14:41,820
we're never going to actually feed the

401
00:14:39,720 --> 00:14:43,680
entire text into Transformer all at once

402
00:14:41,820 --> 00:14:45,839
that would be computationally very

403
00:14:43,680 --> 00:14:47,459
expensive and prohibitive so when we

404
00:14:45,839 --> 00:14:49,740
actually train a Transformer on a lot of

405
00:14:47,459 --> 00:14:51,240
these data sets we only work with chunks

406
00:14:49,740 --> 00:14:53,339
of the data set and when we train the

407
00:14:51,240 --> 00:14:54,839
Transformer we basically sample random

408
00:14:53,339 --> 00:14:57,959
little chunks out of the training set

409
00:14:54,839 --> 00:14:59,699
and train them just chunks at a time and

410
00:14:57,959 --> 00:15:01,199
these chunks have basically some kind of

411
00:14:59,699 --> 00:15:04,560
a length

412
00:15:01,199 --> 00:15:06,480
and as a maximum length now the maximum

413
00:15:04,560 --> 00:15:08,459
length typically at least in the code I

414
00:15:06,480 --> 00:15:10,980
usually write is called block size

415
00:15:08,459 --> 00:15:12,600
you can you can find it on the different

416
00:15:10,980 --> 00:15:14,160
names like context length or something

417
00:15:12,600 --> 00:15:16,199
like that let's start with the block

418
00:15:14,160 --> 00:15:18,779
size of just eight and let me look at

419
00:15:16,199 --> 00:15:20,399
the first train data characters the

420
00:15:18,779 --> 00:15:23,760
first block size plus one characters

421
00:15:20,399 --> 00:15:26,100
I'll explain why plus one in a second

422
00:15:23,760 --> 00:15:29,160
so this is the first nine characters in

423
00:15:26,100 --> 00:15:30,660
the sequence in the training set

424
00:15:29,160 --> 00:15:32,399
now what I'd like to point out is that

425
00:15:30,660 --> 00:15:34,680
when you sample a chunk of data like

426
00:15:32,399 --> 00:15:36,180
this so say that these nine characters

427
00:15:34,680 --> 00:15:38,220
out of the training set

428
00:15:36,180 --> 00:15:39,839
this actually has multiple examples

429
00:15:38,220 --> 00:15:41,639
packed into it

430
00:15:39,839 --> 00:15:43,500
and that's because all of these

431
00:15:41,639 --> 00:15:46,980
characters follow each other

432
00:15:43,500 --> 00:15:49,260
and so what this thing is going to say

433
00:15:46,980 --> 00:15:50,459
when we plug it into a Transformer is

434
00:15:49,260 --> 00:15:52,500
we're going to actually simultaneously

435
00:15:50,459 --> 00:15:54,180
train it to make prediction at every one

436
00:15:52,500 --> 00:15:57,180
of these positions

437
00:15:54,180 --> 00:15:59,279
now in the in a chunk of nine characters

438
00:15:57,180 --> 00:16:01,139
there's actually eight individual

439
00:15:59,279 --> 00:16:04,199
examples packed in there

440
00:16:01,139 --> 00:16:07,500
so there's the example that one 18 when

441
00:16:04,199 --> 00:16:10,800
in the context of 18 47 likely comes

442
00:16:07,500 --> 00:16:14,940
next in the context of 18 and 47 56

443
00:16:10,800 --> 00:16:18,000
comes next in the context of 1847-56 57

444
00:16:14,940 --> 00:16:20,220
can come next and so on so that's the

445
00:16:18,000 --> 00:16:22,680
eight individual examples let me

446
00:16:20,220 --> 00:16:25,019
actually spell it out with code

447
00:16:22,680 --> 00:16:27,360
so here's a chunk of code to illustrate

448
00:16:25,019 --> 00:16:28,740
X are the inputs to the Transformer it

449
00:16:27,360 --> 00:16:30,120
will just be the first block size

450
00:16:28,740 --> 00:16:33,959
characters

451
00:16:30,120 --> 00:16:36,000
y will be the next block size characters

452
00:16:33,959 --> 00:16:38,699
so it's offset by one

453
00:16:36,000 --> 00:16:41,399
and that's because y are the targets for

454
00:16:38,699 --> 00:16:43,380
each position in the input

455
00:16:41,399 --> 00:16:46,139
and then here I'm iterating over all the

456
00:16:43,380 --> 00:16:49,019
block size of 8. and the context is

457
00:16:46,139 --> 00:16:50,699
always all the characters in X up to T

458
00:16:49,019 --> 00:16:52,740
and including t

459
00:16:50,699 --> 00:16:56,160
and the target is always the teeth

460
00:16:52,740 --> 00:16:58,259
character but in the targets array why

461
00:16:56,160 --> 00:16:59,940
so let me just run this

462
00:16:58,259 --> 00:17:02,160
and basically it spells out what I've

463
00:16:59,940 --> 00:17:04,500
said in words these are the eight

464
00:17:02,160 --> 00:17:08,459
examples hidden in a chunk of nine

465
00:17:04,500 --> 00:17:09,959
characters that we uh sampled from the

466
00:17:08,459 --> 00:17:12,120
training set

467
00:17:09,959 --> 00:17:14,640
I want to mention one more thing we

468
00:17:12,120 --> 00:17:17,220
train on all the eight examples here

469
00:17:14,640 --> 00:17:18,900
with context between one all the way up

470
00:17:17,220 --> 00:17:20,520
to context of block size

471
00:17:18,900 --> 00:17:21,959
and we train on that not just for

472
00:17:20,520 --> 00:17:23,280
computational reasons because we happen

473
00:17:21,959 --> 00:17:24,780
to have the sequence already or

474
00:17:23,280 --> 00:17:28,260
something like that it's not just done

475
00:17:24,780 --> 00:17:30,600
for efficiency it's also done to make

476
00:17:28,260 --> 00:17:32,820
the Transformer Network be used to

477
00:17:30,600 --> 00:17:35,880
seeing contexts all the way from as

478
00:17:32,820 --> 00:17:37,620
little as one all the way to block size

479
00:17:35,880 --> 00:17:39,780
and we'd like the transform to be used

480
00:17:37,620 --> 00:17:41,039
to seeing everything in between and

481
00:17:39,780 --> 00:17:43,020
that's going to be useful later during

482
00:17:41,039 --> 00:17:45,240
inference because while we're sampling

483
00:17:43,020 --> 00:17:46,980
we can start the sampling generation

484
00:17:45,240 --> 00:17:48,960
with as little as one character of

485
00:17:46,980 --> 00:17:50,760
context and the Transformer knows how to

486
00:17:48,960 --> 00:17:53,100
predict the next character with all the

487
00:17:50,760 --> 00:17:54,600
way up to just one context of one and so

488
00:17:53,100 --> 00:17:56,760
then it can predict everything up to

489
00:17:54,600 --> 00:17:58,500
block size and after block size we have

490
00:17:56,760 --> 00:18:01,020
to start truncating because the

491
00:17:58,500 --> 00:18:03,120
Transformer will never receive more than

492
00:18:01,020 --> 00:18:04,620
block size inputs when it's predicting

493
00:18:03,120 --> 00:18:06,600
the next character

494
00:18:04,620 --> 00:18:08,340
Okay so we've looked at the time

495
00:18:06,600 --> 00:18:09,480
dimension of the tensors that are going

496
00:18:08,340 --> 00:18:10,980
to be feeding into the Transformer

497
00:18:09,480 --> 00:18:13,320
there's one more Dimension to care about

498
00:18:10,980 --> 00:18:15,059
and that is the batch dimension and so

499
00:18:13,320 --> 00:18:17,640
as we're sampling these chunks of text

500
00:18:15,059 --> 00:18:18,600
we're going to be actually every time

501
00:18:17,640 --> 00:18:20,220
we're going to feed them into a

502
00:18:18,600 --> 00:18:22,320
Transformer we're going to have many

503
00:18:20,220 --> 00:18:23,820
batches of multiple chunks of text that

504
00:18:22,320 --> 00:18:25,440
are all like stacked up in a single

505
00:18:23,820 --> 00:18:27,120
tensor and that's just done for

506
00:18:25,440 --> 00:18:29,880
efficiency just so that we can keep the

507
00:18:27,120 --> 00:18:31,919
gpus busy because they are very good at

508
00:18:29,880 --> 00:18:34,500
parallel processing of

509
00:18:31,919 --> 00:18:36,480
um of data and so we just want to

510
00:18:34,500 --> 00:18:38,280
process multiple chunks all at the same

511
00:18:36,480 --> 00:18:39,539
time but those chunks are processed

512
00:18:38,280 --> 00:18:42,179
completely independently they don't talk

513
00:18:39,539 --> 00:18:43,679
to each other and so on so let me

514
00:18:42,179 --> 00:18:45,539
basically just generalize this and

515
00:18:43,679 --> 00:18:47,160
introduce a batch Dimension here's a

516
00:18:45,539 --> 00:18:48,960
chunk of code

517
00:18:47,160 --> 00:18:51,679
let me just run it and then I'm going to

518
00:18:48,960 --> 00:18:51,679
explain what it does

519
00:18:51,780 --> 00:18:55,140
so here because we're going to start

520
00:18:53,280 --> 00:18:57,660
sampling random locations in the data

521
00:18:55,140 --> 00:18:59,520
set to pull chunks from I am setting the

522
00:18:57,660 --> 00:19:01,320
seed so that

523
00:18:59,520 --> 00:19:02,820
um in the random number generator so

524
00:19:01,320 --> 00:19:04,620
that the numbers I see here are going to

525
00:19:02,820 --> 00:19:06,299
be the same numbers you see later if you

526
00:19:04,620 --> 00:19:07,860
try to reproduce this

527
00:19:06,299 --> 00:19:09,900
now the back size here is how many

528
00:19:07,860 --> 00:19:11,940
independent sequences we are processing

529
00:19:09,900 --> 00:19:13,620
every forward backward pass of the

530
00:19:11,940 --> 00:19:15,480
Transformer

531
00:19:13,620 --> 00:19:17,400
the block size as I explained is the

532
00:19:15,480 --> 00:19:18,360
maximum context length to make those

533
00:19:17,400 --> 00:19:21,240
predictions

534
00:19:18,360 --> 00:19:22,980
so let's say by size 4 block size 8 and

535
00:19:21,240 --> 00:19:25,559
then here's how we get batch

536
00:19:22,980 --> 00:19:26,880
for any arbitrary split if the split is

537
00:19:25,559 --> 00:19:28,260
a training split then we're going to

538
00:19:26,880 --> 00:19:29,880
look at train data otherwise and

539
00:19:28,260 --> 00:19:33,299
validata

540
00:19:29,880 --> 00:19:35,880
that gets us the data array and then

541
00:19:33,299 --> 00:19:37,200
when I Generate random positions to grab

542
00:19:35,880 --> 00:19:39,480
a chunk out of

543
00:19:37,200 --> 00:19:41,460
I actually grab I actually generate

544
00:19:39,480 --> 00:19:43,200
batch size number of

545
00:19:41,460 --> 00:19:46,080
random offsets

546
00:19:43,200 --> 00:19:48,299
so because this is four we are IX is

547
00:19:46,080 --> 00:19:50,940
going to be a four numbers that are

548
00:19:48,299 --> 00:19:52,980
randomly generated between 0 and Len of

549
00:19:50,940 --> 00:19:55,679
data minus block size so it's just

550
00:19:52,980 --> 00:19:58,679
random offsets into the training set

551
00:19:55,679 --> 00:20:01,380
and then X's as I explained are the

552
00:19:58,679 --> 00:20:02,580
first block size characters starting at

553
00:20:01,380 --> 00:20:06,360
I

554
00:20:02,580 --> 00:20:08,100
the Y's are the offset by one of that so

555
00:20:06,360 --> 00:20:10,260
just add plus one

556
00:20:08,100 --> 00:20:13,860
and then we're going to get those chunks

557
00:20:10,260 --> 00:20:17,419
for every one of integers I in IX and

558
00:20:13,860 --> 00:20:20,340
use a torch.stack to take all those

559
00:20:17,419 --> 00:20:21,960
one-dimensional tensors as we saw here

560
00:20:20,340 --> 00:20:24,960
and we're going to

561
00:20:21,960 --> 00:20:27,660
um stack them up at rows

562
00:20:24,960 --> 00:20:29,580
and so they all become a row in a four

563
00:20:27,660 --> 00:20:31,799
by eight tensor

564
00:20:29,580 --> 00:20:34,559
so here's where I'm printing then

565
00:20:31,799 --> 00:20:37,620
when I sample a batch XP and YB

566
00:20:34,559 --> 00:20:41,100
the input the Transformer now are

567
00:20:37,620 --> 00:20:44,940
the input X is the four by eight tensor

568
00:20:41,100 --> 00:20:47,520
four uh rows of eight columns

569
00:20:44,940 --> 00:20:49,559
and each one of these is a chunk of the

570
00:20:47,520 --> 00:20:52,500
training set

571
00:20:49,559 --> 00:20:54,539
and then the targets here are in the

572
00:20:52,500 --> 00:20:55,919
associated array Y and they will come in

573
00:20:54,539 --> 00:20:59,880
through the Transformer all the way at

574
00:20:55,919 --> 00:21:01,799
the end to create the loss function so

575
00:20:59,880 --> 00:21:04,620
they will give us the correct answer for

576
00:21:01,799 --> 00:21:07,140
every single position inside X

577
00:21:04,620 --> 00:21:08,820
and then these are the four independent

578
00:21:07,140 --> 00:21:12,059
rows

579
00:21:08,820 --> 00:21:15,059
so spelled out as we did before

580
00:21:12,059 --> 00:21:17,580
this four by eight array contains a

581
00:21:15,059 --> 00:21:19,380
total of 32 examples and they're

582
00:21:17,580 --> 00:21:21,179
completely independent as far as the

583
00:21:19,380 --> 00:21:24,240
Transformer is concerned

584
00:21:21,179 --> 00:21:27,299
uh so when the

585
00:21:24,240 --> 00:21:30,000
input is 24 the target is 43 or rather

586
00:21:27,299 --> 00:21:32,760
43 here in the Y array when the input is

587
00:21:30,000 --> 00:21:35,580
2443 the target is 58.

588
00:21:32,760 --> 00:21:39,600
when the input is 24 43.58 the target is

589
00:21:35,580 --> 00:21:41,760
5 Etc or like when it is a 5258 one the

590
00:21:39,600 --> 00:21:43,679
target is 58.

591
00:21:41,760 --> 00:21:46,020
right so you can sort of see this

592
00:21:43,679 --> 00:21:48,840
spelled out these are the 32 independent

593
00:21:46,020 --> 00:21:51,600
examples packed in to a single batch of

594
00:21:48,840 --> 00:21:53,820
the input X and then the desired targets

595
00:21:51,600 --> 00:21:59,520
are in y

596
00:21:53,820 --> 00:22:01,320
and so now this integer tensor of X is

597
00:21:59,520 --> 00:22:03,179
going to feed into the Transformer

598
00:22:01,320 --> 00:22:04,620
and that Transformer is going to

599
00:22:03,179 --> 00:22:07,140
simultaneously process all these

600
00:22:04,620 --> 00:22:09,360
examples and then look up the correct

601
00:22:07,140 --> 00:22:12,419
um integers to predict in every one of

602
00:22:09,360 --> 00:22:14,460
these positions in the tensor y okay so

603
00:22:12,419 --> 00:22:15,720
now that we have our batch of input that

604
00:22:14,460 --> 00:22:17,640
we'd like to feed into a Transformer

605
00:22:15,720 --> 00:22:19,860
let's start basically feeding this into

606
00:22:17,640 --> 00:22:21,600
neural networks now we're going to start

607
00:22:19,860 --> 00:22:22,799
off with the simplest possible neural

608
00:22:21,600 --> 00:22:24,539
network which in the case of language

609
00:22:22,799 --> 00:22:26,159
modeling in my opinion is the bigram

610
00:22:24,539 --> 00:22:27,539
language model and we've covered the

611
00:22:26,159 --> 00:22:30,360
background language model in my make

612
00:22:27,539 --> 00:22:32,400
more series in a lot of depth and so

613
00:22:30,360 --> 00:22:34,020
here I'm going to sort of go faster and

614
00:22:32,400 --> 00:22:36,179
let's just implement the pytorch module

615
00:22:34,020 --> 00:22:37,860
directly that implements the bigram

616
00:22:36,179 --> 00:22:40,620
language model

617
00:22:37,860 --> 00:22:41,820
so I'm importing the pytorch and then

618
00:22:40,620 --> 00:22:44,159
module

619
00:22:41,820 --> 00:22:45,780
uh for reproducibility

620
00:22:44,159 --> 00:22:47,640
and then here I'm constructing a diagram

621
00:22:45,780 --> 00:22:49,020
language model which is a subclass of NN

622
00:22:47,640 --> 00:22:51,480
module

623
00:22:49,020 --> 00:22:53,760
and then I'm calling it and I'm passing

624
00:22:51,480 --> 00:22:55,799
in the inputs and the targets

625
00:22:53,760 --> 00:22:57,720
and I'm just printing now when the

626
00:22:55,799 --> 00:22:59,880
inputs and targets come here you see

627
00:22:57,720 --> 00:23:03,240
that I'm just taking the index the

628
00:22:59,880 --> 00:23:04,559
inputs X here which I rename to idx and

629
00:23:03,240 --> 00:23:06,059
I'm just passing them into this token

630
00:23:04,559 --> 00:23:08,280
embedding table

631
00:23:06,059 --> 00:23:09,480
so what's going on here is that here in

632
00:23:08,280 --> 00:23:11,940
the Constructor

633
00:23:09,480 --> 00:23:14,580
we are creating a token embedding table

634
00:23:11,940 --> 00:23:16,320
and it is of size vocab size by vocab

635
00:23:14,580 --> 00:23:18,900
size

636
00:23:16,320 --> 00:23:20,940
and we're using nn.embedding which is a

637
00:23:18,900 --> 00:23:23,280
very thin wrapper around basically a

638
00:23:20,940 --> 00:23:24,059
tensor of shape both capsized by vocab

639
00:23:23,280 --> 00:23:25,980
size

640
00:23:24,059 --> 00:23:28,799
and what's happening here is that when

641
00:23:25,980 --> 00:23:30,780
we pass idx here every single integer in

642
00:23:28,799 --> 00:23:32,940
our input is going to refer to this

643
00:23:30,780 --> 00:23:34,740
embedding table and is going to pluck

644
00:23:32,940 --> 00:23:38,039
out a row of that embedding table

645
00:23:34,740 --> 00:23:39,840
corresponding to its index so 24 here

646
00:23:38,039 --> 00:23:42,840
we'll go to the embedding table and

647
00:23:39,840 --> 00:23:45,480
we'll pluck out the 24th row and then 43

648
00:23:42,840 --> 00:23:47,520
will go here and block out the 43rd row

649
00:23:45,480 --> 00:23:50,220
Etc and then Pi torch is going to

650
00:23:47,520 --> 00:23:53,940
arrange all of this into a batch by Time

651
00:23:50,220 --> 00:23:58,380
by Channel tensor in this case batch is

652
00:23:53,940 --> 00:24:01,559
4 time is 8 and C which is the channels

653
00:23:58,380 --> 00:24:02,880
is vocab size or 65. and so we're just

654
00:24:01,559 --> 00:24:06,240
going to pluck out all those rows

655
00:24:02,880 --> 00:24:07,500
arrange them in a b by T by C and now

656
00:24:06,240 --> 00:24:09,620
we're going to interpret this as the

657
00:24:07,500 --> 00:24:12,600
logits which are basically the scores

658
00:24:09,620 --> 00:24:14,880
for the next character in the sequence

659
00:24:12,600 --> 00:24:17,400
and so what's happening here is we are

660
00:24:14,880 --> 00:24:19,200
predicting what comes next based on just

661
00:24:17,400 --> 00:24:22,080
the individual identity of a single

662
00:24:19,200 --> 00:24:23,880
token and you can do that because

663
00:24:22,080 --> 00:24:25,380
um I mean currently the tokens are not

664
00:24:23,880 --> 00:24:26,880
talking to each other and they're not

665
00:24:25,380 --> 00:24:29,700
seeing any context except for they're

666
00:24:26,880 --> 00:24:32,280
just seeing themselves so I'm a I'm a

667
00:24:29,700 --> 00:24:33,720
token number five and then I can

668
00:24:32,280 --> 00:24:35,460
actually make pretty decent predictions

669
00:24:33,720 --> 00:24:37,260
about what comes next just by knowing

670
00:24:35,460 --> 00:24:40,080
that I'm token five because some

671
00:24:37,260 --> 00:24:43,140
characters know cert follow other

672
00:24:40,080 --> 00:24:45,179
characters in in typical scenarios so we

673
00:24:43,140 --> 00:24:47,100
saw a lot of this in a lot more depth in

674
00:24:45,179 --> 00:24:49,559
the make more series and here if I just

675
00:24:47,100 --> 00:24:53,340
run this then we currently get the

676
00:24:49,559 --> 00:24:55,200
predictions the scores the logits for

677
00:24:53,340 --> 00:24:56,820
every one of the four by eight positions

678
00:24:55,200 --> 00:24:58,620
now that we've made predictions about

679
00:24:56,820 --> 00:25:00,480
what comes next we'd like to evaluate

680
00:24:58,620 --> 00:25:02,700
the loss function and so in make more

681
00:25:00,480 --> 00:25:04,559
series we saw that a good way to measure

682
00:25:02,700 --> 00:25:06,720
a loss or like a quality of the

683
00:25:04,559 --> 00:25:08,640
predictions is to use the negative log

684
00:25:06,720 --> 00:25:10,140
likelihood loss which is also

685
00:25:08,640 --> 00:25:11,760
implemented in pytorch under the name

686
00:25:10,140 --> 00:25:14,460
cross entropy

687
00:25:11,760 --> 00:25:17,039
so what we'd like to do here is

688
00:25:14,460 --> 00:25:19,200
loss is the cross entropy on the

689
00:25:17,039 --> 00:25:21,360
predictions and the targets and so this

690
00:25:19,200 --> 00:25:24,059
measures the quality of the logits with

691
00:25:21,360 --> 00:25:25,440
respect to the Targets in other words we

692
00:25:24,059 --> 00:25:28,320
have the identity of the next character

693
00:25:25,440 --> 00:25:30,419
so how well are we predicting the next

694
00:25:28,320 --> 00:25:32,520
character based on Illusions and

695
00:25:30,419 --> 00:25:36,240
intuitively the correct

696
00:25:32,520 --> 00:25:37,679
um the correct dimension of logits uh

697
00:25:36,240 --> 00:25:39,480
depending on whatever the target is

698
00:25:37,679 --> 00:25:40,919
should have a very high number and all

699
00:25:39,480 --> 00:25:42,539
the other dimensions should be very low

700
00:25:40,919 --> 00:25:44,159
number right

701
00:25:42,539 --> 00:25:46,260
now the issue is that this won't

702
00:25:44,159 --> 00:25:50,539
actually this is what we want we want to

703
00:25:46,260 --> 00:25:50,539
basically output the logits and the loss

704
00:25:51,000 --> 00:25:55,740
this is what we want but unfortunately

705
00:25:52,700 --> 00:25:58,080
uh this won't actually run

706
00:25:55,740 --> 00:26:01,320
we get an error message but intuitively

707
00:25:58,080 --> 00:26:04,620
we want to measure this now when we go

708
00:26:01,320 --> 00:26:06,960
to the pi torch cross entropy

709
00:26:04,620 --> 00:26:07,500
a documentation here

710
00:26:06,960 --> 00:26:09,480
um

711
00:26:07,500 --> 00:26:11,580
we're trying to call the cross entropy

712
00:26:09,480 --> 00:26:13,200
in its functional form so that means we

713
00:26:11,580 --> 00:26:14,159
don't have to create like a module for

714
00:26:13,200 --> 00:26:16,679
it

715
00:26:14,159 --> 00:26:18,360
but here when we go to the documentation

716
00:26:16,679 --> 00:26:20,580
you have to look into the details of how

717
00:26:18,360 --> 00:26:22,799
pytorch expects these inputs and

718
00:26:20,580 --> 00:26:24,900
basically the issue here is by torch

719
00:26:22,799 --> 00:26:26,880
expects if you have multi-dimensional

720
00:26:24,900 --> 00:26:29,460
input which we do because we have a b by

721
00:26:26,880 --> 00:26:32,700
T by C tensor then it actually really

722
00:26:29,460 --> 00:26:34,679
wants the channels to be the second

723
00:26:32,700 --> 00:26:38,580
dimension here

724
00:26:34,679 --> 00:26:42,000
so if you um so basically it wants a b

725
00:26:38,580 --> 00:26:44,100
by C by T instead of a b by T by C

726
00:26:42,000 --> 00:26:45,900
and so it's just the details of how

727
00:26:44,100 --> 00:26:49,200
pytorch treats

728
00:26:45,900 --> 00:26:51,360
um these kinds of inputs and so we don't

729
00:26:49,200 --> 00:26:52,679
actually want to deal with that so what

730
00:26:51,360 --> 00:26:54,840
we're going to do instead is we need to

731
00:26:52,679 --> 00:26:56,100
basically reshape our logits so here's

732
00:26:54,840 --> 00:26:58,140
what I like to do I like to take

733
00:26:56,100 --> 00:27:01,080
basically give names to the dimensions

734
00:26:58,140 --> 00:27:02,520
so launches.shape is B by T by C and

735
00:27:01,080 --> 00:27:05,460
unpack those numbers

736
00:27:02,520 --> 00:27:07,020
and then let's say that logits equals

737
00:27:05,460 --> 00:27:09,720
logits.view

738
00:27:07,020 --> 00:27:13,620
and we want it to be a b times c b times

739
00:27:09,720 --> 00:27:15,960
T by C so just a two-dimensional array

740
00:27:13,620 --> 00:27:17,940
right so we're going to take all the

741
00:27:15,960 --> 00:27:18,840
we're going to take all of these

742
00:27:17,940 --> 00:27:20,820
um

743
00:27:18,840 --> 00:27:22,200
positions here and we're going to uh

744
00:27:20,820 --> 00:27:23,640
stretch them out in a one-dimensional

745
00:27:22,200 --> 00:27:25,799
sequence

746
00:27:23,640 --> 00:27:27,779
and preserve the channel Dimension as

747
00:27:25,799 --> 00:27:29,039
the second dimension

748
00:27:27,779 --> 00:27:30,480
so we're just kind of like stretching

749
00:27:29,039 --> 00:27:32,039
out the array so it's two-dimensional

750
00:27:30,480 --> 00:27:34,799
and in that case it's going to better

751
00:27:32,039 --> 00:27:36,179
conform to what pi torch sort of expects

752
00:27:34,799 --> 00:27:38,159
in its dimensions

753
00:27:36,179 --> 00:27:40,340
now we have to do the same to targets

754
00:27:38,159 --> 00:27:45,600
because currently targets

755
00:27:40,340 --> 00:27:48,900
are of shape B by T and we want it to be

756
00:27:45,600 --> 00:27:50,340
just B times T so one dimensional now

757
00:27:48,900 --> 00:27:53,220
alternatively you could always still

758
00:27:50,340 --> 00:27:54,659
just do -1 because Pi torch will guess

759
00:27:53,220 --> 00:27:56,640
what this should be if you want to lay

760
00:27:54,659 --> 00:27:58,080
it out but let me just be explicit on

761
00:27:56,640 --> 00:28:00,419
say Q times t

762
00:27:58,080 --> 00:28:02,760
once we've reshaped this it will match

763
00:28:00,419 --> 00:28:04,200
the cross entropy case

764
00:28:02,760 --> 00:28:06,740
and then we should be able to evaluate

765
00:28:04,200 --> 00:28:06,740
our loss

766
00:28:06,960 --> 00:28:12,240
okay so with that right now and we can

767
00:28:09,779 --> 00:28:14,460
do loss and So currently we see that the

768
00:28:12,240 --> 00:28:17,760
loss is 4.87

769
00:28:14,460 --> 00:28:19,679
now because our we have 65 possible

770
00:28:17,760 --> 00:28:22,020
vocabulary elements we can actually

771
00:28:19,679 --> 00:28:22,980
guess at what the loss should be and in

772
00:28:22,020 --> 00:28:25,260
particular

773
00:28:22,980 --> 00:28:28,440
we covered negative log likelihood in a

774
00:28:25,260 --> 00:28:30,419
lot of detail we are expecting log or

775
00:28:28,440 --> 00:28:33,840
long of

776
00:28:30,419 --> 00:28:35,960
um 1 over 65 and negative of that

777
00:28:33,840 --> 00:28:39,539
so we're expecting the loss to be about

778
00:28:35,960 --> 00:28:40,799
4.1217 but we're getting 4.87 and so

779
00:28:39,539 --> 00:28:42,779
that's telling us that the initial

780
00:28:40,799 --> 00:28:44,940
predictions are not super diffuse

781
00:28:42,779 --> 00:28:46,679
they've got a little bit of entropy and

782
00:28:44,940 --> 00:28:50,580
so we're guessing wrong

783
00:28:46,679 --> 00:28:53,220
uh so uh yes but actually we're I able

784
00:28:50,580 --> 00:28:55,200
we are able to evaluate the loss okay so

785
00:28:53,220 --> 00:28:57,900
now that we can evaluate the quality of

786
00:28:55,200 --> 00:28:59,580
the model on some data we'd likely also

787
00:28:57,900 --> 00:29:01,860
be able to generate from the model so

788
00:28:59,580 --> 00:29:03,059
let's do the generation now I'm going to

789
00:29:01,860 --> 00:29:04,799
go again a little bit faster here

790
00:29:03,059 --> 00:29:06,600
because I covered all this already in

791
00:29:04,799 --> 00:29:08,279
previous videos

792
00:29:06,600 --> 00:29:12,020
so

793
00:29:08,279 --> 00:29:12,020
here's a generate function for the model

794
00:29:12,299 --> 00:29:17,279
so we take some uh we take the the same

795
00:29:15,360 --> 00:29:19,320
kind of input idx here

796
00:29:17,279 --> 00:29:22,620
and basically

797
00:29:19,320 --> 00:29:25,679
this is the current context of some

798
00:29:22,620 --> 00:29:28,440
characters in a batch in some batch

799
00:29:25,679 --> 00:29:30,720
so it's also B by T and the job of

800
00:29:28,440 --> 00:29:32,399
generate is to basically take this B by

801
00:29:30,720 --> 00:29:34,320
T and extend it to be B by T plus one

802
00:29:32,399 --> 00:29:36,299
plus two plus three and so it's just

803
00:29:34,320 --> 00:29:38,100
basically it contains the generation in

804
00:29:36,299 --> 00:29:39,179
all the batch dimensions in the time

805
00:29:38,100 --> 00:29:41,399
dimension

806
00:29:39,179 --> 00:29:42,960
So that's its job and we'll do that for

807
00:29:41,399 --> 00:29:44,340
Max new tokens

808
00:29:42,960 --> 00:29:46,140
so you can see here on the bottom

809
00:29:44,340 --> 00:29:48,899
there's going to be some stuff here but

810
00:29:46,140 --> 00:29:51,899
on the bottom whatever is predicted is

811
00:29:48,899 --> 00:29:53,580
concatenated on top of the previous idx

812
00:29:51,899 --> 00:29:55,559
along the First Dimension which is the

813
00:29:53,580 --> 00:29:56,520
time Dimension to create a b by T plus

814
00:29:55,559 --> 00:29:58,799
one

815
00:29:56,520 --> 00:30:00,840
so that becomes the new idx so the job

816
00:29:58,799 --> 00:30:03,539
of generators to take a b by T and make

817
00:30:00,840 --> 00:30:05,940
it a b by T plus one plus two plus three

818
00:30:03,539 --> 00:30:08,159
as many as we want maximum tokens so

819
00:30:05,940 --> 00:30:09,779
this is the generation from the model

820
00:30:08,159 --> 00:30:11,460
now inside the generation what we're

821
00:30:09,779 --> 00:30:13,440
what are we doing we're taking the

822
00:30:11,460 --> 00:30:16,740
current indices we're getting the

823
00:30:13,440 --> 00:30:17,940
predictions so we get those are in the

824
00:30:16,740 --> 00:30:19,740
logits

825
00:30:17,940 --> 00:30:20,940
and then the loss here is going to be

826
00:30:19,740 --> 00:30:22,620
ignored because

827
00:30:20,940 --> 00:30:24,539
um we're not we're not using that and we

828
00:30:22,620 --> 00:30:26,220
have no targets that are sort of ground

829
00:30:24,539 --> 00:30:28,440
truth targets that we're going to be

830
00:30:26,220 --> 00:30:30,659
comparing with

831
00:30:28,440 --> 00:30:33,600
then once we get the logits we are only

832
00:30:30,659 --> 00:30:36,000
focusing on the last step so instead of

833
00:30:33,600 --> 00:30:38,940
a b by T by C we're going to pluck out

834
00:30:36,000 --> 00:30:40,020
the negative one the last element in the

835
00:30:38,940 --> 00:30:41,279
time dimension

836
00:30:40,020 --> 00:30:42,659
because those are the predictions for

837
00:30:41,279 --> 00:30:44,580
what comes next

838
00:30:42,659 --> 00:30:47,760
so that this is the logits which we then

839
00:30:44,580 --> 00:30:49,200
convert to probabilities via softmax and

840
00:30:47,760 --> 00:30:51,059
then we use torch that multinomial to

841
00:30:49,200 --> 00:30:53,760
sample from those probabilities and we

842
00:30:51,059 --> 00:30:56,700
ask by torch to give us one sample

843
00:30:53,760 --> 00:30:59,340
and so idx next will become a b by one

844
00:30:56,700 --> 00:31:01,260
because in each one of the batch

845
00:30:59,340 --> 00:31:03,059
Dimensions we're going to have a single

846
00:31:01,260 --> 00:31:05,580
prediction for what comes next so this

847
00:31:03,059 --> 00:31:07,200
num samples equals one will make this be

848
00:31:05,580 --> 00:31:08,460
a one

849
00:31:07,200 --> 00:31:10,559
and then we're going to take those

850
00:31:08,460 --> 00:31:11,940
integers that come from the sampling

851
00:31:10,559 --> 00:31:13,440
process according to the probability

852
00:31:11,940 --> 00:31:15,480
distribution given here

853
00:31:13,440 --> 00:31:17,039
and those integers got just concatenated

854
00:31:15,480 --> 00:31:19,559
on top of the current sort of like

855
00:31:17,039 --> 00:31:21,600
running stream of integers and this

856
00:31:19,559 --> 00:31:24,179
gives us a p by T plus one

857
00:31:21,600 --> 00:31:26,460
and then we can return that now one

858
00:31:24,179 --> 00:31:29,580
thing here is you see how I'm calling

859
00:31:26,460 --> 00:31:31,799
self of idx which will end up going to

860
00:31:29,580 --> 00:31:33,960
the forward function I'm not providing

861
00:31:31,799 --> 00:31:36,779
any Targets So currently this would give

862
00:31:33,960 --> 00:31:39,299
an error because targets is uh is uh

863
00:31:36,779 --> 00:31:41,399
sort of like not given so target has to

864
00:31:39,299 --> 00:31:44,640
be optional so targets is none by

865
00:31:41,399 --> 00:31:47,580
default and then if targets is none then

866
00:31:44,640 --> 00:31:50,520
there's no loss to create so it's just

867
00:31:47,580 --> 00:31:52,919
loss is none but else all of this

868
00:31:50,520 --> 00:31:54,899
happens and we can create a loss

869
00:31:52,919 --> 00:31:55,500
so this will make it so

870
00:31:54,899 --> 00:31:57,179
um

871
00:31:55,500 --> 00:31:58,740
if we have the targets we provide them

872
00:31:57,179 --> 00:32:01,440
and get a loss if we have no targets

873
00:31:58,740 --> 00:32:03,240
we'll just get the logits

874
00:32:01,440 --> 00:32:04,620
so this here will generate from the

875
00:32:03,240 --> 00:32:09,380
model

876
00:32:04,620 --> 00:32:09,380
um and let's take that for a ride now

877
00:32:09,419 --> 00:32:12,480
oops

878
00:32:10,679 --> 00:32:14,340
so I have another code chunk here which

879
00:32:12,480 --> 00:32:16,380
will generate for the model from the

880
00:32:14,340 --> 00:32:19,260
model and okay this is kind of crazy so

881
00:32:16,380 --> 00:32:23,059
maybe let me let me break this down

882
00:32:19,260 --> 00:32:23,059
so these are the idx right

883
00:32:24,779 --> 00:32:29,640
I'm creating a batch will be just one

884
00:32:27,360 --> 00:32:31,500
time will be just one

885
00:32:29,640 --> 00:32:34,080
so I'm creating a little one by one

886
00:32:31,500 --> 00:32:37,200
tensor and it's holding a zero

887
00:32:34,080 --> 00:32:39,299
and the D type the data type is integer

888
00:32:37,200 --> 00:32:42,840
so 0 is going to be how we kick off the

889
00:32:39,299 --> 00:32:45,120
generation and remember that zero is uh

890
00:32:42,840 --> 00:32:46,500
is the element standing for a new line

891
00:32:45,120 --> 00:32:48,480
character so it's kind of like a

892
00:32:46,500 --> 00:32:50,760
reasonable thing to to feed in as the

893
00:32:48,480 --> 00:32:52,799
very first character in a sequence to be

894
00:32:50,760 --> 00:32:55,260
the new line

895
00:32:52,799 --> 00:32:56,640
um so it's going to be idx which we're

896
00:32:55,260 --> 00:32:58,620
going to feed in here then we're going

897
00:32:56,640 --> 00:33:00,600
to ask for 100 tokens

898
00:32:58,620 --> 00:33:01,559
and then enter generate will continue

899
00:33:00,600 --> 00:33:05,220
that

900
00:33:01,559 --> 00:33:07,679
now because uh generate works on the

901
00:33:05,220 --> 00:33:10,500
level of batches we then have to index

902
00:33:07,679 --> 00:33:11,760
into the zero throw to basically unplug

903
00:33:10,500 --> 00:33:14,940
the um

904
00:33:11,760 --> 00:33:18,779
the single bash Dimension that exists

905
00:33:14,940 --> 00:33:20,820
and then that gives us a um

906
00:33:18,779 --> 00:33:22,919
time steps it's just a one-dimensional

907
00:33:20,820 --> 00:33:25,380
array of all the indices which we will

908
00:33:22,919 --> 00:33:28,260
convert to simple python list

909
00:33:25,380 --> 00:33:31,279
from pytorch tensor so that that can

910
00:33:28,260 --> 00:33:33,960
feed into our decode function and

911
00:33:31,279 --> 00:33:36,000
convert those integers into text

912
00:33:33,960 --> 00:33:38,760
so let me bring this back and we're

913
00:33:36,000 --> 00:33:41,100
generating 100 tokens let's run

914
00:33:38,760 --> 00:33:43,559
and uh here's the generation that we

915
00:33:41,100 --> 00:33:45,059
achieved so obviously it's garbage and

916
00:33:43,559 --> 00:33:47,100
the reason it's garbage is because this

917
00:33:45,059 --> 00:33:49,200
is a totally random model so next up

918
00:33:47,100 --> 00:33:50,580
we're going to want to train this model

919
00:33:49,200 --> 00:33:52,019
now one more thing I wanted to point out

920
00:33:50,580 --> 00:33:53,820
here is

921
00:33:52,019 --> 00:33:56,039
this function is written to be General

922
00:33:53,820 --> 00:33:57,960
but it's kind of like ridiculous right

923
00:33:56,039 --> 00:33:59,760
now because

924
00:33:57,960 --> 00:34:02,159
we're feeding in all this we're building

925
00:33:59,760 --> 00:34:04,799
out this context and we're concatenating

926
00:34:02,159 --> 00:34:06,659
it all and we're always feeding it all

927
00:34:04,799 --> 00:34:08,460
into the model

928
00:34:06,659 --> 00:34:09,899
but that's kind of ridiculous because

929
00:34:08,460 --> 00:34:11,639
this is just a simple background model

930
00:34:09,899 --> 00:34:14,700
so to make for example this prediction

931
00:34:11,639 --> 00:34:16,020
about K we only needed this W but

932
00:34:14,700 --> 00:34:18,359
actually what we fed into the model is

933
00:34:16,020 --> 00:34:20,760
we fed the entire sequence and then we

934
00:34:18,359 --> 00:34:22,139
only looked at the very last piece and

935
00:34:20,760 --> 00:34:24,060
predicted k

936
00:34:22,139 --> 00:34:26,099
so the only reason I'm writing it in

937
00:34:24,060 --> 00:34:28,080
this way is because right now this is a

938
00:34:26,099 --> 00:34:30,379
bygram model but I'd like to keep this

939
00:34:28,080 --> 00:34:34,040
function fixed and I'd like it to work

940
00:34:30,379 --> 00:34:36,599
later when our character is actually

941
00:34:34,040 --> 00:34:38,760
basically look further in the history

942
00:34:36,599 --> 00:34:41,460
and so right now the history is not used

943
00:34:38,760 --> 00:34:43,919
so this looks silly but eventually the

944
00:34:41,460 --> 00:34:46,260
history will be used and so that's why

945
00:34:43,919 --> 00:34:48,899
we want to do it this way so just a

946
00:34:46,260 --> 00:34:51,419
quick comment on that so now we see that

947
00:34:48,899 --> 00:34:53,220
this is um random so let's train the

948
00:34:51,419 --> 00:34:55,679
model so it becomes a bit less random

949
00:34:53,220 --> 00:34:56,879
okay let's Now train the model so first

950
00:34:55,679 --> 00:34:59,700
what I'm going to do is I'm going to

951
00:34:56,879 --> 00:35:02,400
create a pytorch optimization object

952
00:34:59,700 --> 00:35:03,680
so here we are using the optimizer

953
00:35:02,400 --> 00:35:06,000
atom W

954
00:35:03,680 --> 00:35:07,500
now in the make more series we've only

955
00:35:06,000 --> 00:35:09,240
ever used stochastic gradient descent

956
00:35:07,500 --> 00:35:12,000
the simplest possible Optimizer which

957
00:35:09,240 --> 00:35:13,200
you can get using the SGD instead but I

958
00:35:12,000 --> 00:35:15,420
want to use Adam which is a much more

959
00:35:13,200 --> 00:35:18,720
advanced and popular Optimizer and it

960
00:35:15,420 --> 00:35:20,339
works extremely well for a typical good

961
00:35:18,720 --> 00:35:23,280
setting for the learning rate is roughly

962
00:35:20,339 --> 00:35:25,079
3E negative four but for very very small

963
00:35:23,280 --> 00:35:26,579
networks luck is the case here you can

964
00:35:25,079 --> 00:35:28,200
get away with much much higher learning

965
00:35:26,579 --> 00:35:29,640
rates running negative three or even

966
00:35:28,200 --> 00:35:32,280
higher probably

967
00:35:29,640 --> 00:35:34,320
but let me create the optimizer object

968
00:35:32,280 --> 00:35:36,240
which will basically take the gradients

969
00:35:34,320 --> 00:35:37,680
and update the parameters using the

970
00:35:36,240 --> 00:35:39,540
gradients

971
00:35:37,680 --> 00:35:41,640
and then here

972
00:35:39,540 --> 00:35:42,839
our batch size up above was only four so

973
00:35:41,640 --> 00:35:45,180
let me actually use something bigger

974
00:35:42,839 --> 00:35:46,260
let's say 32 and then for some number of

975
00:35:45,180 --> 00:35:48,300
steps

976
00:35:46,260 --> 00:35:51,900
um we are sampling a new batch of data

977
00:35:48,300 --> 00:35:53,040
we're evaluating the loss we're zeroing

978
00:35:51,900 --> 00:35:55,320
out all the gradients from the previous

979
00:35:53,040 --> 00:35:57,060
step getting the gradients for all the

980
00:35:55,320 --> 00:35:58,980
parameters and then using those

981
00:35:57,060 --> 00:36:01,140
gradients to update our parameters so

982
00:35:58,980 --> 00:36:02,520
typical training loop as we saw in the

983
00:36:01,140 --> 00:36:05,460
make more series

984
00:36:02,520 --> 00:36:07,560
so let me now uh run this

985
00:36:05,460 --> 00:36:10,640
for say 100 iterations and let's see

986
00:36:07,560 --> 00:36:10,640
what kind of losses we're gonna get

987
00:36:10,680 --> 00:36:15,720
so we started around 4.7

988
00:36:13,560 --> 00:36:16,920
and now we're going to down to like 4.6

989
00:36:15,720 --> 00:36:18,900
4.5

990
00:36:16,920 --> 00:36:20,640
Etc so the optimization is definitely

991
00:36:18,900 --> 00:36:23,760
happening but

992
00:36:20,640 --> 00:36:25,380
um let's uh sort of try to increase the

993
00:36:23,760 --> 00:36:26,880
number of iterations and only print at

994
00:36:25,380 --> 00:36:28,200
the end

995
00:36:26,880 --> 00:36:30,300
because we probably will not train for

996
00:36:28,200 --> 00:36:34,460
longer

997
00:36:30,300 --> 00:36:34,460
okay so we're down to 3.6 roughly

998
00:36:35,460 --> 00:36:38,720
roughly down to three

999
00:36:41,520 --> 00:36:45,200
this is the most janky optimization

1000
00:36:47,160 --> 00:36:51,380
okay it's working let's just do ten

1001
00:36:48,960 --> 00:36:51,380
thousand

1002
00:36:51,420 --> 00:36:57,180
and then from here we want to copy this

1003
00:36:55,200 --> 00:36:58,380
and hopefully we're going to get

1004
00:36:57,180 --> 00:37:00,119
something reasonable and of course it's

1005
00:36:58,380 --> 00:37:01,740
not going to be Shakespeare from a

1006
00:37:00,119 --> 00:37:05,040
background model but at least we see

1007
00:37:01,740 --> 00:37:06,180
that the loss is improving and hopefully

1008
00:37:05,040 --> 00:37:07,680
we're expecting something a bit more

1009
00:37:06,180 --> 00:37:09,960
reasonable

1010
00:37:07,680 --> 00:37:11,520
okay so we're down there about 2.5 ish

1011
00:37:09,960 --> 00:37:12,599
let's see what we get

1012
00:37:11,520 --> 00:37:14,520
okay

1013
00:37:12,599 --> 00:37:15,660
dramatic improvements certainly on what

1014
00:37:14,520 --> 00:37:18,000
we had here

1015
00:37:15,660 --> 00:37:19,020
so let me just increase the number of

1016
00:37:18,000 --> 00:37:20,520
tokens

1017
00:37:19,020 --> 00:37:22,740
okay so we see that we're starting to

1018
00:37:20,520 --> 00:37:24,599
get something at least like

1019
00:37:22,740 --> 00:37:26,700
reasonable ish

1020
00:37:24,599 --> 00:37:29,700
um

1021
00:37:26,700 --> 00:37:31,619
certainly not Shakespeare but the model

1022
00:37:29,700 --> 00:37:34,079
is making progress so that is the

1023
00:37:31,619 --> 00:37:37,560
simplest possible model

1024
00:37:34,079 --> 00:37:39,480
so now what I'd like to do is

1025
00:37:37,560 --> 00:37:40,980
obviously that this is a very simple

1026
00:37:39,480 --> 00:37:42,660
model because the tokens are not talking

1027
00:37:40,980 --> 00:37:45,359
to each other so given the previous

1028
00:37:42,660 --> 00:37:46,680
context of whatever was generated we're

1029
00:37:45,359 --> 00:37:48,180
only looking at the very last character

1030
00:37:46,680 --> 00:37:50,940
to make the predictions about what comes

1031
00:37:48,180 --> 00:37:53,220
next so now these uh now these tokens

1032
00:37:50,940 --> 00:37:55,500
have to start talking to each other and

1033
00:37:53,220 --> 00:37:56,520
figuring out what is in the context so

1034
00:37:55,500 --> 00:37:58,140
that they can make better predictions

1035
00:37:56,520 --> 00:38:00,180
for what comes next and this is how

1036
00:37:58,140 --> 00:38:02,400
we're going to kick off the Transformer

1037
00:38:00,180 --> 00:38:03,900
okay so next I took the code that we

1038
00:38:02,400 --> 00:38:06,119
developed in this Jupiter notebook and I

1039
00:38:03,900 --> 00:38:08,220
converted it to be a script and I'm

1040
00:38:06,119 --> 00:38:10,260
doing this because I just want to

1041
00:38:08,220 --> 00:38:11,760
simplify our intermediate work into just

1042
00:38:10,260 --> 00:38:13,079
the final product that we have at this

1043
00:38:11,760 --> 00:38:15,720
point

1044
00:38:13,079 --> 00:38:17,099
so in the top here I put all the hyper

1045
00:38:15,720 --> 00:38:18,480
parameters that we've defined I

1046
00:38:17,099 --> 00:38:20,820
introduced a few and I'm going to speak

1047
00:38:18,480 --> 00:38:21,839
to that in a little bit otherwise a lot

1048
00:38:20,820 --> 00:38:24,000
of this should be recognizable

1049
00:38:21,839 --> 00:38:26,700
reproducibility

1050
00:38:24,000 --> 00:38:29,820
read data get the encoder in the decoder

1051
00:38:26,700 --> 00:38:32,820
create the training test splits I use

1052
00:38:29,820 --> 00:38:35,760
the uh kind of like data loader that

1053
00:38:32,820 --> 00:38:37,859
gets a batch of the inputs and targets

1054
00:38:35,760 --> 00:38:38,940
this is new and I'll talk about it in a

1055
00:38:37,859 --> 00:38:40,260
second

1056
00:38:38,940 --> 00:38:42,420
now this is the background language

1057
00:38:40,260 --> 00:38:44,220
model that we developed and it can

1058
00:38:42,420 --> 00:38:46,619
forward and give us a logits and loss

1059
00:38:44,220 --> 00:38:48,420
and it can generate

1060
00:38:46,619 --> 00:38:51,839
and then here we are creating the

1061
00:38:48,420 --> 00:38:53,640
optimizer and this is the training Loop

1062
00:38:51,839 --> 00:38:55,500
so everything here should look pretty

1063
00:38:53,640 --> 00:38:58,079
familiar now some of the small things

1064
00:38:55,500 --> 00:39:00,420
that I added number one I added the

1065
00:38:58,079 --> 00:39:02,940
ability to run on a GPU if you have it

1066
00:39:00,420 --> 00:39:05,220
so if you have a GPU then you can this

1067
00:39:02,940 --> 00:39:07,619
will use Cuda instead of just CPU and

1068
00:39:05,220 --> 00:39:09,960
everything will be a lot more faster now

1069
00:39:07,619 --> 00:39:11,400
when device becomes screwed up then we

1070
00:39:09,960 --> 00:39:13,920
need to make sure that when we load the

1071
00:39:11,400 --> 00:39:16,740
data we move it to device

1072
00:39:13,920 --> 00:39:19,020
when we create the model we want to move

1073
00:39:16,740 --> 00:39:21,720
the model parameters to device

1074
00:39:19,020 --> 00:39:23,400
so as an example here we have the NN

1075
00:39:21,720 --> 00:39:26,400
embedding table and it's got a double

1076
00:39:23,400 --> 00:39:28,260
weight inside it which stores the sort

1077
00:39:26,400 --> 00:39:30,240
of lookup table so that would be moved

1078
00:39:28,260 --> 00:39:32,640
to the GPU so that all the calculations

1079
00:39:30,240 --> 00:39:33,900
here happen on the GPU and they can be a

1080
00:39:32,640 --> 00:39:35,339
lot faster

1081
00:39:33,900 --> 00:39:37,800
and then finally here when I'm creating

1082
00:39:35,339 --> 00:39:39,180
the context that feeds into generate I

1083
00:39:37,800 --> 00:39:40,320
have to make sure that I create on the

1084
00:39:39,180 --> 00:39:43,220
device

1085
00:39:40,320 --> 00:39:47,520
number two what I introduced is

1086
00:39:43,220 --> 00:39:50,820
the fact that here in the training Loop

1087
00:39:47,520 --> 00:39:51,720
here I was just printing the Lost dot

1088
00:39:50,820 --> 00:39:53,700
item

1089
00:39:51,720 --> 00:39:54,900
inside the training Loop but this is a

1090
00:39:53,700 --> 00:39:57,300
very noisy measurement of the current

1091
00:39:54,900 --> 00:39:58,440
loss because every batch will be more or

1092
00:39:57,300 --> 00:40:02,160
less lucky

1093
00:39:58,440 --> 00:40:04,920
and so what I want to do usually is I

1094
00:40:02,160 --> 00:40:07,800
have an estimate loss function and the

1095
00:40:04,920 --> 00:40:08,880
estimated loss basically then goes up

1096
00:40:07,800 --> 00:40:11,940
here

1097
00:40:08,880 --> 00:40:13,560
and it averages up the loss over

1098
00:40:11,940 --> 00:40:15,720
multiple batches

1099
00:40:13,560 --> 00:40:18,180
so in particular we're going to iterate

1100
00:40:15,720 --> 00:40:19,859
invalider times and we're going to

1101
00:40:18,180 --> 00:40:21,660
basically get our loss and then we're

1102
00:40:19,859 --> 00:40:23,760
going to get the average loss for both

1103
00:40:21,660 --> 00:40:25,020
splits and so this will be a lot less

1104
00:40:23,760 --> 00:40:27,000
noisy

1105
00:40:25,020 --> 00:40:28,920
so here what we call the estimate loss

1106
00:40:27,000 --> 00:40:31,920
we're going to report the pretty

1107
00:40:28,920 --> 00:40:34,020
accurate train and validation loss

1108
00:40:31,920 --> 00:40:35,880
now when we come back up you'll notice a

1109
00:40:34,020 --> 00:40:38,460
few things here I'm setting the model to

1110
00:40:35,880 --> 00:40:40,740
evaluation phase and down here I'm

1111
00:40:38,460 --> 00:40:42,960
resetting it back to training phase

1112
00:40:40,740 --> 00:40:43,800
now right now for our model as is this

1113
00:40:42,960 --> 00:40:46,020
this doesn't actually do anything

1114
00:40:43,800 --> 00:40:49,619
because the only thing inside this model

1115
00:40:46,020 --> 00:40:52,920
is this nn.embedding and

1116
00:40:49,619 --> 00:40:54,540
um this this network would behave both

1117
00:40:52,920 --> 00:40:56,760
would be have the same in both

1118
00:40:54,540 --> 00:40:58,140
evaluation mode and training mode we

1119
00:40:56,760 --> 00:41:00,420
have no Dropout layers we have no

1120
00:40:58,140 --> 00:41:02,880
bathroom layers Etc but it is a good

1121
00:41:00,420 --> 00:41:05,160
practice to Think Through what mode your

1122
00:41:02,880 --> 00:41:07,380
neural network is in because some layers

1123
00:41:05,160 --> 00:41:09,480
will have different Behavior at

1124
00:41:07,380 --> 00:41:10,980
inference time or training time

1125
00:41:09,480 --> 00:41:12,720
and

1126
00:41:10,980 --> 00:41:14,700
there's also this context manager

1127
00:41:12,720 --> 00:41:16,079
torch.nograd and this is just telling

1128
00:41:14,700 --> 00:41:18,359
pytorch that everything that happens

1129
00:41:16,079 --> 00:41:21,660
inside this function we will not call

1130
00:41:18,359 --> 00:41:23,099
that backward on and so Patrick can be a

1131
00:41:21,660 --> 00:41:25,260
lot more efficient with its memory use

1132
00:41:23,099 --> 00:41:27,180
because it doesn't have to store all the

1133
00:41:25,260 --> 00:41:29,400
intermediate variables because we're

1134
00:41:27,180 --> 00:41:30,720
never going to call backward and so it

1135
00:41:29,400 --> 00:41:32,700
can it can be a lot more memory

1136
00:41:30,720 --> 00:41:35,280
efficient in that way so also a good

1137
00:41:32,700 --> 00:41:37,500
practice to tell Pi torch when we don't

1138
00:41:35,280 --> 00:41:41,520
intend to do back propagation

1139
00:41:37,500 --> 00:41:43,680
so right now the script is about 120

1140
00:41:41,520 --> 00:41:45,119
lines of code of and that's kind of our

1141
00:41:43,680 --> 00:41:47,400
starter code

1142
00:41:45,119 --> 00:41:49,500
I'm calling it background.pi and I'm

1143
00:41:47,400 --> 00:41:52,260
going to release it later now running

1144
00:41:49,500 --> 00:41:53,640
this script gives us output in the

1145
00:41:52,260 --> 00:41:54,660
terminal and it looks something like

1146
00:41:53,640 --> 00:41:57,599
this

1147
00:41:54,660 --> 00:41:59,460
it basically as I ran this code it was

1148
00:41:57,599 --> 00:42:01,200
giving me the train loss and Val loss

1149
00:41:59,460 --> 00:42:02,760
and we see that we convert to somewhere

1150
00:42:01,200 --> 00:42:05,040
around 2.5

1151
00:42:02,760 --> 00:42:08,280
with the migrant model and then here's

1152
00:42:05,040 --> 00:42:10,200
the sample that we produced at the end

1153
00:42:08,280 --> 00:42:11,940
and so we have everything packaged up in

1154
00:42:10,200 --> 00:42:14,160
the script and we're in a good position

1155
00:42:11,940 --> 00:42:16,320
now to iterate on this okay so we are

1156
00:42:14,160 --> 00:42:18,420
almost ready to start writing our very

1157
00:42:16,320 --> 00:42:21,000
first self-attention block for

1158
00:42:18,420 --> 00:42:24,000
processing these tokens

1159
00:42:21,000 --> 00:42:26,099
now before we actually get there I want

1160
00:42:24,000 --> 00:42:27,660
to get you used to a mathematical trick

1161
00:42:26,099 --> 00:42:29,940
that is used in the self attention

1162
00:42:27,660 --> 00:42:32,160
inside a Transformer and is really just

1163
00:42:29,940 --> 00:42:34,980
like at the heart of an efficient

1164
00:42:32,160 --> 00:42:36,540
implementation of self-attention and so

1165
00:42:34,980 --> 00:42:38,700
I want to work with this toy example you

1166
00:42:36,540 --> 00:42:40,079
just get used to this operation and then

1167
00:42:38,700 --> 00:42:43,680
it's going to make it much more clear

1168
00:42:40,079 --> 00:42:45,300
once we actually get to um to it in the

1169
00:42:43,680 --> 00:42:48,180
script again

1170
00:42:45,300 --> 00:42:49,980
so let's create a b by T by C where B T

1171
00:42:48,180 --> 00:42:52,800
and C are just 4 8 and 2 in the story

1172
00:42:49,980 --> 00:42:56,160
example and these are basically channels

1173
00:42:52,800 --> 00:42:57,599
and we have batches and we have the time

1174
00:42:56,160 --> 00:43:01,920
component and we have some information

1175
00:42:57,599 --> 00:43:03,720
at each point in the sequence so C

1176
00:43:01,920 --> 00:43:06,420
now what we would like to do is we would

1177
00:43:03,720 --> 00:43:09,060
like these um tokens so we have up to

1178
00:43:06,420 --> 00:43:10,619
eight tokens here in a batch and these

1179
00:43:09,060 --> 00:43:12,060
eight tokens are currently not talking

1180
00:43:10,619 --> 00:43:13,440
to each other and we would like them to

1181
00:43:12,060 --> 00:43:14,700
talk to each other we'd like to couple

1182
00:43:13,440 --> 00:43:18,060
them

1183
00:43:14,700 --> 00:43:20,339
and in particular we don't we we want to

1184
00:43:18,060 --> 00:43:22,020
couple them in a very specific way so

1185
00:43:20,339 --> 00:43:24,660
the token for example at the fifth

1186
00:43:22,020 --> 00:43:26,640
location it should not communicate with

1187
00:43:24,660 --> 00:43:27,660
tokens in the sixth seventh and eighth

1188
00:43:26,640 --> 00:43:30,060
location

1189
00:43:27,660 --> 00:43:30,900
because those are future tokens in the

1190
00:43:30,060 --> 00:43:32,640
sequence

1191
00:43:30,900 --> 00:43:34,500
the token on the fifth location should

1192
00:43:32,640 --> 00:43:36,000
only talk to the one in the fourth third

1193
00:43:34,500 --> 00:43:38,460
second and first

1194
00:43:36,000 --> 00:43:40,260
so it's only so information only flows

1195
00:43:38,460 --> 00:43:42,240
from previous context to the current

1196
00:43:40,260 --> 00:43:43,740
timestamp and we cannot get any

1197
00:43:42,240 --> 00:43:46,440
information from the future because we

1198
00:43:43,740 --> 00:43:47,700
are about to try to predict the future

1199
00:43:46,440 --> 00:43:49,920
so

1200
00:43:47,700 --> 00:43:52,680
what is the easiest way for tokens to

1201
00:43:49,920 --> 00:43:55,140
communicate okay the easiest way I would

1202
00:43:52,680 --> 00:43:56,760
say is okay if we are up to if we're a

1203
00:43:55,140 --> 00:43:58,980
fifth token and I'd like to communicate

1204
00:43:56,760 --> 00:44:01,800
with my past the simplest way we can do

1205
00:43:58,980 --> 00:44:04,980
that is to just do a weight is to just

1206
00:44:01,800 --> 00:44:07,020
do an average of all the um of all the

1207
00:44:04,980 --> 00:44:09,119
preceding elements so for example if I'm

1208
00:44:07,020 --> 00:44:12,240
the fifth token I would like to take the

1209
00:44:09,119 --> 00:44:14,700
channels that make up that are

1210
00:44:12,240 --> 00:44:16,440
information at my step but then also the

1211
00:44:14,700 --> 00:44:18,300
channels from the four step third step

1212
00:44:16,440 --> 00:44:20,460
second step in the first step I'd like

1213
00:44:18,300 --> 00:44:22,079
to average those up and then that would

1214
00:44:20,460 --> 00:44:24,839
become sort of like a feature Vector

1215
00:44:22,079 --> 00:44:25,740
that summarizes me in the context of my

1216
00:44:24,839 --> 00:44:28,020
history

1217
00:44:25,740 --> 00:44:30,060
now of course just doing a sum or like

1218
00:44:28,020 --> 00:44:31,740
an average is an extremely weak form of

1219
00:44:30,060 --> 00:44:33,780
interaction like this communication is

1220
00:44:31,740 --> 00:44:34,800
extremely lossy we've lost a ton of

1221
00:44:33,780 --> 00:44:37,440
information about the spatial

1222
00:44:34,800 --> 00:44:39,000
Arrangements of all those tokens but

1223
00:44:37,440 --> 00:44:40,920
that's okay for now we'll see how we can

1224
00:44:39,000 --> 00:44:43,200
bring that information back later

1225
00:44:40,920 --> 00:44:44,280
for now what we would like to do is

1226
00:44:43,200 --> 00:44:45,960
for every single batch element

1227
00:44:44,280 --> 00:44:48,720
independently

1228
00:44:45,960 --> 00:44:51,720
for every teeth token in that sequence

1229
00:44:48,720 --> 00:44:54,660
we'd like to now calculate the average

1230
00:44:51,720 --> 00:44:57,300
of all the vectors in all the previous

1231
00:44:54,660 --> 00:44:59,880
tokens and also at this token

1232
00:44:57,300 --> 00:45:01,740
so let's write that out

1233
00:44:59,880 --> 00:45:03,960
um I have a small snippet here and

1234
00:45:01,740 --> 00:45:06,540
instead of just fumbling around let me

1235
00:45:03,960 --> 00:45:08,280
just copy paste it and talk to it

1236
00:45:06,540 --> 00:45:09,720
so in other words we're going to create

1237
00:45:08,280 --> 00:45:12,060
X

1238
00:45:09,720 --> 00:45:15,119
and bow is short for backup words

1239
00:45:12,060 --> 00:45:16,260
because backup words is um is kind of

1240
00:45:15,119 --> 00:45:18,180
like um

1241
00:45:16,260 --> 00:45:19,800
a term that people use when you are just

1242
00:45:18,180 --> 00:45:22,440
averaging up things so it's just a bag

1243
00:45:19,800 --> 00:45:24,240
of words basically there's a word stored

1244
00:45:22,440 --> 00:45:25,800
on every one of these eight locations

1245
00:45:24,240 --> 00:45:27,359
and we're doing a bag of words such as

1246
00:45:25,800 --> 00:45:29,040
averaging

1247
00:45:27,359 --> 00:45:30,839
so in the beginning we're going to say

1248
00:45:29,040 --> 00:45:32,400
that it's just initialized at Zero and

1249
00:45:30,839 --> 00:45:33,900
then I'm doing a for Loop here so we're

1250
00:45:32,400 --> 00:45:36,180
not being efficient yet that's coming

1251
00:45:33,900 --> 00:45:37,619
but for now we're just iterating over

1252
00:45:36,180 --> 00:45:40,020
all the batch Dimensions independently

1253
00:45:37,619 --> 00:45:44,040
iterating over time

1254
00:45:40,020 --> 00:45:47,520
and then the previous tokens are at this

1255
00:45:44,040 --> 00:45:50,940
batch Dimension and then everything up

1256
00:45:47,520 --> 00:45:54,300
to and including the teeth token okay

1257
00:45:50,940 --> 00:45:56,640
so when we slice out X in this way xrev

1258
00:45:54,300 --> 00:45:59,220
Becomes of shape

1259
00:45:56,640 --> 00:46:02,220
um how many T elements there were in the

1260
00:45:59,220 --> 00:46:03,839
past and then of course C so all the two

1261
00:46:02,220 --> 00:46:05,160
dimensional information from these log

1262
00:46:03,839 --> 00:46:08,579
tokens

1263
00:46:05,160 --> 00:46:11,760
so that's the previous sort of chunk of

1264
00:46:08,579 --> 00:46:13,859
um tokens from my current sequence

1265
00:46:11,760 --> 00:46:15,900
and then I'm just doing the average or

1266
00:46:13,859 --> 00:46:18,420
the mean over the zeroth dimension so

1267
00:46:15,900 --> 00:46:20,700
I'm averaging out the time here

1268
00:46:18,420 --> 00:46:22,200
and I'm just going to get a little C

1269
00:46:20,700 --> 00:46:25,079
one-dimensional Vector which I'm going

1270
00:46:22,200 --> 00:46:28,079
to store in X background words

1271
00:46:25,079 --> 00:46:31,079
so I can run this and uh this is not

1272
00:46:28,079 --> 00:46:33,119
going to be very informative because

1273
00:46:31,079 --> 00:46:36,060
let's see so this is x sub 0. so this is

1274
00:46:33,119 --> 00:46:38,339
the zeroth batch element and then expo

1275
00:46:36,060 --> 00:46:40,859
at zero now

1276
00:46:38,339 --> 00:46:44,099
you see how the at the first location

1277
00:46:40,859 --> 00:46:45,780
here you see that the two are equal and

1278
00:46:44,099 --> 00:46:47,579
that's because it's we're just doing an

1279
00:46:45,780 --> 00:46:50,460
average of this one token

1280
00:46:47,579 --> 00:46:51,720
but here this one is now an average of

1281
00:46:50,460 --> 00:46:54,420
these two

1282
00:46:51,720 --> 00:46:55,800
and now this one is an average of these

1283
00:46:54,420 --> 00:46:57,780
three

1284
00:46:55,800 --> 00:47:00,660
and so on

1285
00:46:57,780 --> 00:47:03,060
so uh and this last one is the average

1286
00:47:00,660 --> 00:47:04,920
of all of these elements so vertical

1287
00:47:03,060 --> 00:47:08,400
average just averaging up all the tokens

1288
00:47:04,920 --> 00:47:10,680
now gives this outcome here

1289
00:47:08,400 --> 00:47:13,319
so this is all well and good but this is

1290
00:47:10,680 --> 00:47:14,640
very inefficient now the trick is that

1291
00:47:13,319 --> 00:47:17,220
we can be very very efficient about

1292
00:47:14,640 --> 00:47:19,500
doing this using matrix multiplication

1293
00:47:17,220 --> 00:47:21,240
so that's the mathematical trick and let

1294
00:47:19,500 --> 00:47:22,920
me show you what I mean let's work with

1295
00:47:21,240 --> 00:47:25,260
the toy example here

1296
00:47:22,920 --> 00:47:28,079
let me run it and I'll explain

1297
00:47:25,260 --> 00:47:30,060
I have a simple Matrix here that is a

1298
00:47:28,079 --> 00:47:32,040
three by three of all ones

1299
00:47:30,060 --> 00:47:33,359
a matrix B of just random numbers and

1300
00:47:32,040 --> 00:47:35,160
it's a three by two

1301
00:47:33,359 --> 00:47:37,380
and a matrix C which will be three by

1302
00:47:35,160 --> 00:47:39,300
three multiply three by two which will

1303
00:47:37,380 --> 00:47:41,099
give out a three by two

1304
00:47:39,300 --> 00:47:41,700
so here we're just using

1305
00:47:41,099 --> 00:47:43,380
um

1306
00:47:41,700 --> 00:47:46,859
matrix multiplication

1307
00:47:43,380 --> 00:47:50,960
so a multiply B gives us C

1308
00:47:46,859 --> 00:47:54,000
okay so how are these numbers in C

1309
00:47:50,960 --> 00:47:57,420
achieved right so this number in the top

1310
00:47:54,000 --> 00:48:00,000
left is the first row of a DOT product

1311
00:47:57,420 --> 00:48:02,400
with the First Column of B

1312
00:48:00,000 --> 00:48:03,780
and since all the the row of a right now

1313
00:48:02,400 --> 00:48:06,240
is all just once

1314
00:48:03,780 --> 00:48:08,880
then the dot product here with with this

1315
00:48:06,240 --> 00:48:11,520
column of B is just going to do a sum of

1316
00:48:08,880 --> 00:48:13,500
these of this column so 2 plus 6 plus 6

1317
00:48:11,520 --> 00:48:15,780
is 14.

1318
00:48:13,500 --> 00:48:17,880
the element here and the output of C is

1319
00:48:15,780 --> 00:48:20,099
also the first column here the first row

1320
00:48:17,880 --> 00:48:23,640
of a multiplied now with the second

1321
00:48:20,099 --> 00:48:24,839
column of B so 7 plus 4 plus plus 5 is

1322
00:48:23,640 --> 00:48:26,040
16.

1323
00:48:24,839 --> 00:48:28,020
now you see that there's repeating

1324
00:48:26,040 --> 00:48:29,880
elements here so this 14 again is

1325
00:48:28,020 --> 00:48:31,500
because this row is again all once and

1326
00:48:29,880 --> 00:48:35,099
it's multiplying the First Column of B

1327
00:48:31,500 --> 00:48:37,619
so we get 14. and this one is and so on

1328
00:48:35,099 --> 00:48:40,260
so this last number here is the last row

1329
00:48:37,619 --> 00:48:43,200
dot product last column

1330
00:48:40,260 --> 00:48:46,020
now the trick here is uh the following

1331
00:48:43,200 --> 00:48:48,540
this is just a boring number of

1332
00:48:46,020 --> 00:48:51,079
um it's just a boring array of all ones

1333
00:48:48,540 --> 00:48:54,780
but torch has this function called trell

1334
00:48:51,079 --> 00:48:57,180
which is short for a triangular

1335
00:48:54,780 --> 00:48:58,800
uh something like that and you can wrap

1336
00:48:57,180 --> 00:49:01,440
it in torched at once and it will just

1337
00:48:58,800 --> 00:49:02,520
return the lower triangular portion of

1338
00:49:01,440 --> 00:49:04,560
this

1339
00:49:02,520 --> 00:49:06,839
okay

1340
00:49:04,560 --> 00:49:08,460
so now it will basically zero out uh

1341
00:49:06,839 --> 00:49:11,520
these guys here so we just get the lower

1342
00:49:08,460 --> 00:49:13,880
triangular part well what happens if we

1343
00:49:11,520 --> 00:49:13,880
do that

1344
00:49:15,119 --> 00:49:19,440
so now we'll have a like this and B like

1345
00:49:17,880 --> 00:49:20,220
this and now what are we getting here in

1346
00:49:19,440 --> 00:49:22,380
C

1347
00:49:20,220 --> 00:49:25,680
well what is this number well this is

1348
00:49:22,380 --> 00:49:28,260
the first row times the First Column and

1349
00:49:25,680 --> 00:49:30,660
because this is zeros

1350
00:49:28,260 --> 00:49:32,280
uh these elements here are now ignored

1351
00:49:30,660 --> 00:49:34,380
so we just get a two

1352
00:49:32,280 --> 00:49:37,079
and then this number here is the first

1353
00:49:34,380 --> 00:49:38,819
row times the second column and because

1354
00:49:37,079 --> 00:49:41,099
these are zeros they get ignored and

1355
00:49:38,819 --> 00:49:42,359
it's just seven the seven multiplies

1356
00:49:41,099 --> 00:49:44,099
this one

1357
00:49:42,359 --> 00:49:46,980
but look what happened here because this

1358
00:49:44,099 --> 00:49:48,480
is one and then zeros we what ended up

1359
00:49:46,980 --> 00:49:50,760
happening is we're just plucking out the

1360
00:49:48,480 --> 00:49:52,020
row of this row of B and that's what we

1361
00:49:50,760 --> 00:49:57,060
got

1362
00:49:52,020 --> 00:49:58,859
now here we have 1 1 0. so here one one

1363
00:49:57,060 --> 00:50:00,540
zero dot product with these two columns

1364
00:49:58,859 --> 00:50:03,240
will now give us two plus six which is

1365
00:50:00,540 --> 00:50:05,819
eight and seven plus four which is 11.

1366
00:50:03,240 --> 00:50:08,700
and because this is one one one we ended

1367
00:50:05,819 --> 00:50:10,560
up with the addition of all of them

1368
00:50:08,700 --> 00:50:12,900
and so basically depending on how many

1369
00:50:10,560 --> 00:50:16,920
ones and zeros we have here we are

1370
00:50:12,900 --> 00:50:19,020
basically doing a sum currently of a

1371
00:50:16,920 --> 00:50:21,660
variable number of these rows and that

1372
00:50:19,020 --> 00:50:23,280
gets deposited into C

1373
00:50:21,660 --> 00:50:25,200
So currently we're doing sums because

1374
00:50:23,280 --> 00:50:27,720
these are ones but we can also do

1375
00:50:25,200 --> 00:50:30,560
average right and you can start to see

1376
00:50:27,720 --> 00:50:33,420
how we could do average of the rows of B

1377
00:50:30,560 --> 00:50:35,339
uh sort of in an incremental fashion

1378
00:50:33,420 --> 00:50:37,680
because we don't have to we can

1379
00:50:35,339 --> 00:50:39,119
basically normalize these rows so that

1380
00:50:37,680 --> 00:50:40,260
they sum to one and then we're going to

1381
00:50:39,119 --> 00:50:43,079
get an average

1382
00:50:40,260 --> 00:50:46,200
so if we took a and then we did a equals

1383
00:50:43,079 --> 00:50:48,540
a divide a torch.sum

1384
00:50:46,200 --> 00:50:52,740
in the um

1385
00:50:48,540 --> 00:50:55,740
of a in the warmth

1386
00:50:52,740 --> 00:50:57,900
Dimension and then let's keep them as

1387
00:50:55,740 --> 00:50:58,800
true so therefore the broadcasting will

1388
00:50:57,900 --> 00:51:01,260
work out

1389
00:50:58,800 --> 00:51:04,619
so if I rerun this you see now that

1390
00:51:01,260 --> 00:51:07,559
these rows now sum to one so this row is

1391
00:51:04,619 --> 00:51:08,880
one this row is 0.5.50 and here we get

1392
00:51:07,559 --> 00:51:11,400
one thirds

1393
00:51:08,880 --> 00:51:12,480
and now when we do a multiply B what are

1394
00:51:11,400 --> 00:51:14,099
we getting

1395
00:51:12,480 --> 00:51:15,660
here we are just getting the first row

1396
00:51:14,099 --> 00:51:18,240
first row

1397
00:51:15,660 --> 00:51:21,000
here now we are getting the average of

1398
00:51:18,240 --> 00:51:23,940
the first two rows

1399
00:51:21,000 --> 00:51:25,859
okay so 2 and 6 average is four and four

1400
00:51:23,940 --> 00:51:28,020
and seven average is 5.5

1401
00:51:25,859 --> 00:51:31,500
and on the bottom here we are now

1402
00:51:28,020 --> 00:51:33,839
getting the average of these three rows

1403
00:51:31,500 --> 00:51:36,359
so the average of all of elements of B

1404
00:51:33,839 --> 00:51:38,720
are now deposited here

1405
00:51:36,359 --> 00:51:41,520
and so you can see that by manipulating

1406
00:51:38,720 --> 00:51:44,220
these uh elements of this multiplying

1407
00:51:41,520 --> 00:51:47,400
Matrix and then multiplying it with any

1408
00:51:44,220 --> 00:51:49,619
given Matrix we can do these averages in

1409
00:51:47,400 --> 00:51:50,400
this incremental fashion because we just

1410
00:51:49,619 --> 00:51:51,359
get

1411
00:51:50,400 --> 00:51:53,520
um

1412
00:51:51,359 --> 00:51:55,500
and we can manipulate that based on the

1413
00:51:53,520 --> 00:51:57,839
elements of a okay so that's very

1414
00:51:55,500 --> 00:51:59,579
convenient so let's swing back up here

1415
00:51:57,839 --> 00:52:01,200
and see how we can vectorize this and

1416
00:51:59,579 --> 00:52:02,220
make it much more efficient using what

1417
00:52:01,200 --> 00:52:04,559
we've learned

1418
00:52:02,220 --> 00:52:07,319
so in particular

1419
00:52:04,559 --> 00:52:09,000
we are going to produce an array a but

1420
00:52:07,319 --> 00:52:10,079
here I'm going to call it way short for

1421
00:52:09,000 --> 00:52:12,599
weights

1422
00:52:10,079 --> 00:52:15,059
but this is r a

1423
00:52:12,599 --> 00:52:17,280
and this is how much of every row we

1424
00:52:15,059 --> 00:52:18,420
want to average up and it's going to be

1425
00:52:17,280 --> 00:52:21,059
an average because you can see it in

1426
00:52:18,420 --> 00:52:23,880
these rows sum to 1.

1427
00:52:21,059 --> 00:52:26,040
so this is our a and then our B in this

1428
00:52:23,880 --> 00:52:27,780
example of course is

1429
00:52:26,040 --> 00:52:29,579
X

1430
00:52:27,780 --> 00:52:32,760
so it's going to happen here now is that

1431
00:52:29,579 --> 00:52:35,700
we are going to have an expo 2.

1432
00:52:32,760 --> 00:52:37,500
and this Expo 2 is going to be way

1433
00:52:35,700 --> 00:52:39,420
multiplying

1434
00:52:37,500 --> 00:52:42,059
RX

1435
00:52:39,420 --> 00:52:44,819
so let's think this through way is T by

1436
00:52:42,059 --> 00:52:48,119
T and this is Matrix multiplying in pi

1437
00:52:44,819 --> 00:52:49,680
torch a b by T by C

1438
00:52:48,119 --> 00:52:52,800
and it's giving us

1439
00:52:49,680 --> 00:52:53,880
uh the what shape so pytorch will come

1440
00:52:52,800 --> 00:52:56,339
here and then we'll see that these

1441
00:52:53,880 --> 00:52:58,619
shapes are not the same so it will

1442
00:52:56,339 --> 00:53:00,960
create a batch Dimension here and this

1443
00:52:58,619 --> 00:53:02,460
is a batched matrix multiply

1444
00:53:00,960 --> 00:53:04,559
and so it will apply this matrix

1445
00:53:02,460 --> 00:53:06,480
multiplication in all the batch elements

1446
00:53:04,559 --> 00:53:09,059
in parallel

1447
00:53:06,480 --> 00:53:10,980
and individually and then for each batch

1448
00:53:09,059 --> 00:53:13,800
element there will be a t by T

1449
00:53:10,980 --> 00:53:16,400
multiplying T by C exactly as we had

1450
00:53:13,800 --> 00:53:16,400
below

1451
00:53:16,740 --> 00:53:21,359
so this will now create

1452
00:53:18,660 --> 00:53:24,420
B by T by C

1453
00:53:21,359 --> 00:53:26,520
and X both 2 will now become identical

1454
00:53:24,420 --> 00:53:28,940
to Expo

1455
00:53:26,520 --> 00:53:28,940
so

1456
00:53:28,980 --> 00:53:37,319
we can see that torch.all close

1457
00:53:31,859 --> 00:53:39,599
of Expo and Expo 2 should be true now

1458
00:53:37,319 --> 00:53:42,720
so this kind of like misses us that uh

1459
00:53:39,599 --> 00:53:47,640
these are in fact the same

1460
00:53:42,720 --> 00:53:49,920
so Expo and Expo 2 if I just print them

1461
00:53:47,640 --> 00:53:51,900
uh okay we're not going to be able to

1462
00:53:49,920 --> 00:53:53,880
okay we're not going to be able to just

1463
00:53:51,900 --> 00:53:55,079
stare it down but

1464
00:53:53,880 --> 00:53:56,940
um

1465
00:53:55,079 --> 00:53:58,440
well let me try Expo basically just at

1466
00:53:56,940 --> 00:54:00,059
the zeroth element and Expo two at the

1467
00:53:58,440 --> 00:54:02,760
zeroth element so just the first batch

1468
00:54:00,059 --> 00:54:05,339
and we should see that this and that

1469
00:54:02,760 --> 00:54:07,980
should be identical which they are

1470
00:54:05,339 --> 00:54:10,020
right so what happened here the trick is

1471
00:54:07,980 --> 00:54:10,980
we were able to use batched Matrix

1472
00:54:10,020 --> 00:54:14,940
multiply

1473
00:54:10,980 --> 00:54:17,579
to do this uh aggregation really and

1474
00:54:14,940 --> 00:54:21,300
it's awaited aggregation and the weights

1475
00:54:17,579 --> 00:54:23,640
are specified in this T by T array

1476
00:54:21,300 --> 00:54:27,000
and we're basically doing weighted sums

1477
00:54:23,640 --> 00:54:29,700
and uh these weighted sums are according

1478
00:54:27,000 --> 00:54:32,040
to the weights inside here they take on

1479
00:54:29,700 --> 00:54:34,079
sort of this triangular form

1480
00:54:32,040 --> 00:54:36,900
and so that means that a token at the

1481
00:54:34,079 --> 00:54:39,660
teeth Dimension will only get uh sort of

1482
00:54:36,900 --> 00:54:41,640
um information from the um tokens

1483
00:54:39,660 --> 00:54:43,619
preceding it so that's exactly what we

1484
00:54:41,640 --> 00:54:45,359
want and finally I would like to rewrite

1485
00:54:43,619 --> 00:54:48,000
it in one more way

1486
00:54:45,359 --> 00:54:50,220
and we're going to see why that's useful

1487
00:54:48,000 --> 00:54:51,900
so this is the third version and it's

1488
00:54:50,220 --> 00:54:54,359
also identical to the first and second

1489
00:54:51,900 --> 00:54:55,559
but let me talk through it it uses

1490
00:54:54,359 --> 00:54:56,760
softmax

1491
00:54:55,559 --> 00:55:00,780
so

1492
00:54:56,760 --> 00:55:02,640
Trill here is this Matrix lower

1493
00:55:00,780 --> 00:55:06,180
triangular ones

1494
00:55:02,640 --> 00:55:08,099
way begins as all zero

1495
00:55:06,180 --> 00:55:10,380
okay so if I just print way in the

1496
00:55:08,099 --> 00:55:12,540
beginning it's all zero

1497
00:55:10,380 --> 00:55:14,099
then I used

1498
00:55:12,540 --> 00:55:16,079
masked fill

1499
00:55:14,099 --> 00:55:18,540
so what this is doing is

1500
00:55:16,079 --> 00:55:20,760
wait that masked fill it's all zeros and

1501
00:55:18,540 --> 00:55:24,000
I'm saying for all the elements where

1502
00:55:20,760 --> 00:55:25,380
Trill is equals equals zero make them be

1503
00:55:24,000 --> 00:55:27,599
negative Infinity

1504
00:55:25,380 --> 00:55:30,119
so all the elements where Trill is zero

1505
00:55:27,599 --> 00:55:32,099
will become negative Infinity now

1506
00:55:30,119 --> 00:55:36,920
so this is what we get

1507
00:55:32,099 --> 00:55:36,920
and then the final one here is softmax

1508
00:55:37,200 --> 00:55:41,220
so if I take a soft Max along every

1509
00:55:39,240 --> 00:55:42,780
single so dim is negative one so along

1510
00:55:41,220 --> 00:55:45,240
every single row

1511
00:55:42,780 --> 00:55:47,359
if I do a soft Max what is that going to

1512
00:55:45,240 --> 00:55:47,359
do

1513
00:55:47,460 --> 00:55:53,160
well softmax is um

1514
00:55:50,400 --> 00:55:54,540
it's also like a normalization operation

1515
00:55:53,160 --> 00:55:56,819
right

1516
00:55:54,540 --> 00:55:58,859
and so spoiler alert you get the exact

1517
00:55:56,819 --> 00:56:01,200
same Matrix

1518
00:55:58,859 --> 00:56:03,180
let me bring back the softmax

1519
00:56:01,200 --> 00:56:04,740
and recall that in softmax we're going

1520
00:56:03,180 --> 00:56:05,700
to exponentiate every single one of

1521
00:56:04,740 --> 00:56:07,200
these

1522
00:56:05,700 --> 00:56:08,160
and then we're going to divide by the

1523
00:56:07,200 --> 00:56:10,260
sum

1524
00:56:08,160 --> 00:56:11,700
and so for if we exponentiate every

1525
00:56:10,260 --> 00:56:14,040
single element here we're going to get a

1526
00:56:11,700 --> 00:56:15,900
one and here we're going to get uh

1527
00:56:14,040 --> 00:56:16,980
basically zero zero zero zero zero

1528
00:56:15,900 --> 00:56:19,260
everywhere else

1529
00:56:16,980 --> 00:56:22,140
and then when we normalize we just get

1530
00:56:19,260 --> 00:56:25,079
one here we're going to get 1 1 and then

1531
00:56:22,140 --> 00:56:28,079
zeros and then softmax will again divide

1532
00:56:25,079 --> 00:56:31,079
and this will give us 0.5.5 and so on

1533
00:56:28,079 --> 00:56:33,300
and so this is also the uh the same way

1534
00:56:31,079 --> 00:56:34,920
to produce this mask

1535
00:56:33,300 --> 00:56:36,420
now the reason that this is a bit more

1536
00:56:34,920 --> 00:56:38,640
interesting and the reason we're going

1537
00:56:36,420 --> 00:56:40,440
to end up using it and solve a tension

1538
00:56:38,640 --> 00:56:43,980
is that

1539
00:56:40,440 --> 00:56:45,480
these weights here begin uh with zero

1540
00:56:43,980 --> 00:56:48,240
and you can think of this as like an

1541
00:56:45,480 --> 00:56:51,119
interaction strength or like an affinity

1542
00:56:48,240 --> 00:56:54,180
so basically it's telling us how much of

1543
00:56:51,119 --> 00:56:57,059
each token from the past do we want to

1544
00:56:54,180 --> 00:57:00,240
Aggregate and average up

1545
00:56:57,059 --> 00:57:02,760
and then this line is saying tokens from

1546
00:57:00,240 --> 00:57:04,559
the past cannot communicate by setting

1547
00:57:02,760 --> 00:57:06,420
them to negative Infinity we're saying

1548
00:57:04,559 --> 00:57:08,160
that we will not aggregate anything from

1549
00:57:06,420 --> 00:57:10,020
those tokens

1550
00:57:08,160 --> 00:57:11,700
and so basically this then goes through

1551
00:57:10,020 --> 00:57:13,140
softmax and through the weighted and

1552
00:57:11,700 --> 00:57:14,940
this is the aggregation through matrix

1553
00:57:13,140 --> 00:57:17,280
multiplication

1554
00:57:14,940 --> 00:57:18,660
and so what this is now is you can think

1555
00:57:17,280 --> 00:57:21,180
of these as

1556
00:57:18,660 --> 00:57:24,480
um these zeros are currently just set by

1557
00:57:21,180 --> 00:57:26,940
us to be zero but a quick preview is

1558
00:57:24,480 --> 00:57:28,619
that these affinities between the tokens

1559
00:57:26,940 --> 00:57:30,960
are not going to be just constant at

1560
00:57:28,619 --> 00:57:32,640
zero they're going to be data dependent

1561
00:57:30,960 --> 00:57:34,920
these tokens are going to start looking

1562
00:57:32,640 --> 00:57:37,440
at each other and some tokens will find

1563
00:57:34,920 --> 00:57:39,420
other tokens more or less interesting

1564
00:57:37,440 --> 00:57:40,800
and depending on what their values are

1565
00:57:39,420 --> 00:57:43,140
they're going to find each other

1566
00:57:40,800 --> 00:57:45,119
interesting to different amounts and I'm

1567
00:57:43,140 --> 00:57:47,220
going to call those affinities I think

1568
00:57:45,119 --> 00:57:49,319
and then here we are saying the future

1569
00:57:47,220 --> 00:57:51,000
cannot communicate with the past we're

1570
00:57:49,319 --> 00:57:53,520
going to clamp them

1571
00:57:51,000 --> 00:57:55,920
and then when we normalize and sum we're

1572
00:57:53,520 --> 00:57:57,480
going to aggregate sort of their values

1573
00:57:55,920 --> 00:57:58,380
depending on how interesting they find

1574
00:57:57,480 --> 00:57:59,640
each other

1575
00:57:58,380 --> 00:58:03,180
and so that's the preview for

1576
00:57:59,640 --> 00:58:04,859
self-attention and basically long story

1577
00:58:03,180 --> 00:58:07,859
short from this entire section is that

1578
00:58:04,859 --> 00:58:09,359
you can do weighted aggregations of your

1579
00:58:07,859 --> 00:58:12,420
past elements

1580
00:58:09,359 --> 00:58:15,359
by having by using matrix multiplication

1581
00:58:12,420 --> 00:58:17,640
of a lower triangular fashion

1582
00:58:15,359 --> 00:58:19,559
and then the elements here in the lower

1583
00:58:17,640 --> 00:58:23,339
triangular part are telling you how much

1584
00:58:19,559 --> 00:58:24,960
of each element fuses into this position

1585
00:58:23,339 --> 00:58:27,240
so we're going to use this trick now to

1586
00:58:24,960 --> 00:58:28,740
develop the self-attention block so

1587
00:58:27,240 --> 00:58:30,359
first let's get some quick preliminaries

1588
00:58:28,740 --> 00:58:31,859
out of the way

1589
00:58:30,359 --> 00:58:33,420
first the thing I'm kind of bothered by

1590
00:58:31,859 --> 00:58:35,220
is that you see how we're passing in

1591
00:58:33,420 --> 00:58:36,660
vocab size into the Constructor there's

1592
00:58:35,220 --> 00:58:39,000
no need to do that because vocab size

1593
00:58:36,660 --> 00:58:40,680
has already defined up top as a global

1594
00:58:39,000 --> 00:58:42,480
variable so there's no need to pass this

1595
00:58:40,680 --> 00:58:45,059
stuff around

1596
00:58:42,480 --> 00:58:46,920
next one I want to do is I don't want to

1597
00:58:45,059 --> 00:58:48,480
actually create I want to create like a

1598
00:58:46,920 --> 00:58:51,079
level of interaction here where we don't

1599
00:58:48,480 --> 00:58:53,400
directly go to the embedding for the um

1600
00:58:51,079 --> 00:58:54,960
logits but instead we go through this

1601
00:58:53,400 --> 00:58:57,900
intermediate phase because we're going

1602
00:58:54,960 --> 00:59:01,200
to start making that bigger so let me

1603
00:58:57,900 --> 00:59:02,520
introduce a new variable and embed a

1604
00:59:01,200 --> 00:59:03,660
short for a number of embedding

1605
00:59:02,520 --> 00:59:05,339
dimensions

1606
00:59:03,660 --> 00:59:06,720
so an embed

1607
00:59:05,339 --> 00:59:09,960
here

1608
00:59:06,720 --> 00:59:12,720
will be say 32. that was a suggestion

1609
00:59:09,960 --> 00:59:15,420
from GitHub by the way it also showed us

1610
00:59:12,720 --> 00:59:16,980
to 32 which is a good number

1611
00:59:15,420 --> 00:59:19,920
so this is an embedding table and only

1612
00:59:16,980 --> 00:59:21,660
32 dimensional embeddings

1613
00:59:19,920 --> 00:59:24,000
so then here this is not going to give

1614
00:59:21,660 --> 00:59:25,980
us logits directly instead this is going

1615
00:59:24,000 --> 00:59:27,480
to give us token embeddings

1616
00:59:25,980 --> 00:59:29,220
that's what I'm going to call it and

1617
00:59:27,480 --> 00:59:30,960
then to go from the token embeddings to

1618
00:59:29,220 --> 00:59:34,020
the logits we're going to need a linear

1619
00:59:30,960 --> 00:59:36,119
layer so self.lm head let's call it

1620
00:59:34,020 --> 00:59:38,760
short for language modeling head

1621
00:59:36,119 --> 00:59:39,780
is n linear from an embed up to vocab

1622
00:59:38,760 --> 00:59:41,460
size

1623
00:59:39,780 --> 00:59:43,040
and then when we swing over here we're

1624
00:59:41,460 --> 00:59:45,960
actually going to get the logits by

1625
00:59:43,040 --> 00:59:47,339
exactly what the copilot says

1626
00:59:45,960 --> 00:59:50,940
now we have to be careful here because

1627
00:59:47,339 --> 00:59:53,700
this C and this C are not equal

1628
00:59:50,940 --> 00:59:54,960
this is an embedded C and this is vocab

1629
00:59:53,700 --> 00:59:56,819
size

1630
00:59:54,960 --> 00:59:58,740
so let's just say that an embed is equal

1631
00:59:56,819 --> 01:00:01,260
to C

1632
00:59:58,740 --> 01:00:02,880
and then this just creates one spurious

1633
01:00:01,260 --> 01:00:07,460
layer of interaction through a linear

1634
01:00:02,880 --> 01:00:07,460
layer but this should basically run

1635
01:00:12,299 --> 01:00:16,799
so we see that this runs and uh this

1636
01:00:15,359 --> 01:00:19,079
currently looks kind of spurious but

1637
01:00:16,799 --> 01:00:21,540
we're going to build on top of this now

1638
01:00:19,079 --> 01:00:23,640
next up so far we've taken these in in

1639
01:00:21,540 --> 01:00:28,079
the seas and we've encoded them based on

1640
01:00:23,640 --> 01:00:29,760
the identity of the tokens inside idx

1641
01:00:28,079 --> 01:00:31,680
the next thing that people very often do

1642
01:00:29,760 --> 01:00:33,780
is that we're not just encoding the

1643
01:00:31,680 --> 01:00:34,799
identity of these tokens but also their

1644
01:00:33,780 --> 01:00:36,299
position

1645
01:00:34,799 --> 01:00:39,119
so we're going to have a second position

1646
01:00:36,299 --> 01:00:40,980
uh embedding table here so solve that

1647
01:00:39,119 --> 01:00:43,260
position embedding table

1648
01:00:40,980 --> 01:00:45,900
is an embedding of block size by an

1649
01:00:43,260 --> 01:00:47,760
embed and so each position from 0 to

1650
01:00:45,900 --> 01:00:49,440
block size minus 1 will also get its own

1651
01:00:47,760 --> 01:00:52,260
embedding vector

1652
01:00:49,440 --> 01:00:55,319
and then here first let me decode a b by

1653
01:00:52,260 --> 01:00:56,819
T from idx.shape

1654
01:00:55,319 --> 01:00:58,799
and then here we're also going to have a

1655
01:00:56,819 --> 01:01:00,480
positive bedding which is the positional

1656
01:00:58,799 --> 01:01:03,180
embedding and these are this is tour

1657
01:01:00,480 --> 01:01:05,940
Dutch arrange so this will be basically

1658
01:01:03,180 --> 01:01:08,040
just integers from 0 to T minus 1.

1659
01:01:05,940 --> 01:01:09,480
and all of those integers from 0 to T

1660
01:01:08,040 --> 01:01:12,180
minus 1 get embedded through the table

1661
01:01:09,480 --> 01:01:15,119
to create a t by C

1662
01:01:12,180 --> 01:01:17,940
and then here this gets renamed to just

1663
01:01:15,119 --> 01:01:19,619
say x and x will be

1664
01:01:17,940 --> 01:01:21,780
the addition of the token embeddings

1665
01:01:19,619 --> 01:01:23,700
with the positional embeddings

1666
01:01:21,780 --> 01:01:27,000
and here the broadcasting note will work

1667
01:01:23,700 --> 01:01:28,980
out so B by T by C plus T by C this gets

1668
01:01:27,000 --> 01:01:30,900
right aligned a new dimension of one

1669
01:01:28,980 --> 01:01:32,700
gets added and it gets broadcasted

1670
01:01:30,900 --> 01:01:35,280
across batch

1671
01:01:32,700 --> 01:01:37,799
so at this point x holds not just the

1672
01:01:35,280 --> 01:01:39,420
token identities but the positions at

1673
01:01:37,799 --> 01:01:41,280
which these tokens occur

1674
01:01:39,420 --> 01:01:42,720
and this is currently not that useful

1675
01:01:41,280 --> 01:01:44,280
because of course we just have a simple

1676
01:01:42,720 --> 01:01:45,480
migrain model so it doesn't matter if

1677
01:01:44,280 --> 01:01:47,520
you're in the fifth position the second

1678
01:01:45,480 --> 01:01:49,980
position or wherever it's all

1679
01:01:47,520 --> 01:01:51,500
translation invariant at this stage so

1680
01:01:49,980 --> 01:01:53,880
this information currently wouldn't help

1681
01:01:51,500 --> 01:01:55,380
but as we work on the self potential

1682
01:01:53,880 --> 01:01:57,619
block we'll see that this starts to

1683
01:01:55,380 --> 01:01:57,619
matter

1684
01:01:59,760 --> 01:02:03,240
okay so now we get the Crux of

1685
01:02:01,380 --> 01:02:04,799
self-attention so this is probably the

1686
01:02:03,240 --> 01:02:06,359
most important part of this video to

1687
01:02:04,799 --> 01:02:07,619
understand

1688
01:02:06,359 --> 01:02:09,299
we're going to implement a small

1689
01:02:07,619 --> 01:02:11,099
self-attention for a single individual

1690
01:02:09,299 --> 01:02:13,559
head as they're called

1691
01:02:11,099 --> 01:02:15,299
so we start off with where we were so

1692
01:02:13,559 --> 01:02:17,640
all of this code is familiar

1693
01:02:15,299 --> 01:02:18,839
so right now I'm working with an example

1694
01:02:17,640 --> 01:02:21,720
where I change the number of channels

1695
01:02:18,839 --> 01:02:25,380
from 2 to 32 so we have a 4x8

1696
01:02:21,720 --> 01:02:27,180
arrangement of tokens and each and the

1697
01:02:25,380 --> 01:02:29,339
information at each token is currently

1698
01:02:27,180 --> 01:02:31,200
32 dimensional but we just are working

1699
01:02:29,339 --> 01:02:33,720
with random numbers

1700
01:02:31,200 --> 01:02:36,599
now we saw here that

1701
01:02:33,720 --> 01:02:39,660
the code as we had it before does a

1702
01:02:36,599 --> 01:02:42,780
simple weight a simple average of all

1703
01:02:39,660 --> 01:02:44,280
the past tokens and the current token so

1704
01:02:42,780 --> 01:02:45,780
it's just the previous information and

1705
01:02:44,280 --> 01:02:47,160
current information is just being mixed

1706
01:02:45,780 --> 01:02:48,839
together in an average

1707
01:02:47,160 --> 01:02:50,880
and that's what this code currently

1708
01:02:48,839 --> 01:02:53,040
achieves and it does so by creating this

1709
01:02:50,880 --> 01:02:56,760
lower triangular structure which allows

1710
01:02:53,040 --> 01:02:57,720
us to mask out this weight Matrix that

1711
01:02:56,760 --> 01:03:00,240
we create

1712
01:02:57,720 --> 01:03:04,079
so we mask it out and then we normalize

1713
01:03:00,240 --> 01:03:05,579
it and currently when we initialize the

1714
01:03:04,079 --> 01:03:08,700
affinities between all the different

1715
01:03:05,579 --> 01:03:10,559
sort of tokens or nodes I'm going to use

1716
01:03:08,700 --> 01:03:12,420
those terms interchangeably

1717
01:03:10,559 --> 01:03:13,799
so when we initialize the affinities

1718
01:03:12,420 --> 01:03:14,940
between all the different tokens to be

1719
01:03:13,799 --> 01:03:17,180
zero

1720
01:03:14,940 --> 01:03:19,500
then we see that way gives us this

1721
01:03:17,180 --> 01:03:21,000
structure where every single row has

1722
01:03:19,500 --> 01:03:23,520
these um

1723
01:03:21,000 --> 01:03:25,980
uniform numbers and so that's what

1724
01:03:23,520 --> 01:03:28,079
that's what then uh in this Matrix

1725
01:03:25,980 --> 01:03:29,460
multiply makes it so that we're doing a

1726
01:03:28,079 --> 01:03:30,720
simple average

1727
01:03:29,460 --> 01:03:33,299
now

1728
01:03:30,720 --> 01:03:36,660
we don't actually want this to be

1729
01:03:33,299 --> 01:03:39,119
All Uniform because different uh tokens

1730
01:03:36,660 --> 01:03:40,740
will find different other tokens more or

1731
01:03:39,119 --> 01:03:42,839
less interesting and we want that to be

1732
01:03:40,740 --> 01:03:44,940
data dependent so for example if I'm a

1733
01:03:42,839 --> 01:03:47,099
vowel then maybe I'm looking for

1734
01:03:44,940 --> 01:03:48,720
consonants in my past and maybe I want

1735
01:03:47,099 --> 01:03:51,119
to know what those consonants are and I

1736
01:03:48,720 --> 01:03:53,220
want that information to Flow To Me

1737
01:03:51,119 --> 01:03:55,440
and so I want to now gather information

1738
01:03:53,220 --> 01:03:57,059
from the past but I want to do it in a

1739
01:03:55,440 --> 01:03:58,980
data dependent way and this is the

1740
01:03:57,059 --> 01:04:00,900
problem that self-attention solves

1741
01:03:58,980 --> 01:04:04,079
now the way self-attention solves this

1742
01:04:00,900 --> 01:04:06,900
is the following every single node or

1743
01:04:04,079 --> 01:04:08,579
every single token at each position will

1744
01:04:06,900 --> 01:04:11,940
emit two vectors

1745
01:04:08,579 --> 01:04:13,440
it will emit a query and it will emit a

1746
01:04:11,940 --> 01:04:16,319
key

1747
01:04:13,440 --> 01:04:18,359
now the query Vector roughly speaking is

1748
01:04:16,319 --> 01:04:20,460
what am I looking for

1749
01:04:18,359 --> 01:04:22,619
and the key Vector roughly speaking is

1750
01:04:20,460 --> 01:04:24,720
what do I contain

1751
01:04:22,619 --> 01:04:28,440
and then the way we get affinities

1752
01:04:24,720 --> 01:04:30,359
between these tokens now in a sequence

1753
01:04:28,440 --> 01:04:32,579
is we basically just do a DOT product

1754
01:04:30,359 --> 01:04:35,760
between the keys and the queries

1755
01:04:32,579 --> 01:04:38,220
so my query dot products with all the

1756
01:04:35,760 --> 01:04:42,680
keys of all the other tokens and that

1757
01:04:38,220 --> 01:04:42,680
dot product now becomes way

1758
01:04:42,780 --> 01:04:48,180
and so um if the key and the query are

1759
01:04:46,020 --> 01:04:50,700
sort of aligned they will interact to a

1760
01:04:48,180 --> 01:04:53,940
very high amount and then I will get to

1761
01:04:50,700 --> 01:04:55,559
learn more about that specific token as

1762
01:04:53,940 --> 01:04:59,420
opposed to any other token in the

1763
01:04:55,559 --> 01:04:59,420
sequence so let's implement this tab

1764
01:05:01,500 --> 01:05:07,920
we're going to implement a single

1765
01:05:04,559 --> 01:05:10,260
what's called head of self-attention

1766
01:05:07,920 --> 01:05:11,520
so this is just one head there's a hyper

1767
01:05:10,260 --> 01:05:13,440
parameter involved with these heads

1768
01:05:11,520 --> 01:05:15,180
which is the head size

1769
01:05:13,440 --> 01:05:17,819
and then here I'm initializing the

1770
01:05:15,180 --> 01:05:19,559
linear modules and I'm using bias equals

1771
01:05:17,819 --> 01:05:22,680
false so these are just going to apply a

1772
01:05:19,559 --> 01:05:24,960
matrix multiply with some fixed weights

1773
01:05:22,680 --> 01:05:28,559
and now let me produce a

1774
01:05:24,960 --> 01:05:30,839
key and Q K and Q by forwarding these

1775
01:05:28,559 --> 01:05:33,660
modules on x

1776
01:05:30,839 --> 01:05:37,140
so the size of this will not become

1777
01:05:33,660 --> 01:05:42,319
B by T by 16 because that is the head

1778
01:05:37,140 --> 01:05:42,319
size and the same here B by T by 16.

1779
01:05:45,780 --> 01:05:49,440
so this being that size

1780
01:05:47,520 --> 01:05:52,799
so you see here that when I forward this

1781
01:05:49,440 --> 01:05:54,599
linear on top of my X all the tokens in

1782
01:05:52,799 --> 01:05:57,059
all the positions in the B by T

1783
01:05:54,599 --> 01:05:59,579
Arrangement all of them in parallel and

1784
01:05:57,059 --> 01:06:02,460
independently produce a key and a query

1785
01:05:59,579 --> 01:06:04,619
so no communication has happened yet

1786
01:06:02,460 --> 01:06:07,020
but the communication comes now all the

1787
01:06:04,619 --> 01:06:08,520
queries will dot product with all the

1788
01:06:07,020 --> 01:06:10,500
keys

1789
01:06:08,520 --> 01:06:13,140
so basically what we want is we want way

1790
01:06:10,500 --> 01:06:16,380
now or the affinities between these to

1791
01:06:13,140 --> 01:06:18,059
be query multiplying key

1792
01:06:16,380 --> 01:06:19,500
but we have to be careful with uh we

1793
01:06:18,059 --> 01:06:22,740
can't Matrix multiply this we actually

1794
01:06:19,500 --> 01:06:25,559
need to transpose uh K but we have to be

1795
01:06:22,740 --> 01:06:27,240
also careful because these are when you

1796
01:06:25,559 --> 01:06:29,880
have the batch Dimension so in

1797
01:06:27,240 --> 01:06:32,160
particular we want to transpose uh the

1798
01:06:29,880 --> 01:06:33,900
last two Dimensions Dimension negative

1799
01:06:32,160 --> 01:06:37,380
one and dimension negative two

1800
01:06:33,900 --> 01:06:40,319
so negative 2 negative 1.

1801
01:06:37,380 --> 01:06:44,839
and so this Matrix multiplied now will

1802
01:06:40,319 --> 01:06:44,839
basically do the following B by T by 16

1803
01:06:45,180 --> 01:06:50,339
Matrix multiplies B by 16 by T to give

1804
01:06:49,380 --> 01:06:54,200
us

1805
01:06:50,339 --> 01:06:54,200
B by T by T

1806
01:06:54,299 --> 01:06:58,920
right

1807
01:06:55,980 --> 01:07:01,260
so for every row of B we're not going to

1808
01:06:58,920 --> 01:07:03,839
have a t-square matrix giving us the

1809
01:07:01,260 --> 01:07:06,539
affinities and these are now the way

1810
01:07:03,839 --> 01:07:08,520
so they're not zeros they are now coming

1811
01:07:06,539 --> 01:07:09,599
from this dot product between the keys

1812
01:07:08,520 --> 01:07:13,140
and the queries

1813
01:07:09,599 --> 01:07:15,240
so this can now run I can I can run this

1814
01:07:13,140 --> 01:07:17,220
and the weighted aggregation now is a

1815
01:07:15,240 --> 01:07:19,200
function in a data dependent manner

1816
01:07:17,220 --> 01:07:20,220
between the keys and queries of these

1817
01:07:19,200 --> 01:07:23,760
nodes

1818
01:07:20,220 --> 01:07:26,339
so just inspecting what happened here

1819
01:07:23,760 --> 01:07:29,339
the way takes on this form

1820
01:07:26,339 --> 01:07:31,200
and you see that before way was just a

1821
01:07:29,339 --> 01:07:33,420
constant so it was applied in the same

1822
01:07:31,200 --> 01:07:34,859
way to all the batch elements but now

1823
01:07:33,420 --> 01:07:37,980
every single batch elements will have

1824
01:07:34,859 --> 01:07:39,240
different sort of way because uh every

1825
01:07:37,980 --> 01:07:42,059
single batch element contains different

1826
01:07:39,240 --> 01:07:43,920
tokens at different positions and so

1827
01:07:42,059 --> 01:07:47,099
this is not data dependent

1828
01:07:43,920 --> 01:07:49,740
so when we look at just the zeroth row

1829
01:07:47,099 --> 01:07:51,660
for example in the input these are the

1830
01:07:49,740 --> 01:07:52,799
weights that came out and so you can see

1831
01:07:51,660 --> 01:07:54,260
now that they're not just exactly

1832
01:07:52,799 --> 01:07:56,700
uniform

1833
01:07:54,260 --> 01:07:59,099
and in particular as an example here for

1834
01:07:56,700 --> 01:08:01,200
the last row this was the eighth token

1835
01:07:59,099 --> 01:08:02,880
and the eighth token knows what content

1836
01:08:01,200 --> 01:08:04,020
it has and it knows at what position

1837
01:08:02,880 --> 01:08:06,960
it's in

1838
01:08:04,020 --> 01:08:09,539
and now the eighth token based on that

1839
01:08:06,960 --> 01:08:12,180
creates a query hey I'm looking for this

1840
01:08:09,539 --> 01:08:13,380
kind of stuff I'm a vowel I'm on the

1841
01:08:12,180 --> 01:08:16,259
eighth position I'm looking for any

1842
01:08:13,380 --> 01:08:19,380
consonants at positions up to four

1843
01:08:16,259 --> 01:08:21,480
and then all the nodes get to emit keys

1844
01:08:19,380 --> 01:08:23,580
and maybe one of the channels could be I

1845
01:08:21,480 --> 01:08:25,020
am a I am a consonant and I am in a

1846
01:08:23,580 --> 01:08:27,960
position up to four

1847
01:08:25,020 --> 01:08:30,000
and that key would have a high number in

1848
01:08:27,960 --> 01:08:31,440
that specific Channel and that's how the

1849
01:08:30,000 --> 01:08:33,179
query and the key when they dot product

1850
01:08:31,440 --> 01:08:34,620
they can find each other and create a

1851
01:08:33,179 --> 01:08:36,179
high affinity

1852
01:08:34,620 --> 01:08:39,299
and when they have a high Affinity like

1853
01:08:36,179 --> 01:08:42,299
say this token was pretty interesting to

1854
01:08:39,299 --> 01:08:44,160
uh to this eighth token

1855
01:08:42,299 --> 01:08:45,900
when they have a high Affinity then

1856
01:08:44,160 --> 01:08:47,219
through the soft Max I will end up

1857
01:08:45,900 --> 01:08:49,140
aggregating a lot of its information

1858
01:08:47,219 --> 01:08:52,500
into my position

1859
01:08:49,140 --> 01:08:56,279
and so I'll get to learn a lot about it

1860
01:08:52,500 --> 01:08:58,560
now just this we're looking at way after

1861
01:08:56,279 --> 01:08:59,219
this has already happened

1862
01:08:58,560 --> 01:09:01,140
um

1863
01:08:59,219 --> 01:09:03,120
let me erase this operation as well so

1864
01:09:01,140 --> 01:09:04,560
let me erase the masking and the softmax

1865
01:09:03,120 --> 01:09:06,480
just to show you the under the hood

1866
01:09:04,560 --> 01:09:08,699
internals and how that works

1867
01:09:06,480 --> 01:09:11,339
so without the masking in the softmax

1868
01:09:08,699 --> 01:09:13,199
way comes out like this right this is

1869
01:09:11,339 --> 01:09:15,299
the outputs of the dot products

1870
01:09:13,199 --> 01:09:16,980
and these are the raw outputs and they

1871
01:09:15,299 --> 01:09:19,620
take on values from negative you know

1872
01:09:16,980 --> 01:09:22,319
two to positive two Etc

1873
01:09:19,620 --> 01:09:24,239
so that's the raw interactions and raw

1874
01:09:22,319 --> 01:09:27,060
affinities between all the nodes

1875
01:09:24,239 --> 01:09:28,980
but now if I'm a if I'm a fifth node I

1876
01:09:27,060 --> 01:09:30,839
will not want to aggregate anything from

1877
01:09:28,980 --> 01:09:33,000
the six node seventh node and the eighth

1878
01:09:30,839 --> 01:09:35,819
node so actually we use the upper

1879
01:09:33,000 --> 01:09:38,219
triangular masking so those are not

1880
01:09:35,819 --> 01:09:40,739
allowed to communicate

1881
01:09:38,219 --> 01:09:43,380
and now we actually want to have a nice

1882
01:09:40,739 --> 01:09:45,660
uh distribution so we don't want to

1883
01:09:43,380 --> 01:09:47,759
aggregate negative 0.11 of this node

1884
01:09:45,660 --> 01:09:49,859
that's crazy so instead we exponentiate

1885
01:09:47,759 --> 01:09:51,540
and normalize and now we get a nice

1886
01:09:49,859 --> 01:09:53,100
distribution that seems to one

1887
01:09:51,540 --> 01:09:54,840
and this is telling us now in the data

1888
01:09:53,100 --> 01:09:57,179
dependent manner how much of information

1889
01:09:54,840 --> 01:09:59,340
to aggregate from any of these tokens in

1890
01:09:57,179 --> 01:10:02,340
the past

1891
01:09:59,340 --> 01:10:05,340
so that's way and it's not zeros anymore

1892
01:10:02,340 --> 01:10:08,699
but but it's calculated in this way now

1893
01:10:05,340 --> 01:10:10,800
there's one more uh part to a single

1894
01:10:08,699 --> 01:10:12,420
self-attention head and that is that

1895
01:10:10,800 --> 01:10:15,120
when you do the aggregation we don't

1896
01:10:12,420 --> 01:10:17,100
actually aggregate the tokens exactly we

1897
01:10:15,120 --> 01:10:21,060
aggregate we produce one more value here

1898
01:10:17,100 --> 01:10:22,679
and we call that the value

1899
01:10:21,060 --> 01:10:24,540
so in the same way that we produced p

1900
01:10:22,679 --> 01:10:25,860
and query we're also going to create a

1901
01:10:24,540 --> 01:10:27,659
value

1902
01:10:25,860 --> 01:10:28,860
and then

1903
01:10:27,659 --> 01:10:31,260
here

1904
01:10:28,860 --> 01:10:34,620
we don't aggregate

1905
01:10:31,260 --> 01:10:37,860
X we calculate a v which is just

1906
01:10:34,620 --> 01:10:40,560
achieved by propagating this linear on

1907
01:10:37,860 --> 01:10:44,460
top of X again and then we

1908
01:10:40,560 --> 01:10:46,500
output way multiplied by V so V is the

1909
01:10:44,460 --> 01:10:48,060
elements that we aggregate or the the

1910
01:10:46,500 --> 01:10:49,679
vector that we aggregate instead of the

1911
01:10:48,060 --> 01:10:51,900
raw X

1912
01:10:49,679 --> 01:10:53,640
and now of course this will make it so

1913
01:10:51,900 --> 01:10:55,560
that the output here of the single head

1914
01:10:53,640 --> 01:10:58,260
will be 16 dimensional because that is

1915
01:10:55,560 --> 01:11:00,300
the head size

1916
01:10:58,260 --> 01:11:02,159
so you can think of X as kind of like a

1917
01:11:00,300 --> 01:11:04,500
private information to this token if you

1918
01:11:02,159 --> 01:11:06,480
if you think about it that way so X is

1919
01:11:04,500 --> 01:11:08,280
kind of private to this token so I'm a

1920
01:11:06,480 --> 01:11:11,880
fifth token at some and I have some

1921
01:11:08,280 --> 01:11:13,020
identity and my information is kept in

1922
01:11:11,880 --> 01:11:15,179
Vector X

1923
01:11:13,020 --> 01:11:17,100
and now for the purposes of the single

1924
01:11:15,179 --> 01:11:19,440
head here's what I'm interested in

1925
01:11:17,100 --> 01:11:21,719
here's what I have

1926
01:11:19,440 --> 01:11:23,520
and if you find me interesting here's

1927
01:11:21,719 --> 01:11:25,020
what I will communicate to you and

1928
01:11:23,520 --> 01:11:26,520
that's stored in v

1929
01:11:25,020 --> 01:11:28,320
and so V is the thing that gets

1930
01:11:26,520 --> 01:11:31,620
aggregated for the purposes of this

1931
01:11:28,320 --> 01:11:33,300
single head between the different nodes

1932
01:11:31,620 --> 01:11:35,219
and that's uh

1933
01:11:33,300 --> 01:11:38,040
basically the self attention mechanism

1934
01:11:35,219 --> 01:11:39,600
this is this is what it does

1935
01:11:38,040 --> 01:11:42,239
there are a few notes that I would make

1936
01:11:39,600 --> 01:11:44,820
like to make about attention number one

1937
01:11:42,239 --> 01:11:46,739
attention is a communication mechanism

1938
01:11:44,820 --> 01:11:48,659
you can really think about it as a

1939
01:11:46,739 --> 01:11:50,520
communication mechanism where you have a

1940
01:11:48,659 --> 01:11:52,679
number of nodes in a directed graph

1941
01:11:50,520 --> 01:11:54,659
where basically you have edges pointing

1942
01:11:52,679 --> 01:11:57,179
between those like this

1943
01:11:54,659 --> 01:11:59,159
and what happens is every node has some

1944
01:11:57,179 --> 01:12:01,739
Vector of information and it gets to

1945
01:11:59,159 --> 01:12:04,679
aggregate information via a weighted sum

1946
01:12:01,739 --> 01:12:06,360
from all the nodes that point to it

1947
01:12:04,679 --> 01:12:08,460
and this is done in a data dependent

1948
01:12:06,360 --> 01:12:09,659
manner so depending on whatever data is

1949
01:12:08,460 --> 01:12:11,100
actually stored at each node at any

1950
01:12:09,659 --> 01:12:12,600
point in time

1951
01:12:11,100 --> 01:12:14,219
now

1952
01:12:12,600 --> 01:12:16,199
our graph doesn't look like this our

1953
01:12:14,219 --> 01:12:18,060
graph has a different structure we have

1954
01:12:16,199 --> 01:12:21,120
eight nodes because the block size is

1955
01:12:18,060 --> 01:12:24,000
eight and there's always eight tokens

1956
01:12:21,120 --> 01:12:26,340
and the first node is only pointed to by

1957
01:12:24,000 --> 01:12:28,620
itself the second node is pointed to by

1958
01:12:26,340 --> 01:12:30,600
the first node and itself all the way up

1959
01:12:28,620 --> 01:12:33,600
to the eighth node which is pointed to

1960
01:12:30,600 --> 01:12:35,460
by all the previous nodes and itself

1961
01:12:33,600 --> 01:12:37,679
and so that's the structure that our

1962
01:12:35,460 --> 01:12:39,300
directed graph has or happens happens to

1963
01:12:37,679 --> 01:12:41,699
have in other aggressive sort of

1964
01:12:39,300 --> 01:12:43,020
scenario like language modeling but in

1965
01:12:41,699 --> 01:12:44,580
principle attention can be applied to

1966
01:12:43,020 --> 01:12:46,080
any arbitrary directed graph and it's

1967
01:12:44,580 --> 01:12:47,040
just a communication mechanism between

1968
01:12:46,080 --> 01:12:49,080
the nodes

1969
01:12:47,040 --> 01:12:51,719
the second note is that notice that

1970
01:12:49,080 --> 01:12:54,239
there is no notion of space so attention

1971
01:12:51,719 --> 01:12:56,820
simply acts over like a set of vectors

1972
01:12:54,239 --> 01:12:58,199
in this graph and so by default these

1973
01:12:56,820 --> 01:12:59,940
nodes have no idea where they are

1974
01:12:58,199 --> 01:13:02,159
positioned in a space and that's why we

1975
01:12:59,940 --> 01:13:03,900
need to encode them positionally and

1976
01:13:02,159 --> 01:13:05,940
sort of give them some information that

1977
01:13:03,900 --> 01:13:08,219
is anchored to a specific position so

1978
01:13:05,940 --> 01:13:10,080
that they sort of know where they are

1979
01:13:08,219 --> 01:13:12,060
and this is different than for example

1980
01:13:10,080 --> 01:13:13,320
from convolution because if you run for

1981
01:13:12,060 --> 01:13:16,080
example a convolution operation over

1982
01:13:13,320 --> 01:13:18,719
some input there's a very specific sort

1983
01:13:16,080 --> 01:13:20,820
of layout of the information in space in

1984
01:13:18,719 --> 01:13:24,060
the convolutional filters sort of act in

1985
01:13:20,820 --> 01:13:26,219
space and so it's it's not like an

1986
01:13:24,060 --> 01:13:28,020
attention in attention is just a set of

1987
01:13:26,219 --> 01:13:29,940
vectors out there in space they

1988
01:13:28,020 --> 01:13:31,380
communicate and if you want them to have

1989
01:13:29,940 --> 01:13:33,420
a notion of space you need to

1990
01:13:31,380 --> 01:13:36,000
specifically add it which is what we've

1991
01:13:33,420 --> 01:13:38,580
done when we calculated the um relative

1992
01:13:36,000 --> 01:13:40,140
the position loan code encodings and

1993
01:13:38,580 --> 01:13:41,760
added that information to the vectors

1994
01:13:40,140 --> 01:13:43,860
the next thing that I hope is very clear

1995
01:13:41,760 --> 01:13:45,840
is that the elements across the batch

1996
01:13:43,860 --> 01:13:47,460
Dimension which are independent examples

1997
01:13:45,840 --> 01:13:49,380
never talk to each other don't always

1998
01:13:47,460 --> 01:13:51,179
processed independently and this is a

1999
01:13:49,380 --> 01:13:53,400
bashed Matrix multiply that applies

2000
01:13:51,179 --> 01:13:54,480
basically a matrix multiplication kind

2001
01:13:53,400 --> 01:13:56,280
of in parallel across the batch

2002
01:13:54,480 --> 01:13:58,440
Dimension so maybe it would be more

2003
01:13:56,280 --> 01:14:00,840
accurate to say that in this analogy of

2004
01:13:58,440 --> 01:14:02,820
a directed graph we really have because

2005
01:14:00,840 --> 01:14:05,880
the batch size is four we really have

2006
01:14:02,820 --> 01:14:07,140
four separate pools of eight nodes and

2007
01:14:05,880 --> 01:14:09,000
those eight nodes only talk to each

2008
01:14:07,140 --> 01:14:11,460
other but in total there's like 32 nodes

2009
01:14:09,000 --> 01:14:13,860
that are being processed but there's um

2010
01:14:11,460 --> 01:14:15,360
sort of four separate pools of eight you

2011
01:14:13,860 --> 01:14:18,000
can look at it that way

2012
01:14:15,360 --> 01:14:20,699
the next note is that here in the case

2013
01:14:18,000 --> 01:14:22,800
of language modeling uh we have this

2014
01:14:20,699 --> 01:14:24,840
specific structure of directed graph

2015
01:14:22,800 --> 01:14:27,600
where the future tokens will not

2016
01:14:24,840 --> 01:14:28,800
communicate to the Past tokens but this

2017
01:14:27,600 --> 01:14:30,840
doesn't necessarily have to be the

2018
01:14:28,800 --> 01:14:32,820
constraint in the general case and in

2019
01:14:30,840 --> 01:14:35,159
fact in many cases you may want to have

2020
01:14:32,820 --> 01:14:37,679
all of the nodes talk to each other

2021
01:14:35,159 --> 01:14:39,060
fully so as an example if you're doing

2022
01:14:37,679 --> 01:14:41,159
sentiment analysis or something like

2023
01:14:39,060 --> 01:14:42,900
that with a Transformer you might have a

2024
01:14:41,159 --> 01:14:44,760
number of tokens and you may want to

2025
01:14:42,900 --> 01:14:46,739
have them all talk to each other fully

2026
01:14:44,760 --> 01:14:48,480
because later you are predicting for

2027
01:14:46,739 --> 01:14:50,940
example the sentiment of the sentence

2028
01:14:48,480 --> 01:14:51,840
and so it's okay for these nodes to talk

2029
01:14:50,940 --> 01:14:54,420
to each other

2030
01:14:51,840 --> 01:14:57,900
and so in those cases you will use an

2031
01:14:54,420 --> 01:14:59,940
encoder block of self-attention and all

2032
01:14:57,900 --> 01:15:01,620
it means that it's an encoder block is

2033
01:14:59,940 --> 01:15:03,540
that you will delete this line of code

2034
01:15:01,620 --> 01:15:05,159
allowing all the nodes to completely

2035
01:15:03,540 --> 01:15:06,600
talk to each other what we're

2036
01:15:05,159 --> 01:15:09,800
implementing here is sometimes called a

2037
01:15:06,600 --> 01:15:13,679
decoder block and it's called a decoder

2038
01:15:09,800 --> 01:15:15,360
because it is sort of like a decoding

2039
01:15:13,679 --> 01:15:17,580
language and it's got this Auto

2040
01:15:15,360 --> 01:15:21,060
aggressive format where you have to mask

2041
01:15:17,580 --> 01:15:22,860
with the Triangular Matrix so that nodes

2042
01:15:21,060 --> 01:15:25,380
from the future never talk to the Past

2043
01:15:22,860 --> 01:15:27,719
because they would give away the answer

2044
01:15:25,380 --> 01:15:29,580
and so basically in encoder blocks you

2045
01:15:27,719 --> 01:15:31,560
would delete this allow all the nodes to

2046
01:15:29,580 --> 01:15:33,540
talk in decoder blocks this will always

2047
01:15:31,560 --> 01:15:35,520
be present so that you have this

2048
01:15:33,540 --> 01:15:36,840
triangular structure but both are

2049
01:15:35,520 --> 01:15:38,280
allowed and attention doesn't care

2050
01:15:36,840 --> 01:15:39,780
attention supports arbitrary

2051
01:15:38,280 --> 01:15:41,520
connectivity between nodes

2052
01:15:39,780 --> 01:15:43,440
the next thing I wanted to comment on is

2053
01:15:41,520 --> 01:15:45,420
you keep me you keep hearing me say

2054
01:15:43,440 --> 01:15:46,560
attention self-attention Etc there's

2055
01:15:45,420 --> 01:15:48,320
actually also something called cross

2056
01:15:46,560 --> 01:15:50,060
attention what is the difference

2057
01:15:48,320 --> 01:15:53,159
so

2058
01:15:50,060 --> 01:15:56,219
basically the reason this attention is

2059
01:15:53,159 --> 01:15:58,260
self-attention is because the keys

2060
01:15:56,219 --> 01:16:01,679
queries and the values are all coming

2061
01:15:58,260 --> 01:16:03,659
from the same Source from X so the same

2062
01:16:01,679 --> 01:16:06,480
Source X produces case queries and

2063
01:16:03,659 --> 01:16:09,000
values so these nodes are self-attending

2064
01:16:06,480 --> 01:16:10,920
but in principle attention is much more

2065
01:16:09,000 --> 01:16:13,500
General than that so for example an

2066
01:16:10,920 --> 01:16:15,420
encoder decoder Transformers uh you can

2067
01:16:13,500 --> 01:16:17,580
have a case where the queries are

2068
01:16:15,420 --> 01:16:18,900
produced from X but the keys and the

2069
01:16:17,580 --> 01:16:21,179
values come from a whole separate

2070
01:16:18,900 --> 01:16:23,880
external source and sometimes from

2071
01:16:21,179 --> 01:16:25,920
encoder blocks that encode some context

2072
01:16:23,880 --> 01:16:27,120
that we'd like to condition on and so

2073
01:16:25,920 --> 01:16:29,280
the keys and the values will actually

2074
01:16:27,120 --> 01:16:31,440
come from a whole separate Source those

2075
01:16:29,280 --> 01:16:33,480
are nodes on the side and here we're

2076
01:16:31,440 --> 01:16:35,219
just producing queries and we're reading

2077
01:16:33,480 --> 01:16:37,679
off information from the side

2078
01:16:35,219 --> 01:16:41,040
so cross attention is used when there's

2079
01:16:37,679 --> 01:16:44,280
a separate source of nodes we'd like to

2080
01:16:41,040 --> 01:16:45,540
pull information from into our nodes and

2081
01:16:44,280 --> 01:16:46,860
it's self-attention if we just have

2082
01:16:45,540 --> 01:16:48,239
nodes that would like to look at each

2083
01:16:46,860 --> 01:16:50,400
other and talk to each other

2084
01:16:48,239 --> 01:16:52,739
so this attention here happens to be

2085
01:16:50,400 --> 01:16:54,420
self-attention

2086
01:16:52,739 --> 01:16:55,260
but in principle

2087
01:16:54,420 --> 01:16:57,300
um

2088
01:16:55,260 --> 01:16:59,340
attention is a lot more General okay in

2089
01:16:57,300 --> 01:17:00,480
the last note at this stage is if we

2090
01:16:59,340 --> 01:17:02,460
come to the attention is all you need

2091
01:17:00,480 --> 01:17:04,760
paper here we've already implemented

2092
01:17:02,460 --> 01:17:07,800
attention so given query key and value

2093
01:17:04,760 --> 01:17:09,900
we've multiplied the query on the key

2094
01:17:07,800 --> 01:17:11,699
we've softmaxed it and then we are

2095
01:17:09,900 --> 01:17:12,780
aggregating the values

2096
01:17:11,699 --> 01:17:14,159
there's one more thing that we're

2097
01:17:12,780 --> 01:17:16,260
missing here which is the dividing by

2098
01:17:14,159 --> 01:17:19,080
one over square root of the head size

2099
01:17:16,260 --> 01:17:21,300
the DK here is the head size why aren't

2100
01:17:19,080 --> 01:17:23,940
they doing this once it's important so

2101
01:17:21,300 --> 01:17:25,380
they call it a scaled attention

2102
01:17:23,940 --> 01:17:27,780
and it's kind of like an important

2103
01:17:25,380 --> 01:17:30,000
normalization to basically have

2104
01:17:27,780 --> 01:17:32,520
the problem is if you have unit gaussian

2105
01:17:30,000 --> 01:17:34,380
inputs so zero mean unit variance K and

2106
01:17:32,520 --> 01:17:36,960
Q are unit caution and if you just do

2107
01:17:34,380 --> 01:17:39,060
way naively then you see that your way

2108
01:17:36,960 --> 01:17:40,679
actually will be uh the variance will be

2109
01:17:39,060 --> 01:17:42,300
on the order of head size which in our

2110
01:17:40,679 --> 01:17:44,340
case is 16.

2111
01:17:42,300 --> 01:17:46,440
but if you multiply by one over head

2112
01:17:44,340 --> 01:17:48,179
size square root so this is square root

2113
01:17:46,440 --> 01:17:50,940
and this is one over

2114
01:17:48,179 --> 01:17:52,800
then the variance of way will be one so

2115
01:17:50,940 --> 01:17:55,199
it will be preserved

2116
01:17:52,800 --> 01:17:57,060
now why is this important you'll notice

2117
01:17:55,199 --> 01:17:59,580
that way here

2118
01:17:57,060 --> 01:18:01,140
will feed into softmax

2119
01:17:59,580 --> 01:18:03,600
and so it's really important especially

2120
01:18:01,140 --> 01:18:04,679
at initialization that way be fairly

2121
01:18:03,600 --> 01:18:07,560
diffuse

2122
01:18:04,679 --> 01:18:10,860
so in our case here we sort of lucked

2123
01:18:07,560 --> 01:18:13,020
out here and weigh had a fairly diffuse

2124
01:18:10,860 --> 01:18:15,480
numbers here so

2125
01:18:13,020 --> 01:18:17,460
um like this now the problem is that

2126
01:18:15,480 --> 01:18:19,080
because of softmax if weight takes on

2127
01:18:17,460 --> 01:18:21,840
very positive and very negative numbers

2128
01:18:19,080 --> 01:18:25,020
inside it softmax will actually converge

2129
01:18:21,840 --> 01:18:27,239
towards one hot vectors and so I can

2130
01:18:25,020 --> 01:18:28,080
illustrate that here

2131
01:18:27,239 --> 01:18:30,540
um

2132
01:18:28,080 --> 01:18:32,159
say we are applying softmax to a tensor

2133
01:18:30,540 --> 01:18:33,840
of values that are very close to zero

2134
01:18:32,159 --> 01:18:35,520
then we're going to get a diffuse thing

2135
01:18:33,840 --> 01:18:36,960
out of softmax

2136
01:18:35,520 --> 01:18:38,880
but the moment I take the exact same

2137
01:18:36,960 --> 01:18:40,320
thing and I start sharpening it making

2138
01:18:38,880 --> 01:18:42,600
it bigger by multiplying these numbers

2139
01:18:40,320 --> 01:18:44,699
by eight for example you'll see that the

2140
01:18:42,600 --> 01:18:47,040
soft Max will start to sharpen and in

2141
01:18:44,699 --> 01:18:48,480
fact it will sharpen towards the max so

2142
01:18:47,040 --> 01:18:50,100
it will sharpen towards whatever number

2143
01:18:48,480 --> 01:18:51,420
here is the highest

2144
01:18:50,100 --> 01:18:52,679
and so

2145
01:18:51,420 --> 01:18:54,000
um basically we don't want these values

2146
01:18:52,679 --> 01:18:55,980
to be too extreme especially the

2147
01:18:54,000 --> 01:18:58,620
initialization otherwise softmax will be

2148
01:18:55,980 --> 01:19:00,000
way too peaky and you're basically

2149
01:18:58,620 --> 01:19:01,860
aggregating

2150
01:19:00,000 --> 01:19:03,360
um information from like a single node

2151
01:19:01,860 --> 01:19:05,219
every node just Aggregates information

2152
01:19:03,360 --> 01:19:06,900
from a single other node that's not what

2153
01:19:05,219 --> 01:19:09,600
we want especially its initialization

2154
01:19:06,900 --> 01:19:11,340
and so the scaling is used just to

2155
01:19:09,600 --> 01:19:13,920
control the variance at initialization

2156
01:19:11,340 --> 01:19:15,900
okay so having said all that let's now

2157
01:19:13,920 --> 01:19:18,060
take our soft retention knowledge and

2158
01:19:15,900 --> 01:19:20,400
let's take it for a spin

2159
01:19:18,060 --> 01:19:22,800
so here in the code I created this head

2160
01:19:20,400 --> 01:19:24,060
module and implements a single head of

2161
01:19:22,800 --> 01:19:26,400
self-attention

2162
01:19:24,060 --> 01:19:28,020
so you give it a head size and then here

2163
01:19:26,400 --> 01:19:30,360
it creates the key query and the value

2164
01:19:28,020 --> 01:19:32,159
linear layers typically people don't use

2165
01:19:30,360 --> 01:19:33,900
biases in these

2166
01:19:32,159 --> 01:19:36,300
so those are the linear projections that

2167
01:19:33,900 --> 01:19:38,340
we're going to apply to all of our nodes

2168
01:19:36,300 --> 01:19:40,800
now here I'm creating this Trill

2169
01:19:38,340 --> 01:19:42,719
variable Trill is not a parameter of the

2170
01:19:40,800 --> 01:19:45,060
module so in sort of pythonomic

2171
01:19:42,719 --> 01:19:46,980
conventions this is called a buffer it's

2172
01:19:45,060 --> 01:19:48,179
not a parameter and you have to call it

2173
01:19:46,980 --> 01:19:50,400
you have to assign it to the module

2174
01:19:48,179 --> 01:19:51,300
using a register buffer so that creates

2175
01:19:50,400 --> 01:19:54,360
the trail

2176
01:19:51,300 --> 01:19:56,219
uh the triangle lower triangular Matrix

2177
01:19:54,360 --> 01:19:57,659
and when we're given the input X this

2178
01:19:56,219 --> 01:19:59,940
should look very familiar now we

2179
01:19:57,659 --> 01:20:01,800
calculate the keys the queries we call

2180
01:19:59,940 --> 01:20:04,380
it clock in the attentions course inside

2181
01:20:01,800 --> 01:20:06,060
way we normalize it so we're using

2182
01:20:04,380 --> 01:20:08,280
scaled attention here

2183
01:20:06,060 --> 01:20:10,140
then we make sure that a feature doesn't

2184
01:20:08,280 --> 01:20:11,940
communicate with the past so this makes

2185
01:20:10,140 --> 01:20:14,159
it a decoder block

2186
01:20:11,940 --> 01:20:16,440
and then softmax and then aggregate the

2187
01:20:14,159 --> 01:20:17,820
value and output

2188
01:20:16,440 --> 01:20:20,580
then here in the language model I'm

2189
01:20:17,820 --> 01:20:22,800
creating a head in the Constructor and

2190
01:20:20,580 --> 01:20:24,960
I'm calling it self attention head and

2191
01:20:22,800 --> 01:20:28,380
the head size I'm going to keep as the

2192
01:20:24,960 --> 01:20:31,260
same and embed just for now

2193
01:20:28,380 --> 01:20:33,120
and then here once we've encoded the

2194
01:20:31,260 --> 01:20:34,920
information with the token embeddings

2195
01:20:33,120 --> 01:20:35,880
and the position embeddings we're simply

2196
01:20:34,920 --> 01:20:37,980
going to feed it into the

2197
01:20:35,880 --> 01:20:40,620
self-attentioned head and then the

2198
01:20:37,980 --> 01:20:43,380
output of that is going to go into uh

2199
01:20:40,620 --> 01:20:45,600
the decoder language modeling head and

2200
01:20:43,380 --> 01:20:47,219
create the logits so this is the sort of

2201
01:20:45,600 --> 01:20:49,620
the simplest way to plug in a

2202
01:20:47,219 --> 01:20:50,940
self-attention component into our

2203
01:20:49,620 --> 01:20:53,400
Network right now

2204
01:20:50,940 --> 01:20:55,020
I had to make one more change which is

2205
01:20:53,400 --> 01:20:57,540
that here

2206
01:20:55,020 --> 01:21:00,840
in the generate we have to make sure

2207
01:20:57,540 --> 01:21:02,280
that our idx that we feed into the model

2208
01:21:00,840 --> 01:21:04,560
because now we're using positional

2209
01:21:02,280 --> 01:21:07,679
embeddings we can never have more than

2210
01:21:04,560 --> 01:21:09,480
block size coming in because if idx is

2211
01:21:07,679 --> 01:21:11,280
more than block size then our position

2212
01:21:09,480 --> 01:21:12,780
embedding table is going to run out of

2213
01:21:11,280 --> 01:21:14,280
scope because it only has embeddings for

2214
01:21:12,780 --> 01:21:16,440
up to block size

2215
01:21:14,280 --> 01:21:18,840
and so therefore I added some code here

2216
01:21:16,440 --> 01:21:21,020
to crop the context that we're going to

2217
01:21:18,840 --> 01:21:23,940
feed into self

2218
01:21:21,020 --> 01:21:25,560
so that we never pass in more than block

2219
01:21:23,940 --> 01:21:27,179
size elements

2220
01:21:25,560 --> 01:21:29,219
so those are the changes and let's Now

2221
01:21:27,179 --> 01:21:31,080
train the network okay so I also came up

2222
01:21:29,219 --> 01:21:33,000
to the script here and I decreased the

2223
01:21:31,080 --> 01:21:34,800
learning rate because the self-attention

2224
01:21:33,000 --> 01:21:37,199
can't tolerate very very high learning

2225
01:21:34,800 --> 01:21:38,159
rates and then I also increase the

2226
01:21:37,199 --> 01:21:39,840
number of iterations because the

2227
01:21:38,159 --> 01:21:41,460
learning rate is lower and then I

2228
01:21:39,840 --> 01:21:43,860
trained it and previously we were only

2229
01:21:41,460 --> 01:21:46,440
able to get to up to 2.5 and now we are

2230
01:21:43,860 --> 01:21:48,239
down to 2.4 so we definitely see a

2231
01:21:46,440 --> 01:21:50,880
little bit of an improvement from 2.5 to

2232
01:21:48,239 --> 01:21:53,640
2.4 roughly but the text is still not

2233
01:21:50,880 --> 01:21:55,739
amazing so clearly the self-attention

2234
01:21:53,640 --> 01:21:57,540
head is doing some useful communication

2235
01:21:55,739 --> 01:21:59,520
but

2236
01:21:57,540 --> 01:22:00,659
um we still have a long way to go okay

2237
01:21:59,520 --> 01:22:02,940
so now we've implemented the

2238
01:22:00,659 --> 01:22:04,380
scale.product attention now next up in

2239
01:22:02,940 --> 01:22:06,179
the attention is all you need paper

2240
01:22:04,380 --> 01:22:07,920
there's something called multi-head

2241
01:22:06,179 --> 01:22:10,320
attention and what is multi-head

2242
01:22:07,920 --> 01:22:12,780
attention it's just applying multiple

2243
01:22:10,320 --> 01:22:13,920
attentions in parallel and concatenating

2244
01:22:12,780 --> 01:22:15,600
the results

2245
01:22:13,920 --> 01:22:17,659
so they have a little bit of diagram

2246
01:22:15,600 --> 01:22:20,640
here I don't know if this is super clear

2247
01:22:17,659 --> 01:22:21,659
it's really just multiple attentions in

2248
01:22:20,640 --> 01:22:23,760
parallel

2249
01:22:21,659 --> 01:22:25,260
so let's Implement that fairly

2250
01:22:23,760 --> 01:22:27,540
straightforward

2251
01:22:25,260 --> 01:22:28,980
if we want a multi-head attention then

2252
01:22:27,540 --> 01:22:30,840
we want multiple heads of self-attention

2253
01:22:28,980 --> 01:22:33,420
running in parallel

2254
01:22:30,840 --> 01:22:36,120
so in pytorch we can do this by simply

2255
01:22:33,420 --> 01:22:38,400
creating multiple heads

2256
01:22:36,120 --> 01:22:39,960
so however heads how many however many

2257
01:22:38,400 --> 01:22:41,520
heads you want and then what is the head

2258
01:22:39,960 --> 01:22:44,100
size of each

2259
01:22:41,520 --> 01:22:47,100
and then we run all of them in parallel

2260
01:22:44,100 --> 01:22:49,500
into a list and simply concatenate all

2261
01:22:47,100 --> 01:22:51,659
of the outputs and we're concatenating

2262
01:22:49,500 --> 01:22:53,699
over the channel dimension

2263
01:22:51,659 --> 01:22:55,920
so the way this looks now is we don't

2264
01:22:53,699 --> 01:22:58,980
have just a single attention

2265
01:22:55,920 --> 01:23:01,500
that has a hit size of 32 because

2266
01:22:58,980 --> 01:23:03,719
remember and in bed is 32.

2267
01:23:01,500 --> 01:23:06,179
instead of having one Communication

2268
01:23:03,719 --> 01:23:08,880
channel we now have four communication

2269
01:23:06,179 --> 01:23:10,280
channels in parallel and each one of

2270
01:23:08,880 --> 01:23:14,820
these communication channels typically

2271
01:23:10,280 --> 01:23:16,020
will be smaller correspondingly so

2272
01:23:14,820 --> 01:23:17,940
because we have four communication

2273
01:23:16,020 --> 01:23:20,040
channels we want eight dimensional

2274
01:23:17,940 --> 01:23:21,420
self-attention and so from each

2275
01:23:20,040 --> 01:23:23,640
Communication channel we're going to

2276
01:23:21,420 --> 01:23:25,080
gather eight dimensional vectors and

2277
01:23:23,640 --> 01:23:27,360
then we have four of them and that

2278
01:23:25,080 --> 01:23:28,980
concatenates to give us 32 which is the

2279
01:23:27,360 --> 01:23:31,320
original and embed

2280
01:23:28,980 --> 01:23:32,580
and so this is kind of similar to um if

2281
01:23:31,320 --> 01:23:33,800
you're familiar with convolutions this

2282
01:23:32,580 --> 01:23:36,540
is kind of like a group convolution

2283
01:23:33,800 --> 01:23:38,280
because basically instead of having one

2284
01:23:36,540 --> 01:23:40,860
large convolution we do convolutional

2285
01:23:38,280 --> 01:23:42,360
groups and uh that's multi-headed

2286
01:23:40,860 --> 01:23:45,480
self-attention

2287
01:23:42,360 --> 01:23:47,640
and so then here we just use sa heads

2288
01:23:45,480 --> 01:23:50,820
self-attussion Heads instead

2289
01:23:47,640 --> 01:23:52,140
now I actually ran it and uh scrolling

2290
01:23:50,820 --> 01:23:53,940
down

2291
01:23:52,140 --> 01:23:58,020
I ran the same thing and then we now get

2292
01:23:53,940 --> 01:23:59,340
this down to 2.28 roughly and the output

2293
01:23:58,020 --> 01:24:01,320
is still the generation is still not

2294
01:23:59,340 --> 01:24:03,719
amazing but clearly the validation loss

2295
01:24:01,320 --> 01:24:04,739
is improving because we were at 2.4 just

2296
01:24:03,719 --> 01:24:06,480
now

2297
01:24:04,739 --> 01:24:08,159
and so it helps to have multiple

2298
01:24:06,480 --> 01:24:10,199
communication channels because obviously

2299
01:24:08,159 --> 01:24:12,360
these tokens have a lot to talk about

2300
01:24:10,199 --> 01:24:14,100
and they want to find the consonants the

2301
01:24:12,360 --> 01:24:16,440
vowels they want to find the vowels just

2302
01:24:14,100 --> 01:24:19,020
from certain positions they want to find

2303
01:24:16,440 --> 01:24:20,520
any kinds of different things and so it

2304
01:24:19,020 --> 01:24:22,380
helps to create multiple independent

2305
01:24:20,520 --> 01:24:25,260
channels of communication gather lots of

2306
01:24:22,380 --> 01:24:27,120
different types of data and then decode

2307
01:24:25,260 --> 01:24:28,800
the output now going back to the paper

2308
01:24:27,120 --> 01:24:31,080
for a second of course I didn't explain

2309
01:24:28,800 --> 01:24:32,280
this figure in full detail but we are

2310
01:24:31,080 --> 01:24:33,840
starting to see some components of what

2311
01:24:32,280 --> 01:24:35,580
we've already implemented we have the

2312
01:24:33,840 --> 01:24:38,040
positional encodings the token encodings

2313
01:24:35,580 --> 01:24:41,159
that add we have the masked multi-headed

2314
01:24:38,040 --> 01:24:42,900
attention implemented now here's another

2315
01:24:41,159 --> 01:24:45,120
multi-headed tension which is a cross

2316
01:24:42,900 --> 01:24:46,679
attention to an encoder which we haven't

2317
01:24:45,120 --> 01:24:48,120
we're not going to implement in this

2318
01:24:46,679 --> 01:24:49,320
case I'm going to come back to that

2319
01:24:48,120 --> 01:24:50,880
later

2320
01:24:49,320 --> 01:24:52,920
but I want you to notice that there's a

2321
01:24:50,880 --> 01:24:54,600
feed forward part here and then this is

2322
01:24:52,920 --> 01:24:55,739
grouped into a block that gets repeated

2323
01:24:54,600 --> 01:24:57,659
again and again

2324
01:24:55,739 --> 01:25:00,239
now the feed forward part here is just a

2325
01:24:57,659 --> 01:25:01,620
simple multi-layer perceptron

2326
01:25:00,239 --> 01:25:03,840
um

2327
01:25:01,620 --> 01:25:06,360
so the multi-headed so here position

2328
01:25:03,840 --> 01:25:08,040
wise feed forward networks is just a

2329
01:25:06,360 --> 01:25:09,840
simple little MLP

2330
01:25:08,040 --> 01:25:11,820
so I want to start basically in a

2331
01:25:09,840 --> 01:25:13,199
similar fashion also adding computation

2332
01:25:11,820 --> 01:25:15,360
into the network

2333
01:25:13,199 --> 01:25:17,340
and this computation is on the per node

2334
01:25:15,360 --> 01:25:19,020
level so

2335
01:25:17,340 --> 01:25:20,640
I've already implemented it and you can

2336
01:25:19,020 --> 01:25:22,800
see the diff highlighted on the left

2337
01:25:20,640 --> 01:25:25,260
here when I've added or changed things

2338
01:25:22,800 --> 01:25:26,340
now before we had the multi-headed

2339
01:25:25,260 --> 01:25:28,739
self-attention that did the

2340
01:25:26,340 --> 01:25:31,860
communication but we went way too fast

2341
01:25:28,739 --> 01:25:32,940
to calculate the logits so the tokens

2342
01:25:31,860 --> 01:25:35,400
looked at each other but didn't really

2343
01:25:32,940 --> 01:25:37,440
have a lot of time to think on what they

2344
01:25:35,400 --> 01:25:40,199
found from the other tokens

2345
01:25:37,440 --> 01:25:42,659
and so what I've implemented here is a

2346
01:25:40,199 --> 01:25:44,400
little feed forward single layer and

2347
01:25:42,659 --> 01:25:46,500
this little layer is just a linear

2348
01:25:44,400 --> 01:25:47,880
followed by a relative nonlinearity and

2349
01:25:46,500 --> 01:25:51,000
that's that's it

2350
01:25:47,880 --> 01:25:52,980
so it's just a little layer and then I

2351
01:25:51,000 --> 01:25:54,840
call it feed forward

2352
01:25:52,980 --> 01:25:56,280
and embed

2353
01:25:54,840 --> 01:25:57,840
and then this feed forward is just

2354
01:25:56,280 --> 01:26:00,719
called sequentially right after the

2355
01:25:57,840 --> 01:26:02,699
self-attention so we self-attend then we

2356
01:26:00,719 --> 01:26:04,199
feed forward and you'll notice that the

2357
01:26:02,699 --> 01:26:06,780
feet forward here when it's applying

2358
01:26:04,199 --> 01:26:09,480
linear this is on a per token level all

2359
01:26:06,780 --> 01:26:11,639
the tokens do this independently so the

2360
01:26:09,480 --> 01:26:13,080
self-attention is the communication and

2361
01:26:11,639 --> 01:26:14,699
then once they've gathered all the data

2362
01:26:13,080 --> 01:26:16,020
now they need to think on that data

2363
01:26:14,699 --> 01:26:17,699
individually

2364
01:26:16,020 --> 01:26:20,639
and so that's what feed forward is doing

2365
01:26:17,699 --> 01:26:22,500
and that's why I've added it here now

2366
01:26:20,639 --> 01:26:24,179
when I train this the validation loss

2367
01:26:22,500 --> 01:26:28,199
actually continues to go down now to

2368
01:26:24,179 --> 01:26:30,239
2.24 which is down from 2.28 the output

2369
01:26:28,199 --> 01:26:31,980
still look kind of terrible but at least

2370
01:26:30,239 --> 01:26:34,080
we've improved the situation

2371
01:26:31,980 --> 01:26:36,080
and so as a preview

2372
01:26:34,080 --> 01:26:39,239
we're going to now start to intersperse

2373
01:26:36,080 --> 01:26:41,460
the communication with the computation

2374
01:26:39,239 --> 01:26:43,860
and that's also what the Transformer

2375
01:26:41,460 --> 01:26:46,739
does when it has blocks that communicate

2376
01:26:43,860 --> 01:26:48,600
and then compute and it groups them and

2377
01:26:46,739 --> 01:26:50,699
replicates them

2378
01:26:48,600 --> 01:26:52,560
okay so let me show you what we like to

2379
01:26:50,699 --> 01:26:54,480
do we'd like to do something like this

2380
01:26:52,560 --> 01:26:56,820
we have a block and this block is

2381
01:26:54,480 --> 01:26:58,500
basically this part here except for the

2382
01:26:56,820 --> 01:27:00,480
cross attention

2383
01:26:58,500 --> 01:27:02,880
now the block basically intersperses

2384
01:27:00,480 --> 01:27:04,500
communication and then computation the

2385
01:27:02,880 --> 01:27:06,719
computation the communication is done

2386
01:27:04,500 --> 01:27:08,400
using multi-headed self-attention and

2387
01:27:06,719 --> 01:27:10,380
then the computation is done using the

2388
01:27:08,400 --> 01:27:12,360
feed forward Network on all the tokens

2389
01:27:10,380 --> 01:27:15,120
independently

2390
01:27:12,360 --> 01:27:17,159
now what I've added here also is you'll

2391
01:27:15,120 --> 01:27:18,840
notice

2392
01:27:17,159 --> 01:27:20,159
this takes the number of embeddings in

2393
01:27:18,840 --> 01:27:21,600
the embedding Dimension and number of

2394
01:27:20,159 --> 01:27:23,420
heads that we would like which is kind

2395
01:27:21,600 --> 01:27:25,860
of like group size in group convolution

2396
01:27:23,420 --> 01:27:29,219
and I'm saying that number of heads we'd

2397
01:27:25,860 --> 01:27:31,020
like is for and so because this is 32 we

2398
01:27:29,219 --> 01:27:33,480
calculate that because this 32 the

2399
01:27:31,020 --> 01:27:35,280
number of hats should be four

2400
01:27:33,480 --> 01:27:36,900
um there's num the head size should be

2401
01:27:35,280 --> 01:27:38,940
eight so that everything sort of works

2402
01:27:36,900 --> 01:27:40,199
out Channel wise

2403
01:27:38,940 --> 01:27:42,840
um so this is how the Transformer

2404
01:27:40,199 --> 01:27:44,219
structures uh sort of the uh the sizes

2405
01:27:42,840 --> 01:27:45,900
typically

2406
01:27:44,219 --> 01:27:47,280
so the head size will become eight and

2407
01:27:45,900 --> 01:27:49,500
then this is how we want to intersperse

2408
01:27:47,280 --> 01:27:51,540
them and then here I'm trying to create

2409
01:27:49,500 --> 01:27:54,239
blocks which is just a sequential

2410
01:27:51,540 --> 01:27:55,980
application of block block so that we're

2411
01:27:54,239 --> 01:27:58,199
interspersing communication feed forward

2412
01:27:55,980 --> 01:27:59,340
many many times and then finally we

2413
01:27:58,199 --> 01:28:02,219
decode

2414
01:27:59,340 --> 01:28:03,540
now actually try to run this and the

2415
01:28:02,219 --> 01:28:06,540
problem is this doesn't actually give a

2416
01:28:03,540 --> 01:28:08,219
very good uh answer a very good result

2417
01:28:06,540 --> 01:28:09,840
and the reason for that is we're

2418
01:28:08,219 --> 01:28:12,480
starting to actually get like a pretty

2419
01:28:09,840 --> 01:28:14,100
deep neural net and deep neural Nets uh

2420
01:28:12,480 --> 01:28:15,120
suffer from optimization issues and I

2421
01:28:14,100 --> 01:28:17,219
think that's where we're kind of like

2422
01:28:15,120 --> 01:28:19,320
slightly starting to run into so we need

2423
01:28:17,219 --> 01:28:20,219
one more idea that we can borrow from

2424
01:28:19,320 --> 01:28:21,960
the

2425
01:28:20,219 --> 01:28:23,639
um Transformer paper to resolve those

2426
01:28:21,960 --> 01:28:25,620
difficulties now there are two

2427
01:28:23,639 --> 01:28:27,659
optimizations that dramatically help

2428
01:28:25,620 --> 01:28:29,400
with the depth of these networks and

2429
01:28:27,659 --> 01:28:31,080
make sure that the networks remain

2430
01:28:29,400 --> 01:28:31,860
optimizable let's talk about the first

2431
01:28:31,080 --> 01:28:33,900
one

2432
01:28:31,860 --> 01:28:35,219
the first one in this diagram is you see

2433
01:28:33,900 --> 01:28:37,920
this Arrow here

2434
01:28:35,219 --> 01:28:39,600
and then this arrow and this Arrow those

2435
01:28:37,920 --> 01:28:41,400
are skip connections or sometimes called

2436
01:28:39,600 --> 01:28:43,739
residual connections

2437
01:28:41,400 --> 01:28:45,179
they come from this paper uh the

2438
01:28:43,739 --> 01:28:48,780
procedural learning form and recognition

2439
01:28:45,179 --> 01:28:49,800
from about 2015. that introduced the

2440
01:28:48,780 --> 01:28:53,040
concept

2441
01:28:49,800 --> 01:28:55,020
now these are basically what it means is

2442
01:28:53,040 --> 01:28:57,420
you transform the data but then you have

2443
01:28:55,020 --> 01:29:00,060
a skip connection with addition

2444
01:28:57,420 --> 01:29:03,239
from the previous features now the way I

2445
01:29:00,060 --> 01:29:04,980
like to visualize it that I prefer is

2446
01:29:03,239 --> 01:29:08,040
the following here the computation

2447
01:29:04,980 --> 01:29:10,020
happens from the top to bottom and

2448
01:29:08,040 --> 01:29:12,480
basically you have this uh residual

2449
01:29:10,020 --> 01:29:14,159
pathway and you are free to Fork off

2450
01:29:12,480 --> 01:29:16,080
from the residual pathway perform some

2451
01:29:14,159 --> 01:29:18,360
computation and then project back to the

2452
01:29:16,080 --> 01:29:22,139
residual pathway via addition

2453
01:29:18,360 --> 01:29:25,320
and so you go from the the inputs to the

2454
01:29:22,139 --> 01:29:26,820
targets only the plus and plus and plus

2455
01:29:25,320 --> 01:29:28,860
and the reason this is useful is because

2456
01:29:26,820 --> 01:29:31,620
during that propagation remember from

2457
01:29:28,860 --> 01:29:33,780
our micrograd video earlier addition

2458
01:29:31,620 --> 01:29:36,659
distributes gradients equally to both of

2459
01:29:33,780 --> 01:29:40,320
its branches that that fat as the input

2460
01:29:36,659 --> 01:29:42,900
and so the supervision or the gradients

2461
01:29:40,320 --> 01:29:45,360
from the loss basically hop

2462
01:29:42,900 --> 01:29:48,659
through every addition node all the way

2463
01:29:45,360 --> 01:29:51,179
to the input and then also Fork off into

2464
01:29:48,659 --> 01:29:52,800
the residual blocks

2465
01:29:51,179 --> 01:29:54,600
but basically you have this gradient

2466
01:29:52,800 --> 01:29:56,699
Super Highway that goes directly from

2467
01:29:54,600 --> 01:29:59,460
the supervision all the way to the input

2468
01:29:56,699 --> 01:30:00,840
unimpeded and then these virtual blocks

2469
01:29:59,460 --> 01:30:03,000
are usually initialized in the beginning

2470
01:30:00,840 --> 01:30:04,980
so they contribute very very little if

2471
01:30:03,000 --> 01:30:07,380
anything to the residual pathway they

2472
01:30:04,980 --> 01:30:08,880
they are initialized that way so in the

2473
01:30:07,380 --> 01:30:10,739
beginning they are sort of almost kind

2474
01:30:08,880 --> 01:30:13,460
of like not there but then during the

2475
01:30:10,739 --> 01:30:16,920
optimization they come online over time

2476
01:30:13,460 --> 01:30:18,540
and they start to contribute but at

2477
01:30:16,920 --> 01:30:20,400
least at the initialization you can go

2478
01:30:18,540 --> 01:30:23,100
from directly supervision to the input

2479
01:30:20,400 --> 01:30:26,280
gradient is unimpeded and just close and

2480
01:30:23,100 --> 01:30:27,780
then the blocks over time kick in and so

2481
01:30:26,280 --> 01:30:30,060
that dramatically helps with the

2482
01:30:27,780 --> 01:30:31,920
optimization so let's implement this so

2483
01:30:30,060 --> 01:30:34,620
coming back to our block here basically

2484
01:30:31,920 --> 01:30:36,540
what we want to do is we want to do x

2485
01:30:34,620 --> 01:30:38,520
equals X Plus

2486
01:30:36,540 --> 01:30:40,620
solve the tension and x equals X Plus

2487
01:30:38,520 --> 01:30:44,040
solve that feed forward

2488
01:30:40,620 --> 01:30:46,080
so this is X and then we Fork off and do

2489
01:30:44,040 --> 01:30:47,639
some communication and come back and we

2490
01:30:46,080 --> 01:30:48,900
Fork off and we do some computation and

2491
01:30:47,639 --> 01:30:51,179
come back

2492
01:30:48,900 --> 01:30:53,219
so those are residual connections and

2493
01:30:51,179 --> 01:30:54,540
then swinging back up here

2494
01:30:53,219 --> 01:30:55,980
we also have to introduce this

2495
01:30:54,540 --> 01:30:58,380
projection

2496
01:30:55,980 --> 01:31:01,920
so nn.linear

2497
01:30:58,380 --> 01:31:03,719
and this is going to be from

2498
01:31:01,920 --> 01:31:05,940
after we concatenate this this is the

2499
01:31:03,719 --> 01:31:07,800
precise and embed so this is the output

2500
01:31:05,940 --> 01:31:10,560
of the soft tension itself

2501
01:31:07,800 --> 01:31:12,120
but then we actually want the uh to

2502
01:31:10,560 --> 01:31:14,280
apply the projection

2503
01:31:12,120 --> 01:31:15,659
and that's the result

2504
01:31:14,280 --> 01:31:17,100
so the projection is just a linear

2505
01:31:15,659 --> 01:31:18,840
transformation of the outcome of this

2506
01:31:17,100 --> 01:31:20,460
layer

2507
01:31:18,840 --> 01:31:21,780
so that's the projection back into the

2508
01:31:20,460 --> 01:31:23,580
residual pathway

2509
01:31:21,780 --> 01:31:25,500
and then here in the feed forward it's

2510
01:31:23,580 --> 01:31:28,139
going to be the same thing I could have

2511
01:31:25,500 --> 01:31:29,580
a soft.projection here as well but let

2512
01:31:28,139 --> 01:31:31,400
me just simplify it

2513
01:31:29,580 --> 01:31:33,719
and let me

2514
01:31:31,400 --> 01:31:34,679
couple it inside the same sequential

2515
01:31:33,719 --> 01:31:36,420
container

2516
01:31:34,679 --> 01:31:39,060
and so this is the projection layer

2517
01:31:36,420 --> 01:31:40,860
going back into the residual pathway

2518
01:31:39,060 --> 01:31:43,380
and so

2519
01:31:40,860 --> 01:31:44,940
that's uh well that's it so now we can

2520
01:31:43,380 --> 01:31:47,340
train this so I implemented one more

2521
01:31:44,940 --> 01:31:49,739
small change when you look into the

2522
01:31:47,340 --> 01:31:51,600
paper again you see that the

2523
01:31:49,739 --> 01:31:53,580
dimensionality of input and output is

2524
01:31:51,600 --> 01:31:55,380
512 for them and they're saying that the

2525
01:31:53,580 --> 01:31:57,719
inner layer here in the feed forward has

2526
01:31:55,380 --> 01:31:59,219
dimensionality of 2048. so there's a

2527
01:31:57,719 --> 01:32:01,260
multiplier of four

2528
01:31:59,219 --> 01:32:02,639
and so the inner layer of the feed

2529
01:32:01,260 --> 01:32:04,260
forward Network

2530
01:32:02,639 --> 01:32:06,179
should be multiplied by four in terms of

2531
01:32:04,260 --> 01:32:08,580
Channel sizes so I came here and I

2532
01:32:06,179 --> 01:32:10,440
multiplied to four times embed here for

2533
01:32:08,580 --> 01:32:12,420
the feed forward and then from four

2534
01:32:10,440 --> 01:32:14,400
times n embed coming back down to an

2535
01:32:12,420 --> 01:32:16,260
embed when we go back to the project to

2536
01:32:14,400 --> 01:32:18,480
the projection so adding a bit of

2537
01:32:16,260 --> 01:32:20,400
computation here and growing that layer

2538
01:32:18,480 --> 01:32:22,920
that is in the residual block on the

2539
01:32:20,400 --> 01:32:24,659
side of the residual pathway

2540
01:32:22,920 --> 01:32:27,120
and then I trained this and we actually

2541
01:32:24,659 --> 01:32:29,280
get down all the way to uh 2.08

2542
01:32:27,120 --> 01:32:30,420
validation loss and we also see that the

2543
01:32:29,280 --> 01:32:32,400
network is starting to get big enough

2544
01:32:30,420 --> 01:32:33,900
that our train loss is getting ahead of

2545
01:32:32,400 --> 01:32:36,000
validation loss so we're starting to see

2546
01:32:33,900 --> 01:32:39,500
like a little bit of overfitting

2547
01:32:36,000 --> 01:32:41,699
and um our our um

2548
01:32:39,500 --> 01:32:43,199
Generations here are still not amazing

2549
01:32:41,699 --> 01:32:46,380
but at least you see that we can see

2550
01:32:43,199 --> 01:32:48,179
like is here this now grieve sank

2551
01:32:46,380 --> 01:32:49,920
like this starts to almost look like

2552
01:32:48,179 --> 01:32:51,060
English so

2553
01:32:49,920 --> 01:32:52,920
um yeah we're starting to really get

2554
01:32:51,060 --> 01:32:54,780
there okay and the second Innovation

2555
01:32:52,920 --> 01:32:57,360
that is very helpful for optimizing very

2556
01:32:54,780 --> 01:32:58,440
deep neural networks is right here so we

2557
01:32:57,360 --> 01:33:00,719
have this addition now that's the

2558
01:32:58,440 --> 01:33:02,400
residual part but this Norm is referring

2559
01:33:00,719 --> 01:33:04,440
to something called layer Norm

2560
01:33:02,400 --> 01:33:06,659
so layer Norm is implemented in pi torch

2561
01:33:04,440 --> 01:33:08,699
it's a paper that came out a while back

2562
01:33:06,659 --> 01:33:10,080
here

2563
01:33:08,699 --> 01:33:11,760
um

2564
01:33:10,080 --> 01:33:14,880
and layer Norm is very very similar to

2565
01:33:11,760 --> 01:33:16,860
Bachelor so remember back to our make

2566
01:33:14,880 --> 01:33:18,540
more series part three we implemented

2567
01:33:16,860 --> 01:33:20,400
batch normalization

2568
01:33:18,540 --> 01:33:23,340
and patch normalization basically just

2569
01:33:20,400 --> 01:33:27,080
made sure that across the batch

2570
01:33:23,340 --> 01:33:28,880
Dimension any individual neuron had unit

2571
01:33:27,080 --> 01:33:31,679
gaussian

2572
01:33:28,880 --> 01:33:33,719
distribution so it was zero mean and

2573
01:33:31,679 --> 01:33:35,699
unit standard deviation one standard

2574
01:33:33,719 --> 01:33:37,920
deviation output

2575
01:33:35,699 --> 01:33:39,600
so what I did here is I'm copy pasting

2576
01:33:37,920 --> 01:33:40,860
The Bachelor 1D that we developed in our

2577
01:33:39,600 --> 01:33:43,679
makemore series

2578
01:33:40,860 --> 01:33:45,659
and see here we can initialize for

2579
01:33:43,679 --> 01:33:48,120
example this module and we can have a

2580
01:33:45,659 --> 01:33:50,159
batch of 32 100 dimensional vectors

2581
01:33:48,120 --> 01:33:53,400
feeding through the bathroom layer

2582
01:33:50,159 --> 01:33:55,260
so what this does is it guarantees

2583
01:33:53,400 --> 01:33:56,340
that when we look at just the zeroth

2584
01:33:55,260 --> 01:33:59,219
column

2585
01:33:56,340 --> 01:34:01,860
it's a zero mean one standard deviation

2586
01:33:59,219 --> 01:34:03,840
so it's normalizing every single column

2587
01:34:01,860 --> 01:34:06,480
of this input

2588
01:34:03,840 --> 01:34:08,219
now the rows are not going to be

2589
01:34:06,480 --> 01:34:10,380
normalized by default because we're just

2590
01:34:08,219 --> 01:34:12,600
normalizing columns so let's now

2591
01:34:10,380 --> 01:34:15,600
implement the layer Norm uh it's very

2592
01:34:12,600 --> 01:34:18,900
complicated look we come here we change

2593
01:34:15,600 --> 01:34:21,659
this from 0 to 1. so we don't normalize

2594
01:34:18,900 --> 01:34:24,960
The Columns we normalize the rows

2595
01:34:21,659 --> 01:34:27,600
and now we've implemented layer Norm

2596
01:34:24,960 --> 01:34:29,420
so now the columns are not going to be

2597
01:34:27,600 --> 01:34:32,280
normalized

2598
01:34:29,420 --> 01:34:34,139
but the rows are going to be normalized

2599
01:34:32,280 --> 01:34:36,600
for every individual example it's 100

2600
01:34:34,139 --> 01:34:39,420
dimensional Vector is normalized in this

2601
01:34:36,600 --> 01:34:42,300
way and because our computation Now does

2602
01:34:39,420 --> 01:34:45,960
not span across examples we can delete

2603
01:34:42,300 --> 01:34:48,840
all of this buffers stuff because we can

2604
01:34:45,960 --> 01:34:51,120
always apply this operation and don't

2605
01:34:48,840 --> 01:34:52,980
need to maintain any running buffers so

2606
01:34:51,120 --> 01:34:56,340
we don't need the buffers

2607
01:34:52,980 --> 01:34:58,880
we don't There's no distinction between

2608
01:34:56,340 --> 01:35:01,320
training and test time

2609
01:34:58,880 --> 01:35:04,199
and we don't need these running buffers

2610
01:35:01,320 --> 01:35:05,760
we do keep gamma and beta we don't need

2611
01:35:04,199 --> 01:35:07,260
the momentum we don't care if it's

2612
01:35:05,760 --> 01:35:10,620
training or not

2613
01:35:07,260 --> 01:35:13,199
and this is now a layer Norm

2614
01:35:10,620 --> 01:35:15,540
and it normalizes the rows instead of

2615
01:35:13,199 --> 01:35:19,380
the columns and this here

2616
01:35:15,540 --> 01:35:21,840
is identical to basically this here

2617
01:35:19,380 --> 01:35:23,580
so let's now Implement layer Norm in our

2618
01:35:21,840 --> 01:35:25,739
Transformer before I incorporate the

2619
01:35:23,580 --> 01:35:27,420
layer Norm I just wanted to note that as

2620
01:35:25,739 --> 01:35:28,620
I said very few details about the

2621
01:35:27,420 --> 01:35:30,000
Transformer have changed in the last

2622
01:35:28,620 --> 01:35:31,500
five years but this is actually

2623
01:35:30,000 --> 01:35:33,960
something that slightly departs from the

2624
01:35:31,500 --> 01:35:37,080
original paper you see that the ADD and

2625
01:35:33,960 --> 01:35:41,040
Norm is applied after the transformation

2626
01:35:37,080 --> 01:35:43,500
but um in now it is a bit more basically

2627
01:35:41,040 --> 01:35:45,000
common to apply the layer Norm before

2628
01:35:43,500 --> 01:35:47,280
the transformation so there's a

2629
01:35:45,000 --> 01:35:48,840
reshuffling of the layer Norms uh so

2630
01:35:47,280 --> 01:35:50,040
this is called the pre-norm formulation

2631
01:35:48,840 --> 01:35:51,540
and that's the one that we're going to

2632
01:35:50,040 --> 01:35:53,219
implement as well so slight deviation

2633
01:35:51,540 --> 01:35:55,320
from the original paper

2634
01:35:53,219 --> 01:35:59,460
basically we need two layer Norms layer

2635
01:35:55,320 --> 01:36:00,719
Norm one is an N dot layer norm and we

2636
01:35:59,460 --> 01:36:02,639
tell it how many

2637
01:36:00,719 --> 01:36:05,100
um what is the embedding dimension

2638
01:36:02,639 --> 01:36:07,440
and we need the second layer Norm

2639
01:36:05,100 --> 01:36:09,300
and then here the layer rooms are

2640
01:36:07,440 --> 01:36:12,840
applied immediately on x

2641
01:36:09,300 --> 01:36:14,699
so self.layer number one in applied on x

2642
01:36:12,840 --> 01:36:17,159
and salt on layer number two applied on

2643
01:36:14,699 --> 01:36:18,840
X before it goes into sulfur tension and

2644
01:36:17,159 --> 01:36:21,420
feed forward

2645
01:36:18,840 --> 01:36:24,060
and the size of the layer Norm here is

2646
01:36:21,420 --> 01:36:27,420
an embeds of 32. so when the layer Norm

2647
01:36:24,060 --> 01:36:30,120
is normalizing our features it is the

2648
01:36:27,420 --> 01:36:32,580
normalization here

2649
01:36:30,120 --> 01:36:35,100
happens the mean and the variance are

2650
01:36:32,580 --> 01:36:37,679
taking over 32 numbers so the batch and

2651
01:36:35,100 --> 01:36:40,940
the time act as batch Dimensions both of

2652
01:36:37,679 --> 01:36:43,139
them so this is kind of like a per token

2653
01:36:40,940 --> 01:36:46,679
transformation that just normalizes the

2654
01:36:43,139 --> 01:36:49,260
features and makes them a unit mean unit

2655
01:36:46,679 --> 01:36:51,000
gaussian at initialization

2656
01:36:49,260 --> 01:36:53,460
but of course because these layer Norms

2657
01:36:51,000 --> 01:36:54,960
inside it have these gamma and beta

2658
01:36:53,460 --> 01:36:58,679
trainable parameters

2659
01:36:54,960 --> 01:37:00,600
the layer normal eventually create

2660
01:36:58,679 --> 01:37:03,420
outputs that might not be unit gaussian

2661
01:37:00,600 --> 01:37:05,699
but the optimization will determine that

2662
01:37:03,420 --> 01:37:07,320
so for now this is the uh this is

2663
01:37:05,699 --> 01:37:10,020
incorporating the layer norms and let's

2664
01:37:07,320 --> 01:37:12,360
train them up okay so I let it run and

2665
01:37:10,020 --> 01:37:14,639
we see that we get down to 2.06 which is

2666
01:37:12,360 --> 01:37:16,080
better than the previous 2.08 so a

2667
01:37:14,639 --> 01:37:18,600
slight Improvement by adding the layer

2668
01:37:16,080 --> 01:37:20,940
norms and I'd expect that they help even

2669
01:37:18,600 --> 01:37:23,040
more if we had bigger and deeper Network

2670
01:37:20,940 --> 01:37:25,020
one more thing I forgot to add is that

2671
01:37:23,040 --> 01:37:27,300
there should be a layer Norm here also

2672
01:37:25,020 --> 01:37:29,219
typically as at the end of the

2673
01:37:27,300 --> 01:37:31,500
Transformer and right before the final

2674
01:37:29,219 --> 01:37:35,100
linear layer that decodes into

2675
01:37:31,500 --> 01:37:36,300
vocabulary so I added that as well so at

2676
01:37:35,100 --> 01:37:38,219
this stage we actually have a pretty

2677
01:37:36,300 --> 01:37:40,320
complete Transformer according to the

2678
01:37:38,219 --> 01:37:42,420
original paper and it's a decoder only

2679
01:37:40,320 --> 01:37:45,000
Transformer I'll I'll talk about that in

2680
01:37:42,420 --> 01:37:46,500
a second but at this stage the major

2681
01:37:45,000 --> 01:37:48,060
pieces are in place so we can try to

2682
01:37:46,500 --> 01:37:49,199
scale this up and see how well we can

2683
01:37:48,060 --> 01:37:51,239
push this number

2684
01:37:49,199 --> 01:37:52,920
now in order to scale out the model I

2685
01:37:51,239 --> 01:37:55,440
had to perform some cosmetic changes

2686
01:37:52,920 --> 01:37:57,060
here to make it nicer so I introduced

2687
01:37:55,440 --> 01:37:59,400
this variable called end layer which

2688
01:37:57,060 --> 01:38:01,860
just specifies how many layers of the

2689
01:37:59,400 --> 01:38:03,000
blocks we're going to have I create a

2690
01:38:01,860 --> 01:38:05,340
bunch of blocks and we have a new

2691
01:38:03,000 --> 01:38:07,860
variable number of heads as well

2692
01:38:05,340 --> 01:38:10,139
I pulled out the layer Norm here and so

2693
01:38:07,860 --> 01:38:13,139
this is identical now one thing that I

2694
01:38:10,139 --> 01:38:15,420
did briefly change is I added a dropout

2695
01:38:13,139 --> 01:38:17,639
so Dropout is something that you can add

2696
01:38:15,420 --> 01:38:18,780
right before the residual connection

2697
01:38:17,639 --> 01:38:20,100
back

2698
01:38:18,780 --> 01:38:21,600
or right before the connection back into

2699
01:38:20,100 --> 01:38:23,580
the original pathway

2700
01:38:21,600 --> 01:38:24,659
so we can drop out that as the last

2701
01:38:23,580 --> 01:38:27,300
layer here

2702
01:38:24,659 --> 01:38:29,340
we can drop out uh here at the end of

2703
01:38:27,300 --> 01:38:32,400
the multi-headed extension as well

2704
01:38:29,340 --> 01:38:35,340
and we can also drop out here when we

2705
01:38:32,400 --> 01:38:37,560
calculate the um basically affinities

2706
01:38:35,340 --> 01:38:39,780
and after the soft Max we can drop out

2707
01:38:37,560 --> 01:38:41,940
some of those so we can randomly prevent

2708
01:38:39,780 --> 01:38:45,120
some of the nodes from communicating

2709
01:38:41,940 --> 01:38:47,340
and so Dropout comes from this paper

2710
01:38:45,120 --> 01:38:51,540
from 2014 or so

2711
01:38:47,340 --> 01:38:53,760
and basically it takes your neural net

2712
01:38:51,540 --> 01:38:57,840
and it randomly every forward backward

2713
01:38:53,760 --> 01:39:00,900
pass shuts off some subset of neurons

2714
01:38:57,840 --> 01:39:03,239
so randomly drops them to zero and

2715
01:39:00,900 --> 01:39:05,639
trains without them and what this does

2716
01:39:03,239 --> 01:39:07,020
effectively is because the mask of

2717
01:39:05,639 --> 01:39:08,880
what's being dropped out has changed

2718
01:39:07,020 --> 01:39:11,820
every single forward backward pass it

2719
01:39:08,880 --> 01:39:14,219
ends up kind of training an ensemble of

2720
01:39:11,820 --> 01:39:15,900
sub Networks and then at this time

2721
01:39:14,219 --> 01:39:17,460
everything is fully enabled and kind of

2722
01:39:15,900 --> 01:39:19,500
all of those sub networks are merged

2723
01:39:17,460 --> 01:39:21,300
into a single Ensemble if you can if you

2724
01:39:19,500 --> 01:39:22,739
want to think about it that way so I

2725
01:39:21,300 --> 01:39:24,840
would read the paper to get the full

2726
01:39:22,739 --> 01:39:26,760
detail for now we're just going to stay

2727
01:39:24,840 --> 01:39:29,159
on the level of this is a regularization

2728
01:39:26,760 --> 01:39:30,659
technique and I added it because I'm

2729
01:39:29,159 --> 01:39:33,239
about to scale up the model quite a bit

2730
01:39:30,659 --> 01:39:36,060
and I was concerned about overfitting

2731
01:39:33,239 --> 01:39:37,440
so now when we scroll up to the top uh

2732
01:39:36,060 --> 01:39:39,300
we'll see that I changed a number of

2733
01:39:37,440 --> 01:39:41,280
hyper parameters here about our neural

2734
01:39:39,300 --> 01:39:43,080
net so I made the batch size B much

2735
01:39:41,280 --> 01:39:45,840
larger now with 64.

2736
01:39:43,080 --> 01:39:47,340
I changed the block size to be 256 so

2737
01:39:45,840 --> 01:39:49,860
previously it was just eight eight

2738
01:39:47,340 --> 01:39:51,780
characters of context now it is 256

2739
01:39:49,860 --> 01:39:53,820
characters of context to predict the

2740
01:39:51,780 --> 01:39:56,159
257th

2741
01:39:53,820 --> 01:39:57,480
uh I brought down the learning rate a

2742
01:39:56,159 --> 01:39:58,980
little bit because the neural net is now

2743
01:39:57,480 --> 01:40:00,120
much bigger so I brought down the

2744
01:39:58,980 --> 01:40:02,520
learning rate

2745
01:40:00,120 --> 01:40:06,120
the embedding Dimension is now 384 and

2746
01:40:02,520 --> 01:40:08,760
there are six heads so 384 divide 6

2747
01:40:06,120 --> 01:40:10,980
means that every head is 64 dimensional

2748
01:40:08,760 --> 01:40:12,719
as it as a standard

2749
01:40:10,980 --> 01:40:13,920
and then there are going to be six

2750
01:40:12,719 --> 01:40:15,960
layers of that

2751
01:40:13,920 --> 01:40:17,880
and the Dropout will be of 0.2 so every

2752
01:40:15,960 --> 01:40:19,739
forward backward passed 20 percent of

2753
01:40:17,880 --> 01:40:22,199
all of these um

2754
01:40:19,739 --> 01:40:23,940
intermediate calculations are disabled

2755
01:40:22,199 --> 01:40:25,800
and dropped to zero

2756
01:40:23,940 --> 01:40:28,500
and then I already trained this and I

2757
01:40:25,800 --> 01:40:29,580
ran it so uh drum roll how well does it

2758
01:40:28,500 --> 01:40:32,460
perform

2759
01:40:29,580 --> 01:40:36,120
so let me just scroll up here

2760
01:40:32,460 --> 01:40:37,199
we get a validation loss of 1.48 which

2761
01:40:36,120 --> 01:40:38,580
is actually quite a bit of an

2762
01:40:37,199 --> 01:40:41,639
improvement on what we had before which

2763
01:40:38,580 --> 01:40:43,800
I think was 2.07 so we went from 2.07

2764
01:40:41,639 --> 01:40:45,480
all the way down to 1.48 just by scaling

2765
01:40:43,800 --> 01:40:47,760
up this neural nut with the code that we

2766
01:40:45,480 --> 01:40:49,860
have and this of course ran for a lot

2767
01:40:47,760 --> 01:40:53,040
longer this may be trained for I want to

2768
01:40:49,860 --> 01:40:55,080
say about 15 minutes on my a100 GPU so

2769
01:40:53,040 --> 01:40:56,219
that's a pretty good GPU and if you

2770
01:40:55,080 --> 01:40:58,980
don't have a GPU you're not going to be

2771
01:40:56,219 --> 01:40:59,940
able to reproduce this on a CPU this

2772
01:40:58,980 --> 01:41:01,739
would be

2773
01:40:59,940 --> 01:41:03,179
um I would not run this on the CPU or a

2774
01:41:01,739 --> 01:41:05,280
Macbook or something like that you'll

2775
01:41:03,179 --> 01:41:08,159
have to break down the number of layers

2776
01:41:05,280 --> 01:41:10,139
and the embedding Dimension and so on

2777
01:41:08,159 --> 01:41:12,300
but in about 15 minutes we can get this

2778
01:41:10,139 --> 01:41:13,860
kind of a result and

2779
01:41:12,300 --> 01:41:15,659
um I'm printing

2780
01:41:13,860 --> 01:41:17,760
some of the Shakespeare here but what I

2781
01:41:15,659 --> 01:41:20,460
did also is I printed 10 000 characters

2782
01:41:17,760 --> 01:41:24,139
so a lot more and I wrote them to a file

2783
01:41:20,460 --> 01:41:24,139
and so here we see some of the outputs

2784
01:41:24,179 --> 01:41:29,280
so it's a lot more recognizable as the

2785
01:41:27,060 --> 01:41:31,679
input text file so the input text file

2786
01:41:29,280 --> 01:41:33,659
just for reference looked like this

2787
01:41:31,679 --> 01:41:36,540
so there's always like someone speaking

2788
01:41:33,659 --> 01:41:39,360
in this matter and uh

2789
01:41:36,540 --> 01:41:40,920
our predictions now take on that form

2790
01:41:39,360 --> 01:41:42,840
except of course they're they're

2791
01:41:40,920 --> 01:41:44,520
nonsensical when you actually read them

2792
01:41:42,840 --> 01:41:47,940
so

2793
01:41:44,520 --> 01:41:52,440
it is every crimpy bee house oh those

2794
01:41:47,940 --> 01:41:55,159
preparation we give heed

2795
01:41:52,440 --> 01:41:55,159
um you know

2796
01:41:56,040 --> 01:42:00,020
Oho sent me you mighty Lord

2797
01:42:00,659 --> 01:42:04,860
anyway so you can read through this

2798
01:42:02,699 --> 01:42:06,659
um it's nonsensical of course but this

2799
01:42:04,860 --> 01:42:08,699
is just a Transformer trained on the

2800
01:42:06,659 --> 01:42:10,679
Character level for 1 million characters

2801
01:42:08,699 --> 01:42:12,480
that come from Shakespeare so they're

2802
01:42:10,679 --> 01:42:14,760
sort of like blabbers on and Shakespeare

2803
01:42:12,480 --> 01:42:16,619
like manner but it doesn't of course

2804
01:42:14,760 --> 01:42:19,199
make sense at this scale

2805
01:42:16,619 --> 01:42:21,659
uh but I think I think still a pretty

2806
01:42:19,199 --> 01:42:24,300
good demonstration of what's possible

2807
01:42:21,659 --> 01:42:26,340
so now

2808
01:42:24,300 --> 01:42:28,860
I think uh that kind of like concludes

2809
01:42:26,340 --> 01:42:30,659
the programming section of this video we

2810
01:42:28,860 --> 01:42:33,480
basically kind of did a pretty good job

2811
01:42:30,659 --> 01:42:36,239
in um of implementing this Transformer

2812
01:42:33,480 --> 01:42:37,980
but the picture doesn't exactly match up

2813
01:42:36,239 --> 01:42:40,260
to what we've done so what's going on

2814
01:42:37,980 --> 01:42:41,639
with all these additional Parts here so

2815
01:42:40,260 --> 01:42:44,040
let me finish explaining this

2816
01:42:41,639 --> 01:42:46,199
architecture and why it looks so funky

2817
01:42:44,040 --> 01:42:48,360
basically what's happening here is what

2818
01:42:46,199 --> 01:42:51,179
we implemented here is a decoder only

2819
01:42:48,360 --> 01:42:53,280
Transformer so there's no component here

2820
01:42:51,179 --> 01:42:55,320
this part is called the encoder and

2821
01:42:53,280 --> 01:42:58,320
there's no cross attention block here

2822
01:42:55,320 --> 01:43:00,420
our block only has a self-attention and

2823
01:42:58,320 --> 01:43:03,480
the feed forward so it is missing this

2824
01:43:00,420 --> 01:43:05,460
third in between piece here this piece

2825
01:43:03,480 --> 01:43:07,260
does cross attention so we don't have it

2826
01:43:05,460 --> 01:43:09,060
and we don't have the encoder we just

2827
01:43:07,260 --> 01:43:10,679
have the decoder and the reason we have

2828
01:43:09,060 --> 01:43:13,080
a decoder only

2829
01:43:10,679 --> 01:43:15,000
is because we are just generating text

2830
01:43:13,080 --> 01:43:16,739
and it's unconditioned on anything we're

2831
01:43:15,000 --> 01:43:18,239
just we're just blabbering on according

2832
01:43:16,739 --> 01:43:20,400
to a given data set

2833
01:43:18,239 --> 01:43:22,860
what makes it a decoder is that we are

2834
01:43:20,400 --> 01:43:24,780
using the Triangular mask in our

2835
01:43:22,860 --> 01:43:26,940
Transformer so it has this Auto

2836
01:43:24,780 --> 01:43:28,500
regressive property where we can just go

2837
01:43:26,940 --> 01:43:29,940
and sample from it

2838
01:43:28,500 --> 01:43:32,580
so the fact that it's using the

2839
01:43:29,940 --> 01:43:34,860
Triangular triangular mask to mask out

2840
01:43:32,580 --> 01:43:37,139
the attention makes it a decoder and it

2841
01:43:34,860 --> 01:43:39,119
can be used for language modeling now

2842
01:43:37,139 --> 01:43:41,159
the reason that the original paper had

2843
01:43:39,119 --> 01:43:42,420
an encoder decoder architecture is

2844
01:43:41,159 --> 01:43:44,580
because it is a machine translation

2845
01:43:42,420 --> 01:43:46,739
paper so it is concerned with a

2846
01:43:44,580 --> 01:43:50,520
different setting in particular

2847
01:43:46,739 --> 01:43:51,960
it expects some tokens that encode say

2848
01:43:50,520 --> 01:43:54,179
for example French

2849
01:43:51,960 --> 01:43:55,800
and then it is expected to decode the

2850
01:43:54,179 --> 01:43:58,199
translation in English

2851
01:43:55,800 --> 01:44:00,780
so so you typically these here are

2852
01:43:58,199 --> 01:44:03,719
special tokens so you are expected to

2853
01:44:00,780 --> 01:44:05,280
read in this and condition on it and

2854
01:44:03,719 --> 01:44:07,619
then you start off the generation with a

2855
01:44:05,280 --> 01:44:10,619
special token called start so this is a

2856
01:44:07,619 --> 01:44:12,300
special new token that you introduce and

2857
01:44:10,619 --> 01:44:14,780
always place in the beginning

2858
01:44:12,300 --> 01:44:17,820
and then the network is expected to put

2859
01:44:14,780 --> 01:44:21,000
neural networks are awesome and then a

2860
01:44:17,820 --> 01:44:23,760
special end token to finish a generation

2861
01:44:21,000 --> 01:44:25,980
so this part here will be decoded

2862
01:44:23,760 --> 01:44:27,659
exactly as we we've done it neural

2863
01:44:25,980 --> 01:44:28,860
networks are awesome will be identical

2864
01:44:27,659 --> 01:44:31,560
to what we did

2865
01:44:28,860 --> 01:44:34,500
but unlike what we did they want to

2866
01:44:31,560 --> 01:44:36,420
condition the generation on some

2867
01:44:34,500 --> 01:44:38,159
additional information and in that case

2868
01:44:36,420 --> 01:44:39,360
this additional information is the

2869
01:44:38,159 --> 01:44:40,560
French sentence that they should be

2870
01:44:39,360 --> 01:44:42,420
translating

2871
01:44:40,560 --> 01:44:45,060
so what they do now

2872
01:44:42,420 --> 01:44:48,300
is they bring in the encoder now the

2873
01:44:45,060 --> 01:44:49,800
encoder reads this part here so we're

2874
01:44:48,300 --> 01:44:52,860
only going to take the part of French

2875
01:44:49,800 --> 01:44:54,840
and we're going to create tokens from it

2876
01:44:52,860 --> 01:44:56,880
exactly as we've seen in our video and

2877
01:44:54,840 --> 01:44:58,679
we're going to put a Transformer on it

2878
01:44:56,880 --> 01:45:00,900
but there's going to be no triangular

2879
01:44:58,679 --> 01:45:02,219
mask and so all the tokens are allowed

2880
01:45:00,900 --> 01:45:04,020
to talk to each other as much as they

2881
01:45:02,219 --> 01:45:06,179
want and they're just encoding

2882
01:45:04,020 --> 01:45:08,100
whatever's the content of this French

2883
01:45:06,179 --> 01:45:10,199
sentence

2884
01:45:08,100 --> 01:45:12,420
once they've encoded it

2885
01:45:10,199 --> 01:45:13,260
they've they basically come out in the

2886
01:45:12,420 --> 01:45:15,060
top here

2887
01:45:13,260 --> 01:45:18,300
and then what happens here is in our

2888
01:45:15,060 --> 01:45:21,360
decoder which does the language modeling

2889
01:45:18,300 --> 01:45:23,340
there's an additional connection here to

2890
01:45:21,360 --> 01:45:25,980
the outputs of the encoder

2891
01:45:23,340 --> 01:45:27,000
and that is brought in through a cross

2892
01:45:25,980 --> 01:45:28,739
attention

2893
01:45:27,000 --> 01:45:31,199
so the queries are still generated from

2894
01:45:28,739 --> 01:45:33,179
X but now the keys and the values are

2895
01:45:31,199 --> 01:45:35,100
coming from the side the keys and the

2896
01:45:33,179 --> 01:45:37,139
values are coming from the top

2897
01:45:35,100 --> 01:45:39,000
generated by the nodes that came outside

2898
01:45:37,139 --> 01:45:41,699
of the encoder

2899
01:45:39,000 --> 01:45:43,739
and those tops the keys and the values

2900
01:45:41,699 --> 01:45:45,900
there the top of it

2901
01:45:43,739 --> 01:45:48,300
feeding on the side into every single

2902
01:45:45,900 --> 01:45:49,739
block of the decoder and so that's why

2903
01:45:48,300 --> 01:45:51,840
there's an additional cross attention

2904
01:45:49,739 --> 01:45:54,420
and really what it's doing is it's

2905
01:45:51,840 --> 01:45:57,239
conditioning the decoding not just on

2906
01:45:54,420 --> 01:46:01,080
the past of this current decoding but

2907
01:45:57,239 --> 01:46:02,780
also on having seen the full fully

2908
01:46:01,080 --> 01:46:04,920
encoded French

2909
01:46:02,780 --> 01:46:06,600
prompt sort of

2910
01:46:04,920 --> 01:46:07,619
and so it's an encoder decoder model

2911
01:46:06,600 --> 01:46:09,960
which is why we have those two

2912
01:46:07,619 --> 01:46:12,119
Transformers an additional block and so

2913
01:46:09,960 --> 01:46:13,920
on so we did not do this because we have

2914
01:46:12,119 --> 01:46:15,540
no we have nothing to encode there's no

2915
01:46:13,920 --> 01:46:17,100
conditioning we just have a text file

2916
01:46:15,540 --> 01:46:19,020
and we just want to imitate it and

2917
01:46:17,100 --> 01:46:22,679
that's why we are using a decoder only

2918
01:46:19,020 --> 01:46:24,719
Transformer exactly as done in GPT

2919
01:46:22,679 --> 01:46:26,699
okay so now I wanted to do a very brief

2920
01:46:24,719 --> 01:46:29,580
walkthrough of Nano GPT which you can

2921
01:46:26,699 --> 01:46:31,500
find on my GitHub and uh Nano GPT is

2922
01:46:29,580 --> 01:46:34,739
basically two files of Interest there's

2923
01:46:31,500 --> 01:46:36,420
train.pi and model.pi trained at Pi is

2924
01:46:34,739 --> 01:46:38,580
all the boilerplate code for training

2925
01:46:36,420 --> 01:46:40,860
the network it is basically all the

2926
01:46:38,580 --> 01:46:41,760
stuff that we had here it's the training

2927
01:46:40,860 --> 01:46:42,960
Loop

2928
01:46:41,760 --> 01:46:44,699
it's just that it's a lot more

2929
01:46:42,960 --> 01:46:46,199
complicated because we're saving and

2930
01:46:44,699 --> 01:46:48,780
loading checkpoints and pre-trained

2931
01:46:46,199 --> 01:46:50,580
weights and we are decaying the learning

2932
01:46:48,780 --> 01:46:52,080
rate and compiling the model and using

2933
01:46:50,580 --> 01:46:54,840
distributed training across multiple

2934
01:46:52,080 --> 01:46:56,820
nodes or gpus so the training that Pi

2935
01:46:54,840 --> 01:46:59,460
gets a little bit more hairy complicated

2936
01:46:56,820 --> 01:47:02,280
there's more options Etc

2937
01:46:59,460 --> 01:47:04,560
but the model.pi should look very very

2938
01:47:02,280 --> 01:47:07,199
um similar to what we've done here in

2939
01:47:04,560 --> 01:47:09,719
fact the model is is almost identical

2940
01:47:07,199 --> 01:47:11,880
so first here we have the causal

2941
01:47:09,719 --> 01:47:13,320
self-attention block and all of this

2942
01:47:11,880 --> 01:47:15,600
should look very very recognizable to

2943
01:47:13,320 --> 01:47:18,179
you we're producing queries Keys values

2944
01:47:15,600 --> 01:47:20,940
we're doing Dot products we're masking

2945
01:47:18,179 --> 01:47:24,239
applying softmax optionally dropping out

2946
01:47:20,940 --> 01:47:26,460
and here we are pooling the values

2947
01:47:24,239 --> 01:47:27,420
what is different here is that in our

2948
01:47:26,460 --> 01:47:30,780
code

2949
01:47:27,420 --> 01:47:32,639
I have separated out the multi-headed

2950
01:47:30,780 --> 01:47:33,480
attention into just a single individual

2951
01:47:32,639 --> 01:47:35,699
head

2952
01:47:33,480 --> 01:47:37,800
and then here I have multiple heads and

2953
01:47:35,699 --> 01:47:40,560
I explicitly concatenate them

2954
01:47:37,800 --> 01:47:42,719
whereas here all of it is implemented in

2955
01:47:40,560 --> 01:47:44,820
a batched manner inside a single causal

2956
01:47:42,719 --> 01:47:47,159
self-attention and so we don't just have

2957
01:47:44,820 --> 01:47:48,540
a b and a T and A C Dimension we also

2958
01:47:47,159 --> 01:47:49,800
end up with a fourth dimension which is

2959
01:47:48,540 --> 01:47:52,139
the heads

2960
01:47:49,800 --> 01:47:53,460
and so it just gets a lot more sort of

2961
01:47:52,139 --> 01:47:57,179
hairy because we have four dimensional

2962
01:47:53,460 --> 01:47:59,280
array tensors now but it is equivalent

2963
01:47:57,179 --> 01:48:01,260
mathematically so the exact same thing

2964
01:47:59,280 --> 01:48:02,940
is happening is what we have it's just

2965
01:48:01,260 --> 01:48:04,500
it's a bit more efficient because all

2966
01:48:02,940 --> 01:48:06,659
the heads are not treated as a batch

2967
01:48:04,500 --> 01:48:08,580
Dimension as well

2968
01:48:06,659 --> 01:48:10,920
then we have to multiply perceptron it's

2969
01:48:08,580 --> 01:48:13,619
using the gallon nonlinearity which is

2970
01:48:10,920 --> 01:48:15,119
defined here except instead of relu and

2971
01:48:13,619 --> 01:48:16,320
this is done just because openingi used

2972
01:48:15,119 --> 01:48:17,699
it and I want to be able to load their

2973
01:48:16,320 --> 01:48:19,739
checkpoints

2974
01:48:17,699 --> 01:48:21,420
uh the blocks of the Transformer are

2975
01:48:19,739 --> 01:48:23,219
identical the communicate and the

2976
01:48:21,420 --> 01:48:25,080
compute phase as we saw

2977
01:48:23,219 --> 01:48:26,699
and then the GPT will be identical we

2978
01:48:25,080 --> 01:48:29,219
have the position encodings token

2979
01:48:26,699 --> 01:48:32,219
encodings the blocks the layer Norm at

2980
01:48:29,219 --> 01:48:33,840
the end the final linear layer

2981
01:48:32,219 --> 01:48:34,920
and this should look all very

2982
01:48:33,840 --> 01:48:36,719
recognizable

2983
01:48:34,920 --> 01:48:38,100
and there's a bit more here because I'm

2984
01:48:36,719 --> 01:48:40,260
loading checkpoints and stuff like that

2985
01:48:38,100 --> 01:48:41,820
I'm separating out the parameters into

2986
01:48:40,260 --> 01:48:44,159
building that should be weight decayed

2987
01:48:41,820 --> 01:48:45,659
and those that shouldn't

2988
01:48:44,159 --> 01:48:48,000
um but the generate function should also

2989
01:48:45,659 --> 01:48:49,320
be very very similar so a few details

2990
01:48:48,000 --> 01:48:52,139
are different but you should definitely

2991
01:48:49,320 --> 01:48:53,400
be able to look at this uh file and be

2992
01:48:52,139 --> 01:48:55,440
able to understand a lot of the pieces

2993
01:48:53,400 --> 01:48:56,639
now so let's now bring things back to

2994
01:48:55,440 --> 01:48:58,260
chat GPT

2995
01:48:56,639 --> 01:49:00,239
what would it look like if we wanted to

2996
01:48:58,260 --> 01:49:02,400
train chatgpt ourselves and how does it

2997
01:49:00,239 --> 01:49:04,020
relate to what we learned today

2998
01:49:02,400 --> 01:49:06,000
well to train in chat GPT there are

2999
01:49:04,020 --> 01:49:07,800
roughly two stages first is the

3000
01:49:06,000 --> 01:49:10,400
pre-training stage and then the fine

3001
01:49:07,800 --> 01:49:12,900
tuning stage in the pre-training stage

3002
01:49:10,400 --> 01:49:15,420
we are training on a large chunk of

3003
01:49:12,900 --> 01:49:18,239
internet and just trying to get a first

3004
01:49:15,420 --> 01:49:20,880
decoder only Transformer to Babel text

3005
01:49:18,239 --> 01:49:22,260
so it's very very similar to what we've

3006
01:49:20,880 --> 01:49:24,300
done ourselves

3007
01:49:22,260 --> 01:49:26,300
except we've done like a tiny little

3008
01:49:24,300 --> 01:49:30,239
baby pre-training step

3009
01:49:26,300 --> 01:49:32,340
and so in our case uh this is how you

3010
01:49:30,239 --> 01:49:34,500
print a number of parameters I printed

3011
01:49:32,340 --> 01:49:36,239
it and it's about 10 million so this

3012
01:49:34,500 --> 01:49:38,520
Transformer that I created here to

3013
01:49:36,239 --> 01:49:40,739
create little Shakespeare

3014
01:49:38,520 --> 01:49:43,739
um Transformer was about 10 million

3015
01:49:40,739 --> 01:49:45,840
parameters our data set is roughly 1

3016
01:49:43,739 --> 01:49:47,699
million uh characters so roughly 1

3017
01:49:45,840 --> 01:49:49,560
million tokens but you have to remember

3018
01:49:47,699 --> 01:49:51,239
that opening uses different vocabulary

3019
01:49:49,560 --> 01:49:54,780
they're not on the Character level they

3020
01:49:51,239 --> 01:49:56,520
use these um subword chunks of words and

3021
01:49:54,780 --> 01:49:58,980
so they have a vocabulary of 50 000

3022
01:49:56,520 --> 01:50:01,080
roughly elements and so their sequences

3023
01:49:58,980 --> 01:50:03,540
are a bit more condensed

3024
01:50:01,080 --> 01:50:06,119
so our data set the Shakespeare data set

3025
01:50:03,540 --> 01:50:09,179
would be probably around 300 000 tokens

3026
01:50:06,119 --> 01:50:11,699
in the openai vocabulary roughly

3027
01:50:09,179 --> 01:50:14,219
so we trained about 10 million parameter

3028
01:50:11,699 --> 01:50:17,460
model and roughly 300 000 tokens

3029
01:50:14,219 --> 01:50:20,280
now when you go to the gpd3 paper

3030
01:50:17,460 --> 01:50:21,719
and you look at the Transformers that

3031
01:50:20,280 --> 01:50:23,219
they trained

3032
01:50:21,719 --> 01:50:25,380
they trained a number of Transformers of

3033
01:50:23,219 --> 01:50:27,420
different sizes but the biggest

3034
01:50:25,380 --> 01:50:30,659
Transformer here has 175 billion

3035
01:50:27,420 --> 01:50:32,639
parameters so ours is again 10 million

3036
01:50:30,659 --> 01:50:35,040
they used this number of layers in the

3037
01:50:32,639 --> 01:50:37,440
Transformer This is the End embed

3038
01:50:35,040 --> 01:50:39,060
this is the number of heads and this is

3039
01:50:37,440 --> 01:50:42,000
the head size

3040
01:50:39,060 --> 01:50:44,040
and then this is the batch size so ours

3041
01:50:42,000 --> 01:50:46,260
was 65.

3042
01:50:44,040 --> 01:50:48,000
and the learning rate is similar now

3043
01:50:46,260 --> 01:50:50,460
when they train this Transformer they

3044
01:50:48,000 --> 01:50:52,980
trained on 300 billion tokens

3045
01:50:50,460 --> 01:50:56,040
so again remember ours is about 300 000

3046
01:50:52,980 --> 01:50:57,960
so this is uh about a million fold

3047
01:50:56,040 --> 01:50:59,340
increase and this number would not be

3048
01:50:57,960 --> 01:51:01,560
even that large by today's standards

3049
01:50:59,340 --> 01:51:02,580
you'd be going up uh one trillion and

3050
01:51:01,560 --> 01:51:05,520
above

3051
01:51:02,580 --> 01:51:07,199
so they are training a significantly

3052
01:51:05,520 --> 01:51:10,500
larger model

3053
01:51:07,199 --> 01:51:12,600
on a good chunk of the internet and that

3054
01:51:10,500 --> 01:51:14,159
is the pre-training stage but otherwise

3055
01:51:12,600 --> 01:51:16,139
these hyper parameters should be fairly

3056
01:51:14,159 --> 01:51:17,880
recognizable to you and the architecture

3057
01:51:16,139 --> 01:51:19,800
is actually like nearly identical to

3058
01:51:17,880 --> 01:51:21,239
what we implemented ourselves but of

3059
01:51:19,800 --> 01:51:23,520
course it's a massive infrastructure

3060
01:51:21,239 --> 01:51:26,040
challenge to train this you're talking

3061
01:51:23,520 --> 01:51:28,380
about typically thousands of gpus having

3062
01:51:26,040 --> 01:51:30,540
to you know talk to each other to train

3063
01:51:28,380 --> 01:51:32,520
models of this size so that's just a

3064
01:51:30,540 --> 01:51:34,860
pre-training stage now after you

3065
01:51:32,520 --> 01:51:36,719
complete the pre-training stage you

3066
01:51:34,860 --> 01:51:38,639
don't get something that responds to

3067
01:51:36,719 --> 01:51:41,100
your questions with answers and is not

3068
01:51:38,639 --> 01:51:44,760
helpful and Etc you get a document

3069
01:51:41,100 --> 01:51:46,500
completer right so it babbles but it

3070
01:51:44,760 --> 01:51:48,360
doesn't Babble Shakespeare in Babel's

3071
01:51:46,500 --> 01:51:50,280
internet it will create arbitrary news

3072
01:51:48,360 --> 01:51:51,659
articles and documents and it will try

3073
01:51:50,280 --> 01:51:52,800
to complete documents because that's

3074
01:51:51,659 --> 01:51:54,840
what it's trained for it's trying to

3075
01:51:52,800 --> 01:51:56,580
complete the sequence so when you give

3076
01:51:54,840 --> 01:51:58,739
it a question it would just uh

3077
01:51:56,580 --> 01:52:00,600
potentially just give you more questions

3078
01:51:58,739 --> 01:52:02,880
it would follow with more questions it

3079
01:52:00,600 --> 01:52:05,460
will do whatever it looks like the some

3080
01:52:02,880 --> 01:52:07,380
closed document would do in the training

3081
01:52:05,460 --> 01:52:08,639
data on the internet and so who knows

3082
01:52:07,380 --> 01:52:11,219
you're getting kind of like undefined

3083
01:52:08,639 --> 01:52:13,320
Behavior it might basically answer with

3084
01:52:11,219 --> 01:52:15,179
two questions with other questions it

3085
01:52:13,320 --> 01:52:17,219
might ignore your question it might just

3086
01:52:15,179 --> 01:52:19,739
try to complete some news article it's

3087
01:52:17,219 --> 01:52:21,960
totally underlined as we say

3088
01:52:19,739 --> 01:52:24,719
so the second fine tuning stage is to

3089
01:52:21,960 --> 01:52:26,699
actually align it to be an assistant and

3090
01:52:24,719 --> 01:52:29,219
this is the second stage

3091
01:52:26,699 --> 01:52:30,960
and so this Chachi PT blog post from

3092
01:52:29,219 --> 01:52:34,199
open AI talks a little bit about how the

3093
01:52:30,960 --> 01:52:35,219
stage is achieved we basically

3094
01:52:34,199 --> 01:52:36,900
um

3095
01:52:35,219 --> 01:52:39,119
there's roughly three steps to the to

3096
01:52:36,900 --> 01:52:41,340
this stage uh so what they do here is

3097
01:52:39,119 --> 01:52:43,020
they start to collect training data that

3098
01:52:41,340 --> 01:52:44,699
looks specifically like what an

3099
01:52:43,020 --> 01:52:46,260
assistant would do so if you have

3100
01:52:44,699 --> 01:52:48,000
documents that have the format where the

3101
01:52:46,260 --> 01:52:50,219
question is on top and then an answer is

3102
01:52:48,000 --> 01:52:52,020
below and they have a large number of

3103
01:52:50,219 --> 01:52:53,880
these but probably not on the order of

3104
01:52:52,020 --> 01:52:56,880
the internet this is probably on the

3105
01:52:53,880 --> 01:52:59,880
order of maybe thousands of examples

3106
01:52:56,880 --> 01:53:03,000
and so they they then fine-tuned the

3107
01:52:59,880 --> 01:53:04,920
model to basically only focus on

3108
01:53:03,000 --> 01:53:06,600
documents that look like that and so

3109
01:53:04,920 --> 01:53:08,219
you're starting to slowly align it so

3110
01:53:06,600 --> 01:53:09,719
it's going to expect a question at the

3111
01:53:08,219 --> 01:53:10,679
top and it's going to expect to complete

3112
01:53:09,719 --> 01:53:13,440
the answer

3113
01:53:10,679 --> 01:53:15,239
and uh these very very large models are

3114
01:53:13,440 --> 01:53:17,280
very sample efficient during their fine

3115
01:53:15,239 --> 01:53:19,320
tuning so this actually somehow works

3116
01:53:17,280 --> 01:53:21,239
but that's just step one that's just

3117
01:53:19,320 --> 01:53:23,760
fine-tuning so then they actually have

3118
01:53:21,239 --> 01:53:25,739
more steps where okay the second step is

3119
01:53:23,760 --> 01:53:27,239
you let the model respond and then

3120
01:53:25,739 --> 01:53:29,520
different Raiders look at the different

3121
01:53:27,239 --> 01:53:30,960
responses and rank them for their

3122
01:53:29,520 --> 01:53:33,060
preference as to which one is better

3123
01:53:30,960 --> 01:53:34,920
than the other they use that to train a

3124
01:53:33,060 --> 01:53:37,380
reward model so they can predict

3125
01:53:34,920 --> 01:53:41,760
basically using a different network how

3126
01:53:37,380 --> 01:53:42,780
much of any candidate response would be

3127
01:53:41,760 --> 01:53:44,400
desirable

3128
01:53:42,780 --> 01:53:47,400
and then once they have a reward model

3129
01:53:44,400 --> 01:53:49,199
they run PPO which is a form of policy

3130
01:53:47,400 --> 01:53:50,580
policy gradient um reinforcement

3131
01:53:49,199 --> 01:53:54,480
learning optimizer

3132
01:53:50,580 --> 01:53:57,119
to fine-tune this sampling policy so

3133
01:53:54,480 --> 01:54:00,540
that the answers that the GPT GPT now

3134
01:53:57,119 --> 01:54:03,540
generates are expected to score a high

3135
01:54:00,540 --> 01:54:05,219
reward according to the reward model

3136
01:54:03,540 --> 01:54:07,500
and so basically there's a whole the

3137
01:54:05,219 --> 01:54:09,420
lining stage here or fine-tuning stage

3138
01:54:07,500 --> 01:54:12,060
it's got multiple steps in between there

3139
01:54:09,420 --> 01:54:14,940
as well and it takes the model from

3140
01:54:12,060 --> 01:54:17,639
being a document completer to a question

3141
01:54:14,940 --> 01:54:19,679
answer and that's like a whole separate

3142
01:54:17,639 --> 01:54:21,780
stage a lot of this data is not

3143
01:54:19,679 --> 01:54:24,179
available publicly it is internal to

3144
01:54:21,780 --> 01:54:26,580
open Ai and it's much harder to

3145
01:54:24,179 --> 01:54:28,260
replicate this stage

3146
01:54:26,580 --> 01:54:31,260
um and so that's roughly what would give

3147
01:54:28,260 --> 01:54:33,000
you a child GPD and Nano GPT focuses on

3148
01:54:31,260 --> 01:54:34,460
the pre-training stage okay and that's

3149
01:54:33,000 --> 01:54:38,820
everything that I wanted to cover today

3150
01:54:34,460 --> 01:54:41,760
so we trained to summarize a decoder

3151
01:54:38,820 --> 01:54:43,380
only Transformer following this famous

3152
01:54:41,760 --> 01:54:44,820
paper attention is all you need from

3153
01:54:43,380 --> 01:54:47,820
2017.

3154
01:54:44,820 --> 01:54:50,460
and so that's basically a GPT we trained

3155
01:54:47,820 --> 01:54:52,679
it on a tiny Shakespeare and got

3156
01:54:50,460 --> 01:54:55,380
sensible results

3157
01:54:52,679 --> 01:54:58,320
all of the training code is roughly

3158
01:54:55,380 --> 01:55:02,340
200 lines of code I will be releasing

3159
01:54:58,320 --> 01:55:04,800
this um code base so also it comes with

3160
01:55:02,340 --> 01:55:06,719
all the git log commits along the way as

3161
01:55:04,800 --> 01:55:08,280
we built it up

3162
01:55:06,719 --> 01:55:11,100
in addition to this code I'm going to

3163
01:55:08,280 --> 01:55:12,480
release the notebook of course the

3164
01:55:11,100 --> 01:55:14,159
Google collab

3165
01:55:12,480 --> 01:55:15,300
and I hope that gave you a sense for how

3166
01:55:14,159 --> 01:55:16,199
you can train

3167
01:55:15,300 --> 01:55:18,920
um

3168
01:55:16,199 --> 01:55:21,060
these models like say gpt3 there will be

3169
01:55:18,920 --> 01:55:22,560
architecturally basically identical to

3170
01:55:21,060 --> 01:55:24,179
what we have but they are somewhere

3171
01:55:22,560 --> 01:55:26,699
between ten thousand and one million

3172
01:55:24,179 --> 01:55:30,780
times bigger depending on how you count

3173
01:55:26,699 --> 01:55:32,219
and so that's all I have for now we did

3174
01:55:30,780 --> 01:55:33,960
not talk about any of the fine tuning

3175
01:55:32,219 --> 01:55:35,699
stages that would typically go on top of

3176
01:55:33,960 --> 01:55:36,600
this so if you're interested in

3177
01:55:35,699 --> 01:55:38,520
something that's not just language

3178
01:55:36,600 --> 01:55:41,100
modeling but you actually want to you

3179
01:55:38,520 --> 01:55:43,619
know say perform tasks or you want them

3180
01:55:41,100 --> 01:55:45,900
to be aligned in a specific way or you

3181
01:55:43,619 --> 01:55:47,580
want to detect sentiment or anything

3182
01:55:45,900 --> 01:55:48,960
like that basically anytime you don't

3183
01:55:47,580 --> 01:55:51,119
want something that's just a document

3184
01:55:48,960 --> 01:55:52,860
completer you have to complete further

3185
01:55:51,119 --> 01:55:53,820
stages of fine tuning which we did not

3186
01:55:52,860 --> 01:55:55,920
cover

3187
01:55:53,820 --> 01:55:57,540
uh and that could be simple supervised

3188
01:55:55,920 --> 01:55:59,400
fine tuning or it can be something more

3189
01:55:57,540 --> 01:56:01,320
fancy like we see in chargept we

3190
01:55:59,400 --> 01:56:03,719
actually train a reward model and then

3191
01:56:01,320 --> 01:56:05,340
do rounds of PPO to align it with

3192
01:56:03,719 --> 01:56:06,659
respect to the reward model

3193
01:56:05,340 --> 01:56:08,400
so there's a lot more that can be done

3194
01:56:06,659 --> 01:56:09,900
on top of it I think for now we're

3195
01:56:08,400 --> 01:56:11,699
starting to get to about two hours Mark

3196
01:56:09,900 --> 01:56:14,040
so I'm going to

3197
01:56:11,699 --> 01:56:16,679
um kind of finish here

3198
01:56:14,040 --> 01:56:18,960
I hope you enjoyed the lecture and uh

3199
01:56:16,679 --> 01:56:21,020
yeah go forth and transform see you

3200
01:56:18,960 --> 01:56:21,020
later

