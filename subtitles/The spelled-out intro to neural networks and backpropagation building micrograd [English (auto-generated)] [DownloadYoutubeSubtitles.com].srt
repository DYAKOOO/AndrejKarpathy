1
00:00:00,080 --> 00:00:02,960
hello my name is andre

2
00:00:01,839 --> 00:00:04,880
and i've been training deep neural

3
00:00:02,960 --> 00:00:06,799
networks for a bit more than a decade

4
00:00:04,880 --> 00:00:08,639
and in this lecture i'd like to show you

5
00:00:06,799 --> 00:00:10,880
what neural network training looks like

6
00:00:08,639 --> 00:00:12,240
under the hood so in particular we are

7
00:00:10,880 --> 00:00:14,320
going to start with a blank jupiter

8
00:00:12,240 --> 00:00:16,640
notebook and by the end of this lecture

9
00:00:14,320 --> 00:00:18,160
we will define and train in neural net

10
00:00:16,640 --> 00:00:20,240
and you'll get to see everything that

11
00:00:18,160 --> 00:00:21,680
goes on under the hood and exactly

12
00:00:20,240 --> 00:00:22,640
sort of how that works on an intuitive

13
00:00:21,680 --> 00:00:24,160
level

14
00:00:22,640 --> 00:00:26,560
now specifically what i would like to do

15
00:00:24,160 --> 00:00:29,359
is i would like to take you through

16
00:00:26,560 --> 00:00:30,880
building of micrograd now micrograd is

17
00:00:29,359 --> 00:00:32,880
this library that i released on github

18
00:00:30,880 --> 00:00:34,880
about two years ago but at the time i

19
00:00:32,880 --> 00:00:37,280
only uploaded the source code and you'd

20
00:00:34,880 --> 00:00:39,280
have to go in by yourself and really

21
00:00:37,280 --> 00:00:40,640
figure out how it works

22
00:00:39,280 --> 00:00:42,399
so in this lecture i will take you

23
00:00:40,640 --> 00:00:44,399
through it step by step and kind of

24
00:00:42,399 --> 00:00:47,440
comment on all the pieces of it so what

25
00:00:44,399 --> 00:00:48,960
is micrograd and why is it interesting

26
00:00:47,440 --> 00:00:49,840
good

27
00:00:48,960 --> 00:00:51,520
um

28
00:00:49,840 --> 00:00:53,840
micrograd is basically an autograd

29
00:00:51,520 --> 00:00:55,680
engine autograd is short for automatic

30
00:00:53,840 --> 00:00:57,680
gradient and really what it does is it

31
00:00:55,680 --> 00:00:59,359
implements backpropagation now

32
00:00:57,680 --> 00:01:01,280
backpropagation is this algorithm that

33
00:00:59,359 --> 00:01:03,520
allows you to efficiently evaluate the

34
00:01:01,280 --> 00:01:05,199
gradient of

35
00:01:03,520 --> 00:01:07,200
some kind of a loss function with

36
00:01:05,199 --> 00:01:09,119
respect to the weights of a neural

37
00:01:07,200 --> 00:01:11,040
network and what that allows us to do

38
00:01:09,119 --> 00:01:12,560
then is we can iteratively tune the

39
00:01:11,040 --> 00:01:14,320
weights of that neural network to

40
00:01:12,560 --> 00:01:16,479
minimize the loss function and therefore

41
00:01:14,320 --> 00:01:18,320
improve the accuracy of the network so

42
00:01:16,479 --> 00:01:20,479
back propagation would be at the

43
00:01:18,320 --> 00:01:22,560
mathematical core of any modern deep

44
00:01:20,479 --> 00:01:24,000
neural network library like say pytorch

45
00:01:22,560 --> 00:01:25,600
or jaxx

46
00:01:24,000 --> 00:01:27,600
so the functionality of microgrant is i

47
00:01:25,600 --> 00:01:29,840
think best illustrated by an example so

48
00:01:27,600 --> 00:01:31,200
if we just scroll down here

49
00:01:29,840 --> 00:01:32,960
you'll see that micrograph basically

50
00:01:31,200 --> 00:01:34,240
allows you to build out mathematical

51
00:01:32,960 --> 00:01:36,560
expressions

52
00:01:34,240 --> 00:01:37,840
and um here what we are doing is we have

53
00:01:36,560 --> 00:01:40,880
an expression that we're building out

54
00:01:37,840 --> 00:01:43,280
where you have two inputs a and b

55
00:01:40,880 --> 00:01:46,159
and you'll see that a and b are negative

56
00:01:43,280 --> 00:01:48,640
four and two but we are wrapping those

57
00:01:46,159 --> 00:01:49,920
values into this value object that we

58
00:01:48,640 --> 00:01:51,040
are going to build out as part of

59
00:01:49,920 --> 00:01:53,520
micrograd

60
00:01:51,040 --> 00:01:54,960
so this value object will wrap the

61
00:01:53,520 --> 00:01:56,240
numbers themselves

62
00:01:54,960 --> 00:01:58,479
and then we are going to build out a

63
00:01:56,240 --> 00:02:01,920
mathematical expression here where a and

64
00:01:58,479 --> 00:02:03,920
b are transformed into c d and

65
00:02:01,920 --> 00:02:05,600
eventually e f and g

66
00:02:03,920 --> 00:02:07,040
and i'm showing some of the functions

67
00:02:05,600 --> 00:02:08,959
some of the functionality of micrograph

68
00:02:07,040 --> 00:02:11,039
and the operations that it supports so

69
00:02:08,959 --> 00:02:13,360
you can add two value objects you can

70
00:02:11,039 --> 00:02:15,760
multiply them you can raise them to a

71
00:02:13,360 --> 00:02:18,640
constant power you can offset by one

72
00:02:15,760 --> 00:02:21,520
negate squash at zero

73
00:02:18,640 --> 00:02:22,640
square divide by constant divide by it

74
00:02:21,520 --> 00:02:24,319
etc

75
00:02:22,640 --> 00:02:27,360
and so we're building out an expression

76
00:02:24,319 --> 00:02:30,319
graph with with these two inputs a and b

77
00:02:27,360 --> 00:02:32,640
and we're creating an output value of g

78
00:02:30,319 --> 00:02:34,160
and micrograd will in the background

79
00:02:32,640 --> 00:02:36,400
build out this entire mathematical

80
00:02:34,160 --> 00:02:38,720
expression so it will for example know

81
00:02:36,400 --> 00:02:41,360
that c is also a value

82
00:02:38,720 --> 00:02:42,800
c was a result of an addition operation

83
00:02:41,360 --> 00:02:46,640
and the

84
00:02:42,800 --> 00:02:48,879
child nodes of c are a and b because the

85
00:02:46,640 --> 00:02:50,800
and will maintain pointers to a and b

86
00:02:48,879 --> 00:02:53,360
value objects so we'll basically know

87
00:02:50,800 --> 00:02:55,440
exactly how all of this is laid out

88
00:02:53,360 --> 00:02:57,040
and then not only can we do what we call

89
00:02:55,440 --> 00:02:58,640
the forward pass where we actually look

90
00:02:57,040 --> 00:03:00,560
at the value of g of course that's

91
00:02:58,640 --> 00:03:03,599
pretty straightforward we will access

92
00:03:00,560 --> 00:03:06,080
that using the dot data attribute and so

93
00:03:03,599 --> 00:03:09,360
the output of the forward pass the value

94
00:03:06,080 --> 00:03:11,920
of g is 24.7 it turns out but the big

95
00:03:09,360 --> 00:03:13,840
deal is that we can also take this g

96
00:03:11,920 --> 00:03:14,879
value object and we can call that

97
00:03:13,840 --> 00:03:16,720
backward

98
00:03:14,879 --> 00:03:19,840
and this will basically uh initialize

99
00:03:16,720 --> 00:03:21,360
back propagation at the node g

100
00:03:19,840 --> 00:03:23,200
and what backpropagation is going to do

101
00:03:21,360 --> 00:03:25,200
is it's going to start at g and it's

102
00:03:23,200 --> 00:03:26,720
going to go backwards through that

103
00:03:25,200 --> 00:03:28,799
expression graph and it's going to

104
00:03:26,720 --> 00:03:30,080
recursively apply the chain rule from

105
00:03:28,799 --> 00:03:32,239
calculus

106
00:03:30,080 --> 00:03:34,239
and what that allows us to do then is

107
00:03:32,239 --> 00:03:36,560
we're going to evaluate basically the

108
00:03:34,239 --> 00:03:38,159
derivative of g with respect to all the

109
00:03:36,560 --> 00:03:40,959
internal nodes

110
00:03:38,159 --> 00:03:43,200
like e d and c but also with respect to

111
00:03:40,959 --> 00:03:45,360
the inputs a and b

112
00:03:43,200 --> 00:03:47,920
and then we can actually query this

113
00:03:45,360 --> 00:03:50,159
derivative of g with respect to a for

114
00:03:47,920 --> 00:03:52,480
example that's a dot grad in this case

115
00:03:50,159 --> 00:03:54,239
it happens to be 138 and the derivative

116
00:03:52,480 --> 00:03:57,439
of g with respect to b

117
00:03:54,239 --> 00:03:59,439
which also happens to be here 645

118
00:03:57,439 --> 00:04:01,360
and this derivative we'll see soon is

119
00:03:59,439 --> 00:04:04,879
very important information because it's

120
00:04:01,360 --> 00:04:06,879
telling us how a and b are affecting g

121
00:04:04,879 --> 00:04:08,080
through this mathematical expression so

122
00:04:06,879 --> 00:04:11,840
in particular

123
00:04:08,080 --> 00:04:14,799
a dot grad is 138 so if we slightly

124
00:04:11,840 --> 00:04:18,000
nudge a and make it slightly larger

125
00:04:14,799 --> 00:04:19,440
138 is telling us that g will grow and

126
00:04:18,000 --> 00:04:20,720
the slope of that growth is going to be

127
00:04:19,440 --> 00:04:22,960
138

128
00:04:20,720 --> 00:04:25,600
and the slope of growth of b is going to

129
00:04:22,960 --> 00:04:27,840
be 645. so that's going to tell us about

130
00:04:25,600 --> 00:04:29,759
how g will respond if a and b get

131
00:04:27,840 --> 00:04:31,120
tweaked a tiny amount in a positive

132
00:04:29,759 --> 00:04:33,360
direction

133
00:04:31,120 --> 00:04:33,360
okay

134
00:04:33,440 --> 00:04:36,800
now you might be confused about what

135
00:04:34,880 --> 00:04:38,560
this expression is that we built out

136
00:04:36,800 --> 00:04:40,720
here and this expression by the way is

137
00:04:38,560 --> 00:04:42,240
completely meaningless i just made it up

138
00:04:40,720 --> 00:04:43,759
i'm just flexing about the kinds of

139
00:04:42,240 --> 00:04:44,880
operations that are supported by

140
00:04:43,759 --> 00:04:46,320
micrograd

141
00:04:44,880 --> 00:04:48,080
what we actually really care about are

142
00:04:46,320 --> 00:04:49,840
neural networks but it turns out that

143
00:04:48,080 --> 00:04:51,919
neural networks are just mathematical

144
00:04:49,840 --> 00:04:54,880
expressions just like this one but

145
00:04:51,919 --> 00:04:56,320
actually slightly bit less crazy even

146
00:04:54,880 --> 00:04:59,040
neural networks are just a mathematical

147
00:04:56,320 --> 00:05:00,720
expression they take the input data as

148
00:04:59,040 --> 00:05:02,479
an input and they take the weights of a

149
00:05:00,720 --> 00:05:04,560
neural network as an input and it's a

150
00:05:02,479 --> 00:05:06,320
mathematical expression and the output

151
00:05:04,560 --> 00:05:08,160
are your predictions of your neural net

152
00:05:06,320 --> 00:05:10,560
or the loss function we'll see this in a

153
00:05:08,160 --> 00:05:12,080
bit but basically neural networks just

154
00:05:10,560 --> 00:05:13,759
happen to be a certain class of

155
00:05:12,080 --> 00:05:15,199
mathematical expressions

156
00:05:13,759 --> 00:05:17,120
but back propagation is actually

157
00:05:15,199 --> 00:05:18,800
significantly more general it doesn't

158
00:05:17,120 --> 00:05:20,639
actually care about neural networks at

159
00:05:18,800 --> 00:05:22,400
all it only tells us about arbitrary

160
00:05:20,639 --> 00:05:24,479
mathematical expressions and then we

161
00:05:22,400 --> 00:05:26,800
happen to use that machinery for

162
00:05:24,479 --> 00:05:28,400
training of neural networks now one more

163
00:05:26,800 --> 00:05:30,240
note i would like to make at this stage

164
00:05:28,400 --> 00:05:32,639
is that as you see here micrograd is a

165
00:05:30,240 --> 00:05:34,240
scalar valued auto grant engine so it's

166
00:05:32,639 --> 00:05:36,000
working on the you know level of

167
00:05:34,240 --> 00:05:37,759
individual scalars like negative four

168
00:05:36,000 --> 00:05:39,280
and two and we're taking neural nets and

169
00:05:37,759 --> 00:05:41,360
we're breaking them down all the way to

170
00:05:39,280 --> 00:05:43,120
these atoms of individual scalars and

171
00:05:41,360 --> 00:05:45,600
all the little pluses and times and it's

172
00:05:43,120 --> 00:05:47,120
just excessive and so obviously you

173
00:05:45,600 --> 00:05:48,800
would never be doing any of this in

174
00:05:47,120 --> 00:05:50,800
production it's really just put down for

175
00:05:48,800 --> 00:05:52,320
pedagogical reasons because it allows us

176
00:05:50,800 --> 00:05:54,560
to not have to deal with these

177
00:05:52,320 --> 00:05:56,960
n-dimensional tensors that you would use

178
00:05:54,560 --> 00:05:58,960
in modern deep neural network library so

179
00:05:56,960 --> 00:06:00,720
this is really done so that you

180
00:05:58,960 --> 00:06:02,720
understand and refactor out back

181
00:06:00,720 --> 00:06:04,960
propagation and chain rule and

182
00:06:02,720 --> 00:06:06,720
understanding of neurologic training

183
00:06:04,960 --> 00:06:08,080
and then if you actually want to train

184
00:06:06,720 --> 00:06:09,840
bigger networks you have to be using

185
00:06:08,080 --> 00:06:11,280
these tensors but none of the math

186
00:06:09,840 --> 00:06:13,440
changes this is done purely for

187
00:06:11,280 --> 00:06:14,240
efficiency we are basically taking scale

188
00:06:13,440 --> 00:06:16,000
value

189
00:06:14,240 --> 00:06:17,759
all the scale values we're packaging

190
00:06:16,000 --> 00:06:20,080
them up into tensors which are just

191
00:06:17,759 --> 00:06:22,400
arrays of these scalars and then because

192
00:06:20,080 --> 00:06:24,400
we have these large arrays we're making

193
00:06:22,400 --> 00:06:26,080
operations on those large arrays that

194
00:06:24,400 --> 00:06:28,639
allows us to take advantage of the

195
00:06:26,080 --> 00:06:30,479
parallelism in a computer and all those

196
00:06:28,639 --> 00:06:32,400
operations can be done in parallel and

197
00:06:30,479 --> 00:06:33,680
then the whole thing runs faster but

198
00:06:32,400 --> 00:06:35,680
really none of the math changes and

199
00:06:33,680 --> 00:06:36,880
that's done purely for efficiency so i

200
00:06:35,680 --> 00:06:38,639
don't think that it's pedagogically

201
00:06:36,880 --> 00:06:40,720
useful to be dealing with tensors from

202
00:06:38,639 --> 00:06:42,560
scratch uh and i think and that's why i

203
00:06:40,720 --> 00:06:44,479
fundamentally wrote micrograd because

204
00:06:42,560 --> 00:06:46,400
you can understand how things work uh at

205
00:06:44,479 --> 00:06:48,880
the fundamental level and then you can

206
00:06:46,400 --> 00:06:51,199
speed it up later okay so here's the fun

207
00:06:48,880 --> 00:06:52,800
part my claim is that micrograd is what

208
00:06:51,199 --> 00:06:54,960
you need to train your networks and

209
00:06:52,800 --> 00:06:56,319
everything else is just efficiency so

210
00:06:54,960 --> 00:06:58,560
you'd think that micrograd would be a

211
00:06:56,319 --> 00:07:01,039
very complex piece of code and that

212
00:06:58,560 --> 00:07:03,199
turns out to not be the case

213
00:07:01,039 --> 00:07:05,039
so if we just go to micrograd

214
00:07:03,199 --> 00:07:07,360
and you'll see that there's only two

215
00:07:05,039 --> 00:07:09,120
files here in micrograd this is the

216
00:07:07,360 --> 00:07:10,880
actual engine it doesn't know anything

217
00:07:09,120 --> 00:07:12,720
about neural nuts and this is the entire

218
00:07:10,880 --> 00:07:17,199
neural nets library

219
00:07:12,720 --> 00:07:19,840
on top of micrograd so engine and nn.pi

220
00:07:17,199 --> 00:07:21,199
so the actual backpropagation autograd

221
00:07:19,840 --> 00:07:22,400
engine

222
00:07:21,199 --> 00:07:26,080
that gives you the power of neural

223
00:07:22,400 --> 00:07:28,400
networks is literally

224
00:07:26,080 --> 00:07:30,000
100 lines of code of like very simple

225
00:07:28,400 --> 00:07:31,120
python

226
00:07:30,000 --> 00:07:32,080
which we'll understand by the end of

227
00:07:31,120 --> 00:07:33,840
this lecture

228
00:07:32,080 --> 00:07:35,680
and then nn.pi

229
00:07:33,840 --> 00:07:37,759
this neural network library built on top

230
00:07:35,680 --> 00:07:40,560
of the autograd engine

231
00:07:37,759 --> 00:07:42,720
um is like a joke it's like

232
00:07:40,560 --> 00:07:44,000
we have to define what is a neuron and

233
00:07:42,720 --> 00:07:46,080
then we have to define what is the layer

234
00:07:44,000 --> 00:07:47,840
of neurons and then we define what is a

235
00:07:46,080 --> 00:07:50,400
multi-layer perceptron which is just a

236
00:07:47,840 --> 00:07:52,000
sequence of layers of neurons and so

237
00:07:50,400 --> 00:07:53,440
it's just a total joke

238
00:07:52,000 --> 00:07:55,680
so basically

239
00:07:53,440 --> 00:07:57,599
there's a lot of power that comes from

240
00:07:55,680 --> 00:07:59,120
only 150 lines of code

241
00:07:57,599 --> 00:08:00,879
and that's all you need to understand to

242
00:07:59,120 --> 00:08:02,720
understand neural network training and

243
00:08:00,879 --> 00:08:05,680
everything else is just efficiency and

244
00:08:02,720 --> 00:08:07,120
of course there's a lot to efficiency

245
00:08:05,680 --> 00:08:09,120
but fundamentally that's all that's

246
00:08:07,120 --> 00:08:11,280
happening okay so now let's dive right

247
00:08:09,120 --> 00:08:12,400
in and implement micrograph step by step

248
00:08:11,280 --> 00:08:13,759
the first thing i'd like to do is i'd

249
00:08:12,400 --> 00:08:16,240
like to make sure that you have a very

250
00:08:13,759 --> 00:08:18,240
good understanding intuitively of what a

251
00:08:16,240 --> 00:08:20,560
derivative is and exactly what

252
00:08:18,240 --> 00:08:22,319
information it gives you so let's start

253
00:08:20,560 --> 00:08:25,280
with some basic imports that i copy

254
00:08:22,319 --> 00:08:27,360
paste in every jupiter notebook always

255
00:08:25,280 --> 00:08:28,960
and let's define a function a scalar

256
00:08:27,360 --> 00:08:30,240
valued function

257
00:08:28,960 --> 00:08:31,360
f of x

258
00:08:30,240 --> 00:08:33,200
as follows

259
00:08:31,360 --> 00:08:34,560
so i just make this up randomly i just

260
00:08:33,200 --> 00:08:36,719
want to scale a valid function that

261
00:08:34,560 --> 00:08:38,560
takes a single scalar x and returns a

262
00:08:36,719 --> 00:08:40,080
single scalar y

263
00:08:38,560 --> 00:08:42,800
and we can call this function of course

264
00:08:40,080 --> 00:08:43,919
so we can pass in say 3.0 and get 20

265
00:08:42,800 --> 00:08:45,600
back

266
00:08:43,919 --> 00:08:47,440
now we can also plot this function to

267
00:08:45,600 --> 00:08:48,880
get a sense of its shape you can tell

268
00:08:47,440 --> 00:08:50,720
from the mathematical expression that

269
00:08:48,880 --> 00:08:51,839
this is probably a parabola it's a

270
00:08:50,720 --> 00:08:56,399
quadratic

271
00:08:51,839 --> 00:08:57,680
and so if we just uh create a set of um

272
00:08:56,399 --> 00:08:59,519
um

273
00:08:57,680 --> 00:09:01,120
scale values that we can feed in using

274
00:08:59,519 --> 00:09:03,920
for example a range from negative five

275
00:09:01,120 --> 00:09:06,640
to five in steps of 0.25

276
00:09:03,920 --> 00:09:11,360
so this is so axis is just from negative

277
00:09:06,640 --> 00:09:12,720
5 to 5 not including 5 in steps of 0.25

278
00:09:11,360 --> 00:09:14,640
and we can actually call this function

279
00:09:12,720 --> 00:09:17,680
on this numpy array as well so we get a

280
00:09:14,640 --> 00:09:20,399
set of y's if we call f on axis

281
00:09:17,680 --> 00:09:23,519
and these y's are basically

282
00:09:20,399 --> 00:09:25,360
also applying a function on every one of

283
00:09:23,519 --> 00:09:28,000
these elements independently

284
00:09:25,360 --> 00:09:31,200
and we can plot this using matplotlib so

285
00:09:28,000 --> 00:09:33,760
plt.plot x's and y's and we get a nice

286
00:09:31,200 --> 00:09:36,560
parabola so previously here we fed in

287
00:09:33,760 --> 00:09:39,040
3.0 somewhere here and we received 20

288
00:09:36,560 --> 00:09:40,640
back which is here the y coordinate so

289
00:09:39,040 --> 00:09:42,399
now i'd like to think through

290
00:09:40,640 --> 00:09:44,320
what is the derivative

291
00:09:42,399 --> 00:09:45,519
of this function at any single input

292
00:09:44,320 --> 00:09:47,040
point x

293
00:09:45,519 --> 00:09:49,839
right so what is the derivative at

294
00:09:47,040 --> 00:09:51,200
different points x of this function now

295
00:09:49,839 --> 00:09:52,240
if you remember back to your calculus

296
00:09:51,200 --> 00:09:54,640
class you've probably derived

297
00:09:52,240 --> 00:09:57,200
derivatives so we take this mathematical

298
00:09:54,640 --> 00:09:58,399
expression 3x squared minus 4x plus 5

299
00:09:57,200 --> 00:09:59,920
and you would write out on a piece of

300
00:09:58,399 --> 00:10:01,360
paper and you would you know apply the

301
00:09:59,920 --> 00:10:03,440
product rule and all the other rules and

302
00:10:01,360 --> 00:10:05,440
derive the mathematical expression of

303
00:10:03,440 --> 00:10:06,720
the great derivative of the original

304
00:10:05,440 --> 00:10:08,000
function and then you could plug in

305
00:10:06,720 --> 00:10:09,760
different texts and see what the

306
00:10:08,000 --> 00:10:11,519
derivative is

307
00:10:09,760 --> 00:10:13,680
we're not going to actually do that

308
00:10:11,519 --> 00:10:15,360
because no one in neural networks

309
00:10:13,680 --> 00:10:16,640
actually writes out the expression for

310
00:10:15,360 --> 00:10:18,880
the neural net it would be a massive

311
00:10:16,640 --> 00:10:20,480
expression um it would be you know

312
00:10:18,880 --> 00:10:22,640
thousands tens of thousands of terms no

313
00:10:20,480 --> 00:10:24,399
one actually derives the derivative of

314
00:10:22,640 --> 00:10:26,160
course and so we're not going to take

315
00:10:24,399 --> 00:10:27,360
this kind of like a symbolic approach

316
00:10:26,160 --> 00:10:29,279
instead what i'd like to do is i'd like

317
00:10:27,360 --> 00:10:30,399
to look at the definition of derivative

318
00:10:29,279 --> 00:10:32,320
and just make sure that we really

319
00:10:30,399 --> 00:10:34,800
understand what derivative is measuring

320
00:10:32,320 --> 00:10:38,640
what it's telling you about the function

321
00:10:34,800 --> 00:10:38,640
and so if we just look up derivative

322
00:10:42,320 --> 00:10:44,640
we see that

323
00:10:43,519 --> 00:10:46,079
okay so this is not a very good

324
00:10:44,640 --> 00:10:47,279
definition of derivative this is a

325
00:10:46,079 --> 00:10:48,480
definition of what it means to be

326
00:10:47,279 --> 00:10:50,160
differentiable

327
00:10:48,480 --> 00:10:52,560
but if you remember from your calculus

328
00:10:50,160 --> 00:10:55,920
it is the limit as h goes to zero of f

329
00:10:52,560 --> 00:10:58,399
of x plus h minus f of x over h so

330
00:10:55,920 --> 00:11:00,959
basically what it's saying is if you

331
00:10:58,399 --> 00:11:02,880
slightly bump up you're at some point x

332
00:11:00,959 --> 00:11:04,560
that you're interested in or a and if

333
00:11:02,880 --> 00:11:06,079
you slightly bump up

334
00:11:04,560 --> 00:11:08,079
you know you slightly increase it by

335
00:11:06,079 --> 00:11:09,760
small number h

336
00:11:08,079 --> 00:11:11,440
how does the function respond with what

337
00:11:09,760 --> 00:11:13,600
sensitivity does it respond what is the

338
00:11:11,440 --> 00:11:16,320
slope at that point does the function go

339
00:11:13,600 --> 00:11:18,000
up or does it go down and by how much

340
00:11:16,320 --> 00:11:18,720
and that's the slope of that function

341
00:11:18,000 --> 00:11:21,760
the

342
00:11:18,720 --> 00:11:23,920
the slope of that response at that point

343
00:11:21,760 --> 00:11:26,320
and so we can basically evaluate

344
00:11:23,920 --> 00:11:28,000
the derivative here numerically by

345
00:11:26,320 --> 00:11:30,160
taking a very small h of course the

346
00:11:28,000 --> 00:11:31,760
definition would ask us to take h to

347
00:11:30,160 --> 00:11:34,000
zero we're just going to pick a very

348
00:11:31,760 --> 00:11:35,120
small h 0.001

349
00:11:34,000 --> 00:11:37,760
and let's say we're interested in point

350
00:11:35,120 --> 00:11:38,959
3.0 so we can look at f of x of course

351
00:11:37,760 --> 00:11:40,959
as 20

352
00:11:38,959 --> 00:11:42,800
and now f of x plus h

353
00:11:40,959 --> 00:11:44,560
so if we slightly nudge x in a positive

354
00:11:42,800 --> 00:11:45,600
direction how is the function going to

355
00:11:44,560 --> 00:11:47,680
respond

356
00:11:45,600 --> 00:11:49,200
and just looking at this do you expect

357
00:11:47,680 --> 00:11:51,920
do you expect f of x plus h to be

358
00:11:49,200 --> 00:11:54,639
slightly greater than 20 or do you

359
00:11:51,920 --> 00:11:57,040
expect to be slightly lower than 20

360
00:11:54,639 --> 00:11:59,040
and since this 3 is here and this is 20

361
00:11:57,040 --> 00:12:01,360
if we slightly go positively the

362
00:11:59,040 --> 00:12:03,040
function will respond positively so

363
00:12:01,360 --> 00:12:05,360
you'd expect this to be slightly greater

364
00:12:03,040 --> 00:12:06,480
than 20. and now by how much it's

365
00:12:05,360 --> 00:12:07,839
telling you the

366
00:12:06,480 --> 00:12:09,920
sort of the

367
00:12:07,839 --> 00:12:12,800
the strength of that slope right the the

368
00:12:09,920 --> 00:12:14,800
size of the slope so f of x plus h minus

369
00:12:12,800 --> 00:12:16,000
f of x this is how much the function

370
00:12:14,800 --> 00:12:17,920
responded

371
00:12:16,000 --> 00:12:19,920
in the positive direction and we have to

372
00:12:17,920 --> 00:12:22,320
normalize by the

373
00:12:19,920 --> 00:12:24,800
run so we have the rise over run to get

374
00:12:22,320 --> 00:12:26,800
the slope so this of course is just a

375
00:12:24,800 --> 00:12:28,880
numerical approximation of the slope

376
00:12:26,800 --> 00:12:32,800
because we have to make age very very

377
00:12:28,880 --> 00:12:35,519
small to converge to the exact amount

378
00:12:32,800 --> 00:12:36,800
now if i'm doing too many zeros

379
00:12:35,519 --> 00:12:38,320
at some point

380
00:12:36,800 --> 00:12:39,600
i'm gonna get an incorrect answer

381
00:12:38,320 --> 00:12:41,839
because we're using floating point

382
00:12:39,600 --> 00:12:43,760
arithmetic and the representations of

383
00:12:41,839 --> 00:12:45,600
all these numbers in computer memory is

384
00:12:43,760 --> 00:12:46,399
finite and at some point we get into

385
00:12:45,600 --> 00:12:47,760
trouble

386
00:12:46,399 --> 00:12:50,399
so we can converse towards the right

387
00:12:47,760 --> 00:12:54,560
answer with this approach

388
00:12:50,399 --> 00:12:56,079
but basically um at 3 the slope is 14.

389
00:12:54,560 --> 00:12:58,320
and you can see that by taking 3x

390
00:12:56,079 --> 00:13:00,480
squared minus 4x plus 5 and

391
00:12:58,320 --> 00:13:02,639
differentiating it in our head

392
00:13:00,480 --> 00:13:04,880
so 3x squared would be

393
00:13:02,639 --> 00:13:07,279
6 x minus 4

394
00:13:04,880 --> 00:13:10,880
and then we plug in x equals 3 so that's

395
00:13:07,279 --> 00:13:12,079
18 minus 4 is 14. so this is correct

396
00:13:10,880 --> 00:13:15,760
so that's

397
00:13:12,079 --> 00:13:17,360
at 3. now how about the slope at say

398
00:13:15,760 --> 00:13:19,519
negative 3

399
00:13:17,360 --> 00:13:20,480
would you expect would you expect for

400
00:13:19,519 --> 00:13:22,320
the slope

401
00:13:20,480 --> 00:13:24,959
now telling the exact value is really

402
00:13:22,320 --> 00:13:26,800
hard but what is the sign of that slope

403
00:13:24,959 --> 00:13:28,399
so at negative three

404
00:13:26,800 --> 00:13:30,720
if we slightly go in the positive

405
00:13:28,399 --> 00:13:32,560
direction at x the function would

406
00:13:30,720 --> 00:13:33,920
actually go down and so that tells you

407
00:13:32,560 --> 00:13:36,720
that the slope would be negative so

408
00:13:33,920 --> 00:13:39,120
we'll get a slight number below

409
00:13:36,720 --> 00:13:40,880
below 20. and so if we take the slope we

410
00:13:39,120 --> 00:13:43,839
expect something negative

411
00:13:40,880 --> 00:13:45,519
negative 22. okay

412
00:13:43,839 --> 00:13:47,600
and at some point here of course the

413
00:13:45,519 --> 00:13:48,880
slope would be zero now for this

414
00:13:47,600 --> 00:13:51,279
specific function i looked it up

415
00:13:48,880 --> 00:13:52,160
previously and it's at point two over

416
00:13:51,279 --> 00:13:54,320
three

417
00:13:52,160 --> 00:13:55,920
so at roughly two over three

418
00:13:54,320 --> 00:13:57,120
uh that's somewhere here

419
00:13:55,920 --> 00:13:59,199
um

420
00:13:57,120 --> 00:14:03,440
this derivative be zero

421
00:13:59,199 --> 00:14:04,240
so basically at that precise point

422
00:14:03,440 --> 00:14:06,320
yeah

423
00:14:04,240 --> 00:14:07,760
at that precise point if we nudge in a

424
00:14:06,320 --> 00:14:09,839
positive direction the function doesn't

425
00:14:07,760 --> 00:14:11,839
respond this stays the same almost and

426
00:14:09,839 --> 00:14:14,320
so that's why the slope is zero okay now

427
00:14:11,839 --> 00:14:15,839
let's look at a bit more complex case

428
00:14:14,320 --> 00:14:18,000
so we're going to start you know

429
00:14:15,839 --> 00:14:19,680
complexifying a bit so now we have a

430
00:14:18,000 --> 00:14:20,959
function

431
00:14:19,680 --> 00:14:22,639
here

432
00:14:20,959 --> 00:14:24,160
with output variable d

433
00:14:22,639 --> 00:14:26,160
that is a function of three scalar

434
00:14:24,160 --> 00:14:28,480
inputs a b and c

435
00:14:26,160 --> 00:14:30,639
so a b and c are some specific values

436
00:14:28,480 --> 00:14:32,800
three inputs into our expression graph

437
00:14:30,639 --> 00:14:36,320
and a single output d

438
00:14:32,800 --> 00:14:38,160
and so if we just print d we get four

439
00:14:36,320 --> 00:14:40,240
and now what i have to do is i'd like to

440
00:14:38,160 --> 00:14:42,480
again look at the derivatives of d with

441
00:14:40,240 --> 00:14:44,880
respect to a b and c

442
00:14:42,480 --> 00:14:46,240
and uh think through uh again just the

443
00:14:44,880 --> 00:14:47,519
intuition of what this derivative is

444
00:14:46,240 --> 00:14:49,839
telling us

445
00:14:47,519 --> 00:14:52,079
so in order to evaluate this derivative

446
00:14:49,839 --> 00:14:53,920
we're going to get a bit hacky here

447
00:14:52,079 --> 00:14:55,360
we're going to again have a very small

448
00:14:53,920 --> 00:14:57,360
value of h

449
00:14:55,360 --> 00:14:58,560
and then we're going to fix the inputs

450
00:14:57,360 --> 00:15:00,320
at some

451
00:14:58,560 --> 00:15:02,959
values that we're interested in

452
00:15:00,320 --> 00:15:04,240
so these are the this is the point abc

453
00:15:02,959 --> 00:15:05,440
at which we're going to be evaluating

454
00:15:04,240 --> 00:15:07,760
the the

455
00:15:05,440 --> 00:15:09,760
derivative of d with respect to all a b

456
00:15:07,760 --> 00:15:11,360
and c at that point

457
00:15:09,760 --> 00:15:13,760
so there are the inputs and now we have

458
00:15:11,360 --> 00:15:15,040
d1 is that expression

459
00:15:13,760 --> 00:15:17,120
and then we're going to for example look

460
00:15:15,040 --> 00:15:19,920
at the derivative of d with respect to a

461
00:15:17,120 --> 00:15:22,000
so we'll take a and we'll bump it by h

462
00:15:19,920 --> 00:15:23,839
and then we'll get d2 to be the exact

463
00:15:22,000 --> 00:15:26,800
same function

464
00:15:23,839 --> 00:15:28,720
and now we're going to print um

465
00:15:26,800 --> 00:15:31,120
you know f1

466
00:15:28,720 --> 00:15:32,880
d1 is d1

467
00:15:31,120 --> 00:15:35,199
d2 is d2

468
00:15:32,880 --> 00:15:37,680
and print slope

469
00:15:35,199 --> 00:15:39,680
so the derivative or slope

470
00:15:37,680 --> 00:15:41,199
here will be um

471
00:15:39,680 --> 00:15:42,079
of course

472
00:15:41,199 --> 00:15:44,399
d2

473
00:15:42,079 --> 00:15:47,440
minus d1 divide h

474
00:15:44,399 --> 00:15:48,720
so d2 minus d1 is how much the function

475
00:15:47,440 --> 00:15:50,720
increased

476
00:15:48,720 --> 00:15:51,920
uh when we bumped

477
00:15:50,720 --> 00:15:53,440
the uh

478
00:15:51,920 --> 00:15:55,839
the specific input that we're interested

479
00:15:53,440 --> 00:15:56,720
in by a tiny amount

480
00:15:55,839 --> 00:15:59,199
and

481
00:15:56,720 --> 00:16:02,079
this is then normalized by h

482
00:15:59,199 --> 00:16:02,079
to get the slope

483
00:16:02,800 --> 00:16:05,120
so

484
00:16:03,600 --> 00:16:06,320
um

485
00:16:05,120 --> 00:16:08,880
yeah

486
00:16:06,320 --> 00:16:10,399
so this so if i just run this we're

487
00:16:08,880 --> 00:16:12,240
going to print

488
00:16:10,399 --> 00:16:15,440
d1

489
00:16:12,240 --> 00:16:18,480
which we know is four

490
00:16:15,440 --> 00:16:20,320
now d2 will be bumped a will be bumped

491
00:16:18,480 --> 00:16:22,480
by h

492
00:16:20,320 --> 00:16:26,160
so let's just think through

493
00:16:22,480 --> 00:16:27,600
a little bit uh what d2 will be uh

494
00:16:26,160 --> 00:16:29,279
printed out here

495
00:16:27,600 --> 00:16:31,120
in particular

496
00:16:29,279 --> 00:16:33,519
d1 will be four

497
00:16:31,120 --> 00:16:35,680
will d2 be a number slightly greater

498
00:16:33,519 --> 00:16:37,600
than four or slightly lower than four

499
00:16:35,680 --> 00:16:40,160
and that's going to tell us the sl the

500
00:16:37,600 --> 00:16:42,399
the sign of the derivative

501
00:16:40,160 --> 00:16:42,399
so

502
00:16:42,560 --> 00:16:48,560
we're bumping a by h

503
00:16:45,440 --> 00:16:49,920
b as minus three c is ten

504
00:16:48,560 --> 00:16:51,120
so you can just intuitively think

505
00:16:49,920 --> 00:16:54,800
through this derivative and what it's

506
00:16:51,120 --> 00:16:57,519
doing a will be slightly more positive

507
00:16:54,800 --> 00:17:00,399
and but b is a negative number

508
00:16:57,519 --> 00:17:03,199
so if a is slightly more positive

509
00:17:00,399 --> 00:17:06,079
because b is negative three

510
00:17:03,199 --> 00:17:08,160
we're actually going to be adding less

511
00:17:06,079 --> 00:17:10,640
to d

512
00:17:08,160 --> 00:17:13,760
so you'd actually expect that the value

513
00:17:10,640 --> 00:17:16,480
of the function will go down

514
00:17:13,760 --> 00:17:18,400
so let's just see this

515
00:17:16,480 --> 00:17:20,799
yeah and so we went from 4

516
00:17:18,400 --> 00:17:22,160
to 3.9996

517
00:17:20,799 --> 00:17:23,280
and that tells you that the slope will

518
00:17:22,160 --> 00:17:24,480
be negative

519
00:17:23,280 --> 00:17:26,400
and then

520
00:17:24,480 --> 00:17:27,919
uh will be a negative number

521
00:17:26,400 --> 00:17:29,039
because we went down

522
00:17:27,919 --> 00:17:31,039
and then

523
00:17:29,039 --> 00:17:33,760
the exact number of slope will be

524
00:17:31,039 --> 00:17:35,120
exact amount of slope is negative 3.

525
00:17:33,760 --> 00:17:36,960
and you can also convince yourself that

526
00:17:35,120 --> 00:17:39,039
negative 3 is the right answer

527
00:17:36,960 --> 00:17:41,600
mathematically and analytically because

528
00:17:39,039 --> 00:17:43,840
if you have a times b plus c and you are

529
00:17:41,600 --> 00:17:46,000
you know you have calculus then

530
00:17:43,840 --> 00:17:48,480
differentiating a times b plus c with

531
00:17:46,000 --> 00:17:50,799
respect to a gives you just b

532
00:17:48,480 --> 00:17:52,799
and indeed the value of b is negative 3

533
00:17:50,799 --> 00:17:54,960
which is the derivative that we have so

534
00:17:52,799 --> 00:17:57,760
you can tell that that's correct

535
00:17:54,960 --> 00:17:59,840
so now if we do this with b

536
00:17:57,760 --> 00:18:02,240
so if we bump b by a little bit in a

537
00:17:59,840 --> 00:18:04,559
positive direction we'd get different

538
00:18:02,240 --> 00:18:06,320
slopes so what is the influence of b on

539
00:18:04,559 --> 00:18:08,240
the output d

540
00:18:06,320 --> 00:18:10,160
so if we bump b by a tiny amount in a

541
00:18:08,240 --> 00:18:11,600
positive direction then because a is

542
00:18:10,160 --> 00:18:13,760
positive

543
00:18:11,600 --> 00:18:14,720
we'll be adding more to d

544
00:18:13,760 --> 00:18:17,039
right

545
00:18:14,720 --> 00:18:18,799
so um and now what is the what is the

546
00:18:17,039 --> 00:18:19,840
sensitivity what is the slope of that

547
00:18:18,799 --> 00:18:21,520
addition

548
00:18:19,840 --> 00:18:22,960
and it might not surprise you that this

549
00:18:21,520 --> 00:18:24,240
should be

550
00:18:22,960 --> 00:18:27,679
2

551
00:18:24,240 --> 00:18:30,160
and y is a 2 because d of d

552
00:18:27,679 --> 00:18:31,919
by db differentiating with respect to b

553
00:18:30,160 --> 00:18:34,080
would be would give us a

554
00:18:31,919 --> 00:18:35,440
and the value of a is two so that's also

555
00:18:34,080 --> 00:18:37,440
working well

556
00:18:35,440 --> 00:18:38,559
and then if c gets bumped a tiny amount

557
00:18:37,440 --> 00:18:39,919
in h

558
00:18:38,559 --> 00:18:41,840
by h

559
00:18:39,919 --> 00:18:44,240
then of course a times b is unaffected

560
00:18:41,840 --> 00:18:45,919
and now c becomes slightly bit higher

561
00:18:44,240 --> 00:18:47,280
what does that do to the function it

562
00:18:45,919 --> 00:18:48,799
makes it slightly bit higher because

563
00:18:47,280 --> 00:18:50,480
we're simply adding c

564
00:18:48,799 --> 00:18:53,120
and it makes it slightly bit higher by

565
00:18:50,480 --> 00:18:55,200
the exact same amount that we added to c

566
00:18:53,120 --> 00:18:56,559
and so that tells you that the slope is

567
00:18:55,200 --> 00:18:59,200
one

568
00:18:56,559 --> 00:19:01,039
that will be the

569
00:18:59,200 --> 00:19:04,160
the rate at which

570
00:19:01,039 --> 00:19:05,120
d will increase as we scale

571
00:19:04,160 --> 00:19:06,799
c

572
00:19:05,120 --> 00:19:08,240
okay so we now have some intuitive sense

573
00:19:06,799 --> 00:19:10,000
of what this derivative is telling you

574
00:19:08,240 --> 00:19:11,600
about the function and we'd like to move

575
00:19:10,000 --> 00:19:13,120
to neural networks now as i mentioned

576
00:19:11,600 --> 00:19:15,200
neural networks will be pretty massive

577
00:19:13,120 --> 00:19:16,480
expressions mathematical expressions so

578
00:19:15,200 --> 00:19:17,919
we need some data structures that

579
00:19:16,480 --> 00:19:19,360
maintain these expressions and that's

580
00:19:17,919 --> 00:19:20,480
what we're going to start to build out

581
00:19:19,360 --> 00:19:22,240
now

582
00:19:20,480 --> 00:19:24,080
so we're going to

583
00:19:22,240 --> 00:19:26,240
build out this value object that i

584
00:19:24,080 --> 00:19:27,520
showed you in the readme page of

585
00:19:26,240 --> 00:19:30,720
micrograd

586
00:19:27,520 --> 00:19:33,520
so let me copy paste a skeleton of the

587
00:19:30,720 --> 00:19:36,400
first very simple value object

588
00:19:33,520 --> 00:19:38,320
so class value takes a single

589
00:19:36,400 --> 00:19:39,280
scalar value that it wraps and keeps

590
00:19:38,320 --> 00:19:41,440
track of

591
00:19:39,280 --> 00:19:43,600
and that's it so

592
00:19:41,440 --> 00:19:45,440
we can for example do value of 2.0 and

593
00:19:43,600 --> 00:19:48,559
then we can

594
00:19:45,440 --> 00:19:50,400
get we can look at its content and

595
00:19:48,559 --> 00:19:52,480
python will internally

596
00:19:50,400 --> 00:19:54,320
use the wrapper function

597
00:19:52,480 --> 00:19:56,880
to uh return

598
00:19:54,320 --> 00:19:58,640
uh this string oops

599
00:19:56,880 --> 00:20:00,799
like that

600
00:19:58,640 --> 00:20:03,200
so this is a value object with data

601
00:20:00,799 --> 00:20:04,960
equals two that we're creating here

602
00:20:03,200 --> 00:20:07,440
now we'd like to do is like we'd like to

603
00:20:04,960 --> 00:20:10,080
be able to

604
00:20:07,440 --> 00:20:12,000
have not just like two values

605
00:20:10,080 --> 00:20:13,679
but we'd like to do a bluffy right we'd

606
00:20:12,000 --> 00:20:15,440
like to add them

607
00:20:13,679 --> 00:20:17,760
so currently you would get an error

608
00:20:15,440 --> 00:20:21,440
because python doesn't know how to add

609
00:20:17,760 --> 00:20:22,480
two value objects so we have to tell it

610
00:20:21,440 --> 00:20:24,880
so here's

611
00:20:22,480 --> 00:20:24,880
addition

612
00:20:26,320 --> 00:20:29,520
so you have to basically use these

613
00:20:27,600 --> 00:20:31,840
special double underscore methods in

614
00:20:29,520 --> 00:20:35,440
python to define these operators for

615
00:20:31,840 --> 00:20:39,120
these objects so if we call um

616
00:20:35,440 --> 00:20:43,039
the uh if we use this plus operator

617
00:20:39,120 --> 00:20:43,760
python will internally call a dot add of

618
00:20:43,039 --> 00:20:45,919
b

619
00:20:43,760 --> 00:20:48,960
that's what will happen internally and

620
00:20:45,919 --> 00:20:50,960
so b will be the other and

621
00:20:48,960 --> 00:20:52,159
self will be a

622
00:20:50,960 --> 00:20:54,480
and so we see that what we're going to

623
00:20:52,159 --> 00:20:56,720
return is a new value object and it's

624
00:20:54,480 --> 00:20:58,559
just it's going to be wrapping

625
00:20:56,720 --> 00:20:59,760
the plus of

626
00:20:58,559 --> 00:21:02,000
their data

627
00:20:59,760 --> 00:21:04,320
but remember now because data is the

628
00:21:02,000 --> 00:21:06,880
actual like numbered python number so

629
00:21:04,320 --> 00:21:09,120
this operator here is just the typical

630
00:21:06,880 --> 00:21:11,600
floating point plus addition now it's

631
00:21:09,120 --> 00:21:14,240
not an addition of value objects

632
00:21:11,600 --> 00:21:16,000
and will return a new value so now a

633
00:21:14,240 --> 00:21:17,200
plus b should work and it should print

634
00:21:16,000 --> 00:21:18,400
value of

635
00:21:17,200 --> 00:21:20,240
negative one

636
00:21:18,400 --> 00:21:21,679
because that's two plus minus three

637
00:21:20,240 --> 00:21:24,240
there we go

638
00:21:21,679 --> 00:21:25,919
okay let's now implement multiply

639
00:21:24,240 --> 00:21:26,799
just so we can recreate this expression

640
00:21:25,919 --> 00:21:28,559
here

641
00:21:26,799 --> 00:21:31,600
so multiply i think it won't surprise

642
00:21:28,559 --> 00:21:33,200
you will be fairly similar

643
00:21:31,600 --> 00:21:34,320
so instead of add we're going to be

644
00:21:33,200 --> 00:21:36,000
using mul

645
00:21:34,320 --> 00:21:36,880
and then here of course we want to do

646
00:21:36,000 --> 00:21:38,640
times

647
00:21:36,880 --> 00:21:41,039
and so now we can create a c value

648
00:21:38,640 --> 00:21:44,159
object which will be 10.0 and now we

649
00:21:41,039 --> 00:21:46,720
should be able to do a times b well

650
00:21:44,159 --> 00:21:47,060
let's just do a times b first

651
00:21:46,720 --> 00:21:48,480
um

652
00:21:47,060 --> 00:21:50,720
[Music]

653
00:21:48,480 --> 00:21:52,000
that's value of negative six now

654
00:21:50,720 --> 00:21:53,520
and by the way i skipped over this a

655
00:21:52,000 --> 00:21:55,520
little bit suppose that i didn't have

656
00:21:53,520 --> 00:21:57,440
the wrapper function here

657
00:21:55,520 --> 00:21:59,679
then it's just that you'll get some kind

658
00:21:57,440 --> 00:22:02,080
of an ugly expression so what wrapper is

659
00:21:59,679 --> 00:22:03,440
doing is it's providing us a way to

660
00:22:02,080 --> 00:22:05,120
print out like a nicer looking

661
00:22:03,440 --> 00:22:07,200
expression in python

662
00:22:05,120 --> 00:22:09,280
uh so we don't just have something

663
00:22:07,200 --> 00:22:10,480
cryptic we actually are you know it's

664
00:22:09,280 --> 00:22:14,000
value of

665
00:22:10,480 --> 00:22:16,320
negative six so this gives us a times

666
00:22:14,000 --> 00:22:18,080
and then this we should now be able to

667
00:22:16,320 --> 00:22:20,559
add c to it because we've defined and

668
00:22:18,080 --> 00:22:22,400
told the python how to do mul and add

669
00:22:20,559 --> 00:22:24,880
and so this will call this will

670
00:22:22,400 --> 00:22:26,880
basically be equivalent to a dot

671
00:22:24,880 --> 00:22:27,919
small

672
00:22:26,880 --> 00:22:29,679
of b

673
00:22:27,919 --> 00:22:31,280
and then this new value object will be

674
00:22:29,679 --> 00:22:32,559
dot add

675
00:22:31,280 --> 00:22:34,799
of c

676
00:22:32,559 --> 00:22:36,400
and so let's see if that worked

677
00:22:34,799 --> 00:22:39,200
yep so that worked well that gave us

678
00:22:36,400 --> 00:22:40,960
four which is what we expect from before

679
00:22:39,200 --> 00:22:44,320
and i believe we can just call them

680
00:22:40,960 --> 00:22:45,120
manually as well there we go so

681
00:22:44,320 --> 00:22:46,880
yeah

682
00:22:45,120 --> 00:22:49,039
okay so now what we are missing is the

683
00:22:46,880 --> 00:22:50,240
connective tissue of this expression as

684
00:22:49,039 --> 00:22:52,559
i mentioned we want to keep these

685
00:22:50,240 --> 00:22:54,960
expression graphs so we need to know and

686
00:22:52,559 --> 00:22:56,880
keep pointers about what values produce

687
00:22:54,960 --> 00:22:58,640
what other values

688
00:22:56,880 --> 00:23:00,080
so here for example we are going to

689
00:22:58,640 --> 00:23:02,080
introduce a new variable which we'll

690
00:23:00,080 --> 00:23:03,520
call children and by default it will be

691
00:23:02,080 --> 00:23:04,799
an empty tuple

692
00:23:03,520 --> 00:23:06,400
and then we're actually going to keep a

693
00:23:04,799 --> 00:23:08,559
slightly different variable in the class

694
00:23:06,400 --> 00:23:11,520
which we'll call underscore prev which

695
00:23:08,559 --> 00:23:13,280
will be the set of children

696
00:23:11,520 --> 00:23:15,120
this is how i done i did it in the

697
00:23:13,280 --> 00:23:17,120
original micrograd looking at my code

698
00:23:15,120 --> 00:23:19,120
here i can't remember exactly the reason

699
00:23:17,120 --> 00:23:20,960
i believe it was efficiency but this

700
00:23:19,120 --> 00:23:22,400
underscore children will be a tuple for

701
00:23:20,960 --> 00:23:23,919
convenience but then when we actually

702
00:23:22,400 --> 00:23:27,600
maintain it in the class it will be just

703
00:23:23,919 --> 00:23:28,880
this set yeah i believe for efficiency

704
00:23:27,600 --> 00:23:29,919
um

705
00:23:28,880 --> 00:23:31,760
so now

706
00:23:29,919 --> 00:23:33,440
when we are creating a value like this

707
00:23:31,760 --> 00:23:36,080
with a constructor children will be

708
00:23:33,440 --> 00:23:37,360
empty and prep will be the empty set but

709
00:23:36,080 --> 00:23:39,360
when we're creating a value through

710
00:23:37,360 --> 00:23:42,640
addition or multiplication we're going

711
00:23:39,360 --> 00:23:46,400
to feed in the children of this value

712
00:23:42,640 --> 00:23:48,080
which in this case is self and other

713
00:23:46,400 --> 00:23:50,640
so those are the children

714
00:23:48,080 --> 00:23:52,960
here

715
00:23:50,640 --> 00:23:55,919
so now we can do d dot prev

716
00:23:52,960 --> 00:23:58,880
and we'll see that the children of the

717
00:23:55,919 --> 00:24:00,960
we now know are this value of negative 6

718
00:23:58,880 --> 00:24:03,760
and value of 10 and this of course is

719
00:24:00,960 --> 00:24:06,559
the value resulting from a times b and

720
00:24:03,760 --> 00:24:08,559
the c value which is 10.

721
00:24:06,559 --> 00:24:10,640
now the last piece of information we

722
00:24:08,559 --> 00:24:12,159
don't know so we know that the children

723
00:24:10,640 --> 00:24:14,720
of every single value but we don't know

724
00:24:12,159 --> 00:24:16,559
what operation created this value

725
00:24:14,720 --> 00:24:19,120
so we need one more element here let's

726
00:24:16,559 --> 00:24:21,200
call it underscore pop

727
00:24:19,120 --> 00:24:22,320
and by default this is the empty set for

728
00:24:21,200 --> 00:24:25,600
leaves

729
00:24:22,320 --> 00:24:27,360
and then we'll just maintain it here

730
00:24:25,600 --> 00:24:29,520
and now the operation will be just a

731
00:24:27,360 --> 00:24:31,279
simple string and in the case of

732
00:24:29,520 --> 00:24:33,840
addition it's plus in the case of

733
00:24:31,279 --> 00:24:35,200
multiplication is times

734
00:24:33,840 --> 00:24:37,039
so now we

735
00:24:35,200 --> 00:24:38,640
not just have d dot pref we also have a

736
00:24:37,039 --> 00:24:40,240
d dot up

737
00:24:38,640 --> 00:24:42,799
and we know that d was produced by an

738
00:24:40,240 --> 00:24:44,000
addition of those two values and so now

739
00:24:42,799 --> 00:24:46,080
we have the full

740
00:24:44,000 --> 00:24:47,520
mathematical expression uh and we're

741
00:24:46,080 --> 00:24:49,919
building out this data structure and we

742
00:24:47,520 --> 00:24:51,600
know exactly how each value came to be

743
00:24:49,919 --> 00:24:54,000
by word expression and from what other

744
00:24:51,600 --> 00:24:54,000
values

745
00:24:54,720 --> 00:24:58,080
now because these expressions are about

746
00:24:56,159 --> 00:25:00,400
to get quite a bit larger we'd like a

747
00:24:58,080 --> 00:25:02,240
way to nicely visualize these

748
00:25:00,400 --> 00:25:03,840
expressions that we're building out so

749
00:25:02,240 --> 00:25:06,480
for that i'm going to copy paste a bunch

750
00:25:03,840 --> 00:25:08,640
of slightly scary code that's going to

751
00:25:06,480 --> 00:25:09,600
visualize this these expression graphs

752
00:25:08,640 --> 00:25:11,039
for us

753
00:25:09,600 --> 00:25:13,360
so here's the code and i'll explain it

754
00:25:11,039 --> 00:25:14,880
in a bit but first let me just show you

755
00:25:13,360 --> 00:25:16,720
what this code does

756
00:25:14,880 --> 00:25:19,120
basically what it does is it creates a

757
00:25:16,720 --> 00:25:20,720
new function drawdot that we can call on

758
00:25:19,120 --> 00:25:22,720
some root node

759
00:25:20,720 --> 00:25:24,720
and then it's going to visualize it so

760
00:25:22,720 --> 00:25:27,039
if we call drawdot on d

761
00:25:24,720 --> 00:25:29,760
which is this final value here that is a

762
00:25:27,039 --> 00:25:31,440
times b plus c

763
00:25:29,760 --> 00:25:32,240
it creates something like this so this

764
00:25:31,440 --> 00:25:34,640
is d

765
00:25:32,240 --> 00:25:36,960
and you see that this is a times b

766
00:25:34,640 --> 00:25:40,400
creating an integrated value plus c

767
00:25:36,960 --> 00:25:42,400
gives us this output node d

768
00:25:40,400 --> 00:25:44,320
so that's dried out of d

769
00:25:42,400 --> 00:25:46,159
and i'm not going to go through this in

770
00:25:44,320 --> 00:25:48,960
complete detail you can take a look at

771
00:25:46,159 --> 00:25:51,360
graphless and its api uh graphis is a

772
00:25:48,960 --> 00:25:52,480
open source graph visualization software

773
00:25:51,360 --> 00:25:54,840
and what we're doing here is we're

774
00:25:52,480 --> 00:25:56,960
building out this graph and graphis

775
00:25:54,840 --> 00:25:58,880
api and

776
00:25:56,960 --> 00:26:00,799
you can basically see that trace is this

777
00:25:58,880 --> 00:26:02,799
helper function that enumerates all of

778
00:26:00,799 --> 00:26:04,240
the nodes and edges in the graph

779
00:26:02,799 --> 00:26:06,080
so that just builds a set of all the

780
00:26:04,240 --> 00:26:08,000
nodes and edges and then we iterate for

781
00:26:06,080 --> 00:26:08,880
all the nodes and we create special node

782
00:26:08,000 --> 00:26:11,360
objects

783
00:26:08,880 --> 00:26:13,120
for them in

784
00:26:11,360 --> 00:26:15,679
using dot node

785
00:26:13,120 --> 00:26:16,799
and then we also create edges using dot

786
00:26:15,679 --> 00:26:18,080
dot edge

787
00:26:16,799 --> 00:26:20,400
and the only thing that's like slightly

788
00:26:18,080 --> 00:26:22,400
tricky here is you'll notice that i

789
00:26:20,400 --> 00:26:24,720
basically add these fake nodes which are

790
00:26:22,400 --> 00:26:27,679
these operation nodes so for example

791
00:26:24,720 --> 00:26:28,799
this node here is just like a plus node

792
00:26:27,679 --> 00:26:31,760
and

793
00:26:28,799 --> 00:26:34,400
i create these

794
00:26:31,760 --> 00:26:37,679
special op nodes here

795
00:26:34,400 --> 00:26:39,520
and i connect them accordingly so these

796
00:26:37,679 --> 00:26:41,679
nodes of course are not actual

797
00:26:39,520 --> 00:26:43,919
nodes in the original graph

798
00:26:41,679 --> 00:26:46,159
they're not actually a value object the

799
00:26:43,919 --> 00:26:48,159
only value objects here are the things

800
00:26:46,159 --> 00:26:50,559
in squares those are actual value

801
00:26:48,159 --> 00:26:52,400
objects or representations thereof and

802
00:26:50,559 --> 00:26:55,520
these op nodes are just created in this

803
00:26:52,400 --> 00:26:57,440
drawdot routine so that it looks nice

804
00:26:55,520 --> 00:26:59,919
let's also add labels to these graphs

805
00:26:57,440 --> 00:27:01,760
just so we know what variables are where

806
00:26:59,919 --> 00:27:02,880
so let's create a special underscore

807
00:27:01,760 --> 00:27:03,919
label

808
00:27:02,880 --> 00:27:05,440
um

809
00:27:03,919 --> 00:27:08,400
or let's just do label

810
00:27:05,440 --> 00:27:10,880
equals empty by default and save it in

811
00:27:08,400 --> 00:27:10,880
each node

812
00:27:11,279 --> 00:27:15,360
and then here we're going to do label as

813
00:27:13,279 --> 00:27:17,840
a

814
00:27:15,360 --> 00:27:21,399
label is the

815
00:27:17,840 --> 00:27:21,399
label a c

816
00:27:22,799 --> 00:27:27,679
and then

817
00:27:24,799 --> 00:27:30,720
let's create a special um

818
00:27:27,679 --> 00:27:30,720
e equals a times b

819
00:27:30,799 --> 00:27:35,679
and e dot label will be e

820
00:27:34,080 --> 00:27:38,159
it's kind of naughty

821
00:27:35,679 --> 00:27:40,799
and e will be e plus c

822
00:27:38,159 --> 00:27:42,559
and a d dot label will be

823
00:27:40,799 --> 00:27:44,240
d

824
00:27:42,559 --> 00:27:46,399
okay so nothing really changes i just

825
00:27:44,240 --> 00:27:48,480
added this new e function

826
00:27:46,399 --> 00:27:50,559
a new e variable

827
00:27:48,480 --> 00:27:51,840
and then here when we are

828
00:27:50,559 --> 00:27:54,240
printing this

829
00:27:51,840 --> 00:27:56,080
i'm going to print the label here so

830
00:27:54,240 --> 00:27:56,880
this will be a percent s

831
00:27:56,080 --> 00:28:00,320
bar

832
00:27:56,880 --> 00:28:00,320
and this will be end.label

833
00:28:01,279 --> 00:28:05,200
and so now

834
00:28:03,440 --> 00:28:07,679
we have the label on the left here so it

835
00:28:05,200 --> 00:28:08,720
says a b creating e and then e plus c

836
00:28:07,679 --> 00:28:10,799
creates d

837
00:28:08,720 --> 00:28:12,320
just like we have it here

838
00:28:10,799 --> 00:28:14,320
and finally let's make this expression

839
00:28:12,320 --> 00:28:17,200
just one layer deeper

840
00:28:14,320 --> 00:28:20,000
so d will not be the final output node

841
00:28:17,200 --> 00:28:21,760
instead after d we are going to create a

842
00:28:20,000 --> 00:28:23,679
new value object

843
00:28:21,760 --> 00:28:25,919
called f we're going to start running

844
00:28:23,679 --> 00:28:27,360
out of variables soon f will be negative

845
00:28:25,919 --> 00:28:30,559
2.0

846
00:28:27,360 --> 00:28:34,159
and its label will of course just be f

847
00:28:30,559 --> 00:28:35,440
and then l capital l will be the output

848
00:28:34,159 --> 00:28:38,000
of our graph

849
00:28:35,440 --> 00:28:38,799
and l will be p times f

850
00:28:38,000 --> 00:28:40,320
okay

851
00:28:38,799 --> 00:28:42,720
so l will be negative eight is the

852
00:28:40,320 --> 00:28:42,720
output

853
00:28:42,840 --> 00:28:49,520
so

854
00:28:44,720 --> 00:28:49,520
now we don't just draw a d we draw l

855
00:28:50,000 --> 00:28:54,000
okay

856
00:28:52,000 --> 00:28:56,399
and somehow the label of

857
00:28:54,000 --> 00:28:59,679
l was undefined oops all that label has

858
00:28:56,399 --> 00:29:01,679
to be explicitly sort of given to it

859
00:28:59,679 --> 00:29:03,200
there we go so l is the output

860
00:29:01,679 --> 00:29:04,080
so let's quickly recap what we've done

861
00:29:03,200 --> 00:29:05,679
so far

862
00:29:04,080 --> 00:29:08,000
we are able to build out mathematical

863
00:29:05,679 --> 00:29:09,200
expressions using only plus and times so

864
00:29:08,000 --> 00:29:11,520
far

865
00:29:09,200 --> 00:29:14,240
they are scalar valued along the way

866
00:29:11,520 --> 00:29:16,399
and we can do this forward pass

867
00:29:14,240 --> 00:29:18,640
and build out a mathematical expression

868
00:29:16,399 --> 00:29:19,520
so we have multiple inputs here a b c

869
00:29:18,640 --> 00:29:21,520
and f

870
00:29:19,520 --> 00:29:24,000
going into a mathematical expression

871
00:29:21,520 --> 00:29:26,399
that produces a single output l

872
00:29:24,000 --> 00:29:28,720
and this here is visualizing the forward

873
00:29:26,399 --> 00:29:31,600
pass so the output of the forward pass

874
00:29:28,720 --> 00:29:33,279
is negative eight that's the value

875
00:29:31,600 --> 00:29:35,360
now what we'd like to do next is we'd

876
00:29:33,279 --> 00:29:37,200
like to run back propagation

877
00:29:35,360 --> 00:29:39,600
and in back propagation we are going to

878
00:29:37,200 --> 00:29:40,880
start here at the end and we're going to

879
00:29:39,600 --> 00:29:43,520
reverse

880
00:29:40,880 --> 00:29:45,440
and calculate the gradient along along

881
00:29:43,520 --> 00:29:46,799
all these intermediate values

882
00:29:45,440 --> 00:29:48,880
and really what we're computing for

883
00:29:46,799 --> 00:29:50,960
every single value here

884
00:29:48,880 --> 00:29:55,360
um we're going to compute the derivative

885
00:29:50,960 --> 00:29:56,159
of that node with respect to l

886
00:29:55,360 --> 00:29:58,720
so

887
00:29:56,159 --> 00:30:00,480
the derivative of l with respect to l is

888
00:29:58,720 --> 00:30:01,760
just uh one

889
00:30:00,480 --> 00:30:03,840
and then we're going to derive what is

890
00:30:01,760 --> 00:30:06,320
the derivative of l with respect to f

891
00:30:03,840 --> 00:30:07,600
with respect to d with respect to c with

892
00:30:06,320 --> 00:30:10,320
respect to e

893
00:30:07,600 --> 00:30:12,080
with respect to b and with respect to a

894
00:30:10,320 --> 00:30:13,840
and in the neural network setting you'd

895
00:30:12,080 --> 00:30:16,720
be very interested in the derivative of

896
00:30:13,840 --> 00:30:18,399
basically this loss function l

897
00:30:16,720 --> 00:30:19,360
with respect to the weights of a neural

898
00:30:18,399 --> 00:30:20,720
network

899
00:30:19,360 --> 00:30:22,399
and here of course we have just these

900
00:30:20,720 --> 00:30:23,840
variables a b c and f

901
00:30:22,399 --> 00:30:25,760
but some of these will eventually

902
00:30:23,840 --> 00:30:27,279
represent the weights of a neural net

903
00:30:25,760 --> 00:30:29,120
and so we'll need to know how those

904
00:30:27,279 --> 00:30:31,039
weights are impacting

905
00:30:29,120 --> 00:30:32,240
the loss function so we'll be interested

906
00:30:31,039 --> 00:30:34,640
basically in the derivative of the

907
00:30:32,240 --> 00:30:36,559
output with respect to some of its leaf

908
00:30:34,640 --> 00:30:38,080
nodes and those leaf nodes will be the

909
00:30:36,559 --> 00:30:39,520
weights of the neural net

910
00:30:38,080 --> 00:30:41,679
and the other leaf nodes of course will

911
00:30:39,520 --> 00:30:44,000
be the data itself but usually we will

912
00:30:41,679 --> 00:30:45,760
not want or use the derivative of the

913
00:30:44,000 --> 00:30:47,760
loss function with respect to data

914
00:30:45,760 --> 00:30:50,640
because the data is fixed but the

915
00:30:47,760 --> 00:30:52,559
weights will be iterated on

916
00:30:50,640 --> 00:30:54,240
using the gradient information so next

917
00:30:52,559 --> 00:30:57,039
we are going to create a variable inside

918
00:30:54,240 --> 00:30:59,919
the value class that maintains the

919
00:30:57,039 --> 00:31:00,960
derivative of l with respect to that

920
00:30:59,919 --> 00:31:03,760
value

921
00:31:00,960 --> 00:31:05,840
and we will call this variable grad

922
00:31:03,760 --> 00:31:07,200
so there's a data and there's a

923
00:31:05,840 --> 00:31:09,600
self.grad

924
00:31:07,200 --> 00:31:12,240
and initially it will be zero and

925
00:31:09,600 --> 00:31:14,320
remember that zero is basically means no

926
00:31:12,240 --> 00:31:16,080
effect so at initialization we're

927
00:31:14,320 --> 00:31:18,559
assuming that every value does not

928
00:31:16,080 --> 00:31:19,600
impact does not affect the out the

929
00:31:18,559 --> 00:31:21,360
output

930
00:31:19,600 --> 00:31:23,200
right because if the gradient is zero

931
00:31:21,360 --> 00:31:25,679
that means that changing this variable

932
00:31:23,200 --> 00:31:27,279
is not changing the loss function

933
00:31:25,679 --> 00:31:28,880
so by default we assume that the

934
00:31:27,279 --> 00:31:31,200
gradient is zero

935
00:31:28,880 --> 00:31:36,159
and then

936
00:31:31,200 --> 00:31:36,159
now that we have grad and it's 0.0

937
00:31:36,559 --> 00:31:42,799
we are going to be able to visualize it

938
00:31:38,240 --> 00:31:45,679
here after data so here grad is 0.4 f

939
00:31:42,799 --> 00:31:47,600
and this will be in that graph

940
00:31:45,679 --> 00:31:50,960
and now we are going to be showing both

941
00:31:47,600 --> 00:31:53,679
the data and the grad

942
00:31:50,960 --> 00:31:55,679
initialized at zero

943
00:31:53,679 --> 00:31:57,360
and we are just about getting ready to

944
00:31:55,679 --> 00:31:58,799
calculate the back propagation

945
00:31:57,360 --> 00:32:00,399
and of course this grad again as i

946
00:31:58,799 --> 00:32:02,240
mentioned is representing

947
00:32:00,399 --> 00:32:05,039
the derivative of the output in this

948
00:32:02,240 --> 00:32:06,480
case l with respect to this value so

949
00:32:05,039 --> 00:32:08,559
with respect to so this is the

950
00:32:06,480 --> 00:32:11,360
derivative of l with respect to f with

951
00:32:08,559 --> 00:32:12,960
respect to d and so on so let's now fill

952
00:32:11,360 --> 00:32:14,720
in those gradients and actually do back

953
00:32:12,960 --> 00:32:16,399
propagation manually so let's start

954
00:32:14,720 --> 00:32:18,480
filling in these gradients and start all

955
00:32:16,399 --> 00:32:20,399
the way at the end as i mentioned here

956
00:32:18,480 --> 00:32:22,880
first we are interested to fill in this

957
00:32:20,399 --> 00:32:25,200
gradient here so what is the derivative

958
00:32:22,880 --> 00:32:27,360
of l with respect to l

959
00:32:25,200 --> 00:32:29,039
in other words if i change l by a tiny

960
00:32:27,360 --> 00:32:30,559
amount of h

961
00:32:29,039 --> 00:32:32,320
how much does

962
00:32:30,559 --> 00:32:35,200
l change

963
00:32:32,320 --> 00:32:37,279
it changes by h so it's proportional and

964
00:32:35,200 --> 00:32:39,200
therefore derivative will be one

965
00:32:37,279 --> 00:32:40,960
we can of course measure these or

966
00:32:39,200 --> 00:32:43,200
estimate these numerical gradients

967
00:32:40,960 --> 00:32:45,279
numerically just like we've seen before

968
00:32:43,200 --> 00:32:49,279
so if i take this expression

969
00:32:45,279 --> 00:32:51,200
and i create a def lol function here

970
00:32:49,279 --> 00:32:53,600
and put this here now the reason i'm

971
00:32:51,200 --> 00:32:55,200
creating a gating function hello here is

972
00:32:53,600 --> 00:32:57,200
because i don't want to pollute or mess

973
00:32:55,200 --> 00:32:58,880
up the global scope here this is just

974
00:32:57,200 --> 00:33:00,399
kind of like a little staging area and

975
00:32:58,880 --> 00:33:02,720
as you know in python all of these will

976
00:33:00,399 --> 00:33:04,720
be local variables to this function so

977
00:33:02,720 --> 00:33:05,760
i'm not changing any of the global scope

978
00:33:04,720 --> 00:33:10,000
here

979
00:33:05,760 --> 00:33:13,039
so here l1 will be l

980
00:33:10,000 --> 00:33:16,880
and then copy pasting this expression

981
00:33:13,039 --> 00:33:16,880
we're going to add a small amount h

982
00:33:17,360 --> 00:33:22,640
in for example a

983
00:33:20,559 --> 00:33:25,440
right and this would be measuring the

984
00:33:22,640 --> 00:33:28,159
derivative of l with respect to a

985
00:33:25,440 --> 00:33:29,519
so here this will be l2

986
00:33:28,159 --> 00:33:31,600
and then we want to print this

987
00:33:29,519 --> 00:33:35,200
derivative so print

988
00:33:31,600 --> 00:33:37,679
l2 minus l1 which is how much l changed

989
00:33:35,200 --> 00:33:39,519
and then normalize it by h so this is

990
00:33:37,679 --> 00:33:41,360
the rise over run

991
00:33:39,519 --> 00:33:45,120
and we have to be careful because l is a

992
00:33:41,360 --> 00:33:46,240
value node so we actually want its data

993
00:33:45,120 --> 00:33:48,880
um

994
00:33:46,240 --> 00:33:50,720
so that these are floats dividing by h

995
00:33:48,880 --> 00:33:53,039
and this should print the derivative of

996
00:33:50,720 --> 00:33:55,600
l with respect to a because a is the one

997
00:33:53,039 --> 00:33:57,279
that we bumped a little bit by h

998
00:33:55,600 --> 00:33:59,279
so what is the

999
00:33:57,279 --> 00:34:01,039
derivative of l with respect to a

1000
00:33:59,279 --> 00:34:03,600
it's six

1001
00:34:01,039 --> 00:34:06,960
okay and obviously

1002
00:34:03,600 --> 00:34:09,839
if we change l by h

1003
00:34:06,960 --> 00:34:12,399
then that would be

1004
00:34:09,839 --> 00:34:14,639
here effectively

1005
00:34:12,399 --> 00:34:16,079
this looks really awkward but changing l

1006
00:34:14,639 --> 00:34:20,800
by h

1007
00:34:16,079 --> 00:34:23,200
you see the derivative here is 1. um

1008
00:34:20,800 --> 00:34:24,720
that's kind of like the base case of

1009
00:34:23,200 --> 00:34:26,639
what we are doing here

1010
00:34:24,720 --> 00:34:29,839
so basically we cannot come up here and

1011
00:34:26,639 --> 00:34:31,919
we can manually set l.grad to one this

1012
00:34:29,839 --> 00:34:35,599
is our manual back propagation

1013
00:34:31,919 --> 00:34:37,679
l dot grad is one and let's redraw

1014
00:34:35,599 --> 00:34:39,040
and we'll see that we filled in grad as

1015
00:34:37,679 --> 00:34:40,320
1 for l

1016
00:34:39,040 --> 00:34:42,159
we're now going to continue the back

1017
00:34:40,320 --> 00:34:45,520
propagation so let's here look at the

1018
00:34:42,159 --> 00:34:47,679
derivatives of l with respect to d and f

1019
00:34:45,520 --> 00:34:49,440
let's do a d first

1020
00:34:47,679 --> 00:34:51,839
so what we are interested in if i create

1021
00:34:49,440 --> 00:34:54,240
a markdown on here is we'd like to know

1022
00:34:51,839 --> 00:34:57,280
basically we have that l is d times f

1023
00:34:54,240 --> 00:35:00,240
and we'd like to know what is uh d

1024
00:34:57,280 --> 00:35:01,680
l by d d

1025
00:35:00,240 --> 00:35:03,680
what is that

1026
00:35:01,680 --> 00:35:06,240
and if you know your calculus uh l is d

1027
00:35:03,680 --> 00:35:08,079
times f so what is d l by d d it would

1028
00:35:06,240 --> 00:35:10,240
be f

1029
00:35:08,079 --> 00:35:11,760
and if you don't believe me we can also

1030
00:35:10,240 --> 00:35:14,800
just derive it because the proof would

1031
00:35:11,760 --> 00:35:15,680
be fairly straightforward uh we go to

1032
00:35:14,800 --> 00:35:18,160
the

1033
00:35:15,680 --> 00:35:22,079
definition of the derivative which is f

1034
00:35:18,160 --> 00:35:24,480
of x plus h minus f of x divide h

1035
00:35:22,079 --> 00:35:26,480
as a limit limit of h goes to zero of

1036
00:35:24,480 --> 00:35:28,240
this kind of expression so when we have

1037
00:35:26,480 --> 00:35:31,599
l is d times f

1038
00:35:28,240 --> 00:35:33,920
then increasing d by h

1039
00:35:31,599 --> 00:35:35,760
would give us the output of b plus h

1040
00:35:33,920 --> 00:35:38,960
times f

1041
00:35:35,760 --> 00:35:42,320
that's basically f of x plus h right

1042
00:35:38,960 --> 00:35:44,880
minus d times f

1043
00:35:42,320 --> 00:35:46,560
and then divide h and symbolically

1044
00:35:44,880 --> 00:35:50,000
expanding out here we would have

1045
00:35:46,560 --> 00:35:52,240
basically d times f plus h times f minus

1046
00:35:50,000 --> 00:35:54,400
t times f divide h

1047
00:35:52,240 --> 00:35:57,359
and then you see how the df minus df

1048
00:35:54,400 --> 00:35:58,560
cancels so you're left with h times f

1049
00:35:57,359 --> 00:35:59,920
divide h

1050
00:35:58,560 --> 00:36:03,200
which is f

1051
00:35:59,920 --> 00:36:04,480
so in the limit as h goes to zero of

1052
00:36:03,200 --> 00:36:06,720
you know

1053
00:36:04,480 --> 00:36:09,920
derivative

1054
00:36:06,720 --> 00:36:12,320
definition we just get f in the case of

1055
00:36:09,920 --> 00:36:13,040
d times f

1056
00:36:12,320 --> 00:36:14,320
so

1057
00:36:13,040 --> 00:36:15,920
symmetrically

1058
00:36:14,320 --> 00:36:18,480
dl by d

1059
00:36:15,920 --> 00:36:21,359
f will just be d

1060
00:36:18,480 --> 00:36:24,480
so what we have is that f dot grad

1061
00:36:21,359 --> 00:36:27,839
we see now is just the value of d

1062
00:36:24,480 --> 00:36:27,839
which is 4.

1063
00:36:28,800 --> 00:36:31,680
and we see that

1064
00:36:30,160 --> 00:36:35,599
d dot grad

1065
00:36:31,680 --> 00:36:35,599
is just uh the value of f

1066
00:36:36,880 --> 00:36:44,960
and so the value of f is negative two

1067
00:36:41,200 --> 00:36:44,960
so we'll set those manually

1068
00:36:45,119 --> 00:36:50,400
let me erase this markdown node and then

1069
00:36:47,040 --> 00:36:50,400
let's redraw what we have

1070
00:36:50,880 --> 00:36:53,200
okay

1071
00:36:51,839 --> 00:36:56,400
and let's just make sure that these were

1072
00:36:53,200 --> 00:36:59,520
correct so we seem to think that dl by

1073
00:36:56,400 --> 00:37:02,320
dd is negative two so let's double check

1074
00:36:59,520 --> 00:37:03,359
um let me erase this plus h from before

1075
00:37:02,320 --> 00:37:05,119
and now we want the derivative with

1076
00:37:03,359 --> 00:37:06,880
respect to f

1077
00:37:05,119 --> 00:37:08,960
so let's just come here when i create f

1078
00:37:06,880 --> 00:37:10,800
and let's do a plus h here and this

1079
00:37:08,960 --> 00:37:14,160
should print the derivative of l with

1080
00:37:10,800 --> 00:37:16,320
respect to f so we expect to see four

1081
00:37:14,160 --> 00:37:17,119
yeah and this is four up to floating

1082
00:37:16,320 --> 00:37:18,960
point

1083
00:37:17,119 --> 00:37:21,359
funkiness

1084
00:37:18,960 --> 00:37:25,040
and then dl by dd

1085
00:37:21,359 --> 00:37:26,640
should be f which is negative two

1086
00:37:25,040 --> 00:37:31,520
grad is negative two

1087
00:37:26,640 --> 00:37:31,520
so if we again come here and we change d

1088
00:37:31,839 --> 00:37:37,520
d dot data plus equals h right here

1089
00:37:35,119 --> 00:37:40,079
so we expect so we've added a little h

1090
00:37:37,520 --> 00:37:42,320
and then we see how l changed and we

1091
00:37:40,079 --> 00:37:44,640
expect to print

1092
00:37:42,320 --> 00:37:47,119
uh negative two

1093
00:37:44,640 --> 00:37:47,119
there we go

1094
00:37:47,359 --> 00:37:50,880
so we've numerically verified what we're

1095
00:37:49,520 --> 00:37:53,440
doing here is what kind of like an

1096
00:37:50,880 --> 00:37:54,400
inline gradient check gradient check is

1097
00:37:53,440 --> 00:37:56,400
when we

1098
00:37:54,400 --> 00:37:57,599
are deriving this like back propagation

1099
00:37:56,400 --> 00:38:00,320
and getting the derivative with respect

1100
00:37:57,599 --> 00:38:03,200
to all the intermediate results and then

1101
00:38:00,320 --> 00:38:06,079
numerical gradient is just you know

1102
00:38:03,200 --> 00:38:08,000
estimating it using small step size

1103
00:38:06,079 --> 00:38:10,400
now we're getting to the crux of

1104
00:38:08,000 --> 00:38:12,880
backpropagation so this will be the most

1105
00:38:10,400 --> 00:38:14,320
important node to understand because if

1106
00:38:12,880 --> 00:38:16,079
you understand the gradient for this

1107
00:38:14,320 --> 00:38:17,440
node you understand all of back

1108
00:38:16,079 --> 00:38:19,599
propagation and all of training of

1109
00:38:17,440 --> 00:38:23,119
neural nets basically

1110
00:38:19,599 --> 00:38:24,720
so we need to derive dl by bc

1111
00:38:23,119 --> 00:38:26,560
in other words the derivative of l with

1112
00:38:24,720 --> 00:38:27,920
respect to c

1113
00:38:26,560 --> 00:38:29,440
because we've computed all these other

1114
00:38:27,920 --> 00:38:30,640
gradients already

1115
00:38:29,440 --> 00:38:33,599
now we're coming here and we're

1116
00:38:30,640 --> 00:38:36,160
continuing the back propagation manually

1117
00:38:33,599 --> 00:38:38,400
so we want dl by dc and then we'll also

1118
00:38:36,160 --> 00:38:40,000
derive dl by de

1119
00:38:38,400 --> 00:38:41,920
now here's the problem

1120
00:38:40,000 --> 00:38:44,000
how do we derive dl

1121
00:38:41,920 --> 00:38:46,160
by dc

1122
00:38:44,000 --> 00:38:48,800
we actually know the derivative l with

1123
00:38:46,160 --> 00:38:50,079
respect to d so we know how l assessed

1124
00:38:48,800 --> 00:38:53,119
it to d

1125
00:38:50,079 --> 00:38:55,680
but how is l sensitive to c so if we

1126
00:38:53,119 --> 00:38:58,000
wiggle c how does that impact l

1127
00:38:55,680 --> 00:39:01,839
through d

1128
00:38:58,000 --> 00:39:01,839
so we know dl by dc

1129
00:39:01,920 --> 00:39:06,320
and we also here know how c impacts d

1130
00:39:04,640 --> 00:39:09,200
and so just very intuitively if you know

1131
00:39:06,320 --> 00:39:11,359
the impact that c is having on d and the

1132
00:39:09,200 --> 00:39:12,880
impact that d is having on l

1133
00:39:11,359 --> 00:39:14,400
then you should be able to somehow put

1134
00:39:12,880 --> 00:39:16,480
that information together to figure out

1135
00:39:14,400 --> 00:39:18,480
how c impacts l

1136
00:39:16,480 --> 00:39:20,880
and indeed this is what we can actually

1137
00:39:18,480 --> 00:39:22,960
do so in particular we know just

1138
00:39:20,880 --> 00:39:24,880
concentrating on d first let's look at

1139
00:39:22,960 --> 00:39:27,040
how what is the derivative basically of

1140
00:39:24,880 --> 00:39:30,400
d with respect to c so in other words

1141
00:39:27,040 --> 00:39:30,400
what is dd by dc

1142
00:39:31,599 --> 00:39:35,599
so here we know that d is c times c plus

1143
00:39:34,720 --> 00:39:37,359
e

1144
00:39:35,599 --> 00:39:39,599
that's what we know and now we're

1145
00:39:37,359 --> 00:39:41,359
interested in dd by dc

1146
00:39:39,599 --> 00:39:43,599
if you just know your calculus again and

1147
00:39:41,359 --> 00:39:45,520
you remember that differentiating c plus

1148
00:39:43,599 --> 00:39:46,480
e with respect to c you know that that

1149
00:39:45,520 --> 00:39:47,839
gives you

1150
00:39:46,480 --> 00:39:49,760
1.0

1151
00:39:47,839 --> 00:39:51,760
and we can also go back to the basics

1152
00:39:49,760 --> 00:39:54,640
and derive this because again we can go

1153
00:39:51,760 --> 00:39:56,400
to our f of x plus h minus f of x

1154
00:39:54,640 --> 00:39:58,480
divide by h

1155
00:39:56,400 --> 00:40:00,000
that's the definition of a derivative as

1156
00:39:58,480 --> 00:40:01,440
h goes to zero

1157
00:40:00,000 --> 00:40:04,400
and so here

1158
00:40:01,440 --> 00:40:06,319
focusing on c and its effect on d

1159
00:40:04,400 --> 00:40:07,200
we can basically do the f of x plus h

1160
00:40:06,319 --> 00:40:10,880
will be

1161
00:40:07,200 --> 00:40:12,160
c is incremented by h plus e

1162
00:40:10,880 --> 00:40:14,319
that's the first evaluation of our

1163
00:40:12,160 --> 00:40:16,480
function minus

1164
00:40:14,319 --> 00:40:18,319
c plus e

1165
00:40:16,480 --> 00:40:19,839
and then divide h

1166
00:40:18,319 --> 00:40:21,839
and so what is this

1167
00:40:19,839 --> 00:40:25,440
uh just expanding this out this will be

1168
00:40:21,839 --> 00:40:27,680
c plus h plus e minus c minus e

1169
00:40:25,440 --> 00:40:30,319
divide h and then you see here how c

1170
00:40:27,680 --> 00:40:33,599
minus c cancels e minus e cancels we're

1171
00:40:30,319 --> 00:40:35,280
left with h over h which is 1.0

1172
00:40:33,599 --> 00:40:38,880
and so

1173
00:40:35,280 --> 00:40:39,760
by symmetry also d d by d

1174
00:40:38,880 --> 00:40:42,960
e

1175
00:40:39,760 --> 00:40:44,800
will be 1.0 as well

1176
00:40:42,960 --> 00:40:46,640
so basically the derivative of a sum

1177
00:40:44,800 --> 00:40:49,040
expression is very simple and and this

1178
00:40:46,640 --> 00:40:51,040
is the local derivative so i call this

1179
00:40:49,040 --> 00:40:52,480
the local derivative because we have the

1180
00:40:51,040 --> 00:40:54,400
final output value all the way at the

1181
00:40:52,480 --> 00:40:55,839
end of this graph and we're now like a

1182
00:40:54,400 --> 00:40:58,000
small node here

1183
00:40:55,839 --> 00:41:00,079
and this is a little plus node

1184
00:40:58,000 --> 00:41:02,079
and it the little plus node doesn't know

1185
00:41:00,079 --> 00:41:04,160
anything about the rest of the graph

1186
00:41:02,079 --> 00:41:07,119
that it's embedded in all it knows is

1187
00:41:04,160 --> 00:41:09,119
that it did a plus it took a c and an e

1188
00:41:07,119 --> 00:41:11,200
added them and created d

1189
00:41:09,119 --> 00:41:14,400
and this plus note also knows the local

1190
00:41:11,200 --> 00:41:16,319
influence of c on d or rather rather the

1191
00:41:14,400 --> 00:41:17,040
derivative of d with respect to c and it

1192
00:41:16,319 --> 00:41:18,720
also

1193
00:41:17,040 --> 00:41:21,280
knows the derivative of d with respect

1194
00:41:18,720 --> 00:41:23,359
to e but that's not what we want that's

1195
00:41:21,280 --> 00:41:27,680
just a local derivative what we actually

1196
00:41:23,359 --> 00:41:30,720
want is d l by d c and l could l is here

1197
00:41:27,680 --> 00:41:32,240
just one step away but in a general case

1198
00:41:30,720 --> 00:41:34,640
this little plus note is could be

1199
00:41:32,240 --> 00:41:35,760
embedded in like a massive graph

1200
00:41:34,640 --> 00:41:38,400
so

1201
00:41:35,760 --> 00:41:41,280
again we know how l impacts d and now we

1202
00:41:38,400 --> 00:41:43,359
know how c and e impact d how do we put

1203
00:41:41,280 --> 00:41:46,000
that information together to write dl by

1204
00:41:43,359 --> 00:41:47,760
dc and the answer of course is the chain

1205
00:41:46,000 --> 00:41:50,079
rule in calculus

1206
00:41:47,760 --> 00:41:51,520
and so um

1207
00:41:50,079 --> 00:41:52,960
i pulled up a chain rule here from

1208
00:41:51,520 --> 00:41:53,920
kapedia

1209
00:41:52,960 --> 00:41:54,880
and

1210
00:41:53,920 --> 00:41:57,200
i'm going to go through this very

1211
00:41:54,880 --> 00:41:58,560
briefly so chain rule

1212
00:41:57,200 --> 00:42:00,319
wikipedia sometimes can be very

1213
00:41:58,560 --> 00:42:02,560
confusing and calculus can

1214
00:42:00,319 --> 00:42:03,599
can be very confusing like this is the

1215
00:42:02,560 --> 00:42:05,040
way i

1216
00:42:03,599 --> 00:42:06,400
learned

1217
00:42:05,040 --> 00:42:08,560
chain rule and it was very confusing

1218
00:42:06,400 --> 00:42:10,720
like what is happening it's just

1219
00:42:08,560 --> 00:42:12,960
complicated so i like this expression

1220
00:42:10,720 --> 00:42:15,119
much better

1221
00:42:12,960 --> 00:42:18,079
if a variable z depends on a variable y

1222
00:42:15,119 --> 00:42:20,319
which itself depends on the variable x

1223
00:42:18,079 --> 00:42:22,400
then z depends on x as well obviously

1224
00:42:20,319 --> 00:42:24,319
through the intermediate variable y

1225
00:42:22,400 --> 00:42:25,200
in this case the chain rule is expressed

1226
00:42:24,319 --> 00:42:28,079
as

1227
00:42:25,200 --> 00:42:30,720
if you want dz by dx

1228
00:42:28,079 --> 00:42:33,680
then you take the dz by dy and you

1229
00:42:30,720 --> 00:42:34,960
multiply it by d y by dx

1230
00:42:33,680 --> 00:42:36,160
so the chain rule fundamentally is

1231
00:42:34,960 --> 00:42:37,280
telling you

1232
00:42:36,160 --> 00:42:39,280
how

1233
00:42:37,280 --> 00:42:41,520
we chain these

1234
00:42:39,280 --> 00:42:44,319
uh derivatives together

1235
00:42:41,520 --> 00:42:46,079
correctly so to differentiate through a

1236
00:42:44,319 --> 00:42:48,720
function composition

1237
00:42:46,079 --> 00:42:49,599
we have to apply a multiplication

1238
00:42:48,720 --> 00:42:51,680
of

1239
00:42:49,599 --> 00:42:53,520
those derivatives

1240
00:42:51,680 --> 00:42:54,640
so that's really what chain rule is

1241
00:42:53,520 --> 00:42:56,480
telling us

1242
00:42:54,640 --> 00:42:58,079
and there's a nice little intuitive

1243
00:42:56,480 --> 00:42:59,920
explanation here which i also think is

1244
00:42:58,079 --> 00:43:01,280
kind of cute the chain rule says that

1245
00:42:59,920 --> 00:43:03,760
knowing the instantaneous rate of change

1246
00:43:01,280 --> 00:43:04,800
of z with respect to y and y relative to

1247
00:43:03,760 --> 00:43:06,400
x allows one to calculate the

1248
00:43:04,800 --> 00:43:07,839
instantaneous rate of change of z

1249
00:43:06,400 --> 00:43:09,520
relative to x

1250
00:43:07,839 --> 00:43:10,400
as a product of those two rates of

1251
00:43:09,520 --> 00:43:12,560
change

1252
00:43:10,400 --> 00:43:14,319
simply the product of those two

1253
00:43:12,560 --> 00:43:16,000
so here's a good one

1254
00:43:14,319 --> 00:43:18,160
if a car travels twice as fast as

1255
00:43:16,000 --> 00:43:19,920
bicycle and the bicycle is four times as

1256
00:43:18,160 --> 00:43:22,319
fast as walking man

1257
00:43:19,920 --> 00:43:25,119
then the car travels two times four

1258
00:43:22,319 --> 00:43:27,200
eight times as fast as demand

1259
00:43:25,119 --> 00:43:29,200
and so this makes it very clear that the

1260
00:43:27,200 --> 00:43:30,960
correct thing to do sort of

1261
00:43:29,200 --> 00:43:31,920
is to multiply

1262
00:43:30,960 --> 00:43:33,839
so

1263
00:43:31,920 --> 00:43:36,560
cars twice as fast as bicycle and

1264
00:43:33,839 --> 00:43:38,880
bicycle is four times as fast as man

1265
00:43:36,560 --> 00:43:42,160
so the car will be eight times as fast

1266
00:43:38,880 --> 00:43:44,400
as the man and so we can take these

1267
00:43:42,160 --> 00:43:46,319
intermediate rates of change if you will

1268
00:43:44,400 --> 00:43:48,560
and multiply them together

1269
00:43:46,319 --> 00:43:50,720
and that justifies the

1270
00:43:48,560 --> 00:43:52,640
chain rule intuitively so have a look at

1271
00:43:50,720 --> 00:43:54,640
chain rule about here really what it

1272
00:43:52,640 --> 00:43:56,640
means for us is there's a very simple

1273
00:43:54,640 --> 00:43:59,599
recipe for deriving what we want which

1274
00:43:56,640 --> 00:44:01,520
is dl by dc

1275
00:43:59,599 --> 00:44:03,599
and what we have so far

1276
00:44:01,520 --> 00:44:05,200
is we know

1277
00:44:03,599 --> 00:44:07,440
want

1278
00:44:05,200 --> 00:44:08,880
and we know

1279
00:44:07,440 --> 00:44:12,640
what is the

1280
00:44:08,880 --> 00:44:14,800
impact of d on l so we know d l by

1281
00:44:12,640 --> 00:44:17,440
d d the derivative of l with respect to

1282
00:44:14,800 --> 00:44:19,359
d d we know that that's negative two

1283
00:44:17,440 --> 00:44:21,440
and now because of this local

1284
00:44:19,359 --> 00:44:23,040
reasoning that we've done here we know

1285
00:44:21,440 --> 00:44:24,560
dd by d

1286
00:44:23,040 --> 00:44:27,520
c

1287
00:44:24,560 --> 00:44:29,599
so how does c impact d and in

1288
00:44:27,520 --> 00:44:32,000
particular this is a plus node so the

1289
00:44:29,599 --> 00:44:33,280
local derivative is simply 1.0 it's very

1290
00:44:32,000 --> 00:44:34,319
simple

1291
00:44:33,280 --> 00:44:37,359
and so

1292
00:44:34,319 --> 00:44:40,160
the chain rule tells us that dl by dc

1293
00:44:37,359 --> 00:44:44,000
going through this intermediate variable

1294
00:44:40,160 --> 00:44:44,800
will just be simply d l by

1295
00:44:44,000 --> 00:44:47,200
d

1296
00:44:44,800 --> 00:44:47,200
times

1297
00:44:49,200 --> 00:44:53,280
dd by dc

1298
00:44:51,680 --> 00:44:55,839
that's chain rule

1299
00:44:53,280 --> 00:44:56,720
so this is identical to what's happening

1300
00:44:55,839 --> 00:44:58,480
here

1301
00:44:56,720 --> 00:44:59,920
except

1302
00:44:58,480 --> 00:45:03,760
z is rl

1303
00:44:59,920 --> 00:45:05,200
y is our d and x is rc

1304
00:45:03,760 --> 00:45:06,640
so we literally just have to multiply

1305
00:45:05,200 --> 00:45:09,440
these

1306
00:45:06,640 --> 00:45:09,440
and because

1307
00:45:10,319 --> 00:45:14,720
these local derivatives like dd by dc

1308
00:45:12,480 --> 00:45:17,440
are just one

1309
00:45:14,720 --> 00:45:19,440
we basically just copy over dl by dd

1310
00:45:17,440 --> 00:45:22,160
because this is just times one

1311
00:45:19,440 --> 00:45:25,920
so what does it do so because dl by dd

1312
00:45:22,160 --> 00:45:29,040
is negative two what is dl by dc

1313
00:45:25,920 --> 00:45:31,440
well it's the local gradient 1.0 times

1314
00:45:29,040 --> 00:45:33,760
dl by dd which is negative two

1315
00:45:31,440 --> 00:45:35,520
so literally what a plus node does you

1316
00:45:33,760 --> 00:45:37,520
can look at it that way is it literally

1317
00:45:35,520 --> 00:45:39,680
just routes the gradient

1318
00:45:37,520 --> 00:45:41,920
because the plus nodes local derivatives

1319
00:45:39,680 --> 00:45:43,520
are just one and so in the chain rule

1320
00:45:41,920 --> 00:45:45,680
one times

1321
00:45:43,520 --> 00:45:47,520
dl by dd

1322
00:45:45,680 --> 00:45:50,400
is um

1323
00:45:47,520 --> 00:45:53,280
is uh is just dl by dd and so that

1324
00:45:50,400 --> 00:45:55,440
derivative just gets routed to both c

1325
00:45:53,280 --> 00:45:59,359
and to e in this case

1326
00:45:55,440 --> 00:46:01,040
so basically um we have that that grad

1327
00:45:59,359 --> 00:46:02,480
or let's start with c since that's the

1328
00:46:01,040 --> 00:46:03,920
one we looked at

1329
00:46:02,480 --> 00:46:06,800
is

1330
00:46:03,920 --> 00:46:08,480
negative two times one

1331
00:46:06,800 --> 00:46:11,119
negative two

1332
00:46:08,480 --> 00:46:13,040
and in the same way by symmetry e that

1333
00:46:11,119 --> 00:46:16,960
grad will be negative two that's the

1334
00:46:13,040 --> 00:46:19,359
claim so we can set those

1335
00:46:16,960 --> 00:46:20,880
we can redraw

1336
00:46:19,359 --> 00:46:23,200
and you see how we just assign negative

1337
00:46:20,880 --> 00:46:25,040
to negative two so this backpropagating

1338
00:46:23,200 --> 00:46:26,720
signal which is carrying the information

1339
00:46:25,040 --> 00:46:28,880
of like what is the derivative of l with

1340
00:46:26,720 --> 00:46:30,640
respect to all the intermediate nodes

1341
00:46:28,880 --> 00:46:32,400
we can imagine it almost like flowing

1342
00:46:30,640 --> 00:46:34,319
backwards through the graph and a plus

1343
00:46:32,400 --> 00:46:36,400
node will simply distribute the

1344
00:46:34,319 --> 00:46:39,119
derivative to all the leaf nodes sorry

1345
00:46:36,400 --> 00:46:40,480
to all the children nodes of it

1346
00:46:39,119 --> 00:46:43,359
so this is the claim and now let's

1347
00:46:40,480 --> 00:46:45,119
verify it so let me remove the plus h

1348
00:46:43,359 --> 00:46:46,319
here from before

1349
00:46:45,119 --> 00:46:48,480
and now instead what we're going to do

1350
00:46:46,319 --> 00:46:50,640
is we're going to increment c so c dot

1351
00:46:48,480 --> 00:46:52,400
data will be credited by h

1352
00:46:50,640 --> 00:46:54,319
and when i run this we expect to see

1353
00:46:52,400 --> 00:46:58,880
negative 2

1354
00:46:54,319 --> 00:47:01,119
negative 2. and then of course for e

1355
00:46:58,880 --> 00:47:03,119
so e dot data plus equals h and we

1356
00:47:01,119 --> 00:47:05,440
expect to see negative 2.

1357
00:47:03,119 --> 00:47:05,440
simple

1358
00:47:07,280 --> 00:47:11,359
so those are the derivatives of these

1359
00:47:09,440 --> 00:47:13,760
internal nodes

1360
00:47:11,359 --> 00:47:15,520
and now we're going to recurse our way

1361
00:47:13,760 --> 00:47:17,200
backwards again

1362
00:47:15,520 --> 00:47:19,200
and we're again going to apply the chain

1363
00:47:17,200 --> 00:47:20,960
rule so here we go our second

1364
00:47:19,200 --> 00:47:22,559
application of chain rule and we will

1365
00:47:20,960 --> 00:47:24,000
apply it all the way through the graph

1366
00:47:22,559 --> 00:47:25,359
we just happen to only have one more

1367
00:47:24,000 --> 00:47:27,200
node remaining

1368
00:47:25,359 --> 00:47:28,480
we have that d l

1369
00:47:27,200 --> 00:47:30,240
by d e

1370
00:47:28,480 --> 00:47:32,559
as we have just calculated is negative

1371
00:47:30,240 --> 00:47:33,920
two so we know that

1372
00:47:32,559 --> 00:47:36,480
so we know the derivative of l with

1373
00:47:33,920 --> 00:47:36,480
respect to e

1374
00:47:36,720 --> 00:47:40,559
and now we want dl

1375
00:47:39,440 --> 00:47:41,760
by

1376
00:47:40,559 --> 00:47:42,800
da

1377
00:47:41,760 --> 00:47:44,319
right

1378
00:47:42,800 --> 00:47:48,240
and the chain rule is telling us that

1379
00:47:44,319 --> 00:47:48,240
that's just dl by de

1380
00:47:48,960 --> 00:47:52,720
negative 2

1381
00:47:50,240 --> 00:47:55,520
times the local gradient so what is the

1382
00:47:52,720 --> 00:47:56,960
local gradient basically d e

1383
00:47:55,520 --> 00:48:00,000
by d a

1384
00:47:56,960 --> 00:48:02,319
we have to look at that

1385
00:48:00,000 --> 00:48:04,319
so i'm a little times node

1386
00:48:02,319 --> 00:48:06,880
inside a massive graph

1387
00:48:04,319 --> 00:48:09,040
and i only know that i did a times b and

1388
00:48:06,880 --> 00:48:12,640
i produced an e

1389
00:48:09,040 --> 00:48:14,079
so now what is d e by d a and d e by d b

1390
00:48:12,640 --> 00:48:17,119
that's the only thing that i sort of

1391
00:48:14,079 --> 00:48:17,839
know about that's my local gradient

1392
00:48:17,119 --> 00:48:20,559
so

1393
00:48:17,839 --> 00:48:24,079
because we have that e's a times b we're

1394
00:48:20,559 --> 00:48:26,640
asking what is d e by d a

1395
00:48:24,079 --> 00:48:27,599
and of course we just did that here we

1396
00:48:26,640 --> 00:48:30,079
had a

1397
00:48:27,599 --> 00:48:32,000
times so i'm not going to rederive it

1398
00:48:30,079 --> 00:48:34,559
but if you want to differentiate this

1399
00:48:32,000 --> 00:48:36,640
with respect to a you'll just get b

1400
00:48:34,559 --> 00:48:40,960
right the value of b

1401
00:48:36,640 --> 00:48:40,960
which in this case is negative 3.0

1402
00:48:41,040 --> 00:48:45,119
so

1403
00:48:41,920 --> 00:48:47,200
basically we have that dl by da

1404
00:48:45,119 --> 00:48:49,440
well let me just do it right here we

1405
00:48:47,200 --> 00:48:50,800
have that a dot grad and we are applying

1406
00:48:49,440 --> 00:48:54,240
chain rule here

1407
00:48:50,800 --> 00:48:56,319
is d l by d e which we see here is

1408
00:48:54,240 --> 00:48:57,520
negative two

1409
00:48:56,319 --> 00:48:59,760
times

1410
00:48:57,520 --> 00:49:04,640
what is d e by d a

1411
00:48:59,760 --> 00:49:04,640
it's the value of b which is negative 3.

1412
00:49:04,880 --> 00:49:07,280
that's it

1413
00:49:07,839 --> 00:49:11,440
and then we have b grad is again dl by

1414
00:49:10,559 --> 00:49:13,119
de

1415
00:49:11,440 --> 00:49:14,400
which is negative 2

1416
00:49:13,119 --> 00:49:15,520
just the same way

1417
00:49:14,400 --> 00:49:18,000
times

1418
00:49:15,520 --> 00:49:19,599
what is d e by d

1419
00:49:18,000 --> 00:49:23,520
um db

1420
00:49:19,599 --> 00:49:25,680
is the value of a which is 2.2.0

1421
00:49:23,520 --> 00:49:28,800
as the value of a

1422
00:49:25,680 --> 00:49:30,640
so these are our claimed derivatives

1423
00:49:28,800 --> 00:49:32,079
let's

1424
00:49:30,640 --> 00:49:33,760
redraw

1425
00:49:32,079 --> 00:49:36,000
and we see here that

1426
00:49:33,760 --> 00:49:38,400
a dot grad turns out to be 6 because

1427
00:49:36,000 --> 00:49:41,119
that is negative 2 times negative 3

1428
00:49:38,400 --> 00:49:43,680
and b dot grad is negative 4

1429
00:49:41,119 --> 00:49:45,440
times sorry is negative 2 times 2 which

1430
00:49:43,680 --> 00:49:47,440
is negative 4.

1431
00:49:45,440 --> 00:49:50,480
so those are our claims let's delete

1432
00:49:47,440 --> 00:49:52,559
this and let's verify them

1433
00:49:50,480 --> 00:49:57,280
we have

1434
00:49:52,559 --> 00:49:57,280
a here a dot data plus equals h

1435
00:49:57,520 --> 00:50:01,760
so the claim is that

1436
00:49:59,599 --> 00:50:03,440
a dot grad is six

1437
00:50:01,760 --> 00:50:04,880
let's verify

1438
00:50:03,440 --> 00:50:07,280
six

1439
00:50:04,880 --> 00:50:08,800
and we have beta data

1440
00:50:07,280 --> 00:50:11,119
plus equals h

1441
00:50:08,800 --> 00:50:13,040
so nudging b by h

1442
00:50:11,119 --> 00:50:15,119
and looking at what happens

1443
00:50:13,040 --> 00:50:17,920
we claim it's negative four

1444
00:50:15,119 --> 00:50:20,480
and indeed it's negative four plus minus

1445
00:50:17,920 --> 00:50:21,839
again float oddness

1446
00:50:20,480 --> 00:50:23,280
um

1447
00:50:21,839 --> 00:50:24,559
and uh

1448
00:50:23,280 --> 00:50:26,720
that's it this

1449
00:50:24,559 --> 00:50:28,319
that was the manual

1450
00:50:26,720 --> 00:50:30,559
back propagation

1451
00:50:28,319 --> 00:50:33,280
uh all the way from here to all the leaf

1452
00:50:30,559 --> 00:50:35,280
nodes and we've done it piece by piece

1453
00:50:33,280 --> 00:50:37,280
and really all we've done is as you saw

1454
00:50:35,280 --> 00:50:39,839
we iterated through all the nodes one by

1455
00:50:37,280 --> 00:50:41,760
one and locally applied the chain rule

1456
00:50:39,839 --> 00:50:44,240
we always know what is the derivative of

1457
00:50:41,760 --> 00:50:45,599
l with respect to this little output and

1458
00:50:44,240 --> 00:50:47,599
then we look at how this output was

1459
00:50:45,599 --> 00:50:49,520
produced this output was produced

1460
00:50:47,599 --> 00:50:51,520
through some operation and we have the

1461
00:50:49,520 --> 00:50:52,640
pointers to the children nodes of this

1462
00:50:51,520 --> 00:50:54,720
operation

1463
00:50:52,640 --> 00:50:56,640
and so in this little operation we know

1464
00:50:54,720 --> 00:50:58,559
what the local derivatives are and we

1465
00:50:56,640 --> 00:50:59,599
just multiply them onto the derivative

1466
00:50:58,559 --> 00:51:01,680
always

1467
00:50:59,599 --> 00:51:04,160
so we just go through and recursively

1468
00:51:01,680 --> 00:51:05,920
multiply on the local derivatives and

1469
00:51:04,160 --> 00:51:08,160
that's what back propagation is is just

1470
00:51:05,920 --> 00:51:10,559
a recursive application of chain rule

1471
00:51:08,160 --> 00:51:12,720
backwards through the computation graph

1472
00:51:10,559 --> 00:51:14,640
let's see this power in action just very

1473
00:51:12,720 --> 00:51:15,680
briefly what we're going to do is we're

1474
00:51:14,640 --> 00:51:19,680
going to

1475
00:51:15,680 --> 00:51:21,599
nudge our inputs to try to make l go up

1476
00:51:19,680 --> 00:51:24,160
so in particular what we're doing is we

1477
00:51:21,599 --> 00:51:26,559
want a.data we're going to change it

1478
00:51:24,160 --> 00:51:27,839
and if we want l to go up that means we

1479
00:51:26,559 --> 00:51:29,839
just have to go in the direction of the

1480
00:51:27,839 --> 00:51:30,960
gradient so

1481
00:51:29,839 --> 00:51:32,240
a

1482
00:51:30,960 --> 00:51:34,800
should increase in the direction of

1483
00:51:32,240 --> 00:51:36,480
gradient by like some small step amount

1484
00:51:34,800 --> 00:51:38,160
this is the step size

1485
00:51:36,480 --> 00:51:40,800
and we don't just want this for ba but

1486
00:51:38,160 --> 00:51:40,800
also for b

1487
00:51:41,440 --> 00:51:46,079
also for c

1488
00:51:44,480 --> 00:51:47,200
also for f

1489
00:51:46,079 --> 00:51:49,680
those are

1490
00:51:47,200 --> 00:51:50,640
leaf nodes which we usually have control

1491
00:51:49,680 --> 00:51:52,559
over

1492
00:51:50,640 --> 00:51:54,800
and if we nudge in direction of the

1493
00:51:52,559 --> 00:51:55,680
gradient we expect a positive influence

1494
00:51:54,800 --> 00:51:58,480
on l

1495
00:51:55,680 --> 00:51:59,839
so we expect l to go up

1496
00:51:58,480 --> 00:52:01,760
positively

1497
00:51:59,839 --> 00:52:03,680
so it should become less negative it

1498
00:52:01,760 --> 00:52:05,920
should go up to say negative you know

1499
00:52:03,680 --> 00:52:08,079
six or something like that

1500
00:52:05,920 --> 00:52:09,599
uh it's hard to tell exactly and we'd

1501
00:52:08,079 --> 00:52:12,319
have to rewrite the forward pass so let

1502
00:52:09,599 --> 00:52:13,920
me just um

1503
00:52:12,319 --> 00:52:16,160
do that here

1504
00:52:13,920 --> 00:52:16,160
um

1505
00:52:16,559 --> 00:52:20,800
this would be the forward pass f would

1506
00:52:18,640 --> 00:52:24,720
be unchanged this is effectively the

1507
00:52:20,800 --> 00:52:27,200
forward pass and now if we print l.data

1508
00:52:24,720 --> 00:52:28,800
we expect because we nudged all the

1509
00:52:27,200 --> 00:52:30,960
values all the inputs in the rational

1510
00:52:28,800 --> 00:52:32,880
gradient we expected a less negative l

1511
00:52:30,960 --> 00:52:34,880
we expect it to go up

1512
00:52:32,880 --> 00:52:36,640
so maybe it's negative six or so let's

1513
00:52:34,880 --> 00:52:38,319
see what happens

1514
00:52:36,640 --> 00:52:41,839
okay negative seven

1515
00:52:38,319 --> 00:52:43,839
and uh this is basically one step of an

1516
00:52:41,839 --> 00:52:46,160
optimization that we'll end up running

1517
00:52:43,839 --> 00:52:47,680
and really does gradient just give us

1518
00:52:46,160 --> 00:52:49,520
some power because we know how to

1519
00:52:47,680 --> 00:52:50,800
influence the final outcome and this

1520
00:52:49,520 --> 00:52:52,880
will be extremely useful for training

1521
00:52:50,800 --> 00:52:55,440
knowledge as well as you'll see

1522
00:52:52,880 --> 00:52:58,079
so now i would like to do one more uh

1523
00:52:55,440 --> 00:53:02,319
example of manual backpropagation using

1524
00:52:58,079 --> 00:53:04,079
a bit more complex and uh useful example

1525
00:53:02,319 --> 00:53:05,359
we are going to back propagate through a

1526
00:53:04,079 --> 00:53:07,440
neuron

1527
00:53:05,359 --> 00:53:08,880
so

1528
00:53:07,440 --> 00:53:10,880
we want to eventually build up neural

1529
00:53:08,880 --> 00:53:12,559
networks and in the simplest case these

1530
00:53:10,880 --> 00:53:15,599
are multilateral perceptrons as they're

1531
00:53:12,559 --> 00:53:17,040
called so this is a two layer neural net

1532
00:53:15,599 --> 00:53:18,640
and it's got these hidden layers made up

1533
00:53:17,040 --> 00:53:20,000
of neurons and these neurons are fully

1534
00:53:18,640 --> 00:53:21,599
connected to each other

1535
00:53:20,000 --> 00:53:23,599
now biologically neurons are very

1536
00:53:21,599 --> 00:53:26,079
complicated devices but we have very

1537
00:53:23,599 --> 00:53:27,520
simple mathematical models of them

1538
00:53:26,079 --> 00:53:29,680
and so this is a very simple

1539
00:53:27,520 --> 00:53:31,760
mathematical model of a neuron you have

1540
00:53:29,680 --> 00:53:33,680
some inputs axis

1541
00:53:31,760 --> 00:53:36,640
and then you have these synapses that

1542
00:53:33,680 --> 00:53:39,599
have weights on them so

1543
00:53:36,640 --> 00:53:40,800
the w's are weights

1544
00:53:39,599 --> 00:53:42,640
and then

1545
00:53:40,800 --> 00:53:44,880
the synapse interacts with the input to

1546
00:53:42,640 --> 00:53:47,200
this neuron multiplicatively so what

1547
00:53:44,880 --> 00:53:49,920
flows to the cell body

1548
00:53:47,200 --> 00:53:51,359
of this neuron is w times x

1549
00:53:49,920 --> 00:53:53,920
but there's multiple inputs so there's

1550
00:53:51,359 --> 00:53:54,960
many w times x's flowing into the cell

1551
00:53:53,920 --> 00:53:56,960
body

1552
00:53:54,960 --> 00:53:57,920
the cell body then has also like some

1553
00:53:56,960 --> 00:53:59,680
bias

1554
00:53:57,920 --> 00:54:02,400
so this is kind of like the

1555
00:53:59,680 --> 00:54:04,640
inert innate sort of trigger happiness

1556
00:54:02,400 --> 00:54:06,079
of this neuron so this bias can make it

1557
00:54:04,640 --> 00:54:08,400
a bit more trigger happy or a bit less

1558
00:54:06,079 --> 00:54:10,240
trigger happy regardless of the input

1559
00:54:08,400 --> 00:54:11,359
but basically we're taking all the w

1560
00:54:10,240 --> 00:54:13,839
times x

1561
00:54:11,359 --> 00:54:15,599
of all the inputs adding the bias and

1562
00:54:13,839 --> 00:54:16,960
then we take it through an activation

1563
00:54:15,599 --> 00:54:18,400
function

1564
00:54:16,960 --> 00:54:20,319
and this activation function is usually

1565
00:54:18,400 --> 00:54:22,480
some kind of a squashing function

1566
00:54:20,319 --> 00:54:24,720
like a sigmoid or 10h or something like

1567
00:54:22,480 --> 00:54:26,319
that so as an example

1568
00:54:24,720 --> 00:54:28,160
we're going to use the 10h in this

1569
00:54:26,319 --> 00:54:29,680
example

1570
00:54:28,160 --> 00:54:31,280
numpy has a

1571
00:54:29,680 --> 00:54:32,480
np.10h

1572
00:54:31,280 --> 00:54:34,960
so

1573
00:54:32,480 --> 00:54:36,559
we can call it on a range

1574
00:54:34,960 --> 00:54:38,480
and we can plot it

1575
00:54:36,559 --> 00:54:41,119
this is the 10h function and you see

1576
00:54:38,480 --> 00:54:44,319
that the inputs as they come in

1577
00:54:41,119 --> 00:54:45,280
get squashed on the y coordinate here so

1578
00:54:44,319 --> 00:54:47,359
um

1579
00:54:45,280 --> 00:54:49,599
right at zero we're going to get exactly

1580
00:54:47,359 --> 00:54:50,799
zero and then as you go more positive in

1581
00:54:49,599 --> 00:54:52,400
the input

1582
00:54:50,799 --> 00:54:55,760
then you'll see that the function will

1583
00:54:52,400 --> 00:54:57,839
only go up to one and then plateau out

1584
00:54:55,760 --> 00:55:00,160
and so if you pass in very positive

1585
00:54:57,839 --> 00:55:02,160
inputs we're gonna cap it smoothly at

1586
00:55:00,160 --> 00:55:04,480
one and on the negative side we're gonna

1587
00:55:02,160 --> 00:55:06,400
cap it smoothly to negative one

1588
00:55:04,480 --> 00:55:08,559
so that's 10h

1589
00:55:06,400 --> 00:55:10,720
and that's the squashing function or an

1590
00:55:08,559 --> 00:55:12,640
activation function and what comes out

1591
00:55:10,720 --> 00:55:14,960
of this neuron is just the activation

1592
00:55:12,640 --> 00:55:16,720
function applied to the dot product of

1593
00:55:14,960 --> 00:55:18,079
the weights and the

1594
00:55:16,720 --> 00:55:19,359
inputs

1595
00:55:18,079 --> 00:55:21,040
so let's

1596
00:55:19,359 --> 00:55:22,559
write one out

1597
00:55:21,040 --> 00:55:26,480
um

1598
00:55:22,559 --> 00:55:26,480
i'm going to copy paste because

1599
00:55:27,359 --> 00:55:31,520
i don't want to type too much

1600
00:55:28,960 --> 00:55:33,359
but okay so here we have the inputs

1601
00:55:31,520 --> 00:55:34,799
x1 x2 so this is a two-dimensional

1602
00:55:33,359 --> 00:55:35,920
neuron so two inputs are going to come

1603
00:55:34,799 --> 00:55:37,440
in

1604
00:55:35,920 --> 00:55:38,799
these are thought out as the weights of

1605
00:55:37,440 --> 00:55:41,599
this neuron

1606
00:55:38,799 --> 00:55:43,920
weights w1 w2 and these weights again

1607
00:55:41,599 --> 00:55:45,119
are the synaptic strengths for each

1608
00:55:43,920 --> 00:55:47,440
input

1609
00:55:45,119 --> 00:55:49,119
and this is the bias of the neuron

1610
00:55:47,440 --> 00:55:51,280
b

1611
00:55:49,119 --> 00:55:54,160
and now we want to do is according to

1612
00:55:51,280 --> 00:55:55,520
this model we need to multiply x1 times

1613
00:55:54,160 --> 00:55:57,760
w1

1614
00:55:55,520 --> 00:56:00,400
and x2 times w2

1615
00:55:57,760 --> 00:56:01,440
and then we need to add bias on top of

1616
00:56:00,400 --> 00:56:03,280
it

1617
00:56:01,440 --> 00:56:06,960
and it gets a little messy here but all

1618
00:56:03,280 --> 00:56:07,920
we are trying to do is x1 w1 plus x2 w2

1619
00:56:06,960 --> 00:56:09,839
plus b

1620
00:56:07,920 --> 00:56:12,160
and these are multiply here

1621
00:56:09,839 --> 00:56:13,680
except i'm doing it in small steps so

1622
00:56:12,160 --> 00:56:15,760
that we actually have pointers to all

1623
00:56:13,680 --> 00:56:19,440
these intermediate nodes so we have x1

1624
00:56:15,760 --> 00:56:21,680
w1 variable x times x2 w2 variable and

1625
00:56:19,440 --> 00:56:23,760
i'm also labeling them

1626
00:56:21,680 --> 00:56:25,440
so n is now

1627
00:56:23,760 --> 00:56:26,640
the cell body raw

1628
00:56:25,440 --> 00:56:28,400
raw

1629
00:56:26,640 --> 00:56:30,480
activation without

1630
00:56:28,400 --> 00:56:32,880
the activation function for now

1631
00:56:30,480 --> 00:56:36,559
and this should be enough to basically

1632
00:56:32,880 --> 00:56:36,559
plot it so draw dot of n

1633
00:56:37,839 --> 00:56:43,200
gives us x1 times w1 x2 times w2

1634
00:56:41,920 --> 00:56:45,599
being added

1635
00:56:43,200 --> 00:56:47,280
then the bias gets added on top of this

1636
00:56:45,599 --> 00:56:49,359
and this n

1637
00:56:47,280 --> 00:56:50,880
is this sum

1638
00:56:49,359 --> 00:56:52,559
so we're now going to take it through an

1639
00:56:50,880 --> 00:56:54,720
activation function

1640
00:56:52,559 --> 00:56:56,480
and let's say we use the 10h

1641
00:56:54,720 --> 00:56:58,000
so that we produce the output

1642
00:56:56,480 --> 00:57:01,440
so what we'd like to do here is we'd

1643
00:56:58,000 --> 00:57:03,119
like to do the output and i'll call it o

1644
00:57:01,440 --> 00:57:05,200
is um

1645
00:57:03,119 --> 00:57:08,319
n dot 10h

1646
00:57:05,200 --> 00:57:09,839
okay but we haven't yet written the 10h

1647
00:57:08,319 --> 00:57:12,880
now the reason that we need to implement

1648
00:57:09,839 --> 00:57:14,640
another 10h function here is that

1649
00:57:12,880 --> 00:57:16,720
tanh is a

1650
00:57:14,640 --> 00:57:18,480
hyperbolic function and we've only so

1651
00:57:16,720 --> 00:57:20,799
far implemented a plus and the times and

1652
00:57:18,480 --> 00:57:22,000
you can't make a 10h out of just pluses

1653
00:57:20,799 --> 00:57:25,119
and times

1654
00:57:22,000 --> 00:57:27,200
you also need exponentiation so 10h is

1655
00:57:25,119 --> 00:57:28,720
this kind of a formula here

1656
00:57:27,200 --> 00:57:30,400
you can use either one of these and you

1657
00:57:28,720 --> 00:57:32,640
see that there's exponentiation involved

1658
00:57:30,400 --> 00:57:34,720
which we have not implemented yet for

1659
00:57:32,640 --> 00:57:36,160
our low value node here so we're not

1660
00:57:34,720 --> 00:57:37,520
going to be able to produce 10h yet and

1661
00:57:36,160 --> 00:57:39,040
we have to go back up and implement

1662
00:57:37,520 --> 00:57:42,640
something like it

1663
00:57:39,040 --> 00:57:44,880
now one option here

1664
00:57:42,640 --> 00:57:46,559
is we could actually implement um

1665
00:57:44,880 --> 00:57:49,760
exponentiation

1666
00:57:46,559 --> 00:57:52,400
right and we could return the x of a

1667
00:57:49,760 --> 00:57:54,319
value instead of a 10h of a value

1668
00:57:52,400 --> 00:57:56,960
because if we had x then we have

1669
00:57:54,319 --> 00:57:58,960
everything else that we need so um

1670
00:57:56,960 --> 00:58:00,000
because we know how to add and we know

1671
00:57:58,960 --> 00:58:01,280
how to

1672
00:58:00,000 --> 00:58:02,640
um

1673
00:58:01,280 --> 00:58:04,960
we know how to add and we know how to

1674
00:58:02,640 --> 00:58:06,880
multiply so we'd be able to create 10h

1675
00:58:04,960 --> 00:58:08,319
if we knew how to x

1676
00:58:06,880 --> 00:58:10,000
but for the purposes of this example i

1677
00:58:08,319 --> 00:58:11,040
specifically wanted to

1678
00:58:10,000 --> 00:58:13,040
show you

1679
00:58:11,040 --> 00:58:15,200
that we don't necessarily need to have

1680
00:58:13,040 --> 00:58:16,000
the most atomic pieces

1681
00:58:15,200 --> 00:58:16,880
in

1682
00:58:16,000 --> 00:58:19,280
um

1683
00:58:16,880 --> 00:58:23,119
in this value object we can actually

1684
00:58:19,280 --> 00:58:24,720
like create functions at arbitrary

1685
00:58:23,119 --> 00:58:26,400
points of abstraction they can be

1686
00:58:24,720 --> 00:58:27,920
complicated functions but they can be

1687
00:58:26,400 --> 00:58:30,319
also very very simple functions like a

1688
00:58:27,920 --> 00:58:31,920
plus and it's totally up to us the only

1689
00:58:30,319 --> 00:58:33,440
thing that matters is that we know how

1690
00:58:31,920 --> 00:58:35,760
to differentiate through any one

1691
00:58:33,440 --> 00:58:37,200
function so we take some inputs and we

1692
00:58:35,760 --> 00:58:38,880
make an output the only thing that

1693
00:58:37,200 --> 00:58:41,359
matters it can be arbitrarily complex

1694
00:58:38,880 --> 00:58:43,280
function as long as you know how to

1695
00:58:41,359 --> 00:58:44,880
create the local derivative if you know

1696
00:58:43,280 --> 00:58:46,559
the local derivative of how the inputs

1697
00:58:44,880 --> 00:58:49,520
impact the output then that's all you

1698
00:58:46,559 --> 00:58:51,440
need so we're going to cluster up

1699
00:58:49,520 --> 00:58:52,640
all of this expression and we're not

1700
00:58:51,440 --> 00:58:54,160
going to break it down to its atomic

1701
00:58:52,640 --> 00:58:55,520
pieces we're just going to directly

1702
00:58:54,160 --> 00:58:57,280
implement tanh

1703
00:58:55,520 --> 00:58:59,200
so let's do that

1704
00:58:57,280 --> 00:59:02,400
depth nh

1705
00:58:59,200 --> 00:59:03,599
and then out will be a value

1706
00:59:02,400 --> 00:59:05,920
of

1707
00:59:03,599 --> 00:59:08,240
and we need this expression here so

1708
00:59:05,920 --> 00:59:08,240
um

1709
00:59:08,480 --> 00:59:13,119
let me actually

1710
00:59:10,319 --> 00:59:13,119
copy paste

1711
00:59:14,160 --> 00:59:18,720
let's grab n which is a cell.theta

1712
00:59:17,200 --> 00:59:21,440
and then this

1713
00:59:18,720 --> 00:59:24,559
i believe is the tan h

1714
00:59:21,440 --> 00:59:25,920
math.x of

1715
00:59:24,559 --> 00:59:27,200
two

1716
00:59:25,920 --> 00:59:28,880
no n

1717
00:59:27,200 --> 00:59:30,559
n minus one over

1718
00:59:28,880 --> 00:59:33,040
two n plus one

1719
00:59:30,559 --> 00:59:35,680
maybe i can call this x

1720
00:59:33,040 --> 00:59:37,520
just so that it matches exactly

1721
00:59:35,680 --> 00:59:40,319
okay and now

1722
00:59:37,520 --> 00:59:42,559
this will be t

1723
00:59:40,319 --> 00:59:44,000
and uh children of this node there's

1724
00:59:42,559 --> 00:59:46,000
just one child

1725
00:59:44,000 --> 00:59:48,559
and i'm wrapping it in a tuple so this

1726
00:59:46,000 --> 00:59:50,799
is a tuple of one object just self

1727
00:59:48,559 --> 00:59:52,240
and here the name of this operation will

1728
00:59:50,799 --> 00:59:55,520
be 10h

1729
00:59:52,240 --> 00:59:55,520
and we're going to return that

1730
00:59:56,319 --> 01:00:02,000
okay

1731
00:59:58,559 --> 01:00:03,280
so now valley should be implementing 10h

1732
01:00:02,000 --> 01:00:04,559
and now we can scroll all the way down

1733
01:00:03,280 --> 01:00:06,880
here

1734
01:00:04,559 --> 01:00:09,200
and we can actually do n.10 h and that's

1735
01:00:06,880 --> 01:00:11,280
going to return the tanhd

1736
01:00:09,200 --> 01:00:12,640
output of n

1737
01:00:11,280 --> 01:00:14,720
and now we should be able to draw it out

1738
01:00:12,640 --> 01:00:17,839
of o not of n

1739
01:00:14,720 --> 01:00:17,839
so let's see how that worked

1740
01:00:18,640 --> 01:00:21,760
there we go

1741
01:00:19,680 --> 01:00:24,240
n went through 10 h

1742
01:00:21,760 --> 01:00:26,160
to produce this output

1743
01:00:24,240 --> 01:00:27,760
so now tan h is a

1744
01:00:26,160 --> 01:00:30,000
sort of

1745
01:00:27,760 --> 01:00:33,119
our little micro grad supported node

1746
01:00:30,000 --> 01:00:35,200
here as an operation

1747
01:00:33,119 --> 01:00:36,160
and as long as we know the derivative of

1748
01:00:35,200 --> 01:00:37,520
10h

1749
01:00:36,160 --> 01:00:39,520
then we'll be able to back propagate

1750
01:00:37,520 --> 01:00:41,599
through it now let's see this 10h in

1751
01:00:39,520 --> 01:00:43,920
action currently it's not squashing too

1752
01:00:41,599 --> 01:00:46,960
much because the input to it is pretty

1753
01:00:43,920 --> 01:00:49,119
low so if the bias was increased to say

1754
01:00:46,960 --> 01:00:51,200
eight

1755
01:00:49,119 --> 01:00:53,040
then we'll see that what's flowing into

1756
01:00:51,200 --> 01:00:54,319
the 10h now is

1757
01:00:53,040 --> 01:00:57,280
two

1758
01:00:54,319 --> 01:00:59,920
and 10h is squashing it to 0.96 so we're

1759
01:00:57,280 --> 01:01:01,599
already hitting the tail of this 10h and

1760
01:00:59,920 --> 01:01:03,359
it will sort of smoothly go up to 1 and

1761
01:01:01,599 --> 01:01:04,640
then plateau out over there

1762
01:01:03,359 --> 01:01:06,559
okay so now i'm going to do something

1763
01:01:04,640 --> 01:01:09,760
slightly strange i'm going to change

1764
01:01:06,559 --> 01:01:11,839
this bias from 8 to this number

1765
01:01:09,760 --> 01:01:13,359
6.88 etc

1766
01:01:11,839 --> 01:01:15,520
and i'm going to do this for specific

1767
01:01:13,359 --> 01:01:16,960
reasons because we're about to start

1768
01:01:15,520 --> 01:01:19,040
back propagation

1769
01:01:16,960 --> 01:01:21,040
and i want to make sure that our numbers

1770
01:01:19,040 --> 01:01:22,559
come out nice they're not like very

1771
01:01:21,040 --> 01:01:24,720
crazy numbers they're nice numbers that

1772
01:01:22,559 --> 01:01:26,960
we can sort of understand in our head

1773
01:01:24,720 --> 01:01:30,000
let me also add a pose label

1774
01:01:26,960 --> 01:01:31,520
o is short for output here

1775
01:01:30,000 --> 01:01:32,640
so that's zero

1776
01:01:31,520 --> 01:01:36,079
okay so

1777
01:01:32,640 --> 01:01:37,119
0.88 flows into 10 h comes out 0.7 so on

1778
01:01:36,079 --> 01:01:38,720
so now we're going to do back

1779
01:01:37,119 --> 01:01:40,240
propagation and we're going to fill in

1780
01:01:38,720 --> 01:01:43,200
all the gradients

1781
01:01:40,240 --> 01:01:44,000
so what is the derivative o with respect

1782
01:01:43,200 --> 01:01:45,040
to

1783
01:01:44,000 --> 01:01:47,119
all the

1784
01:01:45,040 --> 01:01:48,559
inputs here and of course in the typical

1785
01:01:47,119 --> 01:01:51,359
neural network setting what we really

1786
01:01:48,559 --> 01:01:53,280
care about the most is the derivative of

1787
01:01:51,359 --> 01:01:56,319
these neurons on the weights

1788
01:01:53,280 --> 01:01:57,359
specifically the w2 and w1 because those

1789
01:01:56,319 --> 01:01:59,680
are the weights that we're going to be

1790
01:01:57,359 --> 01:02:00,640
changing part of the optimization

1791
01:01:59,680 --> 01:02:02,240
and the other thing that we have to

1792
01:02:00,640 --> 01:02:03,760
remember is here we have only a single

1793
01:02:02,240 --> 01:02:04,960
neuron but in the neural natives

1794
01:02:03,760 --> 01:02:07,119
typically have many neurons and they're

1795
01:02:04,960 --> 01:02:09,119
connected

1796
01:02:07,119 --> 01:02:10,640
so this is only like a one small neuron

1797
01:02:09,119 --> 01:02:12,480
a piece of a much bigger puzzle and

1798
01:02:10,640 --> 01:02:13,839
eventually there's a loss function that

1799
01:02:12,480 --> 01:02:15,280
sort of measures the accuracy of the

1800
01:02:13,839 --> 01:02:16,880
neural net and we're back propagating

1801
01:02:15,280 --> 01:02:19,200
with respect to that accuracy and trying

1802
01:02:16,880 --> 01:02:21,119
to increase it

1803
01:02:19,200 --> 01:02:22,480
so let's start off by propagation here

1804
01:02:21,119 --> 01:02:24,319
in the end

1805
01:02:22,480 --> 01:02:26,960
what is the derivative of o with respect

1806
01:02:24,319 --> 01:02:30,480
to o the base case sort of we know

1807
01:02:26,960 --> 01:02:32,400
always is that the gradient is just 1.0

1808
01:02:30,480 --> 01:02:35,119
so let me fill it in

1809
01:02:32,400 --> 01:02:37,119
and then let me

1810
01:02:35,119 --> 01:02:40,079
split out

1811
01:02:37,119 --> 01:02:42,319
the drawing function

1812
01:02:40,079 --> 01:02:42,319
here

1813
01:02:43,680 --> 01:02:46,960
and then here cell

1814
01:02:47,280 --> 01:02:52,480
clear this output here okay

1815
01:02:50,079 --> 01:02:53,920
so now when we draw o we'll see that oh

1816
01:02:52,480 --> 01:02:55,119
that grad is one

1817
01:02:53,920 --> 01:02:56,640
so now we're going to back propagate

1818
01:02:55,119 --> 01:02:58,640
through the tan h

1819
01:02:56,640 --> 01:03:01,200
so to back propagate through 10h we need

1820
01:02:58,640 --> 01:03:03,680
to know the local derivative of 10h

1821
01:03:01,200 --> 01:03:07,039
so if we have that

1822
01:03:03,680 --> 01:03:08,400
o is 10 h of

1823
01:03:07,039 --> 01:03:12,000
n

1824
01:03:08,400 --> 01:03:13,599
then what is d o by d n

1825
01:03:12,000 --> 01:03:15,280
now what you could do is you could come

1826
01:03:13,599 --> 01:03:16,400
here and you could take this expression

1827
01:03:15,280 --> 01:03:19,119
and you could

1828
01:03:16,400 --> 01:03:21,359
do your calculus derivative taking

1829
01:03:19,119 --> 01:03:23,760
um and that would work but we can also

1830
01:03:21,359 --> 01:03:26,799
just scroll down wikipedia here

1831
01:03:23,760 --> 01:03:28,960
into a section that hopefully tells us

1832
01:03:26,799 --> 01:03:31,680
that derivative uh

1833
01:03:28,960 --> 01:03:33,760
d by dx of 10 h of x is

1834
01:03:31,680 --> 01:03:35,200
any of these i like this one 1 minus 10

1835
01:03:33,760 --> 01:03:37,440
h square of x

1836
01:03:35,200 --> 01:03:39,359
so this is 1 minus 10 h

1837
01:03:37,440 --> 01:03:41,760
of x squared

1838
01:03:39,359 --> 01:03:43,680
so basically what this is saying is that

1839
01:03:41,760 --> 01:03:44,559
d o by d n

1840
01:03:43,680 --> 01:03:47,440
is

1841
01:03:44,559 --> 01:03:48,880
1 minus 10 h

1842
01:03:47,440 --> 01:03:51,039
of n

1843
01:03:48,880 --> 01:03:52,880
squared

1844
01:03:51,039 --> 01:03:54,480
and we already have 10 h of n that's

1845
01:03:52,880 --> 01:03:56,480
just o

1846
01:03:54,480 --> 01:03:59,839
so it's one minus o squared

1847
01:03:56,480 --> 01:04:02,160
so o is the output here so the output is

1848
01:03:59,839 --> 01:04:04,240
this number

1849
01:04:02,160 --> 01:04:06,480
data

1850
01:04:04,240 --> 01:04:08,079
is this number

1851
01:04:06,480 --> 01:04:10,400
and then

1852
01:04:08,079 --> 01:04:11,680
what this is saying is that do by dn is

1853
01:04:10,400 --> 01:04:13,280
1 minus

1854
01:04:11,680 --> 01:04:16,480
this squared so

1855
01:04:13,280 --> 01:04:18,960
one minus of that data squared

1856
01:04:16,480 --> 01:04:21,599
is 0.5 conveniently

1857
01:04:18,960 --> 01:04:24,240
so the local derivative of this 10 h

1858
01:04:21,599 --> 01:04:25,200
operation here is 0.5

1859
01:04:24,240 --> 01:04:27,440
and

1860
01:04:25,200 --> 01:04:28,559
so that would be d o by d n

1861
01:04:27,440 --> 01:04:32,640
so

1862
01:04:28,559 --> 01:04:32,640
we can fill in that in that grad

1863
01:04:33,280 --> 01:04:37,960
is 0.5 we'll just fill in

1864
01:04:42,480 --> 01:04:47,119
so this is exactly 0.5 one half

1865
01:04:45,200 --> 01:04:49,280
so now we're going to continue the back

1866
01:04:47,119 --> 01:04:52,160
propagation

1867
01:04:49,280 --> 01:04:55,280
this is 0.5 and this is a plus node

1868
01:04:52,160 --> 01:04:56,640
so how is backprop going to what is that

1869
01:04:55,280 --> 01:04:58,400
going to do here

1870
01:04:56,640 --> 01:05:01,680
and if you remember our previous example

1871
01:04:58,400 --> 01:05:03,599
a plus is just a distributor of gradient

1872
01:05:01,680 --> 01:05:05,760
so this gradient will simply flow to

1873
01:05:03,599 --> 01:05:07,440
both of these equally and that's because

1874
01:05:05,760 --> 01:05:10,720
the local derivative of this operation

1875
01:05:07,440 --> 01:05:12,880
is one for every one of its nodes so 1

1876
01:05:10,720 --> 01:05:14,960
times 0.5 is 0.5

1877
01:05:12,880 --> 01:05:18,640
so therefore we know that

1878
01:05:14,960 --> 01:05:21,039
this node here which we called this

1879
01:05:18,640 --> 01:05:24,799
its grad is just 0.5

1880
01:05:21,039 --> 01:05:28,400
and we know that b dot grad is also 0.5

1881
01:05:24,799 --> 01:05:28,400
so let's set those and let's draw

1882
01:05:28,880 --> 01:05:32,640
so 0.5

1883
01:05:30,480 --> 01:05:34,960
continuing we have another plus

1884
01:05:32,640 --> 01:05:37,280
0.5 again we'll just distribute it so

1885
01:05:34,960 --> 01:05:39,200
0.5 will flow to both of these

1886
01:05:37,280 --> 01:05:41,520
so we can set

1887
01:05:39,200 --> 01:05:41,520
theirs

1888
01:05:43,799 --> 01:05:50,240
x2w2 as well that grad is 0.5

1889
01:05:47,920 --> 01:05:51,920
and let's redraw pluses are my favorite

1890
01:05:50,240 --> 01:05:53,200
uh operations to back propagate through

1891
01:05:51,920 --> 01:05:55,039
because

1892
01:05:53,200 --> 01:05:56,319
it's very simple

1893
01:05:55,039 --> 01:05:58,480
so now it's flowing into these

1894
01:05:56,319 --> 01:05:59,599
expressions is 0.5 and so really again

1895
01:05:58,480 --> 01:06:01,680
keep in mind what the derivative is

1896
01:05:59,599 --> 01:06:04,480
telling us at every point in time along

1897
01:06:01,680 --> 01:06:06,079
here this is saying that

1898
01:06:04,480 --> 01:06:08,079
if we want the output of this neuron to

1899
01:06:06,079 --> 01:06:08,880
increase

1900
01:06:08,079 --> 01:06:10,880
then

1901
01:06:08,880 --> 01:06:13,359
the influence on these expressions is

1902
01:06:10,880 --> 01:06:15,760
positive on the output both of them are

1903
01:06:13,359 --> 01:06:15,760
positive

1904
01:06:16,960 --> 01:06:20,079
contribution to the output

1905
01:06:20,480 --> 01:06:24,160
so now back propagating to x2 and w2

1906
01:06:23,200 --> 01:06:26,319
first

1907
01:06:24,160 --> 01:06:28,079
this is a times node so we know that the

1908
01:06:26,319 --> 01:06:28,960
local derivative is you know the other

1909
01:06:28,079 --> 01:06:32,799
term

1910
01:06:28,960 --> 01:06:33,599
so if we want to calculate x2.grad

1911
01:06:32,799 --> 01:06:34,960
then

1912
01:06:33,599 --> 01:06:37,200
can you think through what it's going to

1913
01:06:34,960 --> 01:06:37,200
be

1914
01:06:40,880 --> 01:06:44,839
so x2.grad will be

1915
01:06:42,799 --> 01:06:48,240
w2.data

1916
01:06:44,839 --> 01:06:51,119
times this x2w2

1917
01:06:48,240 --> 01:06:52,119
by grad right

1918
01:06:51,119 --> 01:06:55,680
and

1919
01:06:52,119 --> 01:07:00,599
w2.grad will be

1920
01:06:55,680 --> 01:07:00,599
x2 that data times x2w2.grad

1921
01:07:01,359 --> 01:07:06,240
right so that's the local piece of chain

1922
01:07:03,920 --> 01:07:06,240
rule

1923
01:07:07,039 --> 01:07:11,280
let's set them and let's redraw

1924
01:07:09,760 --> 01:07:15,680
so here we see that the gradient on our

1925
01:07:11,280 --> 01:07:18,640
weight 2 is 0 because x2 data was 0

1926
01:07:15,680 --> 01:07:20,559
right but x2 will have the gradient 0.5

1927
01:07:18,640 --> 01:07:22,559
because data here was 1.

1928
01:07:20,559 --> 01:07:25,839
and so what's interesting here right is

1929
01:07:22,559 --> 01:07:28,480
because the input x2 was 0 then because

1930
01:07:25,839 --> 01:07:30,319
of the way the times works

1931
01:07:28,480 --> 01:07:33,200
of course this gradient will be zero and

1932
01:07:30,319 --> 01:07:35,280
think about intuitively why that is

1933
01:07:33,200 --> 01:07:36,079
derivative always tells us the influence

1934
01:07:35,280 --> 01:07:39,760
of

1935
01:07:36,079 --> 01:07:41,520
this on the final output if i wiggle w2

1936
01:07:39,760 --> 01:07:42,720
how is the output changing

1937
01:07:41,520 --> 01:07:44,319
it's not changing because we're

1938
01:07:42,720 --> 01:07:46,000
multiplying by zero

1939
01:07:44,319 --> 01:07:47,599
so because it's not changing there's no

1940
01:07:46,000 --> 01:07:48,799
derivative and zero is the correct

1941
01:07:47,599 --> 01:07:49,760
answer

1942
01:07:48,799 --> 01:07:52,160
because we're

1943
01:07:49,760 --> 01:07:54,559
squashing it at zero

1944
01:07:52,160 --> 01:07:57,680
and let's do it here point five should

1945
01:07:54,559 --> 01:08:01,920
come here and flow through this times

1946
01:07:57,680 --> 01:08:03,680
and so we'll have that x1.grad is

1947
01:08:01,920 --> 01:08:04,559
can you think through a little bit what

1948
01:08:03,680 --> 01:08:07,039
what

1949
01:08:04,559 --> 01:08:07,039
this should be

1950
01:08:07,280 --> 01:08:12,559
the local derivative of times

1951
01:08:09,440 --> 01:08:15,359
with respect to x1 is going to be w1

1952
01:08:12,559 --> 01:08:18,799
so w1 is data times

1953
01:08:15,359 --> 01:08:23,600
x1 w1 dot grad

1954
01:08:18,799 --> 01:08:27,199
and w1.grad will be x1.data times

1955
01:08:23,600 --> 01:08:29,279
x1 w2 w1 with graph

1956
01:08:27,199 --> 01:08:31,679
let's see what those came out to be

1957
01:08:29,279 --> 01:08:34,640
so this is 0.5 so this would be negative

1958
01:08:31,679 --> 01:08:36,480
1.5 and this would be 1.

1959
01:08:34,640 --> 01:08:38,159
and we've back propagated through this

1960
01:08:36,480 --> 01:08:40,880
expression these are the actual final

1961
01:08:38,159 --> 01:08:43,839
derivatives so if we want this neuron's

1962
01:08:40,880 --> 01:08:47,120
output to increase

1963
01:08:43,839 --> 01:08:49,600
we know that what's necessary is that

1964
01:08:47,120 --> 01:08:51,440
w2 we have no gradient w2 doesn't

1965
01:08:49,600 --> 01:08:54,319
actually matter to this neuron right now

1966
01:08:51,440 --> 01:08:55,120
but this neuron this weight should uh go

1967
01:08:54,319 --> 01:08:57,279
up

1968
01:08:55,120 --> 01:08:59,679
so if this weight goes up then this

1969
01:08:57,279 --> 01:09:01,199
neuron's output would have gone up and

1970
01:08:59,679 --> 01:09:03,440
proportionally because the gradient is

1971
01:09:01,199 --> 01:09:05,359
one okay so doing the back propagation

1972
01:09:03,440 --> 01:09:06,880
manually is obviously ridiculous so we

1973
01:09:05,359 --> 01:09:08,880
are now going to put an end to this

1974
01:09:06,880 --> 01:09:11,040
suffering and we're going to see how we

1975
01:09:08,880 --> 01:09:12,719
can implement uh the backward pass a bit

1976
01:09:11,040 --> 01:09:14,880
more automatically we're not going to be

1977
01:09:12,719 --> 01:09:17,040
doing all of it manually out here

1978
01:09:14,880 --> 01:09:18,560
it's now pretty obvious to us by example

1979
01:09:17,040 --> 01:09:20,960
how these pluses and times are back

1980
01:09:18,560 --> 01:09:22,159
property ingredients so let's go up to

1981
01:09:20,960 --> 01:09:24,480
the value

1982
01:09:22,159 --> 01:09:27,120
object and we're going to start

1983
01:09:24,480 --> 01:09:29,520
codifying what we've seen

1984
01:09:27,120 --> 01:09:31,279
in the examples below

1985
01:09:29,520 --> 01:09:34,799
so we're going to do this by storing a

1986
01:09:31,279 --> 01:09:37,120
special cell dot backward

1987
01:09:34,799 --> 01:09:39,520
and underscore backward and this will be

1988
01:09:37,120 --> 01:09:41,440
a function which is going to do that

1989
01:09:39,520 --> 01:09:43,199
little piece of chain rule at each

1990
01:09:41,440 --> 01:09:45,440
little node that compute that took

1991
01:09:43,199 --> 01:09:46,719
inputs and produced output uh we're

1992
01:09:45,440 --> 01:09:49,040
going to store

1993
01:09:46,719 --> 01:09:51,040
how we are going to chain the the

1994
01:09:49,040 --> 01:09:52,239
outputs gradient into the inputs

1995
01:09:51,040 --> 01:09:54,000
gradients

1996
01:09:52,239 --> 01:09:55,600
so by default

1997
01:09:54,000 --> 01:09:58,159
this will be a function

1998
01:09:55,600 --> 01:09:59,920
that uh doesn't do anything

1999
01:09:58,159 --> 01:10:01,199
so um

2000
01:09:59,920 --> 01:10:03,199
and you can also see that here in the

2001
01:10:01,199 --> 01:10:04,000
value in micrograb

2002
01:10:03,199 --> 01:10:06,560
so

2003
01:10:04,000 --> 01:10:08,400
with this backward function by default

2004
01:10:06,560 --> 01:10:10,159
doesn't do anything

2005
01:10:08,400 --> 01:10:11,360
this is an empty function

2006
01:10:10,159 --> 01:10:13,199
and that would be sort of the case for

2007
01:10:11,360 --> 01:10:15,760
example for a leaf node for leaf node

2008
01:10:13,199 --> 01:10:18,320
there's nothing to do

2009
01:10:15,760 --> 01:10:21,679
but now if when we're creating these out

2010
01:10:18,320 --> 01:10:24,159
values these out values are an addition

2011
01:10:21,679 --> 01:10:27,120
of self and other

2012
01:10:24,159 --> 01:10:29,360
and so we will want to sell set

2013
01:10:27,120 --> 01:10:31,280
outs backward to be

2014
01:10:29,360 --> 01:10:33,679
the function that propagates the

2015
01:10:31,280 --> 01:10:33,679
gradient

2016
01:10:34,159 --> 01:10:39,199
so

2017
01:10:35,360 --> 01:10:39,199
let's define what should happen

2018
01:10:40,480 --> 01:10:44,080
and we're going to store it in a closure

2019
01:10:42,159 --> 01:10:45,040
let's define what should happen when we

2020
01:10:44,080 --> 01:10:47,679
call

2021
01:10:45,040 --> 01:10:47,679
outs grad

2022
01:10:47,760 --> 01:10:52,000
for in addition

2023
01:10:50,000 --> 01:10:55,040
our job is to take

2024
01:10:52,000 --> 01:10:57,679
outs grad and propagate it into self's

2025
01:10:55,040 --> 01:11:00,560
grad and other grad so basically we want

2026
01:10:57,679 --> 01:11:02,960
to sell self.grad to something

2027
01:11:00,560 --> 01:11:04,400
and we want to set others.grad to

2028
01:11:02,960 --> 01:11:05,760
something

2029
01:11:04,400 --> 01:11:08,320
okay

2030
01:11:05,760 --> 01:11:10,000
and the way we saw below how chain rule

2031
01:11:08,320 --> 01:11:11,679
works we want to take the local

2032
01:11:10,000 --> 01:11:12,719
derivative times

2033
01:11:11,679 --> 01:11:14,400
the

2034
01:11:12,719 --> 01:11:16,159
sort of global derivative i should call

2035
01:11:14,400 --> 01:11:18,800
it which is the derivative of the final

2036
01:11:16,159 --> 01:11:21,040
output of the expression with respect to

2037
01:11:18,800 --> 01:11:22,960
out's data

2038
01:11:21,040 --> 01:11:24,719
with respect to out

2039
01:11:22,960 --> 01:11:27,280
so

2040
01:11:24,719 --> 01:11:29,600
the local derivative of self in an

2041
01:11:27,280 --> 01:11:31,760
addition is 1.0

2042
01:11:29,600 --> 01:11:34,400
so it's just 1.0 times

2043
01:11:31,760 --> 01:11:35,920
outs grad

2044
01:11:34,400 --> 01:11:38,080
that's the chain rule

2045
01:11:35,920 --> 01:11:39,199
and others.grad will be 1.0 times

2046
01:11:38,080 --> 01:11:40,480
outgrad

2047
01:11:39,199 --> 01:11:42,719
and what you basically what you're

2048
01:11:40,480 --> 01:11:45,520
seeing here is that outscrad

2049
01:11:42,719 --> 01:11:48,159
will simply be copied onto selfs grad

2050
01:11:45,520 --> 01:11:49,920
and others grad as we saw happens for an

2051
01:11:48,159 --> 01:11:51,520
addition operation

2052
01:11:49,920 --> 01:11:53,440
so we're going to later call this

2053
01:11:51,520 --> 01:11:55,760
function to propagate the gradient

2054
01:11:53,440 --> 01:11:57,760
having done an addition

2055
01:11:55,760 --> 01:12:01,360
let's now do multiplication we're going

2056
01:11:57,760 --> 01:12:01,360
to also define that backward

2057
01:12:02,320 --> 01:12:07,280
and we're going to set its backward to

2058
01:12:04,640 --> 01:12:07,280
be backward

2059
01:12:07,679 --> 01:12:13,840
and we want to chain outgrad into

2060
01:12:11,120 --> 01:12:13,840
self.grad

2061
01:12:14,320 --> 01:12:18,480
and others.grad

2062
01:12:17,120 --> 01:12:20,320
and this will be a little piece of chain

2063
01:12:18,480 --> 01:12:21,679
rule for multiplication

2064
01:12:20,320 --> 01:12:23,280
so we'll have

2065
01:12:21,679 --> 01:12:26,000
so what should this be

2066
01:12:23,280 --> 01:12:26,000
can you think through

2067
01:12:28,640 --> 01:12:32,320
so what is the local derivative

2068
01:12:30,800 --> 01:12:35,199
here the local derivative was

2069
01:12:32,320 --> 01:12:35,199
others.data

2070
01:12:35,440 --> 01:12:39,600
and then

2071
01:12:36,560 --> 01:12:42,560
oops others.data and the times of that

2072
01:12:39,600 --> 01:12:44,800
grad that's channel

2073
01:12:42,560 --> 01:12:45,600
and here we have self.data times of that

2074
01:12:44,800 --> 01:12:48,400
grad

2075
01:12:45,600 --> 01:12:48,400
that's what we've been doing

2076
01:12:49,600 --> 01:12:54,400
and finally here for 10 h

2077
01:12:51,679 --> 01:12:54,400
left backward

2078
01:12:54,800 --> 01:13:00,239
and then we want to set out backwards to

2079
01:12:57,360 --> 01:13:00,239
be just backward

2080
01:13:00,480 --> 01:13:04,960
and here we need to

2081
01:13:02,560 --> 01:13:09,440
back propagate we have out that grad and

2082
01:13:04,960 --> 01:13:09,440
we want to chain it into self.grad

2083
01:13:09,679 --> 01:13:13,760
and salt.grad will be

2084
01:13:11,760 --> 01:13:16,159
the local derivative of this operation

2085
01:13:13,760 --> 01:13:17,520
that we've done here which is 10h

2086
01:13:16,159 --> 01:13:20,560
and so we saw that the local the

2087
01:13:17,520 --> 01:13:23,600
gradient is 1 minus the tan h of x

2088
01:13:20,560 --> 01:13:25,440
squared which here is t

2089
01:13:23,600 --> 01:13:27,679
that's the local derivative because

2090
01:13:25,440 --> 01:13:30,000
that's t is the output of this 10 h so 1

2091
01:13:27,679 --> 01:13:32,480
minus t squared is the local derivative

2092
01:13:30,000 --> 01:13:33,840
and then gradient um

2093
01:13:32,480 --> 01:13:34,880
has to be multiplied because of the

2094
01:13:33,840 --> 01:13:36,640
chain rule

2095
01:13:34,880 --> 01:13:39,360
so outgrad is chained through the local

2096
01:13:36,640 --> 01:13:41,840
gradient into salt.grad

2097
01:13:39,360 --> 01:13:44,880
and that should be basically it so we're

2098
01:13:41,840 --> 01:13:46,239
going to redefine our value node

2099
01:13:44,880 --> 01:13:48,080
we're going to swing all the way down

2100
01:13:46,239 --> 01:13:49,920
here

2101
01:13:48,080 --> 01:13:51,199
and we're going to

2102
01:13:49,920 --> 01:13:52,560
redefine

2103
01:13:51,199 --> 01:13:55,280
our expression

2104
01:13:52,560 --> 01:13:56,239
make sure that all the grads are zero

2105
01:13:55,280 --> 01:13:57,600
okay

2106
01:13:56,239 --> 01:13:59,760
but now we don't have to do this

2107
01:13:57,600 --> 01:14:01,280
manually anymore

2108
01:13:59,760 --> 01:14:04,000
we are going to basically be calling the

2109
01:14:01,280 --> 01:14:05,040
dot backward in the right order

2110
01:14:04,000 --> 01:14:07,840
so

2111
01:14:05,040 --> 01:14:11,640
first we want to call os

2112
01:14:07,840 --> 01:14:11,640
dot backwards

2113
01:14:14,000 --> 01:14:20,480
so o was the outcome of 10h

2114
01:14:17,920 --> 01:14:22,159
right so calling all that those who's

2115
01:14:20,480 --> 01:14:23,120
backward

2116
01:14:22,159 --> 01:14:26,000
will be

2117
01:14:23,120 --> 01:14:29,120
this function this is what it will do

2118
01:14:26,000 --> 01:14:31,760
now we have to be careful because

2119
01:14:29,120 --> 01:14:34,239
there's a times out.grad

2120
01:14:31,760 --> 01:14:36,640
and out.grad remember is initialized to

2121
01:14:34,239 --> 01:14:36,640
zero

2122
01:14:38,880 --> 01:14:46,640
so here we see grad zero so as a base

2123
01:14:41,360 --> 01:14:50,080
case we need to set both.grad to 1.0

2124
01:14:46,640 --> 01:14:50,080
to initialize this with 1

2125
01:14:53,520 --> 01:14:57,199
and then once this is 1 we can call oda

2126
01:14:56,080 --> 01:14:58,960
backward

2127
01:14:57,199 --> 01:15:02,159
and what that should do is it should

2128
01:14:58,960 --> 01:15:04,239
propagate this grad through 10h

2129
01:15:02,159 --> 01:15:05,920
so the local derivative times

2130
01:15:04,239 --> 01:15:08,239
the global derivative which is

2131
01:15:05,920 --> 01:15:11,120
initialized at one so

2132
01:15:08,239 --> 01:15:13,360
this should

2133
01:15:11,120 --> 01:15:13,360
um

2134
01:15:15,679 --> 01:15:19,120
a dope

2135
01:15:17,040 --> 01:15:20,560
so i thought about redoing it but i

2136
01:15:19,120 --> 01:15:22,480
figured i should just leave the error in

2137
01:15:20,560 --> 01:15:24,400
here because it's pretty funny why is

2138
01:15:22,480 --> 01:15:27,040
anti-object not callable

2139
01:15:24,400 --> 01:15:29,280
uh it's because

2140
01:15:27,040 --> 01:15:31,920
i screwed up we're trying to save these

2141
01:15:29,280 --> 01:15:33,280
functions so this is correct

2142
01:15:31,920 --> 01:15:34,640
this here

2143
01:15:33,280 --> 01:15:36,080
we don't want to call the function

2144
01:15:34,640 --> 01:15:38,080
because that returns none these

2145
01:15:36,080 --> 01:15:39,600
functions return none we just want to

2146
01:15:38,080 --> 01:15:42,159
store the function

2147
01:15:39,600 --> 01:15:43,760
so let me redefine the value object

2148
01:15:42,159 --> 01:15:46,640
and then we're going to come back in

2149
01:15:43,760 --> 01:15:50,080
redefine the expression draw a dot

2150
01:15:46,640 --> 01:15:53,120
everything is great o dot grad is one

2151
01:15:50,080 --> 01:15:55,679
o dot grad is one and now

2152
01:15:53,120 --> 01:15:58,560
now this should work of course

2153
01:15:55,679 --> 01:16:00,560
okay so all that backward should

2154
01:15:58,560 --> 01:16:03,199
this grant should now be 0.5 if we

2155
01:16:00,560 --> 01:16:05,280
redraw and if everything went correctly

2156
01:16:03,199 --> 01:16:09,760
0.5 yay

2157
01:16:05,280 --> 01:16:09,760
okay so now we need to call ns.grad

2158
01:16:10,159 --> 01:16:14,560
and it's not awkward sorry

2159
01:16:13,040 --> 01:16:17,600
ends backward

2160
01:16:14,560 --> 01:16:17,600
so that seems to have worked

2161
01:16:17,840 --> 01:16:22,800
so instead backward routed the gradient

2162
01:16:21,120 --> 01:16:24,560
to both of these so this is looking

2163
01:16:22,800 --> 01:16:26,239
great

2164
01:16:24,560 --> 01:16:27,520
now we could of course called uh called

2165
01:16:26,239 --> 01:16:30,159
b grad

2166
01:16:27,520 --> 01:16:32,000
beat up backwards sorry

2167
01:16:30,159 --> 01:16:34,480
what's gonna happen

2168
01:16:32,000 --> 01:16:35,600
well b doesn't have it backward b is

2169
01:16:34,480 --> 01:16:37,600
backward

2170
01:16:35,600 --> 01:16:40,159
because b is a leaf node

2171
01:16:37,600 --> 01:16:41,600
b's backward is by initialization the

2172
01:16:40,159 --> 01:16:44,239
empty function

2173
01:16:41,600 --> 01:16:45,840
so nothing would happen but we can call

2174
01:16:44,239 --> 01:16:48,400
call it on it

2175
01:16:45,840 --> 01:16:50,080
but when we call

2176
01:16:48,400 --> 01:16:52,800
this one

2177
01:16:50,080 --> 01:16:52,800
it's backward

2178
01:16:53,440 --> 01:16:57,520
then we expect this 0.5 to get further

2179
01:16:56,239 --> 01:17:00,960
routed

2180
01:16:57,520 --> 01:17:02,719
right so there we go 0.5.5

2181
01:17:00,960 --> 01:17:05,199
and then finally

2182
01:17:02,719 --> 01:17:09,040
we want to call

2183
01:17:05,199 --> 01:17:09,040
it here on x2 w2

2184
01:17:10,320 --> 01:17:14,280
and on x1 w1

2185
01:17:16,000 --> 01:17:19,679
do both of those

2186
01:17:17,760 --> 01:17:23,760
and there we go

2187
01:17:19,679 --> 01:17:26,480
so we get 0 0.5 negative 1.5 and 1

2188
01:17:23,760 --> 01:17:28,400
exactly as we did before but now

2189
01:17:26,480 --> 01:17:30,960
we've done it through

2190
01:17:28,400 --> 01:17:32,400
calling that backward um

2191
01:17:30,960 --> 01:17:34,320
sort of manually

2192
01:17:32,400 --> 01:17:36,400
so we have the lamp one last piece to

2193
01:17:34,320 --> 01:17:38,400
get rid of which is us calling

2194
01:17:36,400 --> 01:17:40,400
underscore backward manually so let's

2195
01:17:38,400 --> 01:17:41,440
think through what we are actually doing

2196
01:17:40,400 --> 01:17:43,199
um

2197
01:17:41,440 --> 01:17:44,880
we've laid out a mathematical expression

2198
01:17:43,199 --> 01:17:46,480
and now we're trying to go backwards

2199
01:17:44,880 --> 01:17:48,800
through that expression

2200
01:17:46,480 --> 01:17:50,480
um so going backwards through the

2201
01:17:48,800 --> 01:17:54,080
expression just means that we never want

2202
01:17:50,480 --> 01:17:55,280
to call a dot backward for any node

2203
01:17:54,080 --> 01:17:58,719
before

2204
01:17:55,280 --> 01:17:59,679
we've done a sort of um everything after

2205
01:17:58,719 --> 01:18:01,280
it

2206
01:17:59,679 --> 01:18:02,800
so we have to do everything after it

2207
01:18:01,280 --> 01:18:04,320
before we're ever going to call that

2208
01:18:02,800 --> 01:18:06,000
backward on any one node we have to get

2209
01:18:04,320 --> 01:18:08,560
all of its full dependencies everything

2210
01:18:06,000 --> 01:18:10,480
that it depends on has to

2211
01:18:08,560 --> 01:18:14,159
propagate to it before we can continue

2212
01:18:10,480 --> 01:18:16,000
back propagation so this ordering of

2213
01:18:14,159 --> 01:18:17,760
graphs can be achieved using something

2214
01:18:16,000 --> 01:18:20,080
called topological sort

2215
01:18:17,760 --> 01:18:23,120
so topological sort

2216
01:18:20,080 --> 01:18:24,880
is basically a laying out of a graph

2217
01:18:23,120 --> 01:18:26,640
such that all the edges go only from

2218
01:18:24,880 --> 01:18:29,360
left to right basically

2219
01:18:26,640 --> 01:18:31,520
so here we have a graph it's a directory

2220
01:18:29,360 --> 01:18:34,000
a cyclic graph a dag

2221
01:18:31,520 --> 01:18:36,239
and this is two different topological

2222
01:18:34,000 --> 01:18:37,520
orders of it i believe where basically

2223
01:18:36,239 --> 01:18:39,360
you'll see that it's laying out of the

2224
01:18:37,520 --> 01:18:41,920
notes such that all the edges go only

2225
01:18:39,360 --> 01:18:44,239
one way from left to right

2226
01:18:41,920 --> 01:18:46,239
and implementing topological sort you

2227
01:18:44,239 --> 01:18:48,800
can look in wikipedia and so on i'm not

2228
01:18:46,239 --> 01:18:51,520
going to go through it in detail

2229
01:18:48,800 --> 01:18:54,159
but basically this is what builds a

2230
01:18:51,520 --> 01:18:56,960
topological graph

2231
01:18:54,159 --> 01:18:59,520
we maintain a set of visited nodes and

2232
01:18:56,960 --> 01:19:02,000
then we are

2233
01:18:59,520 --> 01:19:03,679
going through starting at some root node

2234
01:19:02,000 --> 01:19:05,679
which for us is o that's where we want

2235
01:19:03,679 --> 01:19:08,000
to start the topological sort

2236
01:19:05,679 --> 01:19:10,480
and starting at o we go through all of

2237
01:19:08,000 --> 01:19:12,640
its children and we need to lay them out

2238
01:19:10,480 --> 01:19:14,960
from left to right

2239
01:19:12,640 --> 01:19:17,199
and basically this starts at o

2240
01:19:14,960 --> 01:19:19,120
if it's not visited then it marks it as

2241
01:19:17,199 --> 01:19:20,719
visited and then it iterates through all

2242
01:19:19,120 --> 01:19:24,239
of its children

2243
01:19:20,719 --> 01:19:26,159
and calls build topological on them

2244
01:19:24,239 --> 01:19:28,400
and then uh after it's gone through all

2245
01:19:26,159 --> 01:19:29,920
the children it adds itself

2246
01:19:28,400 --> 01:19:31,920
so basically

2247
01:19:29,920 --> 01:19:34,560
this node that we're going to call it on

2248
01:19:31,920 --> 01:19:37,600
like say o is only going to add itself

2249
01:19:34,560 --> 01:19:39,360
to the topo list after all of the

2250
01:19:37,600 --> 01:19:41,760
children have been processed and that's

2251
01:19:39,360 --> 01:19:43,520
how this function is guaranteeing

2252
01:19:41,760 --> 01:19:45,679
that you're only going to be in the list

2253
01:19:43,520 --> 01:19:46,960
once all your children are in the list

2254
01:19:45,679 --> 01:19:49,440
and that's the invariant that is being

2255
01:19:46,960 --> 01:19:52,159
maintained so if we built upon o and

2256
01:19:49,440 --> 01:19:54,880
then inspect this list

2257
01:19:52,159 --> 01:19:56,480
we're going to see that it ordered our

2258
01:19:54,880 --> 01:19:58,400
value objects

2259
01:19:56,480 --> 01:20:00,480
and the last one

2260
01:19:58,400 --> 01:20:01,760
is the value of 0.707 which is the

2261
01:20:00,480 --> 01:20:04,880
output

2262
01:20:01,760 --> 01:20:07,120
so this is o and then this is n

2263
01:20:04,880 --> 01:20:09,760
and then all the other nodes get laid

2264
01:20:07,120 --> 01:20:12,000
out before it

2265
01:20:09,760 --> 01:20:13,920
so that builds the topological graph and

2266
01:20:12,000 --> 01:20:16,639
really what we're doing now is we're

2267
01:20:13,920 --> 01:20:19,840
just calling dot underscore backward on

2268
01:20:16,639 --> 01:20:22,000
all of the nodes in a topological order

2269
01:20:19,840 --> 01:20:23,440
so if we just reset the gradients

2270
01:20:22,000 --> 01:20:24,719
they're all zero

2271
01:20:23,440 --> 01:20:27,679
what did we do

2272
01:20:24,719 --> 01:20:29,840
we started by

2273
01:20:27,679 --> 01:20:31,440
setting o dot grad

2274
01:20:29,840 --> 01:20:33,600
to b1

2275
01:20:31,440 --> 01:20:37,920
that's the base case

2276
01:20:33,600 --> 01:20:37,920
then we built the topological order

2277
01:20:38,480 --> 01:20:42,639
and then we went for node

2278
01:20:41,840 --> 01:20:44,000
in

2279
01:20:42,639 --> 01:20:46,400
reversed

2280
01:20:44,000 --> 01:20:47,600
of topo

2281
01:20:46,400 --> 01:20:49,360
now

2282
01:20:47,600 --> 01:20:50,800
in in the reverse order because this

2283
01:20:49,360 --> 01:20:52,400
list goes from

2284
01:20:50,800 --> 01:20:53,920
you know we need to go through it in

2285
01:20:52,400 --> 01:20:56,080
reversed order

2286
01:20:53,920 --> 01:20:58,639
so starting at o

2287
01:20:56,080 --> 01:21:01,360
note that backward

2288
01:20:58,639 --> 01:21:03,360
and this should be

2289
01:21:01,360 --> 01:21:05,520
it

2290
01:21:03,360 --> 01:21:07,120
there we go

2291
01:21:05,520 --> 01:21:08,880
those are the correct derivatives

2292
01:21:07,120 --> 01:21:10,159
finally we are going to hide this

2293
01:21:08,880 --> 01:21:11,679
functionality

2294
01:21:10,159 --> 01:21:13,760
so i'm going to

2295
01:21:11,679 --> 01:21:15,440
copy this and we're going to hide it

2296
01:21:13,760 --> 01:21:18,239
inside the valley class because we don't

2297
01:21:15,440 --> 01:21:19,840
want to have all that code lying around

2298
01:21:18,239 --> 01:21:21,440
so instead of an underscore backward

2299
01:21:19,840 --> 01:21:23,840
we're now going to define an actual

2300
01:21:21,440 --> 01:21:26,239
backward so that's backward without the

2301
01:21:23,840 --> 01:21:27,520
underscore

2302
01:21:26,239 --> 01:21:29,040
and that's going to do all the stuff

2303
01:21:27,520 --> 01:21:30,400
that we just arrived

2304
01:21:29,040 --> 01:21:32,480
so let me just clean this up a little

2305
01:21:30,400 --> 01:21:35,360
bit so

2306
01:21:32,480 --> 01:21:35,360
we're first going to

2307
01:21:37,040 --> 01:21:41,280
build a topological graph

2308
01:21:38,960 --> 01:21:44,159
starting at self

2309
01:21:41,280 --> 01:21:46,560
so build topo of self

2310
01:21:44,159 --> 01:21:49,280
will populate the topological order into

2311
01:21:46,560 --> 01:21:52,960
the topo list which is a local variable

2312
01:21:49,280 --> 01:21:55,360
then we set self.grad to be one

2313
01:21:52,960 --> 01:21:57,679
and then for each node in the reversed

2314
01:21:55,360 --> 01:22:00,000
list so starting at us and going to all

2315
01:21:57,679 --> 01:22:02,400
the children

2316
01:22:00,000 --> 01:22:03,280
underscore backward

2317
01:22:02,400 --> 01:22:06,080
and

2318
01:22:03,280 --> 01:22:08,000
that should be it so

2319
01:22:06,080 --> 01:22:09,199
save

2320
01:22:08,000 --> 01:22:09,910
come down here

2321
01:22:09,199 --> 01:22:11,120
redefine

2322
01:22:09,910 --> 01:22:13,600
[Music]

2323
01:22:11,120 --> 01:22:15,199
okay all the grands are zero

2324
01:22:13,600 --> 01:22:17,120
and now what we can do is oh that

2325
01:22:15,199 --> 01:22:19,360
backward without the underscore

2326
01:22:17,120 --> 01:22:19,360
and

2327
01:22:21,440 --> 01:22:26,639
there we go

2328
01:22:22,880 --> 01:22:28,400
and that's uh that's back propagation

2329
01:22:26,639 --> 01:22:29,920
place for one neuron

2330
01:22:28,400 --> 01:22:32,320
now we shouldn't be too happy with

2331
01:22:29,920 --> 01:22:35,040
ourselves actually because we have a bad

2332
01:22:32,320 --> 01:22:36,800
bug um and we have not surfaced the bug

2333
01:22:35,040 --> 01:22:39,920
because of some specific conditions that

2334
01:22:36,800 --> 01:22:42,000
we are we have to think about right now

2335
01:22:39,920 --> 01:22:43,920
so here's the simplest case that shows

2336
01:22:42,000 --> 01:22:47,760
the bug

2337
01:22:43,920 --> 01:22:47,760
say i create a single node a

2338
01:22:48,000 --> 01:22:54,560
and then i create a b that is a plus a

2339
01:22:51,600 --> 01:22:54,560
and then i called backward

2340
01:22:54,800 --> 01:23:00,000
so what's going to happen is a is 3

2341
01:22:57,280 --> 01:23:03,600
and then a b is a plus a so there's two

2342
01:23:00,000 --> 01:23:03,600
arrows on top of each other here

2343
01:23:03,760 --> 01:23:06,880
then we can see that b is of course the

2344
01:23:05,440 --> 01:23:08,080
forward pass works

2345
01:23:06,880 --> 01:23:10,000
b is just

2346
01:23:08,080 --> 01:23:11,679
a plus a which is six

2347
01:23:10,000 --> 01:23:12,639
but the gradient here is not actually

2348
01:23:11,679 --> 01:23:15,920
correct

2349
01:23:12,639 --> 01:23:17,440
that we calculate it automatically

2350
01:23:15,920 --> 01:23:19,199
and that's because

2351
01:23:17,440 --> 01:23:20,480
um

2352
01:23:19,199 --> 01:23:22,480
of course uh

2353
01:23:20,480 --> 01:23:24,560
just doing calculus in your head the

2354
01:23:22,480 --> 01:23:27,600
derivative of b with respect to a

2355
01:23:24,560 --> 01:23:28,800
should be uh two

2356
01:23:27,600 --> 01:23:30,719
one plus one

2357
01:23:28,800 --> 01:23:32,239
it's not one

2358
01:23:30,719 --> 01:23:34,639
intuitively what's happening here right

2359
01:23:32,239 --> 01:23:36,639
so b is the result of a plus a and then

2360
01:23:34,639 --> 01:23:41,040
we call backward on it

2361
01:23:36,639 --> 01:23:41,040
so let's go up and see what that does

2362
01:23:42,080 --> 01:23:45,520
um

2363
01:23:43,600 --> 01:23:46,840
b is a result of addition

2364
01:23:45,520 --> 01:23:49,520
so out as

2365
01:23:46,840 --> 01:23:50,800
b and then when we called backward what

2366
01:23:49,520 --> 01:23:53,199
happened is

2367
01:23:50,800 --> 01:23:54,400
self.grad was set

2368
01:23:53,199 --> 01:23:57,199
to one

2369
01:23:54,400 --> 01:23:59,840
and then other that grad was set to one

2370
01:23:57,199 --> 01:24:02,000
but because we're doing a plus a

2371
01:23:59,840 --> 01:24:03,360
self and other are actually the exact

2372
01:24:02,000 --> 01:24:06,000
same object

2373
01:24:03,360 --> 01:24:07,600
so we are overriding the gradient we are

2374
01:24:06,000 --> 01:24:10,000
setting it to one and then we are

2375
01:24:07,600 --> 01:24:11,199
setting it again to one and that's why

2376
01:24:10,000 --> 01:24:13,040
it stays

2377
01:24:11,199 --> 01:24:14,639
at one

2378
01:24:13,040 --> 01:24:16,400
so that's a problem

2379
01:24:14,639 --> 01:24:20,000
there's another way to see this in a

2380
01:24:16,400 --> 01:24:20,000
little bit more complicated expression

2381
01:24:21,520 --> 01:24:25,920
so here we have

2382
01:24:23,520 --> 01:24:28,880
a and b

2383
01:24:25,920 --> 01:24:30,800
and then uh d will be the multiplication

2384
01:24:28,880 --> 01:24:32,000
of the two and e will be the addition of

2385
01:24:30,800 --> 01:24:33,040
the two

2386
01:24:32,000 --> 01:24:35,280
and

2387
01:24:33,040 --> 01:24:37,679
then we multiply e times d to get f and

2388
01:24:35,280 --> 01:24:39,600
then we called fda backward

2389
01:24:37,679 --> 01:24:40,800
and these gradients if you check will be

2390
01:24:39,600 --> 01:24:42,480
incorrect

2391
01:24:40,800 --> 01:24:45,040
so fundamentally what's happening here

2392
01:24:42,480 --> 01:24:46,400
again is

2393
01:24:45,040 --> 01:24:49,199
basically we're going to see an issue

2394
01:24:46,400 --> 01:24:51,040
anytime we use a variable more than once

2395
01:24:49,199 --> 01:24:53,199
until now in these expressions above

2396
01:24:51,040 --> 01:24:54,960
every variable is used exactly once so

2397
01:24:53,199 --> 01:24:56,639
we didn't see the issue

2398
01:24:54,960 --> 01:24:57,920
but here if a variable is used more than

2399
01:24:56,639 --> 01:25:00,239
once what's going to happen during

2400
01:24:57,920 --> 01:25:03,280
backward pass we're backpropagating from

2401
01:25:00,239 --> 01:25:05,840
f to e to d so far so good but now

2402
01:25:03,280 --> 01:25:08,000
equals it backward and it deposits its

2403
01:25:05,840 --> 01:25:09,120
gradients to a and b but then we come

2404
01:25:08,000 --> 01:25:11,440
back to d

2405
01:25:09,120 --> 01:25:14,480
and call backward and it overwrites

2406
01:25:11,440 --> 01:25:17,120
those gradients at a and b

2407
01:25:14,480 --> 01:25:19,920
so that's obviously a problem

2408
01:25:17,120 --> 01:25:22,239
and the solution here if you look at

2409
01:25:19,920 --> 01:25:23,920
the multivariate case of the chain rule

2410
01:25:22,239 --> 01:25:26,320
and its generalization there

2411
01:25:23,920 --> 01:25:28,080
the solution there is basically that we

2412
01:25:26,320 --> 01:25:30,080
have to accumulate these gradients these

2413
01:25:28,080 --> 01:25:32,239
gradients add

2414
01:25:30,080 --> 01:25:34,639
and so instead of setting those

2415
01:25:32,239 --> 01:25:37,120
gradients

2416
01:25:34,639 --> 01:25:39,120
we can simply do plus equals we need to

2417
01:25:37,120 --> 01:25:41,920
accumulate those gradients

2418
01:25:39,120 --> 01:25:44,560
plus equals plus equals

2419
01:25:41,920 --> 01:25:44,560
plus equals

2420
01:25:44,639 --> 01:25:48,159
plus equals

2421
01:25:46,480 --> 01:25:50,239
and this will be okay remember because

2422
01:25:48,159 --> 01:25:51,440
we are initializing them at zero so they

2423
01:25:50,239 --> 01:25:53,360
start at zero

2424
01:25:51,440 --> 01:25:54,880
and then any

2425
01:25:53,360 --> 01:25:57,199
contribution

2426
01:25:54,880 --> 01:25:58,800
that flows backwards

2427
01:25:57,199 --> 01:26:01,280
will simply add

2428
01:25:58,800 --> 01:26:03,760
so now if we redefine

2429
01:26:01,280 --> 01:26:06,000
this one

2430
01:26:03,760 --> 01:26:08,239
because the plus equals this now works

2431
01:26:06,000 --> 01:26:11,120
because a.grad started at zero and we

2432
01:26:08,239 --> 01:26:13,120
called beta backward we deposit one and

2433
01:26:11,120 --> 01:26:14,960
then we deposit one again and now this

2434
01:26:13,120 --> 01:26:16,960
is two which is correct

2435
01:26:14,960 --> 01:26:18,400
and here this will also work and we'll

2436
01:26:16,960 --> 01:26:20,159
get correct gradients

2437
01:26:18,400 --> 01:26:21,600
because when we call eta backward we

2438
01:26:20,159 --> 01:26:23,760
will deposit the gradients from this

2439
01:26:21,600 --> 01:26:26,000
branch and then we get to back into

2440
01:26:23,760 --> 01:26:28,159
detail backward it will deposit its own

2441
01:26:26,000 --> 01:26:30,560
gradients and then those gradients

2442
01:26:28,159 --> 01:26:31,920
simply add on top of each other and so

2443
01:26:30,560 --> 01:26:34,080
we just accumulate those gradients and

2444
01:26:31,920 --> 01:26:35,679
that fixes the issue okay now before we

2445
01:26:34,080 --> 01:26:38,800
move on let me actually do a bit of

2446
01:26:35,679 --> 01:26:41,199
cleanup here and delete some of these

2447
01:26:38,800 --> 01:26:42,719
some of this intermediate work so

2448
01:26:41,199 --> 01:26:44,800
we're not gonna need any of this now

2449
01:26:42,719 --> 01:26:45,679
that we've derived all of it

2450
01:26:44,800 --> 01:26:48,000
um

2451
01:26:45,679 --> 01:26:49,679
we are going to keep this because i want

2452
01:26:48,000 --> 01:26:51,280
to come back to it

2453
01:26:49,679 --> 01:26:53,840
delete the 10h

2454
01:26:51,280 --> 01:26:55,840
delete our morning example

2455
01:26:53,840 --> 01:26:59,679
delete the step

2456
01:26:55,840 --> 01:27:02,080
delete this keep the code that draws

2457
01:26:59,679 --> 01:27:03,840
and then delete this example

2458
01:27:02,080 --> 01:27:05,280
and leave behind only the definition of

2459
01:27:03,840 --> 01:27:06,560
value

2460
01:27:05,280 --> 01:27:08,159
and now let's come back to this

2461
01:27:06,560 --> 01:27:10,719
non-linearity here that we implemented

2462
01:27:08,159 --> 01:27:13,760
the tanh now i told you that we could

2463
01:27:10,719 --> 01:27:16,000
have broken down 10h into its explicit

2464
01:27:13,760 --> 01:27:18,320
atoms in terms of other expressions if

2465
01:27:16,000 --> 01:27:20,560
we had the x function so if you remember

2466
01:27:18,320 --> 01:27:22,880
tan h is defined like this and we chose

2467
01:27:20,560 --> 01:27:24,639
to develop tan h as a single function

2468
01:27:22,880 --> 01:27:26,000
and we can do that because we know its

2469
01:27:24,639 --> 01:27:26,960
derivative and we can back propagate

2470
01:27:26,000 --> 01:27:29,199
through it

2471
01:27:26,960 --> 01:27:31,440
but we can also break down tan h into

2472
01:27:29,199 --> 01:27:33,120
and express it as a function of x and i

2473
01:27:31,440 --> 01:27:34,320
would like to do that now because i want

2474
01:27:33,120 --> 01:27:36,800
to prove to you that you get all the

2475
01:27:34,320 --> 01:27:38,239
same results and all those ingredients

2476
01:27:36,800 --> 01:27:40,000
but also because it forces us to

2477
01:27:38,239 --> 01:27:42,960
implement a few more expressions it

2478
01:27:40,000 --> 01:27:44,800
forces us to do exponentiation addition

2479
01:27:42,960 --> 01:27:46,239
subtraction division and things like

2480
01:27:44,800 --> 01:27:48,080
that and i think it's a good exercise to

2481
01:27:46,239 --> 01:27:50,080
go through a few more of these

2482
01:27:48,080 --> 01:27:52,159
okay so let's scroll up

2483
01:27:50,080 --> 01:27:53,840
to the definition of value

2484
01:27:52,159 --> 01:27:56,639
and here one thing that we currently

2485
01:27:53,840 --> 01:27:58,400
can't do is we can do like a value of

2486
01:27:56,639 --> 01:28:00,400
say 2.0

2487
01:27:58,400 --> 01:28:02,560
but we can't do you know here for

2488
01:28:00,400 --> 01:28:05,120
example we want to add constant one and

2489
01:28:02,560 --> 01:28:06,960
we can't do something like this

2490
01:28:05,120 --> 01:28:08,719
and we can't do it because it says

2491
01:28:06,960 --> 01:28:11,120
object has no attribute data that's

2492
01:28:08,719 --> 01:28:12,080
because a plus one comes right here to

2493
01:28:11,120 --> 01:28:14,880
add

2494
01:28:12,080 --> 01:28:16,560
and then other is the integer one and

2495
01:28:14,880 --> 01:28:18,719
then here python is trying to access

2496
01:28:16,560 --> 01:28:20,480
one.data and that's not a thing and

2497
01:28:18,719 --> 01:28:22,239
that's because basically one is not a

2498
01:28:20,480 --> 01:28:24,800
value object and we only have addition

2499
01:28:22,239 --> 01:28:26,800
for value objects so as a matter of

2500
01:28:24,800 --> 01:28:28,239
convenience so that we can create

2501
01:28:26,800 --> 01:28:29,120
expressions like this and make them make

2502
01:28:28,239 --> 01:28:32,320
sense

2503
01:28:29,120 --> 01:28:33,679
we can simply do something like this

2504
01:28:32,320 --> 01:28:35,920
basically

2505
01:28:33,679 --> 01:28:37,760
we let other alone if other is an

2506
01:28:35,920 --> 01:28:39,040
instance of value but if it's not an

2507
01:28:37,760 --> 01:28:40,880
instance of value we're going to assume

2508
01:28:39,040 --> 01:28:43,360
that it's a number like an integer float

2509
01:28:40,880 --> 01:28:45,199
and we're going to simply wrap it in in

2510
01:28:43,360 --> 01:28:46,880
value and then other will just become

2511
01:28:45,199 --> 01:28:49,280
value of other and then other will have

2512
01:28:46,880 --> 01:28:51,840
a data attribute and this should work so

2513
01:28:49,280 --> 01:28:53,199
if i just say this predefined value then

2514
01:28:51,840 --> 01:28:55,280
this should work

2515
01:28:53,199 --> 01:28:57,199
there we go okay now let's do the exact

2516
01:28:55,280 --> 01:28:58,880
same thing for multiply because we can't

2517
01:28:57,199 --> 01:28:59,760
do something like this

2518
01:28:58,880 --> 01:29:01,679
again

2519
01:28:59,760 --> 01:29:04,960
for the exact same reason so we just

2520
01:29:01,679 --> 01:29:07,520
have to go to mole and if other is

2521
01:29:04,960 --> 01:29:10,480
not a value then let's wrap it in value

2522
01:29:07,520 --> 01:29:12,800
let's redefine value and now this works

2523
01:29:10,480 --> 01:29:15,280
now here's a kind of unfortunate and not

2524
01:29:12,800 --> 01:29:19,600
obvious part a times two works we saw

2525
01:29:15,280 --> 01:29:21,920
that but two times a is that gonna work

2526
01:29:19,600 --> 01:29:22,960
you'd expect it to right but actually it

2527
01:29:21,920 --> 01:29:24,239
will not

2528
01:29:22,960 --> 01:29:26,080
and the reason it won't is because

2529
01:29:24,239 --> 01:29:28,480
python doesn't know

2530
01:29:26,080 --> 01:29:31,440
like when when you do a times two

2531
01:29:28,480 --> 01:29:32,880
basically um so a times two python will

2532
01:29:31,440 --> 01:29:34,719
go and it will basically do something

2533
01:29:32,880 --> 01:29:36,560
like a dot mul

2534
01:29:34,719 --> 01:29:39,679
of two that's basically what it will

2535
01:29:36,560 --> 01:29:41,840
call but to it 2 times a is the same as

2536
01:29:39,679 --> 01:29:44,960
2 dot mol of a

2537
01:29:41,840 --> 01:29:46,639
and it doesn't 2 can't multiply

2538
01:29:44,960 --> 01:29:47,360
value and so it's really confused about

2539
01:29:46,639 --> 01:29:49,199
that

2540
01:29:47,360 --> 01:29:51,760
so instead what happens is in python the

2541
01:29:49,199 --> 01:29:54,320
way this works is you are free to define

2542
01:29:51,760 --> 01:29:55,840
something called the r mold

2543
01:29:54,320 --> 01:29:58,320
and our mole

2544
01:29:55,840 --> 01:30:02,400
is kind of like a fallback so if python

2545
01:29:58,320 --> 01:30:05,040
can't do 2 times a it will check if um

2546
01:30:02,400 --> 01:30:07,199
if by any chance a knows how to multiply

2547
01:30:05,040 --> 01:30:08,800
two and that will be called into our

2548
01:30:07,199 --> 01:30:11,199
mole

2549
01:30:08,800 --> 01:30:12,800
so because python can't do two times a

2550
01:30:11,199 --> 01:30:15,360
it will check is there an our mole in

2551
01:30:12,800 --> 01:30:16,800
value and because there is it will now

2552
01:30:15,360 --> 01:30:18,960
call that

2553
01:30:16,800 --> 01:30:21,360
and what we'll do here is we will swap

2554
01:30:18,960 --> 01:30:23,520
the order of the operands so basically

2555
01:30:21,360 --> 01:30:26,080
two times a will redirect to armel and

2556
01:30:23,520 --> 01:30:28,320
our mole will basically call a times two

2557
01:30:26,080 --> 01:30:29,040
and that's how that will work

2558
01:30:28,320 --> 01:30:31,199
so

2559
01:30:29,040 --> 01:30:33,520
redefining now with armor two times a

2560
01:30:31,199 --> 01:30:34,880
becomes four okay now looking at the

2561
01:30:33,520 --> 01:30:36,239
other elements that we still need we

2562
01:30:34,880 --> 01:30:38,400
need to know how to exponentiate and how

2563
01:30:36,239 --> 01:30:40,480
to divide so let's first the explanation

2564
01:30:38,400 --> 01:30:41,760
to the exponentiation part we're going

2565
01:30:40,480 --> 01:30:42,960
to introduce

2566
01:30:41,760 --> 01:30:45,040
a single

2567
01:30:42,960 --> 01:30:47,920
function x here

2568
01:30:45,040 --> 01:30:49,679
and x is going to mirror 10h in the

2569
01:30:47,920 --> 01:30:51,199
sense that it's a simple single function

2570
01:30:49,679 --> 01:30:53,120
that transforms a single scalar value

2571
01:30:51,199 --> 01:30:56,000
and outputs a single scalar value

2572
01:30:53,120 --> 01:30:58,000
so we pop out the python number we use

2573
01:30:56,000 --> 01:30:59,199
math.x to exponentiate it create a new

2574
01:30:58,000 --> 01:31:00,880
value object

2575
01:30:59,199 --> 01:31:02,239
everything that we've seen before the

2576
01:31:00,880 --> 01:31:04,800
tricky part of course is how do you

2577
01:31:02,239 --> 01:31:05,920
propagate through e to the x

2578
01:31:04,800 --> 01:31:07,679
and

2579
01:31:05,920 --> 01:31:09,679
so here you can potentially pause the

2580
01:31:07,679 --> 01:31:11,920
video and think about what should go

2581
01:31:09,679 --> 01:31:11,920
here

2582
01:31:13,280 --> 01:31:18,639
okay so basically we need to know what

2583
01:31:15,840 --> 01:31:21,280
is the local derivative of e to the x so

2584
01:31:18,639 --> 01:31:23,120
d by d x of e to the x is famously just

2585
01:31:21,280 --> 01:31:25,520
e to the x and we've already just

2586
01:31:23,120 --> 01:31:27,760
calculated e to the x and it's inside

2587
01:31:25,520 --> 01:31:28,960
out that data so we can do up that data

2588
01:31:27,760 --> 01:31:29,760
times

2589
01:31:28,960 --> 01:31:32,000
and

2590
01:31:29,760 --> 01:31:33,840
out that grad that's the chain rule

2591
01:31:32,000 --> 01:31:35,280
so we're just chaining on to the current

2592
01:31:33,840 --> 01:31:36,719
running grad

2593
01:31:35,280 --> 01:31:38,639
and this is what the expression looks

2594
01:31:36,719 --> 01:31:40,000
like it looks a little confusing but

2595
01:31:38,639 --> 01:31:41,760
this is what it is and that's the

2596
01:31:40,000 --> 01:31:43,679
exponentiation

2597
01:31:41,760 --> 01:31:45,360
so redefining we should now be able to

2598
01:31:43,679 --> 01:31:46,159
call a.x

2599
01:31:45,360 --> 01:31:47,840
and

2600
01:31:46,159 --> 01:31:49,600
hopefully the backward pass works as

2601
01:31:47,840 --> 01:31:50,960
well okay and the last thing we'd like

2602
01:31:49,600 --> 01:31:52,239
to do of course is we'd like to be able

2603
01:31:50,960 --> 01:31:53,120
to divide

2604
01:31:52,239 --> 01:31:54,480
now

2605
01:31:53,120 --> 01:31:56,000
i actually will implement something

2606
01:31:54,480 --> 01:31:57,840
slightly more powerful than division

2607
01:31:56,000 --> 01:31:59,920
because division is just a special case

2608
01:31:57,840 --> 01:32:02,639
of something a bit more powerful

2609
01:31:59,920 --> 01:32:04,880
so in particular just by rearranging

2610
01:32:02,639 --> 01:32:07,600
if we have some kind of a b equals

2611
01:32:04,880 --> 01:32:09,199
value of 4.0 here we'd like to basically

2612
01:32:07,600 --> 01:32:11,600
be able to do a divide b and we'd like

2613
01:32:09,199 --> 01:32:14,239
this to be able to give us 0.5

2614
01:32:11,600 --> 01:32:17,040
now division actually can be reshuffled

2615
01:32:14,239 --> 01:32:18,719
as follows if we have a divide b that's

2616
01:32:17,040 --> 01:32:19,840
actually the same as a multiplying one

2617
01:32:18,719 --> 01:32:21,600
over b

2618
01:32:19,840 --> 01:32:24,239
and that's the same as a multiplying b

2619
01:32:21,600 --> 01:32:25,760
to the power of negative one

2620
01:32:24,239 --> 01:32:27,520
and so what i'd like to do instead is i

2621
01:32:25,760 --> 01:32:29,760
basically like to implement the

2622
01:32:27,520 --> 01:32:32,560
operation of x to the k for some

2623
01:32:29,760 --> 01:32:35,280
constant uh k so it's an integer or a

2624
01:32:32,560 --> 01:32:36,960
float um and we would like to be able to

2625
01:32:35,280 --> 01:32:40,800
differentiate this and then as a special

2626
01:32:36,960 --> 01:32:42,480
case uh negative one will be division

2627
01:32:40,800 --> 01:32:45,199
and so i'm doing that just because uh

2628
01:32:42,480 --> 01:32:46,960
it's more general and um yeah you might

2629
01:32:45,199 --> 01:32:49,440
as well do it that way so basically what

2630
01:32:46,960 --> 01:32:51,280
i'm saying is we can redefine

2631
01:32:49,440 --> 01:32:54,480
uh division

2632
01:32:51,280 --> 01:32:56,320
which we will put here somewhere

2633
01:32:54,480 --> 01:32:58,080
yeah we can put it here somewhere what

2634
01:32:56,320 --> 01:33:00,800
i'm saying is that we can redefine

2635
01:32:58,080 --> 01:33:03,120
division so self-divide other

2636
01:33:00,800 --> 01:33:05,679
can actually be rewritten as self times

2637
01:33:03,120 --> 01:33:07,360
other to the power of negative one

2638
01:33:05,679 --> 01:33:09,199
and now

2639
01:33:07,360 --> 01:33:11,600
a value raised to the power of negative

2640
01:33:09,199 --> 01:33:12,320
one we have now defined that

2641
01:33:11,600 --> 01:33:13,600
so

2642
01:33:12,320 --> 01:33:15,920
here's

2643
01:33:13,600 --> 01:33:17,280
so we need to implement the pow function

2644
01:33:15,920 --> 01:33:20,000
where am i going to put the power

2645
01:33:17,280 --> 01:33:22,480
function maybe here somewhere

2646
01:33:20,000 --> 01:33:24,320
this is the skeleton for it

2647
01:33:22,480 --> 01:33:26,719
so this function will be called when we

2648
01:33:24,320 --> 01:33:28,639
try to raise a value to some power and

2649
01:33:26,719 --> 01:33:30,560
other will be that power

2650
01:33:28,639 --> 01:33:33,040
now i'd like to make sure that other is

2651
01:33:30,560 --> 01:33:35,360
only an int or a float usually other is

2652
01:33:33,040 --> 01:33:37,360
some kind of a different value object

2653
01:33:35,360 --> 01:33:40,639
but here other will be forced to be an

2654
01:33:37,360 --> 01:33:42,159
end or a float otherwise the math

2655
01:33:40,639 --> 01:33:43,840
won't work for

2656
01:33:42,159 --> 01:33:45,360
for or try to achieve in the specific

2657
01:33:43,840 --> 01:33:47,840
case that would be a different

2658
01:33:45,360 --> 01:33:49,600
derivative expression if we wanted other

2659
01:33:47,840 --> 01:33:51,520
to be a value

2660
01:33:49,600 --> 01:33:53,679
so here we create the output value which

2661
01:33:51,520 --> 01:33:55,440
is just uh you know this data raised to

2662
01:33:53,679 --> 01:33:56,880
the power of other and other here could

2663
01:33:55,440 --> 01:33:59,360
be for example negative one that's what

2664
01:33:56,880 --> 01:34:01,840
we are hoping to achieve

2665
01:33:59,360 --> 01:34:03,840
and then uh this is the backwards stub

2666
01:34:01,840 --> 01:34:07,280
and this is the fun part which is what

2667
01:34:03,840 --> 01:34:09,600
is the uh chain rule expression here for

2668
01:34:07,280 --> 01:34:11,600
back for um

2669
01:34:09,600 --> 01:34:13,920
back propagating through the power

2670
01:34:11,600 --> 01:34:15,679
function where the power is to the power

2671
01:34:13,920 --> 01:34:17,280
of some kind of a constant

2672
01:34:15,679 --> 01:34:18,639
so this is the exercise and maybe pause

2673
01:34:17,280 --> 01:34:20,480
the video here and see if you can figure

2674
01:34:18,639 --> 01:34:22,719
it out yourself as to what we should put

2675
01:34:20,480 --> 01:34:22,719
here

2676
01:34:26,960 --> 01:34:30,800
okay so

2677
01:34:29,040 --> 01:34:32,800
you can actually go here and look at

2678
01:34:30,800 --> 01:34:34,320
derivative rules as an example and we

2679
01:34:32,800 --> 01:34:36,000
see lots of derivatives that you can

2680
01:34:34,320 --> 01:34:37,760
hopefully know from calculus in

2681
01:34:36,000 --> 01:34:39,120
particular what we're looking for is the

2682
01:34:37,760 --> 01:34:40,320
power rule

2683
01:34:39,120 --> 01:34:42,719
because that's telling us that if we're

2684
01:34:40,320 --> 01:34:44,560
trying to take d by dx of x to the n

2685
01:34:42,719 --> 01:34:46,960
which is what we're doing here

2686
01:34:44,560 --> 01:34:48,400
then that is just n times x to the n

2687
01:34:46,960 --> 01:34:49,520
minus 1

2688
01:34:48,400 --> 01:34:50,880
right

2689
01:34:49,520 --> 01:34:51,600
okay

2690
01:34:50,880 --> 01:34:53,280
so

2691
01:34:51,600 --> 01:34:55,920
that's telling us about the local

2692
01:34:53,280 --> 01:34:58,320
derivative of this power operation

2693
01:34:55,920 --> 01:35:00,719
so all we want here

2694
01:34:58,320 --> 01:35:03,440
basically n is now other

2695
01:35:00,719 --> 01:35:06,080
and self.data is x

2696
01:35:03,440 --> 01:35:08,560
and so this now becomes

2697
01:35:06,080 --> 01:35:10,239
other which is n times

2698
01:35:08,560 --> 01:35:13,040
self.data

2699
01:35:10,239 --> 01:35:14,719
which is now a python in torah float

2700
01:35:13,040 --> 01:35:16,159
it's not a valley object we're accessing

2701
01:35:14,719 --> 01:35:17,280
the data attribute

2702
01:35:16,159 --> 01:35:19,760
raised

2703
01:35:17,280 --> 01:35:21,119
to the power of other minus one or n

2704
01:35:19,760 --> 01:35:22,639
minus one

2705
01:35:21,119 --> 01:35:25,280
i can put brackets around this but this

2706
01:35:22,639 --> 01:35:27,360
doesn't matter because

2707
01:35:25,280 --> 01:35:29,440
power takes precedence over multiply and

2708
01:35:27,360 --> 01:35:31,360
python so that would have been okay

2709
01:35:29,440 --> 01:35:33,360
and that's the local derivative only but

2710
01:35:31,360 --> 01:35:34,880
now we have to chain it and we change

2711
01:35:33,360 --> 01:35:36,719
just simply by multiplying by output

2712
01:35:34,880 --> 01:35:40,719
grad that's chain rule

2713
01:35:36,719 --> 01:35:42,960
and this should technically work

2714
01:35:40,719 --> 01:35:43,840
and we're going to find out soon but now

2715
01:35:42,960 --> 01:35:46,719
if we

2716
01:35:43,840 --> 01:35:49,199
do this this should now work

2717
01:35:46,719 --> 01:35:51,440
and we get 0.5 so the forward pass works

2718
01:35:49,199 --> 01:35:52,719
but does the backward pass work and i

2719
01:35:51,440 --> 01:35:54,719
realize that we actually also have to

2720
01:35:52,719 --> 01:35:57,360
know how to subtract so

2721
01:35:54,719 --> 01:36:00,080
right now a minus b will not work

2722
01:35:57,360 --> 01:36:01,760
to make it work we need one more

2723
01:36:00,080 --> 01:36:02,960
piece of code here

2724
01:36:01,760 --> 01:36:05,360
and

2725
01:36:02,960 --> 01:36:06,800
basically this is the

2726
01:36:05,360 --> 01:36:08,800
subtraction and the way we're going to

2727
01:36:06,800 --> 01:36:10,719
implement subtraction is we're going to

2728
01:36:08,800 --> 01:36:12,159
implement it by addition of a negation

2729
01:36:10,719 --> 01:36:14,159
and then to implement negation we're

2730
01:36:12,159 --> 01:36:15,520
gonna multiply by negative one so just

2731
01:36:14,159 --> 01:36:17,840
again using the stuff we've already

2732
01:36:15,520 --> 01:36:20,159
built and just um expressing it in terms

2733
01:36:17,840 --> 01:36:22,320
of what we have and a minus b is now

2734
01:36:20,159 --> 01:36:25,199
working okay so now let's scroll again

2735
01:36:22,320 --> 01:36:26,880
to this expression here for this neuron

2736
01:36:25,199 --> 01:36:28,480
and let's just

2737
01:36:26,880 --> 01:36:30,000
compute the backward pass here once

2738
01:36:28,480 --> 01:36:32,000
we've defined o

2739
01:36:30,000 --> 01:36:33,679
and let's draw it

2740
01:36:32,000 --> 01:36:35,119
so here's the gradients for all these

2741
01:36:33,679 --> 01:36:37,280
leaf nodes for this two-dimensional

2742
01:36:35,119 --> 01:36:39,440
neuron that has a 10h that we've seen

2743
01:36:37,280 --> 01:36:41,920
before so now what i'd like to do is i'd

2744
01:36:39,440 --> 01:36:44,320
like to break up this 10h

2745
01:36:41,920 --> 01:36:46,239
into this expression here

2746
01:36:44,320 --> 01:36:47,360
so let me copy paste this

2747
01:36:46,239 --> 01:36:49,280
here

2748
01:36:47,360 --> 01:36:50,239
and now instead of we'll preserve the

2749
01:36:49,280 --> 01:36:53,679
label

2750
01:36:50,239 --> 01:36:55,040
and we will change how we define o

2751
01:36:53,679 --> 01:36:56,719
so in particular we're going to

2752
01:36:55,040 --> 01:36:58,560
implement this formula here

2753
01:36:56,719 --> 01:37:01,440
so we need e to the 2x

2754
01:36:58,560 --> 01:37:04,719
minus 1 over e to the x plus 1. so e to

2755
01:37:01,440 --> 01:37:07,119
the 2x we need to take 2 times n and we

2756
01:37:04,719 --> 01:37:08,960
need to exponentiate it that's e to the

2757
01:37:07,119 --> 01:37:10,800
two x and then because we're using it

2758
01:37:08,960 --> 01:37:12,560
twice let's create an intermediate

2759
01:37:10,800 --> 01:37:14,960
variable e

2760
01:37:12,560 --> 01:37:16,800
and then define o as

2761
01:37:14,960 --> 01:37:19,199
e plus one over

2762
01:37:16,800 --> 01:37:22,800
e minus one over e plus one

2763
01:37:19,199 --> 01:37:24,719
e minus one over e plus one

2764
01:37:22,800 --> 01:37:26,960
and that should be it and then we should

2765
01:37:24,719 --> 01:37:29,280
be able to draw that of o

2766
01:37:26,960 --> 01:37:30,880
so now before i run this what do we

2767
01:37:29,280 --> 01:37:32,639
expect to see

2768
01:37:30,880 --> 01:37:33,679
number one we're expecting to see a much

2769
01:37:32,639 --> 01:37:35,679
longer

2770
01:37:33,679 --> 01:37:37,600
graph here because we've broken up 10h

2771
01:37:35,679 --> 01:37:39,360
into a bunch of other operations

2772
01:37:37,600 --> 01:37:41,040
but those operations are mathematically

2773
01:37:39,360 --> 01:37:43,760
equivalent and so what we're expecting

2774
01:37:41,040 --> 01:37:45,920
to see is number one the same

2775
01:37:43,760 --> 01:37:47,199
result here so the forward pass works

2776
01:37:45,920 --> 01:37:49,119
and number two because of that

2777
01:37:47,199 --> 01:37:51,040
mathematical equivalence we expect to

2778
01:37:49,119 --> 01:37:53,199
see the same backward pass and the same

2779
01:37:51,040 --> 01:37:55,199
gradients on these leaf nodes so these

2780
01:37:53,199 --> 01:37:58,000
gradients should be identical

2781
01:37:55,199 --> 01:38:00,320
so let's run this

2782
01:37:58,000 --> 01:38:03,280
so number one let's verify that instead

2783
01:38:00,320 --> 01:38:06,880
of a single 10h node we have now x and

2784
01:38:03,280 --> 01:38:08,719
we have plus we have times negative one

2785
01:38:06,880 --> 01:38:10,880
uh this is the division

2786
01:38:08,719 --> 01:38:11,760
and we end up with the same forward pass

2787
01:38:10,880 --> 01:38:13,119
here

2788
01:38:11,760 --> 01:38:14,239
and then the gradients we have to be

2789
01:38:13,119 --> 01:38:16,320
careful because they're in slightly

2790
01:38:14,239 --> 01:38:19,440
different order potentially the

2791
01:38:16,320 --> 01:38:22,000
gradients for w2x2 should be 0 and 0.5

2792
01:38:19,440 --> 01:38:25,199
w2 and x2 are 0 and 0.5

2793
01:38:22,000 --> 01:38:27,199
and w1 x1 are 1 and negative 1.5

2794
01:38:25,199 --> 01:38:28,880
1 and negative 1.5

2795
01:38:27,199 --> 01:38:31,040
so that means that both our forward

2796
01:38:28,880 --> 01:38:33,119
passes and backward passes were correct

2797
01:38:31,040 --> 01:38:34,000
because this turned out to be equivalent

2798
01:38:33,119 --> 01:38:35,760
to

2799
01:38:34,000 --> 01:38:37,280
10h before

2800
01:38:35,760 --> 01:38:39,199
and so the reason i wanted to go through

2801
01:38:37,280 --> 01:38:41,600
this exercise is number one we got to

2802
01:38:39,199 --> 01:38:43,440
practice a few more operations and uh

2803
01:38:41,600 --> 01:38:45,280
writing more backwards passes and number

2804
01:38:43,440 --> 01:38:46,159
two i wanted to illustrate the point

2805
01:38:45,280 --> 01:38:47,760
that

2806
01:38:46,159 --> 01:38:49,520
the um

2807
01:38:47,760 --> 01:38:51,679
the level at which you implement your

2808
01:38:49,520 --> 01:38:53,440
operations is totally up to you you can

2809
01:38:51,679 --> 01:38:54,880
implement backward passes for tiny

2810
01:38:53,440 --> 01:38:56,639
expressions like a single individual

2811
01:38:54,880 --> 01:38:58,960
plus or a single times

2812
01:38:56,639 --> 01:39:00,000
or you can implement them for say

2813
01:38:58,960 --> 01:39:01,679
10h

2814
01:39:00,000 --> 01:39:03,199
which is a kind of a potentially you can

2815
01:39:01,679 --> 01:39:05,280
see it as a composite operation because

2816
01:39:03,199 --> 01:39:07,119
it's made up of all these more atomic

2817
01:39:05,280 --> 01:39:08,800
operations but really all of this is

2818
01:39:07,119 --> 01:39:10,400
kind of like a fake concept all that

2819
01:39:08,800 --> 01:39:11,679
matters is we have some kind of inputs

2820
01:39:10,400 --> 01:39:13,040
and some kind of an output and this

2821
01:39:11,679 --> 01:39:14,800
output is a function of the inputs in

2822
01:39:13,040 --> 01:39:16,639
some way and as long as you can do

2823
01:39:14,800 --> 01:39:19,040
forward pass and the backward pass of

2824
01:39:16,639 --> 01:39:21,199
that little operation it doesn't matter

2825
01:39:19,040 --> 01:39:23,040
what that operation is

2826
01:39:21,199 --> 01:39:24,639
and how composite it is

2827
01:39:23,040 --> 01:39:26,000
if you can write the local gradients you

2828
01:39:24,639 --> 01:39:28,159
can chain the gradient and you can

2829
01:39:26,000 --> 01:39:30,080
continue back propagation so the design

2830
01:39:28,159 --> 01:39:31,920
of what those functions are is

2831
01:39:30,080 --> 01:39:33,280
completely up to you

2832
01:39:31,920 --> 01:39:35,280
so now i would like to show you how you

2833
01:39:33,280 --> 01:39:37,199
can do the exact same thing by using a

2834
01:39:35,280 --> 01:39:40,000
modern deep neural network library like

2835
01:39:37,199 --> 01:39:41,760
for example pytorch which i've roughly

2836
01:39:40,000 --> 01:39:42,800
modeled micrograd

2837
01:39:41,760 --> 01:39:43,679
by

2838
01:39:42,800 --> 01:39:44,960
and so

2839
01:39:43,679 --> 01:39:46,639
pytorch is something you would use in

2840
01:39:44,960 --> 01:39:48,639
production and i'll show you how you can

2841
01:39:46,639 --> 01:39:50,880
do the exact same thing but in pytorch

2842
01:39:48,639 --> 01:39:52,560
api so i'm just going to copy paste it

2843
01:39:50,880 --> 01:39:54,719
in and walk you through it a little bit

2844
01:39:52,560 --> 01:39:56,880
this is what it looks like

2845
01:39:54,719 --> 01:39:59,600
so we're going to import pi torch and

2846
01:39:56,880 --> 01:40:01,679
then we need to define these

2847
01:39:59,600 --> 01:40:04,800
value objects like we have here

2848
01:40:01,679 --> 01:40:07,440
now micrograd is a scalar valued

2849
01:40:04,800 --> 01:40:10,000
engine so we only have scalar values

2850
01:40:07,440 --> 01:40:11,760
like 2.0 but in pi torch everything is

2851
01:40:10,000 --> 01:40:13,840
based around tensors and like i

2852
01:40:11,760 --> 01:40:15,760
mentioned tensors are just n-dimensional

2853
01:40:13,840 --> 01:40:17,600
arrays of scalars

2854
01:40:15,760 --> 01:40:19,840
so that's why things get a little bit

2855
01:40:17,600 --> 01:40:21,760
more complicated here i just need a

2856
01:40:19,840 --> 01:40:23,360
scalar value to tensor a tensor with

2857
01:40:21,760 --> 01:40:25,280
just a single element

2858
01:40:23,360 --> 01:40:28,480
but by default when you work with

2859
01:40:25,280 --> 01:40:30,639
pytorch you would use um

2860
01:40:28,480 --> 01:40:33,600
more complicated tensors like this so if

2861
01:40:30,639 --> 01:40:33,600
i import pytorch

2862
01:40:33,920 --> 01:40:38,320
then i can create tensors like this and

2863
01:40:36,239 --> 01:40:39,760
this tensor for example is a two by

2864
01:40:38,320 --> 01:40:41,119
three array

2865
01:40:39,760 --> 01:40:42,480
of scalar

2866
01:40:41,119 --> 01:40:45,199
scalars

2867
01:40:42,480 --> 01:40:46,560
in a single compact representation so we

2868
01:40:45,199 --> 01:40:48,000
can check its shape we see that it's a

2869
01:40:46,560 --> 01:40:49,440
two by three array

2870
01:40:48,000 --> 01:40:50,800
and so on

2871
01:40:49,440 --> 01:40:54,000
so this is usually what you would work

2872
01:40:50,800 --> 01:40:55,280
with um in the actual libraries so here

2873
01:40:54,000 --> 01:40:58,080
i'm creating

2874
01:40:55,280 --> 01:41:00,480
a tensor that has only a single element

2875
01:40:58,080 --> 01:41:03,520
2.0

2876
01:41:00,480 --> 01:41:05,760
and then i'm casting it to be double

2877
01:41:03,520 --> 01:41:07,360
because python is by default using

2878
01:41:05,760 --> 01:41:08,960
double precision for its floating point

2879
01:41:07,360 --> 01:41:12,400
numbers so i'd like everything to be

2880
01:41:08,960 --> 01:41:14,800
identical by default the data type of

2881
01:41:12,400 --> 01:41:16,719
these tensors will be float32 so it's

2882
01:41:14,800 --> 01:41:18,960
only using a single precision float so

2883
01:41:16,719 --> 01:41:21,199
i'm casting it to double

2884
01:41:18,960 --> 01:41:22,639
so that we have float64 just like in

2885
01:41:21,199 --> 01:41:24,719
python

2886
01:41:22,639 --> 01:41:28,080
so i'm casting to double and then we get

2887
01:41:24,719 --> 01:41:29,600
something similar to value of two the

2888
01:41:28,080 --> 01:41:31,520
next thing i have to do is because these

2889
01:41:29,600 --> 01:41:32,960
are leaf nodes by default pytorch

2890
01:41:31,520 --> 01:41:35,199
assumes that they do not require

2891
01:41:32,960 --> 01:41:36,800
gradients so i need to explicitly say

2892
01:41:35,199 --> 01:41:37,920
that all of these nodes require

2893
01:41:36,800 --> 01:41:39,840
gradients

2894
01:41:37,920 --> 01:41:43,199
okay so this is going to construct

2895
01:41:39,840 --> 01:41:44,480
scalar valued one element tensors

2896
01:41:43,199 --> 01:41:47,119
make sure that fighters knows that they

2897
01:41:44,480 --> 01:41:48,880
require gradients now by default these

2898
01:41:47,119 --> 01:41:50,800
are set to false by the way because of

2899
01:41:48,880 --> 01:41:53,600
efficiency reasons because usually you

2900
01:41:50,800 --> 01:41:55,760
would not want gradients for leaf nodes

2901
01:41:53,600 --> 01:41:57,199
like the inputs to the network and this

2902
01:41:55,760 --> 01:41:59,199
is just trying to be efficient in the

2903
01:41:57,199 --> 01:42:01,440
most common cases

2904
01:41:59,199 --> 01:42:03,920
so once we've defined all of our values

2905
01:42:01,440 --> 01:42:06,000
in python we can perform arithmetic just

2906
01:42:03,920 --> 01:42:07,600
like we can here in microgradlend so

2907
01:42:06,000 --> 01:42:09,679
this will just work and then there's a

2908
01:42:07,600 --> 01:42:12,480
torch.10h also

2909
01:42:09,679 --> 01:42:13,520
and when we get back is a tensor again

2910
01:42:12,480 --> 01:42:15,600
and we can

2911
01:42:13,520 --> 01:42:18,239
just like in micrograd it's got a data

2912
01:42:15,600 --> 01:42:19,920
attribute and it's got grant attributes

2913
01:42:18,239 --> 01:42:22,719
so these tensor objects just like in

2914
01:42:19,920 --> 01:42:23,440
micrograd have a dot data and a dot grad

2915
01:42:22,719 --> 01:42:25,119
and

2916
01:42:23,440 --> 01:42:28,080
the only difference here is that we need

2917
01:42:25,119 --> 01:42:30,400
to call it that item because otherwise

2918
01:42:28,080 --> 01:42:32,159
um pi torch

2919
01:42:30,400 --> 01:42:34,080
that item basically takes

2920
01:42:32,159 --> 01:42:36,239
a single tensor of one element and it

2921
01:42:34,080 --> 01:42:37,760
just returns that element stripping out

2922
01:42:36,239 --> 01:42:39,520
the tensor

2923
01:42:37,760 --> 01:42:41,280
so let me just run this and hopefully we

2924
01:42:39,520 --> 01:42:42,480
are going to get this is going to print

2925
01:42:41,280 --> 01:42:44,880
the forward pass

2926
01:42:42,480 --> 01:42:46,800
which is 0.707

2927
01:42:44,880 --> 01:42:48,320
and this will be the gradients which

2928
01:42:46,800 --> 01:42:51,119
hopefully are

2929
01:42:48,320 --> 01:42:53,760
0.5 0 negative 1.5 and 1.

2930
01:42:51,119 --> 01:42:54,960
so if we just run this

2931
01:42:53,760 --> 01:42:57,440
there we go

2932
01:42:54,960 --> 01:42:59,360
0.7 so the forward pass agrees and then

2933
01:42:57,440 --> 01:43:00,639
point five zero negative one point five

2934
01:42:59,360 --> 01:43:02,639
and one

2935
01:43:00,639 --> 01:43:05,040
so pi torch agrees with us

2936
01:43:02,639 --> 01:43:08,239
and just to show you here basically o

2937
01:43:05,040 --> 01:43:09,520
here's a tensor with a single element

2938
01:43:08,239 --> 01:43:12,000
and it's a double

2939
01:43:09,520 --> 01:43:14,320
and we can call that item on it to just

2940
01:43:12,000 --> 01:43:16,880
get the single number out

2941
01:43:14,320 --> 01:43:18,960
so that's what item does and o is a

2942
01:43:16,880 --> 01:43:20,480
tensor object like i mentioned and it's

2943
01:43:18,960 --> 01:43:22,080
got a backward function just like we've

2944
01:43:20,480 --> 01:43:23,840
implemented

2945
01:43:22,080 --> 01:43:26,080
and then all of these also have a dot

2946
01:43:23,840 --> 01:43:28,080
graph so like x2 for example in the grad

2947
01:43:26,080 --> 01:43:31,440
and it's a tensor and we can pop out the

2948
01:43:28,080 --> 01:43:32,960
individual number with that actin

2949
01:43:31,440 --> 01:43:35,040
so basically

2950
01:43:32,960 --> 01:43:37,199
torches torch can do what we did in

2951
01:43:35,040 --> 01:43:40,400
micrograph is a special case when your

2952
01:43:37,199 --> 01:43:42,159
tensors are all single element tensors

2953
01:43:40,400 --> 01:43:43,280
but the big deal with pytorch is that

2954
01:43:42,159 --> 01:43:45,119
everything is significantly more

2955
01:43:43,280 --> 01:43:47,280
efficient because we are working with

2956
01:43:45,119 --> 01:43:49,199
these tensor objects and we can do lots

2957
01:43:47,280 --> 01:43:51,440
of operations in parallel on all of

2958
01:43:49,199 --> 01:43:53,520
these tensors

2959
01:43:51,440 --> 01:43:55,600
but otherwise what we've built very much

2960
01:43:53,520 --> 01:43:57,199
agrees with the api of pytorch

2961
01:43:55,600 --> 01:43:58,320
okay so now that we have some machinery

2962
01:43:57,199 --> 01:44:00,480
to build out pretty complicated

2963
01:43:58,320 --> 01:44:02,239
mathematical expressions we can also

2964
01:44:00,480 --> 01:44:03,440
start building out neural nets and as i

2965
01:44:02,239 --> 01:44:05,520
mentioned neural nets are just a

2966
01:44:03,440 --> 01:44:07,040
specific class of mathematical

2967
01:44:05,520 --> 01:44:08,159
expressions

2968
01:44:07,040 --> 01:44:09,840
so we're going to start building out a

2969
01:44:08,159 --> 01:44:12,159
neural net piece by piece and eventually

2970
01:44:09,840 --> 01:44:14,239
we'll build out a two-layer multi-layer

2971
01:44:12,159 --> 01:44:15,920
layer perceptron as it's called and i'll

2972
01:44:14,239 --> 01:44:17,119
show you exactly what that means

2973
01:44:15,920 --> 01:44:19,679
let's start with a single individual

2974
01:44:17,119 --> 01:44:21,360
neuron we've implemented one here but

2975
01:44:19,679 --> 01:44:24,159
here i'm going to implement one that

2976
01:44:21,360 --> 01:44:26,239
also subscribes to the pytorch api in

2977
01:44:24,159 --> 01:44:27,280
how it designs its neural network

2978
01:44:26,239 --> 01:44:28,880
modules

2979
01:44:27,280 --> 01:44:31,199
so just like we saw that we can like

2980
01:44:28,880 --> 01:44:33,600
match the api of pytorch

2981
01:44:31,199 --> 01:44:35,840
on the auto grad side we're going to try

2982
01:44:33,600 --> 01:44:38,400
to do that on the neural network modules

2983
01:44:35,840 --> 01:44:40,960
so here's class neuron

2984
01:44:38,400 --> 01:44:42,560
and just for the sake of efficiency i'm

2985
01:44:40,960 --> 01:44:45,520
going to copy paste some sections that

2986
01:44:42,560 --> 01:44:47,520
are relatively straightforward

2987
01:44:45,520 --> 01:44:49,760
so the constructor will take

2988
01:44:47,520 --> 01:44:52,719
number of inputs to this neuron which is

2989
01:44:49,760 --> 01:44:55,119
how many inputs come to a neuron so this

2990
01:44:52,719 --> 01:44:57,199
one for example has three inputs

2991
01:44:55,119 --> 01:44:58,400
and then it's going to create a weight

2992
01:44:57,199 --> 01:45:00,080
there is some random number between

2993
01:44:58,400 --> 01:45:01,199
negative one and one for every one of

2994
01:45:00,080 --> 01:45:03,679
those inputs

2995
01:45:01,199 --> 01:45:06,480
and a bias that controls the overall

2996
01:45:03,679 --> 01:45:08,639
trigger happiness of this neuron

2997
01:45:06,480 --> 01:45:11,280
and then we're going to implement a def

2998
01:45:08,639 --> 01:45:14,000
underscore underscore call

2999
01:45:11,280 --> 01:45:15,840
of self and x some input x

3000
01:45:14,000 --> 01:45:17,360
and really what we don't do here is w

3001
01:45:15,840 --> 01:45:19,520
times x plus b

3002
01:45:17,360 --> 01:45:21,280
where w times x here is a dot product

3003
01:45:19,520 --> 01:45:22,800
specifically

3004
01:45:21,280 --> 01:45:24,000
now if you haven't seen

3005
01:45:22,800 --> 01:45:26,719
call

3006
01:45:24,000 --> 01:45:28,320
let me just return 0.0 here for now the

3007
01:45:26,719 --> 01:45:31,119
way this works now is we can have an x

3008
01:45:28,320 --> 01:45:32,480
which is say like 2.0 3.0 then we can

3009
01:45:31,119 --> 01:45:33,920
initialize a neuron that is

3010
01:45:32,480 --> 01:45:35,679
two-dimensional

3011
01:45:33,920 --> 01:45:37,040
because these are two numbers and then

3012
01:45:35,679 --> 01:45:39,760
we can feed those two numbers into that

3013
01:45:37,040 --> 01:45:42,639
neuron to get an output

3014
01:45:39,760 --> 01:45:45,040
and so when you use this notation n of x

3015
01:45:42,639 --> 01:45:49,040
python will use call

3016
01:45:45,040 --> 01:45:49,040
so currently call just return 0.0

3017
01:45:50,080 --> 01:45:54,880
now we'd like to actually do the forward

3018
01:45:52,080 --> 01:45:57,280
pass of this neuron instead

3019
01:45:54,880 --> 01:45:58,960
so we're going to do here first is we

3020
01:45:57,280 --> 01:46:01,360
need to basically multiply all of the

3021
01:45:58,960 --> 01:46:04,239
elements of w with all of the elements

3022
01:46:01,360 --> 01:46:05,600
of x pairwise we need to multiply them

3023
01:46:04,239 --> 01:46:07,119
so the first thing we're going to do is

3024
01:46:05,600 --> 01:46:09,040
we're going to zip up

3025
01:46:07,119 --> 01:46:12,560
celta w and x

3026
01:46:09,040 --> 01:46:14,480
and in python zip takes two iterators

3027
01:46:12,560 --> 01:46:16,159
and it creates a new iterator that

3028
01:46:14,480 --> 01:46:17,920
iterates over the tuples of the

3029
01:46:16,159 --> 01:46:20,000
corresponding entries

3030
01:46:17,920 --> 01:46:22,000
so for example just to show you we can

3031
01:46:20,000 --> 01:46:26,600
print this list

3032
01:46:22,000 --> 01:46:26,600
and still return 0.0 here

3033
01:46:30,800 --> 01:46:33,040
sorry

3034
01:46:34,080 --> 01:46:41,080
so we see that these w's are paired up

3035
01:46:36,239 --> 01:46:41,080
with the x's w with x

3036
01:46:41,440 --> 01:46:44,639
and now what we want to do is

3037
01:46:47,360 --> 01:46:52,719
for w i x i in

3038
01:46:50,639 --> 01:46:54,880
we want to multiply w times

3039
01:46:52,719 --> 01:46:56,560
w wi times x i

3040
01:46:54,880 --> 01:46:57,520
and then we want to sum all of that

3041
01:46:56,560 --> 01:46:59,600
together

3042
01:46:57,520 --> 01:47:02,159
to come up with an activation

3043
01:46:59,600 --> 01:47:04,239
and add also subnet b on top

3044
01:47:02,159 --> 01:47:05,440
so that's the raw activation and then of

3045
01:47:04,239 --> 01:47:07,280
course we need to pass that through a

3046
01:47:05,440 --> 01:47:09,840
non-linearity so what we're going to be

3047
01:47:07,280 --> 01:47:12,239
returning is act.10h

3048
01:47:09,840 --> 01:47:13,119
and here's out

3049
01:47:12,239 --> 01:47:14,560
so

3050
01:47:13,119 --> 01:47:16,320
now we see that we are getting some

3051
01:47:14,560 --> 01:47:17,679
outputs and we get a different output

3052
01:47:16,320 --> 01:47:19,440
from a neuron each time because we are

3053
01:47:17,679 --> 01:47:21,199
initializing different weights and by

3054
01:47:19,440 --> 01:47:22,639
and biases

3055
01:47:21,199 --> 01:47:25,440
and then to be a bit more efficient here

3056
01:47:22,639 --> 01:47:28,880
actually sum by the way takes a second

3057
01:47:25,440 --> 01:47:31,679
optional parameter which is the start

3058
01:47:28,880 --> 01:47:34,080
and by default the start is zero so

3059
01:47:31,679 --> 01:47:35,760
these elements of this sum will be added

3060
01:47:34,080 --> 01:47:37,199
on top of zero to begin with but

3061
01:47:35,760 --> 01:47:38,400
actually we can just start with cell dot

3062
01:47:37,199 --> 01:47:39,760
b

3063
01:47:38,400 --> 01:47:42,000
and then we just have an expression like

3064
01:47:39,760 --> 01:47:42,000
this

3065
01:47:45,440 --> 01:47:49,360
and then the generator expression here

3066
01:47:47,040 --> 01:47:51,840
must be parenthesized in python

3067
01:47:49,360 --> 01:47:51,840
there we go

3068
01:47:53,760 --> 01:47:57,920
yep so now we can forward a single

3069
01:47:55,679 --> 01:47:59,920
neuron next up we're going to define a

3070
01:47:57,920 --> 01:48:02,480
layer of neurons so here we have a

3071
01:47:59,920 --> 01:48:05,119
schematic for a mlb

3072
01:48:02,480 --> 01:48:07,040
so we see that these mlps each layer

3073
01:48:05,119 --> 01:48:08,639
this is one layer has actually a number

3074
01:48:07,040 --> 01:48:09,760
of neurons and they're not connected to

3075
01:48:08,639 --> 01:48:11,280
each other but all of them are fully

3076
01:48:09,760 --> 01:48:13,440
connected to the input

3077
01:48:11,280 --> 01:48:15,119
so what is a layer of neurons it's just

3078
01:48:13,440 --> 01:48:16,719
it's just a set of neurons evaluated

3079
01:48:15,119 --> 01:48:17,760
independently

3080
01:48:16,719 --> 01:48:20,000
so

3081
01:48:17,760 --> 01:48:23,040
in the interest of time i'm going to do

3082
01:48:20,000 --> 01:48:25,040
something fairly straightforward here

3083
01:48:23,040 --> 01:48:27,679
it's um

3084
01:48:25,040 --> 01:48:28,960
literally a layer is just a list of

3085
01:48:27,679 --> 01:48:30,800
neurons

3086
01:48:28,960 --> 01:48:32,719
and then how many neurons do we have we

3087
01:48:30,800 --> 01:48:34,080
take that as an input argument here how

3088
01:48:32,719 --> 01:48:36,560
many neurons do you want in your layer

3089
01:48:34,080 --> 01:48:38,400
number of outputs in this layer

3090
01:48:36,560 --> 01:48:40,239
and so we just initialize completely

3091
01:48:38,400 --> 01:48:43,119
independent neurons with this given

3092
01:48:40,239 --> 01:48:44,960
dimensionality and when we call on it we

3093
01:48:43,119 --> 01:48:47,760
just independently

3094
01:48:44,960 --> 01:48:49,840
evaluate them so now instead of a neuron

3095
01:48:47,760 --> 01:48:51,199
we can make a layer of neurons they are

3096
01:48:49,840 --> 01:48:52,320
two-dimensional neurons and let's have

3097
01:48:51,199 --> 01:48:53,840
three of them

3098
01:48:52,320 --> 01:48:55,760
and now we see that we have three

3099
01:48:53,840 --> 01:48:57,199
independent evaluations of three

3100
01:48:55,760 --> 01:48:58,800
different neurons

3101
01:48:57,199 --> 01:49:00,880
right

3102
01:48:58,800 --> 01:49:02,639
okay finally let's complete this picture

3103
01:49:00,880 --> 01:49:04,560
and define an entire multi-layer

3104
01:49:02,639 --> 01:49:06,639
perceptron or mlp

3105
01:49:04,560 --> 01:49:07,840
and as we can see here in an mlp these

3106
01:49:06,639 --> 01:49:09,199
layers just feed into each other

3107
01:49:07,840 --> 01:49:11,600
sequentially

3108
01:49:09,199 --> 01:49:14,320
so let's come here and i'm just going to

3109
01:49:11,600 --> 01:49:16,639
copy the code here in interest of time

3110
01:49:14,320 --> 01:49:18,719
so an mlp is very similar

3111
01:49:16,639 --> 01:49:20,800
we're taking the number of inputs

3112
01:49:18,719 --> 01:49:22,639
as before but now instead of taking a

3113
01:49:20,800 --> 01:49:24,719
single n out which is number of neurons

3114
01:49:22,639 --> 01:49:26,960
in a single layer we're going to take a

3115
01:49:24,719 --> 01:49:28,880
list of an outs and this list defines

3116
01:49:26,960 --> 01:49:30,320
the sizes of all the layers that we want

3117
01:49:28,880 --> 01:49:31,920
in our mlp

3118
01:49:30,320 --> 01:49:34,400
so here we just put them all together

3119
01:49:31,920 --> 01:49:36,800
and then iterate over consecutive pairs

3120
01:49:34,400 --> 01:49:37,840
of these sizes and create layer objects

3121
01:49:36,800 --> 01:49:39,199
for them

3122
01:49:37,840 --> 01:49:41,119
and then in the call function we are

3123
01:49:39,199 --> 01:49:42,800
just calling them sequentially so that's

3124
01:49:41,119 --> 01:49:44,159
an mlp really

3125
01:49:42,800 --> 01:49:46,560
and let's actually re-implement this

3126
01:49:44,159 --> 01:49:48,000
picture so we want three input neurons

3127
01:49:46,560 --> 01:49:49,679
and then two layers of four and an

3128
01:49:48,000 --> 01:49:50,560
output unit

3129
01:49:49,679 --> 01:49:52,400
so

3130
01:49:50,560 --> 01:49:54,320
we want

3131
01:49:52,400 --> 01:49:57,520
a three-dimensional input say this is an

3132
01:49:54,320 --> 01:50:00,560
example input we want three inputs into

3133
01:49:57,520 --> 01:50:03,599
two layers of four and one output

3134
01:50:00,560 --> 01:50:05,440
and this of course is an mlp

3135
01:50:03,599 --> 01:50:06,719
and there we go that's a forward pass of

3136
01:50:05,440 --> 01:50:08,400
an mlp

3137
01:50:06,719 --> 01:50:09,920
to make this a little bit nicer you see

3138
01:50:08,400 --> 01:50:11,599
how we have just a single element but

3139
01:50:09,920 --> 01:50:13,599
it's wrapped in a list because layer

3140
01:50:11,599 --> 01:50:15,599
always returns lists

3141
01:50:13,599 --> 01:50:18,639
circle for convenience

3142
01:50:15,599 --> 01:50:20,560
return outs at zero if len out is

3143
01:50:18,639 --> 01:50:22,400
exactly a single element

3144
01:50:20,560 --> 01:50:23,599
else return fullest

3145
01:50:22,400 --> 01:50:25,920
and this will allow us to just get a

3146
01:50:23,599 --> 01:50:28,080
single value out at the last layer that

3147
01:50:25,920 --> 01:50:29,599
only has a single neuron

3148
01:50:28,080 --> 01:50:31,360
and finally we should be able to draw

3149
01:50:29,599 --> 01:50:32,400
dot of n of x

3150
01:50:31,360 --> 01:50:34,080
and

3151
01:50:32,400 --> 01:50:36,719
as you might imagine

3152
01:50:34,080 --> 01:50:38,480
these expressions are now getting

3153
01:50:36,719 --> 01:50:40,239
relatively involved

3154
01:50:38,480 --> 01:50:42,880
so this is an entire mlp that we're

3155
01:50:40,239 --> 01:50:42,880
defining now

3156
01:50:45,280 --> 01:50:49,280
all the way until a single output

3157
01:50:48,239 --> 01:50:50,800
okay

3158
01:50:49,280 --> 01:50:52,719
and so obviously you would never

3159
01:50:50,800 --> 01:50:55,199
differentiate on pen and paper these

3160
01:50:52,719 --> 01:50:56,800
expressions but with micrograd we will

3161
01:50:55,199 --> 01:50:58,080
be able to back propagate all the way

3162
01:50:56,800 --> 01:50:59,760
through this

3163
01:50:58,080 --> 01:51:00,800
and back propagate

3164
01:50:59,760 --> 01:51:02,960
into

3165
01:51:00,800 --> 01:51:04,639
these weights of all these neurons so

3166
01:51:02,960 --> 01:51:06,800
let's see how that works okay so let's

3167
01:51:04,639 --> 01:51:08,639
create ourselves a very simple

3168
01:51:06,800 --> 01:51:11,040
example data set here

3169
01:51:08,639 --> 01:51:13,520
so this data set has four examples

3170
01:51:11,040 --> 01:51:15,360
and so we have four possible

3171
01:51:13,520 --> 01:51:17,679
inputs into the neural net

3172
01:51:15,360 --> 01:51:21,280
and we have four desired targets so we'd

3173
01:51:17,679 --> 01:51:24,159
like the neural net to assign

3174
01:51:21,280 --> 01:51:25,440
or output 1.0 when it's fed this example

3175
01:51:24,159 --> 01:51:26,960
negative one when it's fed these

3176
01:51:25,440 --> 01:51:28,960
examples and one when it's fed this

3177
01:51:26,960 --> 01:51:30,800
example so it's a very simple binary

3178
01:51:28,960 --> 01:51:32,320
classifier neural net basically that we

3179
01:51:30,800 --> 01:51:33,679
would like here

3180
01:51:32,320 --> 01:51:34,960
now let's think what the neural net

3181
01:51:33,679 --> 01:51:36,560
currently thinks about these four

3182
01:51:34,960 --> 01:51:37,840
examples we can just get their

3183
01:51:36,560 --> 01:51:40,320
predictions

3184
01:51:37,840 --> 01:51:42,000
um basically we can just call n of x for

3185
01:51:40,320 --> 01:51:43,440
x in axis

3186
01:51:42,000 --> 01:51:45,040
and then we can

3187
01:51:43,440 --> 01:51:46,639
print

3188
01:51:45,040 --> 01:51:48,800
so these are the outputs of the neural

3189
01:51:46,639 --> 01:51:50,239
net on those four examples

3190
01:51:48,800 --> 01:51:52,719
so

3191
01:51:50,239 --> 01:51:55,280
the first one is 0.91 but we'd like it

3192
01:51:52,719 --> 01:51:58,000
to be one so we should push this one

3193
01:51:55,280 --> 01:52:00,960
higher this one we want to be higher

3194
01:51:58,000 --> 01:52:02,639
this one says 0.88 and we want this to

3195
01:52:00,960 --> 01:52:04,320
be negative one

3196
01:52:02,639 --> 01:52:05,119
this is 0.8 we want it to be negative

3197
01:52:04,320 --> 01:52:08,159
one

3198
01:52:05,119 --> 01:52:10,239
and this one is 0.8 we want it to be one

3199
01:52:08,159 --> 01:52:12,159
so how do we make the neural net and how

3200
01:52:10,239 --> 01:52:12,960
do we tune the weights

3201
01:52:12,159 --> 01:52:16,719
to

3202
01:52:12,960 --> 01:52:18,480
better predict the desired targets

3203
01:52:16,719 --> 01:52:20,480
and the trick used in deep learning to

3204
01:52:18,480 --> 01:52:22,239
achieve this is to

3205
01:52:20,480 --> 01:52:24,400
calculate a single number that somehow

3206
01:52:22,239 --> 01:52:25,920
measures the total performance of your

3207
01:52:24,400 --> 01:52:28,000
neural net and we call this single

3208
01:52:25,920 --> 01:52:29,760
number the loss

3209
01:52:28,000 --> 01:52:31,040
so the loss

3210
01:52:29,760 --> 01:52:32,560
first

3211
01:52:31,040 --> 01:52:34,400
is is a single number that we're going

3212
01:52:32,560 --> 01:52:36,239
to define that basically measures how

3213
01:52:34,400 --> 01:52:37,360
well the neural net is performing right

3214
01:52:36,239 --> 01:52:38,800
now we have the intuitive sense that

3215
01:52:37,360 --> 01:52:40,880
it's not performing very well because

3216
01:52:38,800 --> 01:52:43,040
we're not very much close to this

3217
01:52:40,880 --> 01:52:44,639
so the loss will be high and we'll want

3218
01:52:43,040 --> 01:52:46,080
to minimize the loss

3219
01:52:44,639 --> 01:52:47,760
so in particular in this case what we're

3220
01:52:46,080 --> 01:52:49,679
going to do is we're going to implement

3221
01:52:47,760 --> 01:52:51,360
the mean squared error loss

3222
01:52:49,679 --> 01:52:54,080
so this is doing is we're going to

3223
01:52:51,360 --> 01:52:56,400
basically iterate um

3224
01:52:54,080 --> 01:52:59,920
for y ground truth

3225
01:52:56,400 --> 01:53:01,679
and y output in zip of um

3226
01:52:59,920 --> 01:53:03,119
wise and white red so we're going to

3227
01:53:01,679 --> 01:53:06,080
pair up the

3228
01:53:03,119 --> 01:53:07,840
ground truths with the predictions

3229
01:53:06,080 --> 01:53:08,880
and this zip iterates over tuples of

3230
01:53:07,840 --> 01:53:11,520
them

3231
01:53:08,880 --> 01:53:13,520
and for each

3232
01:53:11,520 --> 01:53:16,239
y ground truth and y output we're going

3233
01:53:13,520 --> 01:53:16,239
to subtract them

3234
01:53:16,960 --> 01:53:20,239
and square them

3235
01:53:18,560 --> 01:53:22,960
so let's first see what these losses are

3236
01:53:20,239 --> 01:53:25,199
these are individual loss components

3237
01:53:22,960 --> 01:53:26,400
and so basically for each

3238
01:53:25,199 --> 01:53:28,480
one of the four

3239
01:53:26,400 --> 01:53:30,800
we are taking the prediction and the

3240
01:53:28,480 --> 01:53:32,639
ground truth we are subtracting them and

3241
01:53:30,800 --> 01:53:33,840
squaring them

3242
01:53:32,639 --> 01:53:36,880
so because

3243
01:53:33,840 --> 01:53:38,719
this one is so close to its target 0.91

3244
01:53:36,880 --> 01:53:40,400
is almost one

3245
01:53:38,719 --> 01:53:41,920
subtracting them gives a very small

3246
01:53:40,400 --> 01:53:43,360
number

3247
01:53:41,920 --> 01:53:45,760
so here we would get like a negative

3248
01:53:43,360 --> 01:53:47,599
point one and then squaring it

3249
01:53:45,760 --> 01:53:49,599
just makes sure

3250
01:53:47,599 --> 01:53:51,520
that regardless of whether we are more

3251
01:53:49,599 --> 01:53:52,840
negative or more positive we always get

3252
01:53:51,520 --> 01:53:55,520
a positive

3253
01:53:52,840 --> 01:53:56,880
number instead of squaring we should we

3254
01:53:55,520 --> 01:53:59,440
could also take for example the absolute

3255
01:53:56,880 --> 01:54:00,880
value we need to discard the sign

3256
01:53:59,440 --> 01:54:03,520
and so you see that the expression is

3257
01:54:00,880 --> 01:54:06,400
ranged so that you only get zero exactly

3258
01:54:03,520 --> 01:54:07,760
when y out is equal to y ground truth

3259
01:54:06,400 --> 01:54:09,599
when those two are equal so your

3260
01:54:07,760 --> 01:54:10,960
prediction is exactly the target you are

3261
01:54:09,599 --> 01:54:12,880
going to get zero

3262
01:54:10,960 --> 01:54:15,040
and if your prediction is not the target

3263
01:54:12,880 --> 01:54:17,119
you are going to get some other number

3264
01:54:15,040 --> 01:54:19,840
so here for example we are way off and

3265
01:54:17,119 --> 01:54:22,560
so that's why the loss is quite high

3266
01:54:19,840 --> 01:54:24,080
and the more off we are the greater the

3267
01:54:22,560 --> 01:54:26,800
loss will be

3268
01:54:24,080 --> 01:54:27,760
so we don't want high loss we want low

3269
01:54:26,800 --> 01:54:30,560
loss

3270
01:54:27,760 --> 01:54:32,000
and so the final loss here will be just

3271
01:54:30,560 --> 01:54:33,280
the sum

3272
01:54:32,000 --> 01:54:34,239
of all of these

3273
01:54:33,280 --> 01:54:36,320
numbers

3274
01:54:34,239 --> 01:54:38,400
so you see that this should be zero

3275
01:54:36,320 --> 01:54:39,679
roughly plus zero roughly

3276
01:54:38,400 --> 01:54:40,800
but plus

3277
01:54:39,679 --> 01:54:43,599
seven

3278
01:54:40,800 --> 01:54:44,800
so loss should be about seven

3279
01:54:43,599 --> 01:54:47,280
here

3280
01:54:44,800 --> 01:54:49,280
and now we want to minimize the loss we

3281
01:54:47,280 --> 01:54:51,520
want the loss to be low

3282
01:54:49,280 --> 01:54:54,000
because if loss is low

3283
01:54:51,520 --> 01:54:56,320
then every one of the predictions is

3284
01:54:54,000 --> 01:54:58,880
equal to its target

3285
01:54:56,320 --> 01:55:01,520
so the loss the lowest it can be is zero

3286
01:54:58,880 --> 01:55:04,080
and the greater it is the worse off the

3287
01:55:01,520 --> 01:55:05,599
neural net is predicting

3288
01:55:04,080 --> 01:55:07,920
so now of course if we do lost that

3289
01:55:05,599 --> 01:55:09,440
backward

3290
01:55:07,920 --> 01:55:10,560
something magical happened when i hit

3291
01:55:09,440 --> 01:55:12,159
enter

3292
01:55:10,560 --> 01:55:14,119
and the magical thing of course that

3293
01:55:12,159 --> 01:55:16,560
happened is that we can look at

3294
01:55:14,119 --> 01:55:18,800
end.layers.neuron and that layers at say

3295
01:55:16,560 --> 01:55:22,000
like the the first layer

3296
01:55:18,800 --> 01:55:22,000
that neurons at zero

3297
01:55:22,400 --> 01:55:26,480
because remember that mlp has the layers

3298
01:55:24,880 --> 01:55:28,080
which is a list

3299
01:55:26,480 --> 01:55:29,760
and each layer has a neurons which is a

3300
01:55:28,080 --> 01:55:30,719
list and that gives us an individual

3301
01:55:29,760 --> 01:55:32,800
neuron

3302
01:55:30,719 --> 01:55:34,480
and then it's got some weights

3303
01:55:32,800 --> 01:55:37,520
and so we can for example look at the

3304
01:55:34,480 --> 01:55:37,520
weights at zero

3305
01:55:38,239 --> 01:55:42,080
um

3306
01:55:40,159 --> 01:55:44,239
oops it's not called weights it's called

3307
01:55:42,080 --> 01:55:46,400
w

3308
01:55:44,239 --> 01:55:48,800
and that's a value but now this value

3309
01:55:46,400 --> 01:55:50,080
also has a groud because of the backward

3310
01:55:48,800 --> 01:55:52,880
pass

3311
01:55:50,080 --> 01:55:54,800
and so we see that because this gradient

3312
01:55:52,880 --> 01:55:56,239
here on this particular weight of this

3313
01:55:54,800 --> 01:55:57,840
particular neuron of this particular

3314
01:55:56,239 --> 01:56:00,080
layer is negative

3315
01:55:57,840 --> 01:56:02,480
we see that its influence on the loss is

3316
01:56:00,080 --> 01:56:04,639
also negative so slightly increasing

3317
01:56:02,480 --> 01:56:08,800
this particular weight of this neuron of

3318
01:56:04,639 --> 01:56:10,159
this layer would make the loss go down

3319
01:56:08,800 --> 01:56:12,400
and we actually have this information

3320
01:56:10,159 --> 01:56:13,760
for every single one of our neurons and

3321
01:56:12,400 --> 01:56:16,880
all their parameters actually it's worth

3322
01:56:13,760 --> 01:56:17,920
looking at also the draw dot loss by the

3323
01:56:16,880 --> 01:56:19,679
way

3324
01:56:17,920 --> 01:56:21,760
so previously we looked at the draw dot

3325
01:56:19,679 --> 01:56:23,760
of a single neural neuron forward pass

3326
01:56:21,760 --> 01:56:25,920
and that was already a large expression

3327
01:56:23,760 --> 01:56:27,360
but what is this expression we actually

3328
01:56:25,920 --> 01:56:29,119
forwarded

3329
01:56:27,360 --> 01:56:30,880
every one of those four examples and

3330
01:56:29,119 --> 01:56:32,800
then we have the loss on top of them

3331
01:56:30,880 --> 01:56:36,159
with the mean squared error

3332
01:56:32,800 --> 01:56:38,239
and so this is a really massive graph

3333
01:56:36,159 --> 01:56:39,679
because this graph that we've built up

3334
01:56:38,239 --> 01:56:41,679
now

3335
01:56:39,679 --> 01:56:42,960
oh my gosh this graph that we've built

3336
01:56:41,679 --> 01:56:44,719
up now

3337
01:56:42,960 --> 01:56:46,400
which is kind of excessive it's

3338
01:56:44,719 --> 01:56:48,239
excessive because it has four forward

3339
01:56:46,400 --> 01:56:50,639
passes of a neural net for every one of

3340
01:56:48,239 --> 01:56:51,440
the examples and then it has the loss on

3341
01:56:50,639 --> 01:56:53,040
top

3342
01:56:51,440 --> 01:56:55,199
and it ends with the value of the loss

3343
01:56:53,040 --> 01:56:56,639
which was 7.12

3344
01:56:55,199 --> 01:56:58,960
and this loss will now back propagate

3345
01:56:56,639 --> 01:57:00,960
through all the four forward passes all

3346
01:56:58,960 --> 01:57:03,119
the way through just every single

3347
01:57:00,960 --> 01:57:05,199
intermediate value of the neural net

3348
01:57:03,119 --> 01:57:06,800
all the way back to of course the

3349
01:57:05,199 --> 01:57:07,920
parameters of the weights which are the

3350
01:57:06,800 --> 01:57:10,320
input

3351
01:57:07,920 --> 01:57:12,320
so these weight parameters here are

3352
01:57:10,320 --> 01:57:13,280
inputs to this neural net

3353
01:57:12,320 --> 01:57:15,119
and

3354
01:57:13,280 --> 01:57:16,880
these numbers here these scalars are

3355
01:57:15,119 --> 01:57:18,880
inputs to the neural net

3356
01:57:16,880 --> 01:57:20,639
so if we went around here

3357
01:57:18,880 --> 01:57:22,880
we'll probably find

3358
01:57:20,639 --> 01:57:25,280
some of these examples this 1.0

3359
01:57:22,880 --> 01:57:26,960
potentially maybe this 1.0 or you know

3360
01:57:25,280 --> 01:57:28,800
some of the others and you'll see that

3361
01:57:26,960 --> 01:57:30,800
they all have gradients as well

3362
01:57:28,800 --> 01:57:33,440
the thing is these gradients on the

3363
01:57:30,800 --> 01:57:36,080
input data are not that useful to us

3364
01:57:33,440 --> 01:57:38,880
and that's because the input data seems

3365
01:57:36,080 --> 01:57:40,880
to be not changeable it's it's a given

3366
01:57:38,880 --> 01:57:42,159
to the problem and so it's a fixed input

3367
01:57:40,880 --> 01:57:43,679
we're not going to be changing it or

3368
01:57:42,159 --> 01:57:46,000
messing with it even though we do have

3369
01:57:43,679 --> 01:57:49,040
gradients for it

3370
01:57:46,000 --> 01:57:50,320
but some of these gradients here

3371
01:57:49,040 --> 01:57:53,280
will be for the neural network

3372
01:57:50,320 --> 01:57:55,760
parameters the ws and the bs and those

3373
01:57:53,280 --> 01:57:58,080
we of course we want to change

3374
01:57:55,760 --> 01:57:59,920
okay so now we're going to want some

3375
01:57:58,080 --> 01:58:01,679
convenience code to gather up all of the

3376
01:57:59,920 --> 01:58:03,360
parameters of the neural net so that we

3377
01:58:01,679 --> 01:58:05,760
can operate on all of them

3378
01:58:03,360 --> 01:58:08,719
simultaneously and every one of them we

3379
01:58:05,760 --> 01:58:10,639
will nudge a tiny amount

3380
01:58:08,719 --> 01:58:11,920
based on the gradient information

3381
01:58:10,639 --> 01:58:14,800
so let's collect the parameters of the

3382
01:58:11,920 --> 01:58:17,840
neural net all in one array

3383
01:58:14,800 --> 01:58:18,840
so let's create a parameters of self

3384
01:58:17,840 --> 01:58:22,239
that just

3385
01:58:18,840 --> 01:58:24,000
returns celta w which is a list

3386
01:58:22,239 --> 01:58:27,119
concatenated with

3387
01:58:24,000 --> 01:58:29,440
a list of self.b

3388
01:58:27,119 --> 01:58:31,360
so this will just return a list

3389
01:58:29,440 --> 01:58:32,159
list plus list just you know gives you a

3390
01:58:31,360 --> 01:58:35,040
list

3391
01:58:32,159 --> 01:58:36,719
so that's parameters of neuron and i'm

3392
01:58:35,040 --> 01:58:38,800
calling it this way because also pi

3393
01:58:36,719 --> 01:58:40,080
torch has a parameters on every single

3394
01:58:38,800 --> 01:58:42,000
and in module

3395
01:58:40,080 --> 01:58:44,000
and uh it does exactly what we're doing

3396
01:58:42,000 --> 01:58:46,000
here it just returns the

3397
01:58:44,000 --> 01:58:48,400
parameter tensors for us as the

3398
01:58:46,000 --> 01:58:50,880
parameter scalars

3399
01:58:48,400 --> 01:58:52,480
now layer is also a module so it will

3400
01:58:50,880 --> 01:58:54,080
have parameters

3401
01:58:52,480 --> 01:58:56,719
itself

3402
01:58:54,080 --> 01:58:59,679
and basically what we want to do here is

3403
01:58:56,719 --> 01:58:59,679
something like this like

3404
01:59:00,239 --> 01:59:07,360
params is here and then for

3405
01:59:03,520 --> 01:59:10,560
neuron in salt out neurons

3406
01:59:07,360 --> 01:59:14,000
we want to get neuron.parameters

3407
01:59:10,560 --> 01:59:16,000
and we want to params.extend

3408
01:59:14,000 --> 01:59:17,920
right so these are the parameters of

3409
01:59:16,000 --> 01:59:21,440
this neuron and then we want to put them

3410
01:59:17,920 --> 01:59:22,560
on top of params so params dot extend

3411
01:59:21,440 --> 01:59:25,360
of peace

3412
01:59:22,560 --> 01:59:28,080
and then we want to return brands

3413
01:59:25,360 --> 01:59:31,599
so this is way too much code so actually

3414
01:59:28,080 --> 01:59:33,920
there's a way to simplify this which is

3415
01:59:31,599 --> 01:59:35,520
return

3416
01:59:33,920 --> 01:59:38,000
p

3417
01:59:35,520 --> 01:59:39,840
for neuron in self

3418
01:59:38,000 --> 01:59:41,360
neurons

3419
01:59:39,840 --> 01:59:45,679
for

3420
01:59:41,360 --> 01:59:47,360
p in neuron dot parameters

3421
01:59:45,679 --> 01:59:49,280
so it's a single list comprehension in

3422
01:59:47,360 --> 01:59:51,599
python you can sort of nest them like

3423
01:59:49,280 --> 01:59:52,800
this and you can um

3424
01:59:51,599 --> 01:59:54,800
then create

3425
01:59:52,800 --> 01:59:57,440
uh the desired

3426
01:59:54,800 --> 02:00:00,000
array so this is these are identical

3427
01:59:57,440 --> 02:00:03,360
we can take this out

3428
02:00:00,000 --> 02:00:03,360
and then let's do the same here

3429
02:00:04,639 --> 02:00:07,920
def parameters

3430
02:00:06,639 --> 02:00:09,440
self

3431
02:00:07,920 --> 02:00:13,599
and return

3432
02:00:09,440 --> 02:00:15,119
a parameter for layer in self dot layers

3433
02:00:13,599 --> 02:00:19,440
for

3434
02:00:15,119 --> 02:00:19,440
p in layer dot parameters

3435
02:00:20,960 --> 02:00:26,400
and that should be good

3436
02:00:23,679 --> 02:00:28,960
now let me pop out this so

3437
02:00:26,400 --> 02:00:31,360
we don't re-initialize our network

3438
02:00:28,960 --> 02:00:33,760
because we need to re-initialize

3439
02:00:31,360 --> 02:00:33,760
our

3440
02:00:35,599 --> 02:00:38,560
okay so unfortunately we will have to

3441
02:00:37,280 --> 02:00:41,520
probably re-initialize the network

3442
02:00:38,560 --> 02:00:43,119
because we just add functionality

3443
02:00:41,520 --> 02:00:45,760
because this class of course we i want

3444
02:00:43,119 --> 02:00:47,280
to get all the and that parameters but

3445
02:00:45,760 --> 02:00:49,840
that's not going to work because this is

3446
02:00:47,280 --> 02:00:50,880
the old class

3447
02:00:49,840 --> 02:00:52,000
okay

3448
02:00:50,880 --> 02:00:53,520
so unfortunately we do have to

3449
02:00:52,000 --> 02:00:55,679
reinitialize the network which will

3450
02:00:53,520 --> 02:00:57,199
change some of the numbers

3451
02:00:55,679 --> 02:00:58,800
but let me do that so that we pick up

3452
02:00:57,199 --> 02:01:00,159
the new api we can now do in the

3453
02:00:58,800 --> 02:01:02,960
parameters

3454
02:01:00,159 --> 02:01:05,760
and these are all the weights and biases

3455
02:01:02,960 --> 02:01:11,280
inside the entire neural net

3456
02:01:05,760 --> 02:01:11,280
so in total this mlp has 41 parameters

3457
02:01:11,520 --> 02:01:15,599
and

3458
02:01:12,880 --> 02:01:18,400
now we'll be able to change them

3459
02:01:15,599 --> 02:01:19,840
if we recalculate the loss here we see

3460
02:01:18,400 --> 02:01:22,400
that unfortunately we have slightly

3461
02:01:19,840 --> 02:01:26,000
different

3462
02:01:22,400 --> 02:01:26,000
predictions and slightly different laws

3463
02:01:26,560 --> 02:01:31,040
but that's okay

3464
02:01:28,560 --> 02:01:33,199
okay so we see that this neurons

3465
02:01:31,040 --> 02:01:36,480
gradient is slightly negative we can

3466
02:01:33,199 --> 02:01:38,480
also look at its data right now

3467
02:01:36,480 --> 02:01:40,639
which is 0.85 so this is the current

3468
02:01:38,480 --> 02:01:43,119
value of this neuron and this is its

3469
02:01:40,639 --> 02:01:45,040
gradient on the loss

3470
02:01:43,119 --> 02:01:47,760
so what we want to do now is we want to

3471
02:01:45,040 --> 02:01:49,920
iterate for every p in

3472
02:01:47,760 --> 02:01:51,840
n dot parameters so for all the 41

3473
02:01:49,920 --> 02:01:55,040
parameters in this neural net

3474
02:01:51,840 --> 02:01:56,159
we actually want to change p data

3475
02:01:55,040 --> 02:01:59,119
slightly

3476
02:01:56,159 --> 02:02:00,320
according to the gradient information

3477
02:01:59,119 --> 02:02:02,400
okay so

3478
02:02:00,320 --> 02:02:05,199
dot dot to do here

3479
02:02:02,400 --> 02:02:08,320
but this will be basically a tiny update

3480
02:02:05,199 --> 02:02:10,560
in this gradient descent scheme in

3481
02:02:08,320 --> 02:02:13,199
gradient descent we are thinking of the

3482
02:02:10,560 --> 02:02:14,320
gradient as a vector pointing in the

3483
02:02:13,199 --> 02:02:15,360
direction

3484
02:02:14,320 --> 02:02:16,800
of

3485
02:02:15,360 --> 02:02:19,119
increased

3486
02:02:16,800 --> 02:02:20,080
loss

3487
02:02:19,119 --> 02:02:22,719
and so

3488
02:02:20,080 --> 02:02:24,320
in gradient descent we are modifying

3489
02:02:22,719 --> 02:02:26,560
p data

3490
02:02:24,320 --> 02:02:28,400
by a small step size in the direction of

3491
02:02:26,560 --> 02:02:29,760
the gradient so the step size as an

3492
02:02:28,400 --> 02:02:32,800
example could be like a very small

3493
02:02:29,760 --> 02:02:35,040
number like 0.01 is the step size times

3494
02:02:32,800 --> 02:02:36,400
p dot grad

3495
02:02:35,040 --> 02:02:37,760
right

3496
02:02:36,400 --> 02:02:38,880
but we have to think through some of the

3497
02:02:37,760 --> 02:02:40,320
signs here

3498
02:02:38,880 --> 02:02:43,040
so uh

3499
02:02:40,320 --> 02:02:44,560
in particular working with this specific

3500
02:02:43,040 --> 02:02:47,040
example here

3501
02:02:44,560 --> 02:02:49,280
we see that if we just left it like this

3502
02:02:47,040 --> 02:02:51,119
then this neuron's value

3503
02:02:49,280 --> 02:02:53,440
would be currently increased by a tiny

3504
02:02:51,119 --> 02:02:56,320
amount of the gradient

3505
02:02:53,440 --> 02:02:58,080
the grain is negative so this value of

3506
02:02:56,320 --> 02:03:00,560
this neuron would go slightly down it

3507
02:02:58,080 --> 02:03:02,880
would become like 0.8 you know four or

3508
02:03:00,560 --> 02:03:06,480
something like that

3509
02:03:02,880 --> 02:03:08,880
but if this neuron's value goes lower

3510
02:03:06,480 --> 02:03:10,639
that would actually

3511
02:03:08,880 --> 02:03:12,480
increase the loss

3512
02:03:10,639 --> 02:03:14,239
that's because

3513
02:03:12,480 --> 02:03:16,560
the derivative of this neuron is

3514
02:03:14,239 --> 02:03:19,199
negative so increasing

3515
02:03:16,560 --> 02:03:21,440
this makes the loss go down so

3516
02:03:19,199 --> 02:03:23,280
increasing it is what we want to do

3517
02:03:21,440 --> 02:03:24,239
instead of decreasing it so basically

3518
02:03:23,280 --> 02:03:26,480
what we're missing here is we're

3519
02:03:24,239 --> 02:03:29,040
actually missing a negative sign

3520
02:03:26,480 --> 02:03:30,320
and again this other interpretation

3521
02:03:29,040 --> 02:03:31,840
and that's because we want to minimize

3522
02:03:30,320 --> 02:03:33,760
the loss we don't want to maximize the

3523
02:03:31,840 --> 02:03:34,960
loss we want to decrease it

3524
02:03:33,760 --> 02:03:36,320
and the other interpretation as i

3525
02:03:34,960 --> 02:03:37,760
mentioned is you can think of the

3526
02:03:36,320 --> 02:03:39,199
gradient vector

3527
02:03:37,760 --> 02:03:40,400
so basically just the vector of all the

3528
02:03:39,199 --> 02:03:42,719
gradients

3529
02:03:40,400 --> 02:03:44,239
as pointing in the direction of

3530
02:03:42,719 --> 02:03:46,400
increasing

3531
02:03:44,239 --> 02:03:47,440
the loss but then we want to decrease it

3532
02:03:46,400 --> 02:03:49,119
so we actually want to go in the

3533
02:03:47,440 --> 02:03:50,320
opposite direction

3534
02:03:49,119 --> 02:03:51,920
and so you can convince yourself that

3535
02:03:50,320 --> 02:03:53,199
this sort of plug does the right thing

3536
02:03:51,920 --> 02:03:55,360
here with the negative because we want

3537
02:03:53,199 --> 02:03:57,119
to minimize the loss

3538
02:03:55,360 --> 02:03:59,760
so if we nudge all the parameters by

3539
02:03:57,119 --> 02:03:59,760
tiny amount

3540
02:04:00,639 --> 02:04:04,079
then we'll see that

3541
02:04:02,079 --> 02:04:06,079
this data will have changed a little bit

3542
02:04:04,079 --> 02:04:08,880
so now this neuron

3543
02:04:06,079 --> 02:04:13,840
is a tiny amount greater

3544
02:04:08,880 --> 02:04:16,079
value so 0.854 went to 0.857

3545
02:04:13,840 --> 02:04:18,079
and that's a good thing because slightly

3546
02:04:16,079 --> 02:04:18,880
increasing this neuron

3547
02:04:18,079 --> 02:04:21,920
uh

3548
02:04:18,880 --> 02:04:23,679
data makes the loss go down according to

3549
02:04:21,920 --> 02:04:26,159
the gradient and so the correct thing

3550
02:04:23,679 --> 02:04:27,920
has happened sign wise

3551
02:04:26,159 --> 02:04:29,119
and so now what we would expect of

3552
02:04:27,920 --> 02:04:30,239
course is that

3553
02:04:29,119 --> 02:04:32,639
because we've changed all these

3554
02:04:30,239 --> 02:04:35,119
parameters we expect that the loss

3555
02:04:32,639 --> 02:04:37,040
should have gone down a bit

3556
02:04:35,119 --> 02:04:39,840
so we want to re-evaluate the loss let

3557
02:04:37,040 --> 02:04:39,840
me basically

3558
02:04:39,920 --> 02:04:44,880
this is just a data definition that

3559
02:04:41,520 --> 02:04:48,560
hasn't changed but the forward pass here

3560
02:04:44,880 --> 02:04:48,560
of the network we can recalculate

3561
02:04:49,679 --> 02:04:52,880
and actually let me do it outside here

3562
02:04:51,520 --> 02:04:54,400
so that we can compare the two loss

3563
02:04:52,880 --> 02:04:57,920
values

3564
02:04:54,400 --> 02:04:59,520
so here if i recalculate the loss

3565
02:04:57,920 --> 02:05:01,440
we'd expect the new loss now to be

3566
02:04:59,520 --> 02:05:03,440
slightly lower than this number so

3567
02:05:01,440 --> 02:05:06,560
hopefully what we're getting now is a

3568
02:05:03,440 --> 02:05:08,159
tiny bit lower than 4.84

3569
02:05:06,560 --> 02:05:10,320
4.36

3570
02:05:08,159 --> 02:05:12,960
okay and remember the way we've arranged

3571
02:05:10,320 --> 02:05:15,119
this is that low loss means that our

3572
02:05:12,960 --> 02:05:16,960
predictions are matching the targets so

3573
02:05:15,119 --> 02:05:18,840
our predictions now are probably

3574
02:05:16,960 --> 02:05:22,239
slightly closer to the

3575
02:05:18,840 --> 02:05:24,079
targets and now all we have to do is we

3576
02:05:22,239 --> 02:05:26,400
have to iterate this process

3577
02:05:24,079 --> 02:05:28,000
so again um we've done the forward pass

3578
02:05:26,400 --> 02:05:30,239
and this is the loss

3579
02:05:28,000 --> 02:05:32,239
now we can lost that backward

3580
02:05:30,239 --> 02:05:34,400
let me take these out and we can do a

3581
02:05:32,239 --> 02:05:35,920
step size

3582
02:05:34,400 --> 02:05:39,760
and now we should have a slightly lower

3583
02:05:35,920 --> 02:05:41,679
loss 4.36 goes to 3.9

3584
02:05:39,760 --> 02:05:43,040
and okay so

3585
02:05:41,679 --> 02:05:44,320
we've done the forward pass here's the

3586
02:05:43,040 --> 02:05:45,679
backward pass

3587
02:05:44,320 --> 02:05:49,719
nudge

3588
02:05:45,679 --> 02:05:49,719
and now the loss is 3.66

3589
02:05:50,159 --> 02:05:54,239
3.47

3590
02:05:52,400 --> 02:05:56,320
and you get the idea we just continue

3591
02:05:54,239 --> 02:05:58,239
doing this and this is uh gradient

3592
02:05:56,320 --> 02:06:01,199
descent we're just iteratively doing

3593
02:05:58,239 --> 02:06:02,880
forward pass backward pass update

3594
02:06:01,199 --> 02:06:04,320
forward pass backward pass update and

3595
02:06:02,880 --> 02:06:05,679
the neural net is improving its

3596
02:06:04,320 --> 02:06:09,599
predictions

3597
02:06:05,679 --> 02:06:12,159
so here if we look at why pred now

3598
02:06:09,599 --> 02:06:12,159
like red

3599
02:06:12,719 --> 02:06:16,000
we see that um

3600
02:06:14,560 --> 02:06:16,800
this value should be getting closer to

3601
02:06:16,000 --> 02:06:17,920
one

3602
02:06:16,800 --> 02:06:19,199
so this value should be getting more

3603
02:06:17,920 --> 02:06:20,560
positive these should be getting more

3604
02:06:19,199 --> 02:06:22,400
negative and this one should be also

3605
02:06:20,560 --> 02:06:23,920
getting more positive so if we just

3606
02:06:22,400 --> 02:06:26,639
iterate this

3607
02:06:23,920 --> 02:06:26,639
a few more times

3608
02:06:26,719 --> 02:06:30,239
actually we may be able to afford go to

3609
02:06:28,239 --> 02:06:33,040
go a bit faster let's try a slightly

3610
02:06:30,239 --> 02:06:33,040
higher learning rate

3611
02:06:34,239 --> 02:06:38,560
oops okay there we go so now we're at

3612
02:06:35,920 --> 02:06:38,560
0.31

3613
02:06:39,199 --> 02:06:43,280
if you go too fast by the way if you try

3614
02:06:41,280 --> 02:06:46,400
to make it too big of a step you may

3615
02:06:43,280 --> 02:06:46,400
actually overstep

3616
02:06:47,040 --> 02:06:50,079
it's overconfidence because again

3617
02:06:48,639 --> 02:06:51,599
remember we don't actually know exactly

3618
02:06:50,079 --> 02:06:53,679
about the loss function the loss

3619
02:06:51,599 --> 02:06:55,679
function has all kinds of structure and

3620
02:06:53,679 --> 02:06:57,199
we only know about the very local

3621
02:06:55,679 --> 02:06:59,360
dependence of all these parameters on

3622
02:06:57,199 --> 02:07:01,520
the loss but if we step too far

3623
02:06:59,360 --> 02:07:03,440
we may step into you know a part of the

3624
02:07:01,520 --> 02:07:04,800
loss that is completely different

3625
02:07:03,440 --> 02:07:08,159
and that can destabilize training and

3626
02:07:04,800 --> 02:07:11,040
make your loss actually blow up even

3627
02:07:08,159 --> 02:07:13,440
so the loss is now 0.04 so actually the

3628
02:07:11,040 --> 02:07:15,119
predictions should be really quite close

3629
02:07:13,440 --> 02:07:17,119
let's take a look

3630
02:07:15,119 --> 02:07:19,760
so you see how this is almost one

3631
02:07:17,119 --> 02:07:21,280
almost negative one almost one we can

3632
02:07:19,760 --> 02:07:22,400
continue going

3633
02:07:21,280 --> 02:07:24,079
uh so

3634
02:07:22,400 --> 02:07:25,679
yep backward

3635
02:07:24,079 --> 02:07:28,159
update

3636
02:07:25,679 --> 02:07:29,760
oops there we go so we went way too fast

3637
02:07:28,159 --> 02:07:31,440
and um

3638
02:07:29,760 --> 02:07:34,719
we actually overstepped

3639
02:07:31,440 --> 02:07:36,320
so we got two uh too eager where are we

3640
02:07:34,719 --> 02:07:37,199
now oops

3641
02:07:36,320 --> 02:07:39,199
okay

3642
02:07:37,199 --> 02:07:41,520
seven e negative nine so this is very

3643
02:07:39,199 --> 02:07:43,119
very low loss

3644
02:07:41,520 --> 02:07:45,520
and the predictions

3645
02:07:43,119 --> 02:07:47,440
are basically perfect

3646
02:07:45,520 --> 02:07:48,880
so somehow we

3647
02:07:47,440 --> 02:07:50,560
basically we were doing way too big

3648
02:07:48,880 --> 02:07:51,920
updates and we briefly exploded but then

3649
02:07:50,560 --> 02:07:54,800
somehow we ended up getting into a

3650
02:07:51,920 --> 02:07:56,880
really good spot so usually this

3651
02:07:54,800 --> 02:07:58,639
learning rate and the tuning of it is a

3652
02:07:56,880 --> 02:08:00,560
subtle art you want to set your learning

3653
02:07:58,639 --> 02:08:02,400
rate if it's too low you're going to

3654
02:08:00,560 --> 02:08:03,840
take way too long to converge but if

3655
02:08:02,400 --> 02:08:05,599
it's too high the whole thing gets

3656
02:08:03,840 --> 02:08:07,040
unstable and you might actually even

3657
02:08:05,599 --> 02:08:08,480
explode the loss

3658
02:08:07,040 --> 02:08:10,800
depending on your loss function

3659
02:08:08,480 --> 02:08:12,639
so finding the step size to be just

3660
02:08:10,800 --> 02:08:14,079
right it's it's a pretty subtle art

3661
02:08:12,639 --> 02:08:15,920
sometimes when you're using sort of

3662
02:08:14,079 --> 02:08:17,840
vanilla gradient descent

3663
02:08:15,920 --> 02:08:19,520
but we happen to get into a good spot we

3664
02:08:17,840 --> 02:08:22,639
can look at

3665
02:08:19,520 --> 02:08:25,440
n-dot parameters

3666
02:08:22,639 --> 02:08:26,560
so this is the setting of weights and

3667
02:08:25,440 --> 02:08:29,119
biases

3668
02:08:26,560 --> 02:08:30,159
that makes our network

3669
02:08:29,119 --> 02:08:31,840
predict

3670
02:08:30,159 --> 02:08:33,760
the desired targets

3671
02:08:31,840 --> 02:08:35,599
very very close

3672
02:08:33,760 --> 02:08:37,119
and

3673
02:08:35,599 --> 02:08:38,800
basically we've successfully trained

3674
02:08:37,119 --> 02:08:40,159
neural net

3675
02:08:38,800 --> 02:08:41,599
okay let's make this a tiny bit more

3676
02:08:40,159 --> 02:08:43,760
respectable and implement an actual

3677
02:08:41,599 --> 02:08:45,199
training loop and what that looks like

3678
02:08:43,760 --> 02:08:47,920
so this is the data definition that

3679
02:08:45,199 --> 02:08:49,440
stays this is the forward pass

3680
02:08:47,920 --> 02:08:52,480
um so

3681
02:08:49,440 --> 02:08:53,920
for uh k in range you know we're going

3682
02:08:52,480 --> 02:08:57,040
to

3683
02:08:53,920 --> 02:08:57,040
take a bunch of steps

3684
02:08:57,599 --> 02:09:03,040
first you do the forward pass

3685
02:09:00,079 --> 02:09:03,040
we validate the loss

3686
02:09:03,679 --> 02:09:06,239
let's re-initialize the neural net from

3687
02:09:05,040 --> 02:09:08,560
scratch

3688
02:09:06,239 --> 02:09:11,280
and here's the data

3689
02:09:08,560 --> 02:09:14,760
and we first do before pass then we do

3690
02:09:11,280 --> 02:09:14,760
the backward pass

3691
02:09:19,599 --> 02:09:23,679
and then we do an update that's gradient

3692
02:09:21,360 --> 02:09:23,679
descent

3693
02:09:26,320 --> 02:09:29,040
and then we should be able to iterate

3694
02:09:27,520 --> 02:09:30,239
this and we should be able to print the

3695
02:09:29,040 --> 02:09:33,840
current step

3696
02:09:30,239 --> 02:09:34,960
the current loss um let's just print the

3697
02:09:33,840 --> 02:09:36,880
sort of

3698
02:09:34,960 --> 02:09:38,239
number of the loss

3699
02:09:36,880 --> 02:09:40,560
and

3700
02:09:38,239 --> 02:09:42,480
that should be it

3701
02:09:40,560 --> 02:09:44,400
and then the learning rate 0.01 is a

3702
02:09:42,480 --> 02:09:46,239
little too small 0.1 we saw is like a

3703
02:09:44,400 --> 02:09:47,920
little bit dangerously too high let's go

3704
02:09:46,239 --> 02:09:50,079
somewhere in between

3705
02:09:47,920 --> 02:09:52,239
and we'll optimize this for

3706
02:09:50,079 --> 02:09:54,560
not 10 steps but let's go for say 20

3707
02:09:52,239 --> 02:09:58,639
steps

3708
02:09:54,560 --> 02:09:58,639
let me erase all of this junk

3709
02:09:59,119 --> 02:10:02,880
and uh let's run the optimization

3710
02:10:03,119 --> 02:10:08,400
and you see how we've actually converged

3711
02:10:05,280 --> 02:10:11,440
slower in a more controlled manner and

3712
02:10:08,400 --> 02:10:12,480
got to a loss that is very low

3713
02:10:11,440 --> 02:10:15,760
so

3714
02:10:12,480 --> 02:10:18,239
i expect white bread to be quite good

3715
02:10:15,760 --> 02:10:18,239
there we go

3716
02:10:19,679 --> 02:10:23,119
um

3717
02:10:22,000 --> 02:10:24,320
and

3718
02:10:23,119 --> 02:10:25,840
that's it

3719
02:10:24,320 --> 02:10:28,239
okay so this is kind of embarrassing but

3720
02:10:25,840 --> 02:10:31,040
we actually have a really terrible bug

3721
02:10:28,239 --> 02:10:33,199
in here and it's a subtle bug and it's a

3722
02:10:31,040 --> 02:10:36,320
very common bug and i can't believe i've

3723
02:10:33,199 --> 02:10:38,159
done it for the 20th time in my life

3724
02:10:36,320 --> 02:10:39,760
especially on camera and i could have

3725
02:10:38,159 --> 02:10:41,840
reshot the whole thing but i think it's

3726
02:10:39,760 --> 02:10:44,480
pretty funny and you know you get to

3727
02:10:41,840 --> 02:10:45,760
appreciate a bit what um working with

3728
02:10:44,480 --> 02:10:47,599
neural nets maybe

3729
02:10:45,760 --> 02:10:50,239
is like sometimes

3730
02:10:47,599 --> 02:10:52,800
we are guilty of

3731
02:10:50,239 --> 02:10:54,639
come bug i've actually tweeted

3732
02:10:52,800 --> 02:10:56,639
the most common neural net mistakes a

3733
02:10:54,639 --> 02:10:57,920
long time ago now

3734
02:10:56,639 --> 02:10:59,520
uh and

3735
02:10:57,920 --> 02:11:01,679
i'm not really

3736
02:10:59,520 --> 02:11:03,760
gonna explain any of these except for we

3737
02:11:01,679 --> 02:11:04,880
are guilty of number three you forgot to

3738
02:11:03,760 --> 02:11:08,800
zero grad

3739
02:11:04,880 --> 02:11:08,800
before that backward what is that

3740
02:11:09,440 --> 02:11:12,000
basically what's happening and it's a

3741
02:11:10,719 --> 02:11:12,800
subtle bug and i'm not sure if you saw

3742
02:11:12,000 --> 02:11:14,400
it

3743
02:11:12,800 --> 02:11:15,679
is that

3744
02:11:14,400 --> 02:11:17,760
all of these

3745
02:11:15,679 --> 02:11:19,280
weights here have a dot data and a dot

3746
02:11:17,760 --> 02:11:22,719
grad

3747
02:11:19,280 --> 02:11:24,320
and that grad starts at zero

3748
02:11:22,719 --> 02:11:25,920
and then we do backward and we fill in

3749
02:11:24,320 --> 02:11:27,760
the gradients

3750
02:11:25,920 --> 02:11:29,760
and then we do an update on the data but

3751
02:11:27,760 --> 02:11:31,599
we don't flush the grad

3752
02:11:29,760 --> 02:11:33,440
it stays there

3753
02:11:31,599 --> 02:11:35,440
so when we do the second

3754
02:11:33,440 --> 02:11:36,880
forward pass and we do backward again

3755
02:11:35,440 --> 02:11:39,440
remember that all the backward

3756
02:11:36,880 --> 02:11:41,280
operations do a plus equals on the grad

3757
02:11:39,440 --> 02:11:44,880
and so these gradients just

3758
02:11:41,280 --> 02:11:47,520
add up and they never get reset to zero

3759
02:11:44,880 --> 02:11:50,079
so basically we didn't zero grad so

3760
02:11:47,520 --> 02:11:51,119
here's how we zero grad before

3761
02:11:50,079 --> 02:11:52,159
backward

3762
02:11:51,119 --> 02:11:54,159
we need to iterate over all the

3763
02:11:52,159 --> 02:11:56,400
parameters

3764
02:11:54,159 --> 02:11:58,639
and we need to make sure that p dot grad

3765
02:11:56,400 --> 02:12:00,880
is set to zero

3766
02:11:58,639 --> 02:12:02,719
we need to reset it to zero just like it

3767
02:12:00,880 --> 02:12:04,400
is in the constructor

3768
02:12:02,719 --> 02:12:07,360
so remember all the way here for all

3769
02:12:04,400 --> 02:12:09,280
these value nodes grad is reset to zero

3770
02:12:07,360 --> 02:12:11,280
and then all these backward passes do a

3771
02:12:09,280 --> 02:12:13,360
plus equals from that grad

3772
02:12:11,280 --> 02:12:15,920
but we need to make sure that

3773
02:12:13,360 --> 02:12:17,360
we reset these graphs to zero so that

3774
02:12:15,920 --> 02:12:18,880
when we do backward

3775
02:12:17,360 --> 02:12:21,920
all of them start at zero and the actual

3776
02:12:18,880 --> 02:12:25,599
backward pass accumulates um

3777
02:12:21,920 --> 02:12:28,880
the loss derivatives into the grads

3778
02:12:25,599 --> 02:12:30,159
so this is zero grad in pytorch

3779
02:12:28,880 --> 02:12:31,599
and uh

3780
02:12:30,159 --> 02:12:33,280
we will slightly get we'll get a

3781
02:12:31,599 --> 02:12:34,880
slightly different optimization let's

3782
02:12:33,280 --> 02:12:37,280
reset the neural net

3783
02:12:34,880 --> 02:12:38,480
the data is the same this is now i think

3784
02:12:37,280 --> 02:12:40,000
correct

3785
02:12:38,480 --> 02:12:42,400
and we get a much more

3786
02:12:40,000 --> 02:12:44,079
you know we get a much more

3787
02:12:42,400 --> 02:12:46,239
slower descent

3788
02:12:44,079 --> 02:12:48,560
we still end up with pretty good results

3789
02:12:46,239 --> 02:12:50,239
and we can continue this a bit more

3790
02:12:48,560 --> 02:12:51,360
to get down lower

3791
02:12:50,239 --> 02:12:53,760
and lower

3792
02:12:51,360 --> 02:12:53,760
and lower

3793
02:12:54,159 --> 02:12:57,440
yeah

3794
02:12:56,239 --> 02:12:59,760
so the only reason that the previous

3795
02:12:57,440 --> 02:13:03,199
thing worked it's extremely buggy um the

3796
02:12:59,760 --> 02:13:05,760
only reason that worked is that

3797
02:13:03,199 --> 02:13:07,760
this is a very very simple problem

3798
02:13:05,760 --> 02:13:09,360
and it's very easy for this neural net

3799
02:13:07,760 --> 02:13:12,079
to fit this data

3800
02:13:09,360 --> 02:13:13,760
and so the grads ended up accumulating

3801
02:13:12,079 --> 02:13:16,079
and it effectively gave us a massive

3802
02:13:13,760 --> 02:13:18,960
step size and it made us converge

3803
02:13:16,079 --> 02:13:18,960
extremely fast

3804
02:13:19,360 --> 02:13:24,320
but basically now we have to do more

3805
02:13:20,880 --> 02:13:26,719
steps to get to very low values of loss

3806
02:13:24,320 --> 02:13:27,920
and get wipe red to be really good we

3807
02:13:26,719 --> 02:13:31,719
can try to

3808
02:13:27,920 --> 02:13:31,719
step a bit greater

3809
02:13:34,320 --> 02:13:38,480
yeah we're gonna get closer and closer

3810
02:13:36,079 --> 02:13:39,840
to one minus one and one

3811
02:13:38,480 --> 02:13:41,599
so

3812
02:13:39,840 --> 02:13:43,280
working with neural nets is sometimes

3813
02:13:41,599 --> 02:13:44,960
tricky because

3814
02:13:43,280 --> 02:13:47,040
uh

3815
02:13:44,960 --> 02:13:49,840
you may have lots of bugs in the code

3816
02:13:47,040 --> 02:13:51,280
and uh your network might actually work

3817
02:13:49,840 --> 02:13:53,280
just like ours worked

3818
02:13:51,280 --> 02:13:55,679
but chances are is that if we had a more

3819
02:13:53,280 --> 02:13:57,520
complex problem then actually this bug

3820
02:13:55,679 --> 02:13:59,119
would have made us not optimize the loss

3821
02:13:57,520 --> 02:14:01,599
very well and we were only able to get

3822
02:13:59,119 --> 02:14:03,360
away with it because

3823
02:14:01,599 --> 02:14:04,960
the problem is very simple

3824
02:14:03,360 --> 02:14:06,880
so let's now bring everything together

3825
02:14:04,960 --> 02:14:09,119
and summarize what we learned

3826
02:14:06,880 --> 02:14:11,440
what are neural nets neural nets are

3827
02:14:09,119 --> 02:14:13,040
these mathematical expressions

3828
02:14:11,440 --> 02:14:15,520
fairly simple mathematical expressions

3829
02:14:13,040 --> 02:14:16,560
in the case of multi-layer perceptron

3830
02:14:15,520 --> 02:14:19,280
that take

3831
02:14:16,560 --> 02:14:20,719
input as the data and they take input

3832
02:14:19,280 --> 02:14:22,800
the weights and the parameters of the

3833
02:14:20,719 --> 02:14:24,840
neural net mathematical expression for

3834
02:14:22,800 --> 02:14:26,960
the forward pass followed by a loss

3835
02:14:24,840 --> 02:14:29,360
function and the loss function tries to

3836
02:14:26,960 --> 02:14:31,679
measure the accuracy of the predictions

3837
02:14:29,360 --> 02:14:32,960
and usually the loss will be low when

3838
02:14:31,679 --> 02:14:34,480
your predictions are matching your

3839
02:14:32,960 --> 02:14:37,280
targets or where the network is

3840
02:14:34,480 --> 02:14:38,800
basically behaving well so we we

3841
02:14:37,280 --> 02:14:40,719
manipulate the loss function so that

3842
02:14:38,800 --> 02:14:42,400
when the loss is low the network is

3843
02:14:40,719 --> 02:14:44,000
doing what you want it to do on your

3844
02:14:42,400 --> 02:14:46,480
problem

3845
02:14:44,000 --> 02:14:48,800
and then we backward the loss

3846
02:14:46,480 --> 02:14:50,000
use backpropagation to get the gradient

3847
02:14:48,800 --> 02:14:52,800
and then we know how to tune all the

3848
02:14:50,000 --> 02:14:54,239
parameters to decrease the loss locally

3849
02:14:52,800 --> 02:14:55,679
but then we have to iterate that process

3850
02:14:54,239 --> 02:14:56,719
many times in what's called the gradient

3851
02:14:55,679 --> 02:14:58,320
descent

3852
02:14:56,719 --> 02:15:01,040
so we simply follow the gradient

3853
02:14:58,320 --> 02:15:02,320
information and that minimizes the loss

3854
02:15:01,040 --> 02:15:04,320
and the loss is arranged so that when

3855
02:15:02,320 --> 02:15:06,840
the loss is minimized the network is

3856
02:15:04,320 --> 02:15:09,199
doing what you want it to do

3857
02:15:06,840 --> 02:15:11,360
and yeah so we just have a blob of

3858
02:15:09,199 --> 02:15:13,280
neural stuff and we can make it do

3859
02:15:11,360 --> 02:15:15,199
arbitrary things and that's what gives

3860
02:15:13,280 --> 02:15:16,719
neural nets their power um

3861
02:15:15,199 --> 02:15:19,119
it's you know this is a very tiny

3862
02:15:16,719 --> 02:15:20,800
network with 41 parameters

3863
02:15:19,119 --> 02:15:24,079
but you can build significantly more

3864
02:15:20,800 --> 02:15:25,360
complicated neural nets with billions

3865
02:15:24,079 --> 02:15:28,079
at this point almost trillions of

3866
02:15:25,360 --> 02:15:31,119
parameters and it's a massive blob of

3867
02:15:28,079 --> 02:15:32,560
neural tissue simulated neural tissue

3868
02:15:31,119 --> 02:15:34,960
roughly speaking

3869
02:15:32,560 --> 02:15:37,360
and you can make it do extremely complex

3870
02:15:34,960 --> 02:15:39,040
problems and these neurons then have all

3871
02:15:37,360 --> 02:15:40,159
kinds of very fascinating emergent

3872
02:15:39,040 --> 02:15:41,520
properties

3873
02:15:40,159 --> 02:15:43,440
in

3874
02:15:41,520 --> 02:15:45,679
when you try to make them do

3875
02:15:43,440 --> 02:15:47,679
significantly hard problems as in the

3876
02:15:45,679 --> 02:15:49,840
case of gpt for example

3877
02:15:47,679 --> 02:15:51,280
we have massive amounts of text from the

3878
02:15:49,840 --> 02:15:53,440
internet and we're trying to get a

3879
02:15:51,280 --> 02:15:55,199
neural net to predict to take like a few

3880
02:15:53,440 --> 02:15:56,639
words and try to predict the next word

3881
02:15:55,199 --> 02:15:57,599
in a sequence that's the learning

3882
02:15:56,639 --> 02:15:58,719
problem

3883
02:15:57,599 --> 02:16:00,800
and it turns out that when you train

3884
02:15:58,719 --> 02:16:02,480
this on all of internet the neural net

3885
02:16:00,800 --> 02:16:04,480
actually has like really remarkable

3886
02:16:02,480 --> 02:16:05,760
emergent properties but that neural net

3887
02:16:04,480 --> 02:16:07,440
would have hundreds of billions of

3888
02:16:05,760 --> 02:16:09,360
parameters

3889
02:16:07,440 --> 02:16:10,560
but it works on fundamentally the exact

3890
02:16:09,360 --> 02:16:12,159
same principles

3891
02:16:10,560 --> 02:16:15,119
the neural net of course will be a bit

3892
02:16:12,159 --> 02:16:17,520
more complex but otherwise the

3893
02:16:15,119 --> 02:16:19,440
value in the gradient is there

3894
02:16:17,520 --> 02:16:21,040
and would be identical and the gradient

3895
02:16:19,440 --> 02:16:23,119
descent would be there and would be

3896
02:16:21,040 --> 02:16:25,119
basically identical but people usually

3897
02:16:23,119 --> 02:16:27,199
use slightly different updates this is a

3898
02:16:25,119 --> 02:16:28,480
very simple stochastic gradient descent

3899
02:16:27,199 --> 02:16:29,440
update

3900
02:16:28,480 --> 02:16:30,639
um

3901
02:16:29,440 --> 02:16:32,000
and the loss function would not be mean

3902
02:16:30,639 --> 02:16:34,000
squared error they would be using

3903
02:16:32,000 --> 02:16:36,000
something called the cross-entropy loss

3904
02:16:34,000 --> 02:16:37,679
for predicting the next token so there's

3905
02:16:36,000 --> 02:16:39,040
a few more details but fundamentally the

3906
02:16:37,679 --> 02:16:42,160
neural network setup and neural network

3907
02:16:39,040 --> 02:16:44,240
training is identical and pervasive and

3908
02:16:42,160 --> 02:16:46,000
now you understand intuitively

3909
02:16:44,240 --> 02:16:47,439
how that works under the hood in the

3910
02:16:46,000 --> 02:16:48,800
beginning of this video i told you that

3911
02:16:47,439 --> 02:16:50,719
by the end of it you would understand

3912
02:16:48,800 --> 02:16:52,639
everything in micrograd and then we'd

3913
02:16:50,719 --> 02:16:54,000
slowly build it up let me briefly prove

3914
02:16:52,639 --> 02:16:55,120
that to you

3915
02:16:54,000 --> 02:16:57,519
so i'm going to step through all the

3916
02:16:55,120 --> 02:16:59,040
code that is in micrograd as of today

3917
02:16:57,519 --> 02:17:00,319
actually potentially some of the code

3918
02:16:59,040 --> 02:17:01,760
will change by the time you watch this

3919
02:17:00,319 --> 02:17:03,439
video because i intend to continue

3920
02:17:01,760 --> 02:17:05,040
developing micrograd

3921
02:17:03,439 --> 02:17:07,760
but let's look at what we have so far at

3922
02:17:05,040 --> 02:17:10,080
least init.pi is empty when you go to

3923
02:17:07,760 --> 02:17:11,200
engine.pi that has the value

3924
02:17:10,080 --> 02:17:13,599
everything here you should mostly

3925
02:17:11,200 --> 02:17:15,920
recognize so we have the data.grad

3926
02:17:13,599 --> 02:17:17,439
attributes we have the backward function

3927
02:17:15,920 --> 02:17:19,599
uh we have the previous set of children

3928
02:17:17,439 --> 02:17:20,800
and the operation that produced this

3929
02:17:19,599 --> 02:17:22,479
value

3930
02:17:20,800 --> 02:17:25,200
we have addition multiplication and

3931
02:17:22,479 --> 02:17:27,040
raising to a scalar power

3932
02:17:25,200 --> 02:17:28,479
we have the relu non-linearity which is

3933
02:17:27,040 --> 02:17:30,880
slightly different type of nonlinearity

3934
02:17:28,479 --> 02:17:32,719
than 10h that we used in this video

3935
02:17:30,880 --> 02:17:34,639
both of them are non-linearities and

3936
02:17:32,719 --> 02:17:37,040
notably 10h is not actually present in

3937
02:17:34,639 --> 02:17:38,639
micrograd as of right now but i intend

3938
02:17:37,040 --> 02:17:40,719
to add it later

3939
02:17:38,639 --> 02:17:42,880
with the backward which is identical and

3940
02:17:40,719 --> 02:17:45,439
then all of these other operations which

3941
02:17:42,880 --> 02:17:47,200
are built up on top of operations here

3942
02:17:45,439 --> 02:17:48,559
so values should be very recognizable

3943
02:17:47,200 --> 02:17:50,080
except for the non-linearity used in

3944
02:17:48,559 --> 02:17:52,160
this video

3945
02:17:50,080 --> 02:17:54,160
um there's no massive difference between

3946
02:17:52,160 --> 02:17:55,920
relu and 10h and sigmoid and these other

3947
02:17:54,160 --> 02:17:58,319
non-linearities they're all roughly

3948
02:17:55,920 --> 02:18:00,399
equivalent and can be used in mlps so i

3949
02:17:58,319 --> 02:18:01,280
use 10h because it's a bit smoother and

3950
02:18:00,399 --> 02:18:03,280
because it's a little bit more

3951
02:18:01,280 --> 02:18:05,920
complicated than relu and therefore it's

3952
02:18:03,280 --> 02:18:07,679
stressed a little bit more the

3953
02:18:05,920 --> 02:18:09,040
local gradients and working with those

3954
02:18:07,679 --> 02:18:10,719
derivatives which i thought would be

3955
02:18:09,040 --> 02:18:12,319
useful

3956
02:18:10,719 --> 02:18:14,240
and then that pi is the neural networks

3957
02:18:12,319 --> 02:18:16,160
library as i mentioned so you should

3958
02:18:14,240 --> 02:18:18,240
recognize identical implementation of

3959
02:18:16,160 --> 02:18:20,639
neuron layer and mlp

3960
02:18:18,240 --> 02:18:22,160
notably or not so much

3961
02:18:20,639 --> 02:18:24,479
we have a class module here there is a

3962
02:18:22,160 --> 02:18:27,120
parent class of all these modules i did

3963
02:18:24,479 --> 02:18:29,120
that because there's an nn.module class

3964
02:18:27,120 --> 02:18:31,679
in pytorch and so this exactly matches

3965
02:18:29,120 --> 02:18:33,679
that api and end.module and pytorch has

3966
02:18:31,679 --> 02:18:36,240
also a zero grad which i've refactored

3967
02:18:33,679 --> 02:18:36,240
out here

3968
02:18:36,319 --> 02:18:40,000
so that's the end of micrograd really

3969
02:18:38,479 --> 02:18:41,439
then there's a test

3970
02:18:40,000 --> 02:18:42,960
which you'll see

3971
02:18:41,439 --> 02:18:45,599
basically creates

3972
02:18:42,960 --> 02:18:47,599
two chunks of code one in micrograd and

3973
02:18:45,599 --> 02:18:49,280
one in pi torch and we'll make sure that

3974
02:18:47,599 --> 02:18:50,399
the forward and the backward pass agree

3975
02:18:49,280 --> 02:18:51,920
identically

3976
02:18:50,399 --> 02:18:53,359
for a slightly less complicated

3977
02:18:51,920 --> 02:18:55,359
expression a slightly more complicated

3978
02:18:53,359 --> 02:18:57,359
expression everything

3979
02:18:55,359 --> 02:18:58,960
agrees so we agree with pytorch on all

3980
02:18:57,359 --> 02:19:01,679
of these operations

3981
02:18:58,960 --> 02:19:03,200
and finally there's a demo.ipymb here

3982
02:19:01,679 --> 02:19:04,559
and it's a bit more complicated binary

3983
02:19:03,200 --> 02:19:07,040
classification demo than the one i

3984
02:19:04,559 --> 02:19:09,920
covered in this lecture so we only had a

3985
02:19:07,040 --> 02:19:11,679
tiny data set of four examples um here

3986
02:19:09,920 --> 02:19:13,679
we have a bit more complicated example

3987
02:19:11,679 --> 02:19:15,599
with lots of blue points and lots of red

3988
02:19:13,679 --> 02:19:18,000
points and we're trying to again build a

3989
02:19:15,599 --> 02:19:20,479
binary classifier to distinguish uh two

3990
02:19:18,000 --> 02:19:22,319
dimensional points as red or blue

3991
02:19:20,479 --> 02:19:24,719
it's a bit more complicated mlp here

3992
02:19:22,319 --> 02:19:26,160
with it's a bigger mlp

3993
02:19:24,719 --> 02:19:27,120
the loss is a bit more complicated

3994
02:19:26,160 --> 02:19:29,200
because

3995
02:19:27,120 --> 02:19:31,200
it supports batches

3996
02:19:29,200 --> 02:19:32,800
so because our dataset was so tiny we

3997
02:19:31,200 --> 02:19:35,200
always did a forward pass on the entire

3998
02:19:32,800 --> 02:19:37,359
data set of four examples but when your

3999
02:19:35,200 --> 02:19:39,840
data set is like a million examples what

4000
02:19:37,359 --> 02:19:41,840
we usually do in practice is we chair we

4001
02:19:39,840 --> 02:19:43,760
basically pick out some random subset we

4002
02:19:41,840 --> 02:19:45,920
call that a batch and then we only

4003
02:19:43,760 --> 02:19:47,599
process the batch forward backward and

4004
02:19:45,920 --> 02:19:49,840
update so we don't have to forward the

4005
02:19:47,599 --> 02:19:51,200
entire training set

4006
02:19:49,840 --> 02:19:53,520
so this supports batching because

4007
02:19:51,200 --> 02:19:55,680
there's a lot more examples here

4008
02:19:53,520 --> 02:19:57,600
we do a forward pass the loss is

4009
02:19:55,680 --> 02:20:00,000
slightly more different this is a max

4010
02:19:57,600 --> 02:20:01,760
margin loss that i implement here

4011
02:20:00,000 --> 02:20:03,200
the one that we used was the mean

4012
02:20:01,760 --> 02:20:04,640
squared error loss because it's the

4013
02:20:03,200 --> 02:20:06,319
simplest one

4014
02:20:04,640 --> 02:20:08,080
there's also the binary cross entropy

4015
02:20:06,319 --> 02:20:10,160
loss all of them can be used for binary

4016
02:20:08,080 --> 02:20:11,600
classification and don't make too much

4017
02:20:10,160 --> 02:20:13,520
of a difference in the simple examples

4018
02:20:11,600 --> 02:20:14,800
that we looked at so far

4019
02:20:13,520 --> 02:20:17,439
there's something called l2

4020
02:20:14,800 --> 02:20:19,359
regularization used here this has to do

4021
02:20:17,439 --> 02:20:21,840
with generalization of the neural net

4022
02:20:19,359 --> 02:20:23,359
and controls the overfitting in machine

4023
02:20:21,840 --> 02:20:24,880
learning setting but i did not cover

4024
02:20:23,359 --> 02:20:26,880
these concepts and concepts in this

4025
02:20:24,880 --> 02:20:27,920
video potentially later

4026
02:20:26,880 --> 02:20:31,040
and the training loop you should

4027
02:20:27,920 --> 02:20:32,319
recognize so forward backward with zero

4028
02:20:31,040 --> 02:20:35,200
grad

4029
02:20:32,319 --> 02:20:36,880
and update and so on you'll notice that

4030
02:20:35,200 --> 02:20:38,640
in the update here the learning rate is

4031
02:20:36,880 --> 02:20:40,560
scaled as a function of number of

4032
02:20:38,640 --> 02:20:41,760
iterations and it

4033
02:20:40,560 --> 02:20:43,040
shrinks

4034
02:20:41,760 --> 02:20:44,800
and this is something called learning

4035
02:20:43,040 --> 02:20:47,040
rate decay so in the beginning you have

4036
02:20:44,800 --> 02:20:49,359
a high learning rate and as the network

4037
02:20:47,040 --> 02:20:50,720
sort of stabilizes near the end you

4038
02:20:49,359 --> 02:20:53,120
bring down the learning rate to get some

4039
02:20:50,720 --> 02:20:54,880
of the fine details in the end

4040
02:20:53,120 --> 02:20:56,640
and in the end we see the decision

4041
02:20:54,880 --> 02:20:58,560
surface of the neural net and we see

4042
02:20:56,640 --> 02:21:00,560
that it learns to separate out the red

4043
02:20:58,560 --> 02:21:01,920
and the blue area based on the data

4044
02:21:00,560 --> 02:21:03,359
points

4045
02:21:01,920 --> 02:21:05,359
so that's the slightly more complicated

4046
02:21:03,359 --> 02:21:07,520
example and then we'll demo that hyper

4047
02:21:05,359 --> 02:21:10,240
ymb that you're free to go over

4048
02:21:07,520 --> 02:21:11,439
but yeah as of today that is micrograd i

4049
02:21:10,240 --> 02:21:13,200
also wanted to show you a little bit of

4050
02:21:11,439 --> 02:21:14,399
real stuff so that you get to see how

4051
02:21:13,200 --> 02:21:16,640
this is actually implemented in

4052
02:21:14,399 --> 02:21:18,479
production grade library like by torch

4053
02:21:16,640 --> 02:21:20,640
uh so in particular i wanted to show i

4054
02:21:18,479 --> 02:21:23,600
wanted to find and show you the backward

4055
02:21:20,640 --> 02:21:25,200
pass for 10h in pytorch so here in

4056
02:21:23,600 --> 02:21:28,479
micrograd we see that the backward

4057
02:21:25,200 --> 02:21:33,040
password 10h is one minus t square

4058
02:21:28,479 --> 02:21:33,040
where t is the output of the tanh of x

4059
02:21:33,200 --> 02:21:36,479
times of that grad which is the chain

4060
02:21:34,880 --> 02:21:38,080
rule so we're looking for something that

4061
02:21:36,479 --> 02:21:39,120
looks like this

4062
02:21:38,080 --> 02:21:42,560
now

4063
02:21:39,120 --> 02:21:45,120
i went to pytorch um which has an open

4064
02:21:42,560 --> 02:21:47,040
source github codebase and uh i looked

4065
02:21:45,120 --> 02:21:49,520
through a lot of its code

4066
02:21:47,040 --> 02:21:51,920
and honestly i i i spent about 15

4067
02:21:49,520 --> 02:21:53,280
minutes and i couldn't find 10h

4068
02:21:51,920 --> 02:21:55,120
and that's because these libraries

4069
02:21:53,280 --> 02:21:57,920
unfortunately they grow in size and

4070
02:21:55,120 --> 02:22:01,040
entropy and if you just search for 10h

4071
02:21:57,920 --> 02:22:04,000
you get apparently 2 800 results and 400

4072
02:22:01,040 --> 02:22:07,760
and 406 files so i don't know what these

4073
02:22:04,000 --> 02:22:09,040
files are doing honestly

4074
02:22:07,760 --> 02:22:11,200
and why there are so many mentions of

4075
02:22:09,040 --> 02:22:12,560
10h but unfortunately these libraries

4076
02:22:11,200 --> 02:22:15,600
are quite complex they're meant to be

4077
02:22:12,560 --> 02:22:18,399
used not really inspected um

4078
02:22:15,600 --> 02:22:21,120
eventually i did stumble on someone

4079
02:22:18,399 --> 02:22:22,640
who tries to change the 10 h backward

4080
02:22:21,120 --> 02:22:24,319
code for some reason

4081
02:22:22,640 --> 02:22:26,319
and someone here pointed to the cpu

4082
02:22:24,319 --> 02:22:27,760
kernel and the kuda kernel for 10 inch

4083
02:22:26,319 --> 02:22:29,520
backward

4084
02:22:27,760 --> 02:22:31,600
so this so basically depends on if

4085
02:22:29,520 --> 02:22:33,280
you're using pi torch on a cpu device or

4086
02:22:31,600 --> 02:22:35,200
on a gpu which these are different

4087
02:22:33,280 --> 02:22:37,760
devices and i haven't covered this but

4088
02:22:35,200 --> 02:22:40,000
this is the 10 h backwards kernel

4089
02:22:37,760 --> 02:22:43,920
for uh cpu

4090
02:22:40,000 --> 02:22:45,439
and the reason it's so large is that

4091
02:22:43,920 --> 02:22:46,960
number one this is like if you're using

4092
02:22:45,439 --> 02:22:48,720
a complex type which we haven't even

4093
02:22:46,960 --> 02:22:50,720
talked about if you're using a specific

4094
02:22:48,720 --> 02:22:52,560
data type of b-float 16 which we haven't

4095
02:22:50,720 --> 02:22:54,960
talked about

4096
02:22:52,560 --> 02:22:57,359
and then if you're not then this is the

4097
02:22:54,960 --> 02:23:00,080
kernel and deep here we see something

4098
02:22:57,359 --> 02:23:02,240
that resembles our backward pass so they

4099
02:23:00,080 --> 02:23:05,359
have a times one minus

4100
02:23:02,240 --> 02:23:07,600
b square uh so this b

4101
02:23:05,359 --> 02:23:10,240
b here must be the output of the 10h and

4102
02:23:07,600 --> 02:23:11,040
this is the health.grad so here we found

4103
02:23:10,240 --> 02:23:14,080
it

4104
02:23:11,040 --> 02:23:15,680
uh deep inside

4105
02:23:14,080 --> 02:23:18,560
pi torch from this location for some

4106
02:23:15,680 --> 02:23:21,200
reason inside binaryops kernel when 10h

4107
02:23:18,560 --> 02:23:24,880
is not actually a binary op

4108
02:23:21,200 --> 02:23:24,880
and then this is the gpu kernel

4109
02:23:25,040 --> 02:23:27,520
we're not complex

4110
02:23:26,640 --> 02:23:29,520
we're

4111
02:23:27,520 --> 02:23:30,640
here and here we go with one line of

4112
02:23:29,520 --> 02:23:33,439
code

4113
02:23:30,640 --> 02:23:34,800
so we did find it but basically

4114
02:23:33,439 --> 02:23:36,240
unfortunately these codepieces are very

4115
02:23:34,800 --> 02:23:38,560
large and

4116
02:23:36,240 --> 02:23:40,399
micrograd is very very simple but if you

4117
02:23:38,560 --> 02:23:41,920
actually want to use real stuff uh

4118
02:23:40,399 --> 02:23:43,920
finding the code for it you'll actually

4119
02:23:41,920 --> 02:23:45,840
find that difficult

4120
02:23:43,920 --> 02:23:47,760
i also wanted to show you a little

4121
02:23:45,840 --> 02:23:49,680
example here where pytorch is showing

4122
02:23:47,760 --> 02:23:51,120
you how can you can register a new type

4123
02:23:49,680 --> 02:23:53,600
of function that you want to add to

4124
02:23:51,120 --> 02:23:55,840
pytorch as a lego building block

4125
02:23:53,600 --> 02:23:59,359
so here if you want to for example add a

4126
02:23:55,840 --> 02:24:00,640
gender polynomial 3

4127
02:23:59,359 --> 02:24:03,600
here's how you could do it you will

4128
02:24:00,640 --> 02:24:06,319
register it as a class that

4129
02:24:03,600 --> 02:24:07,840
subclasses storage.org that function

4130
02:24:06,319 --> 02:24:10,800
and then you have to tell pytorch how to

4131
02:24:07,840 --> 02:24:12,640
forward your new function

4132
02:24:10,800 --> 02:24:14,080
and how to backward through it

4133
02:24:12,640 --> 02:24:15,520
so as long as you can do the forward

4134
02:24:14,080 --> 02:24:17,120
pass of this little function piece that

4135
02:24:15,520 --> 02:24:19,040
you want to add and as long as you know

4136
02:24:17,120 --> 02:24:20,560
the the local derivative the local

4137
02:24:19,040 --> 02:24:22,720
gradients which are implemented in the

4138
02:24:20,560 --> 02:24:24,399
backward pi torch will be able to back

4139
02:24:22,720 --> 02:24:26,399
propagate through your function and then

4140
02:24:24,399 --> 02:24:28,399
you can use this as a lego block in a

4141
02:24:26,399 --> 02:24:31,040
larger lego castle of all the different

4142
02:24:28,399 --> 02:24:32,319
lego blocks that pytorch already has

4143
02:24:31,040 --> 02:24:33,680
and so that's the only thing you have to

4144
02:24:32,319 --> 02:24:35,760
tell pytorch and everything would just

4145
02:24:33,680 --> 02:24:36,800
work and you can register new types of

4146
02:24:35,760 --> 02:24:38,960
functions

4147
02:24:36,800 --> 02:24:40,080
in this way following this example

4148
02:24:38,960 --> 02:24:41,680
and that is everything that i wanted to

4149
02:24:40,080 --> 02:24:42,800
cover in this lecture

4150
02:24:41,680 --> 02:24:44,399
so i hope you enjoyed building out

4151
02:24:42,800 --> 02:24:46,399
micrograd with me i hope you find it

4152
02:24:44,399 --> 02:24:47,920
interesting insightful

4153
02:24:46,399 --> 02:24:50,080
and

4154
02:24:47,920 --> 02:24:51,680
yeah i will post a lot of the links

4155
02:24:50,080 --> 02:24:53,920
that are related to this video in the

4156
02:24:51,680 --> 02:24:55,680
video description below i will also

4157
02:24:53,920 --> 02:24:56,479
probably post a link to a discussion

4158
02:24:55,680 --> 02:24:58,160
forum

4159
02:24:56,479 --> 02:25:00,399
or discussion group where you can ask

4160
02:24:58,160 --> 02:25:02,240
questions related to this video and then

4161
02:25:00,399 --> 02:25:04,399
i can answer or someone else can answer

4162
02:25:02,240 --> 02:25:06,080
your questions and i may also do a

4163
02:25:04,399 --> 02:25:08,479
follow-up video that answers some of the

4164
02:25:06,080 --> 02:25:10,399
most common questions

4165
02:25:08,479 --> 02:25:11,920
but for now that's it i hope you enjoyed

4166
02:25:10,399 --> 02:25:13,600
it if you did then please like and

4167
02:25:11,920 --> 02:25:15,520
subscribe so that youtube knows to

4168
02:25:13,600 --> 02:25:19,359
feature this video to more people

4169
02:25:15,520 --> 02:25:19,359
and that's it for now i'll see you later

4170
02:25:22,399 --> 02:25:25,680
now here's the problem

4171
02:25:24,640 --> 02:25:28,560
we know

4172
02:25:25,680 --> 02:25:31,520
dl by

4173
02:25:28,560 --> 02:25:31,520
wait what is the problem

4174
02:25:31,920 --> 02:25:34,240
and that's everything i wanted to cover

4175
02:25:33,120 --> 02:25:35,840
in this lecture

4176
02:25:34,240 --> 02:25:38,479
so i hope

4177
02:25:35,840 --> 02:25:41,120
you enjoyed us building up microcraft

4178
02:25:38,479 --> 02:25:41,120
micro crab

4179
02:25:42,000 --> 02:25:44,800
okay now let's do the exact same thing

4180
02:25:43,359 --> 02:25:47,840
for multiply because we can't do

4181
02:25:44,800 --> 02:25:50,240
something like a times two

4182
02:25:47,840 --> 02:25:50,240
oops

4183
02:25:50,800 --> 02:25:53,680
i know what happened there

